<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/memnode.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "compiler/compileLog.hpp"
  28 #include "memory/allocation.inline.hpp"
  29 #include "oops/objArrayKlass.hpp"
  30 #include "opto/addnode.hpp"
  31 #include "opto/arraycopynode.hpp"
  32 #include "opto/cfgnode.hpp"
  33 #include "opto/compile.hpp"
  34 #include "opto/connode.hpp"
  35 #include "opto/convertnode.hpp"
  36 #include "opto/loopnode.hpp"
  37 #include "opto/machnode.hpp"
  38 #include "opto/matcher.hpp"
  39 #include "opto/memnode.hpp"
  40 #include "opto/mulnode.hpp"
  41 #include "opto/narrowptrnode.hpp"
  42 #include "opto/phaseX.hpp"
  43 #include "opto/regmask.hpp"
  44 #include "utilities/copy.hpp"
  45 
  46 // Portions of code courtesy of Clifford Click
  47 
  48 // Optimization - Graph Style
  49 
  50 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  51 
  52 //=============================================================================
  53 uint MemNode::size_of() const { return sizeof(*this); }
  54 
  55 const TypePtr *MemNode::adr_type() const {
  56   Node* adr = in(Address);
  57   if (adr == NULL)  return NULL; // node is dead
  58   const TypePtr* cross_check = NULL;
  59   DEBUG_ONLY(cross_check = _adr_type);
  60   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  61 }
  62 
  63 #ifndef PRODUCT
  64 void MemNode::dump_spec(outputStream *st) const {
  65   if (in(Address) == NULL)  return; // node is dead
  66 #ifndef ASSERT
  67   // fake the missing field
  68   const TypePtr* _adr_type = NULL;
  69   if (in(Address) != NULL)
  70     _adr_type = in(Address)-&gt;bottom_type()-&gt;isa_ptr();
  71 #endif
  72   dump_adr_type(this, _adr_type, st);
  73 
  74   Compile* C = Compile::current();
  75   if (C-&gt;alias_type(_adr_type)-&gt;is_volatile()) {
  76     st-&gt;print(" Volatile!");
  77   }
  78   if (_unaligned_access) {
  79     st-&gt;print(" unaligned");
  80   }
  81   if (_mismatched_access) {
  82     st-&gt;print(" mismatched");
  83   }
  84 }
  85 
  86 void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {
  87   st-&gt;print(" @");
  88   if (adr_type == NULL) {
  89     st-&gt;print("NULL");
  90   } else {
  91     adr_type-&gt;dump_on(st);
  92     Compile* C = Compile::current();
  93     Compile::AliasType* atp = NULL;
  94     if (C-&gt;have_alias_type(adr_type))  atp = C-&gt;alias_type(adr_type);
  95     if (atp == NULL)
  96       st-&gt;print(", idx=?\?;");
  97     else if (atp-&gt;index() == Compile::AliasIdxBot)
  98       st-&gt;print(", idx=Bot;");
  99     else if (atp-&gt;index() == Compile::AliasIdxTop)
 100       st-&gt;print(", idx=Top;");
 101     else if (atp-&gt;index() == Compile::AliasIdxRaw)
 102       st-&gt;print(", idx=Raw;");
 103     else {
 104       ciField* field = atp-&gt;field();
 105       if (field) {
 106         st-&gt;print(", name=");
 107         field-&gt;print_name_on(st);
 108       }
 109       st-&gt;print(", idx=%d;", atp-&gt;index());
 110     }
 111   }
 112 }
 113 
 114 extern void print_alias_types();
 115 
 116 #endif
 117 
 118 Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {
 119   assert((t_oop != NULL), "sanity");
 120   bool is_instance = t_oop-&gt;is_known_instance_field();
 121   bool is_boxed_value_load = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp;
 122                              (load != NULL) &amp;&amp; load-&gt;is_Load() &amp;&amp;
 123                              (phase-&gt;is_IterGVN() != NULL);
 124   if (!(is_instance || is_boxed_value_load))
 125     return mchain;  // don't try to optimize non-instance types
 126   uint instance_id = t_oop-&gt;instance_id();
 127   Node *start_mem = phase-&gt;C-&gt;start()-&gt;proj_out(TypeFunc::Memory);
 128   Node *prev = NULL;
 129   Node *result = mchain;
 130   while (prev != result) {
 131     prev = result;
 132     if (result == start_mem)
 133       break;  // hit one of our sentinels
 134     // skip over a call which does not affect this memory slice
 135     if (result-&gt;is_Proj() &amp;&amp; result-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
 136       Node *proj_in = result-&gt;in(0);
 137       if (proj_in-&gt;is_Allocate() &amp;&amp; proj_in-&gt;_idx == instance_id) {
 138         break;  // hit one of our sentinels
 139       } else if (proj_in-&gt;is_Call()) {
 140         // ArrayCopyNodes processed here as well
 141         CallNode *call = proj_in-&gt;as_Call();
 142         if (!call-&gt;may_modify(t_oop, phase)) { // returns false for instances
 143           result = call-&gt;in(TypeFunc::Memory);
 144         }
 145       } else if (proj_in-&gt;is_Initialize()) {
 146         AllocateNode* alloc = proj_in-&gt;as_Initialize()-&gt;allocation();
 147         // Stop if this is the initialization for the object instance which
 148         // contains this memory slice, otherwise skip over it.
 149         if ((alloc == NULL) || (alloc-&gt;_idx == instance_id)) {
 150           break;
 151         }
 152         if (is_instance) {
 153           result = proj_in-&gt;in(TypeFunc::Memory);
 154         } else if (is_boxed_value_load) {
 155           Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
 156           const TypeKlassPtr* tklass = phase-&gt;type(klass)-&gt;is_klassptr();
 157           if (tklass-&gt;klass_is_exact() &amp;&amp; !tklass-&gt;klass()-&gt;equals(t_oop-&gt;klass())) {
 158             result = proj_in-&gt;in(TypeFunc::Memory); // not related allocation
 159           }
 160         }
 161       } else if (proj_in-&gt;is_MemBar()) {
 162         if (ArrayCopyNode::may_modify(t_oop, proj_in-&gt;as_MemBar(), phase)) {
 163           break;
 164         }
 165         result = proj_in-&gt;in(TypeFunc::Memory);
 166       } else {
 167         assert(false, "unexpected projection");
 168       }
 169     } else if (result-&gt;is_ClearArray()) {
 170       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 171         // Can not bypass initialization of the instance
 172         // we are looking for.
 173         break;
 174       }
 175       // Otherwise skip it (the call updated 'result' value).
 176     } else if (result-&gt;is_MergeMem()) {
 177       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 178     }
 179   }
 180   return result;
 181 }
 182 
 183 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 184   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 185   if (t_oop == NULL)
 186     return mchain;  // don't try to optimize non-oop types
 187   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 188   bool is_instance = t_oop-&gt;is_known_instance_field();
 189   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 190   if (is_instance &amp;&amp; igvn != NULL  &amp;&amp; result-&gt;is_Phi()) {
 191     PhiNode *mphi = result-&gt;as_Phi();
 192     assert(mphi-&gt;bottom_type() == Type::MEMORY, "memory phi required");
 193     const TypePtr *t = mphi-&gt;adr_type();
 194     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 195         t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 196         t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 197          -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 198          -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop) {
 199       // clone the Phi with our address type
 200       result = mphi-&gt;split_out_instance(t_adr, igvn);
 201     } else {
 202       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), "correct memory chain");
 203     }
 204   }
 205   return result;
 206 }
 207 
 208 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 209   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 210   Node *mem = mmem;
 211 #ifdef ASSERT
 212   {
 213     // Check that current type is consistent with the alias index used during graph construction
 214     assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not be a bad alias_idx");
 215     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 216                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 217     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 218     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
 219                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;
 220         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 221         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 222           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 223           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 224       // don't assert if it is dead code.
 225       consistent = true;
 226     }
 227     if( !consistent ) {
 228       st-&gt;print("alias_idx==%d, adr_check==", alias_idx);
 229       if( adr_check == NULL ) {
 230         st-&gt;print("NULL");
 231       } else {
 232         adr_check-&gt;dump();
 233       }
 234       st-&gt;cr();
 235       print_alias_types();
 236       assert(consistent, "adr_check must match alias idx");
 237     }
 238   }
 239 #endif
 240   // TypeOopPtr::NOTNULL+any is an OOP with unknown offset - generally
 241   // means an array I have not precisely typed yet.  Do not do any
 242   // alias stuff with it any time soon.
 243   const TypeOopPtr *toop = tp-&gt;isa_oopptr();
 244   if( tp-&gt;base() != Type::AnyPtr &amp;&amp;
 245       !(toop &amp;&amp;
 246         toop-&gt;klass() != NULL &amp;&amp;
 247         toop-&gt;klass()-&gt;is_java_lang_Object() &amp;&amp;
 248         toop-&gt;offset() == Type::OffsetBot) ) {
 249     // compress paths and change unreachable cycles to TOP
 250     // If not, we can update the input infinitely along a MergeMem cycle
 251     // Equivalent code in PhiNode::Ideal
 252     Node* m  = phase-&gt;transform(mmem);
 253     // If transformed to a MergeMem, get the desired slice
 254     // Otherwise the returned node represents memory for every slice
 255     mem = (m-&gt;is_MergeMem())? m-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : m;
 256     // Update input if it is progress over what we have now
 257   }
 258   return mem;
 259 }
 260 
 261 //--------------------------Ideal_common---------------------------------------
 262 // Look for degenerate control and memory inputs.  Bypass MergeMem inputs.
 263 // Unhook non-raw memories from complete (macro-expanded) initializations.
 264 Node *MemNode::Ideal_common(PhaseGVN *phase, bool can_reshape) {
 265   // If our control input is a dead region, kill all below the region
 266   Node *ctl = in(MemNode::Control);
 267   if (ctl &amp;&amp; remove_dead_region(phase, can_reshape))
 268     return this;
 269   ctl = in(MemNode::Control);
 270   // Don't bother trying to transform a dead node
 271   if (ctl &amp;&amp; ctl-&gt;is_top())  return NodeSentinel;
 272 
 273   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 274   // Wait if control on the worklist.
 275   if (ctl &amp;&amp; can_reshape &amp;&amp; igvn != NULL) {
 276     Node* bol = NULL;
 277     Node* cmp = NULL;
 278     if (ctl-&gt;in(0)-&gt;is_If()) {
 279       assert(ctl-&gt;is_IfTrue() || ctl-&gt;is_IfFalse(), "sanity");
 280       bol = ctl-&gt;in(0)-&gt;in(1);
 281       if (bol-&gt;is_Bool())
 282         cmp = ctl-&gt;in(0)-&gt;in(1)-&gt;in(1);
 283     }
 284     if (igvn-&gt;_worklist.member(ctl) ||
 285         (bol != NULL &amp;&amp; igvn-&gt;_worklist.member(bol)) ||
 286         (cmp != NULL &amp;&amp; igvn-&gt;_worklist.member(cmp)) ) {
 287       // This control path may be dead.
 288       // Delay this memory node transformation until the control is processed.
 289       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 290       return NodeSentinel; // caller will return NULL
 291     }
 292   }
 293   // Ignore if memory is dead, or self-loop
 294   Node *mem = in(MemNode::Memory);
 295   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 296   assert(mem != this, "dead loop in MemNode::Ideal");
 297 
 298   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 299     // This memory slice may be dead.
 300     // Delay this mem node transformation until the memory is processed.
 301     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 302     return NodeSentinel; // caller will return NULL
 303   }
 304 
 305   Node *address = in(MemNode::Address);
 306   const Type *t_adr = phase-&gt;type(address);
 307   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 308 
 309   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 310       (igvn-&gt;_worklist.member(address) ||
 311        igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; (t_adr != adr_type())) ) {
 312     // The address's base and type may change when the address is processed.
 313     // Delay this mem node transformation until the address is processed.
 314     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 315     return NodeSentinel; // caller will return NULL
 316   }
 317 
 318   // Do NOT remove or optimize the next lines: ensure a new alias index
 319   // is allocated for an oop pointer type before Escape Analysis.
 320   // Note: C++ will not remove it since the call has side effect.
 321   if (t_adr-&gt;isa_oopptr()) {
 322     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 323   }
 324 
 325   Node* base = NULL;
 326   if (address-&gt;is_AddP()) {
 327     base = address-&gt;in(AddPNode::Base);
 328   }
 329   if (base != NULL &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR) &amp;&amp;
 330       !t_adr-&gt;isa_rawptr()) {
 331     // Note: raw address has TOP base and top-&gt;higher_equal(TypePtr::NULL_PTR) is true.
 332     // Skip this node optimization if its address has TOP base.
 333     return NodeSentinel; // caller will return NULL
 334   }
 335 
 336   // Avoid independent memory operations
 337   Node* old_mem = mem;
 338 
 339   // The code which unhooks non-raw memories from complete (macro-expanded)
 340   // initializations was removed. After macro-expansion all stores catched
 341   // by Initialize node became raw stores and there is no information
 342   // which memory slices they modify. So it is unsafe to move any memory
 343   // operation above these stores. Also in most cases hooked non-raw memories
 344   // were already unhooked by using information from detect_ptr_independence()
 345   // and find_previous_store().
 346 
 347   if (mem-&gt;is_MergeMem()) {
 348     MergeMemNode* mmem = mem-&gt;as_MergeMem();
 349     const TypePtr *tp = t_adr-&gt;is_ptr();
 350 
 351     mem = step_through_mergemem(phase, mmem, tp, adr_type(), tty);
 352   }
 353 
 354   if (mem != old_mem) {
 355     set_req(MemNode::Memory, mem);
 356     if (can_reshape &amp;&amp; old_mem-&gt;outcnt() == 0) {
 357         igvn-&gt;_worklist.push(old_mem);
 358     }
 359     if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel;
 360     return this;
 361   }
 362 
 363   // let the subclass continue analyzing...
 364   return NULL;
 365 }
 366 
 367 // Helper function for proving some simple control dominations.
 368 // Attempt to prove that all control inputs of 'dom' dominate 'sub'.
 369 // Already assumes that 'dom' is available at 'sub', and that 'sub'
 370 // is not a constant (dominated by the method's StartNode).
 371 // Used by MemNode::find_previous_store to prove that the
 372 // control input of a memory operation predates (dominates)
 373 // an allocation it wants to look past.
 374 bool MemNode::all_controls_dominate(Node* dom, Node* sub) {
 375   if (dom == NULL || dom-&gt;is_top() || sub == NULL || sub-&gt;is_top())
 376     return false; // Conservative answer for dead code
 377 
 378   // Check 'dom'. Skip Proj and CatchProj nodes.
 379   dom = dom-&gt;find_exact_control(dom);
 380   if (dom == NULL || dom-&gt;is_top())
 381     return false; // Conservative answer for dead code
 382 
 383   if (dom == sub) {
 384     // For the case when, for example, 'sub' is Initialize and the original
 385     // 'dom' is Proj node of the 'sub'.
 386     return false;
 387   }
 388 
 389   if (dom-&gt;is_Con() || dom-&gt;is_Start() || dom-&gt;is_Root() || dom == sub)
 390     return true;
 391 
 392   // 'dom' dominates 'sub' if its control edge and control edges
 393   // of all its inputs dominate or equal to sub's control edge.
 394 
 395   // Currently 'sub' is either Allocate, Initialize or Start nodes.
 396   // Or Region for the check in LoadNode::Ideal();
 397   // 'sub' should have sub-&gt;in(0) != NULL.
 398   assert(sub-&gt;is_Allocate() || sub-&gt;is_Initialize() || sub-&gt;is_Start() ||
 399          sub-&gt;is_Region() || sub-&gt;is_Call(), "expecting only these nodes");
 400 
 401   // Get control edge of 'sub'.
 402   Node* orig_sub = sub;
 403   sub = sub-&gt;find_exact_control(sub-&gt;in(0));
 404   if (sub == NULL || sub-&gt;is_top())
 405     return false; // Conservative answer for dead code
 406 
 407   assert(sub-&gt;is_CFG(), "expecting control");
 408 
 409   if (sub == dom)
 410     return true;
 411 
 412   if (sub-&gt;is_Start() || sub-&gt;is_Root())
 413     return false;
 414 
 415   {
 416     // Check all control edges of 'dom'.
 417 
 418     ResourceMark rm;
 419     Arena* arena = Thread::current()-&gt;resource_area();
 420     Node_List nlist(arena);
 421     Unique_Node_List dom_list(arena);
 422 
 423     dom_list.push(dom);
 424     bool only_dominating_controls = false;
 425 
 426     for (uint next = 0; next &lt; dom_list.size(); next++) {
 427       Node* n = dom_list.at(next);
 428       if (n == orig_sub)
 429         return false; // One of dom's inputs dominated by sub.
 430       if (!n-&gt;is_CFG() &amp;&amp; n-&gt;pinned()) {
 431         // Check only own control edge for pinned non-control nodes.
 432         n = n-&gt;find_exact_control(n-&gt;in(0));
 433         if (n == NULL || n-&gt;is_top())
 434           return false; // Conservative answer for dead code
 435         assert(n-&gt;is_CFG(), "expecting control");
 436         dom_list.push(n);
 437       } else if (n-&gt;is_Con() || n-&gt;is_Start() || n-&gt;is_Root()) {
 438         only_dominating_controls = true;
 439       } else if (n-&gt;is_CFG()) {
 440         if (n-&gt;dominates(sub, nlist))
 441           only_dominating_controls = true;
 442         else
 443           return false;
 444       } else {
 445         // First, own control edge.
 446         Node* m = n-&gt;find_exact_control(n-&gt;in(0));
 447         if (m != NULL) {
 448           if (m-&gt;is_top())
 449             return false; // Conservative answer for dead code
 450           dom_list.push(m);
 451         }
 452         // Now, the rest of edges.
 453         uint cnt = n-&gt;req();
 454         for (uint i = 1; i &lt; cnt; i++) {
 455           m = n-&gt;find_exact_control(n-&gt;in(i));
 456           if (m == NULL || m-&gt;is_top())
 457             continue;
 458           dom_list.push(m);
 459         }
 460       }
 461     }
 462     return only_dominating_controls;
 463   }
 464 }
 465 
 466 //---------------------detect_ptr_independence---------------------------------
 467 // Used by MemNode::find_previous_store to prove that two base
 468 // pointers are never equal.
 469 // The pointers are accompanied by their associated allocations,
 470 // if any, which have been previously discovered by the caller.
 471 bool MemNode::detect_ptr_independence(Node* p1, AllocateNode* a1,
 472                                       Node* p2, AllocateNode* a2,
 473                                       PhaseTransform* phase) {
 474   // Attempt to prove that these two pointers cannot be aliased.
 475   // They may both manifestly be allocations, and they should differ.
 476   // Or, if they are not both allocations, they can be distinct constants.
 477   // Otherwise, one is an allocation and the other a pre-existing value.
 478   if (a1 == NULL &amp;&amp; a2 == NULL) {           // neither an allocation
 479     return (p1 != p2) &amp;&amp; p1-&gt;is_Con() &amp;&amp; p2-&gt;is_Con();
 480   } else if (a1 != NULL &amp;&amp; a2 != NULL) {    // both allocations
 481     return (a1 != a2);
 482   } else if (a1 != NULL) {                  // one allocation a1
 483     // (Note:  p2-&gt;is_Con implies p2-&gt;in(0)-&gt;is_Root, which dominates.)
 484     return all_controls_dominate(p2, a1);
 485   } else { //(a2 != NULL)                   // one allocation a2
 486     return all_controls_dominate(p1, a2);
 487   }
 488   return false;
 489 }
 490 
 491 
 492 // Find an arraycopy that must have set (can_see_stored_value=true) or
 493 // could have set (can_see_stored_value=false) the value for this load
 494 Node* LoadNode::find_previous_arraycopy(PhaseTransform* phase, Node* ld_alloc, Node*&amp; mem, bool can_see_stored_value) const {
 495   if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) != NULL &amp;&amp; (mem-&gt;in(0)-&gt;Opcode() == Op_MemBarStoreStore ||
 496                                                mem-&gt;in(0)-&gt;Opcode() == Op_MemBarCPUOrder)) {
 497     Node* mb = mem-&gt;in(0);
 498     if (mb-&gt;in(0) != NULL &amp;&amp; mb-&gt;in(0)-&gt;is_Proj() &amp;&amp;
 499         mb-&gt;in(0)-&gt;in(0) != NULL &amp;&amp; mb-&gt;in(0)-&gt;in(0)-&gt;is_ArrayCopy()) {
 500       ArrayCopyNode* ac = mb-&gt;in(0)-&gt;in(0)-&gt;as_ArrayCopy();
 501       if (ac-&gt;is_clonebasic()) {
 502         intptr_t offset;
 503         AllocateNode* alloc = AllocateNode::Ideal_allocation(ac-&gt;in(ArrayCopyNode::Dest), phase, offset);
 504         assert(alloc != NULL &amp;&amp; alloc-&gt;initialization()-&gt;is_complete_with_arraycopy(), "broken allocation");
 505         if (alloc == ld_alloc) {
 506           return ac;
 507         }
 508       }
 509     }
 510   } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) != NULL &amp;&amp; mem-&gt;in(0)-&gt;is_ArrayCopy()) {
 511     ArrayCopyNode* ac = mem-&gt;in(0)-&gt;as_ArrayCopy();
 512 
 513     if (ac-&gt;is_arraycopy_validated() ||
 514         ac-&gt;is_copyof_validated() ||
 515         ac-&gt;is_copyofrange_validated()) {
 516       Node* ld_addp = in(MemNode::Address);
 517       if (ld_addp-&gt;is_AddP()) {
 518         Node* ld_base = ld_addp-&gt;in(AddPNode::Address);
 519         Node* ld_offs = ld_addp-&gt;in(AddPNode::Offset);
 520 
 521         Node* dest = ac-&gt;in(ArrayCopyNode::Dest);
 522 
 523         if (dest == ld_base) {
 524           const TypeX *ld_offs_t = phase-&gt;type(ld_offs)-&gt;isa_intptr_t();
 525           if (ac-&gt;modifies(ld_offs_t-&gt;_lo, ld_offs_t-&gt;_hi, phase, can_see_stored_value)) {
 526             return ac;
 527           }
 528           if (!can_see_stored_value) {
 529             mem = ac-&gt;in(TypeFunc::Memory);
 530           }
 531         }
 532       }
 533     }
 534   }
 535   return NULL;
 536 }
 537 
 538 // The logic for reordering loads and stores uses four steps:
 539 // (a) Walk carefully past stores and initializations which we
 540 //     can prove are independent of this load.
 541 // (b) Observe that the next memory state makes an exact match
 542 //     with self (load or store), and locate the relevant store.
 543 // (c) Ensure that, if we were to wire self directly to the store,
 544 //     the optimizer would fold it up somehow.
 545 // (d) Do the rewiring, and return, depending on some other part of
 546 //     the optimizer to fold up the load.
 547 // This routine handles steps (a) and (b).  Steps (c) and (d) are
 548 // specific to loads and stores, so they are handled by the callers.
 549 // (Currently, only LoadNode::Ideal has steps (c), (d).  More later.)
 550 //
 551 Node* MemNode::find_previous_store(PhaseTransform* phase) {
 552   Node*         ctrl   = in(MemNode::Control);
 553   Node*         adr    = in(MemNode::Address);
 554   intptr_t      offset = 0;
 555   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
 556   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
 557 
 558   if (offset == Type::OffsetBot)
 559     return NULL;            // cannot unalias unless there are precise offsets
 560 
 561   const TypeOopPtr *addr_t = adr-&gt;bottom_type()-&gt;isa_oopptr();
 562 
 563   intptr_t size_in_bytes = memory_size();
 564 
 565   Node* mem = in(MemNode::Memory);   // start searching here...
 566 
 567   int cnt = 50;             // Cycle limiter
 568   for (;;) {                // While we can dance past unrelated stores...
 569     if (--cnt &lt; 0)  break;  // Caught in cycle or a complicated dance?
 570 
 571     Node* prev = mem;
 572     if (mem-&gt;is_Store()) {
 573       Node* st_adr = mem-&gt;in(MemNode::Address);
 574       intptr_t st_offset = 0;
 575       Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);
 576       if (st_base == NULL)
 577         break;              // inscrutable pointer
 578       if (st_offset != offset &amp;&amp; st_offset != Type::OffsetBot) {
 579         const int MAX_STORE = BytesPerLong;
 580         if (st_offset &gt;= offset + size_in_bytes ||
 581             st_offset &lt;= offset - MAX_STORE ||
 582             st_offset &lt;= offset - mem-&gt;as_Store()-&gt;memory_size()) {
 583           // Success:  The offsets are provably independent.
 584           // (You may ask, why not just test st_offset != offset and be done?
 585           // The answer is that stores of different sizes can co-exist
 586           // in the same sequence of RawMem effects.  We sometimes initialize
 587           // a whole 'tile' of array elements with a single jint or jlong.)
 588           mem = mem-&gt;in(MemNode::Memory);
 589           continue;           // (a) advance through independent store memory
 590         }
 591       }
 592       if (st_base != base &amp;&amp;
 593           detect_ptr_independence(base, alloc,
 594                                   st_base,
 595                                   AllocateNode::Ideal_allocation(st_base, phase),
 596                                   phase)) {
 597         // Success:  The bases are provably independent.
 598         mem = mem-&gt;in(MemNode::Memory);
 599         continue;           // (a) advance through independent store memory
 600       }
 601 
 602       // (b) At this point, if the bases or offsets do not agree, we lose,
 603       // since we have not managed to prove 'this' and 'mem' independent.
 604       if (st_base == base &amp;&amp; st_offset == offset) {
 605         return mem;         // let caller handle steps (c), (d)
 606       }
 607 
 608     } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
 609       InitializeNode* st_init = mem-&gt;in(0)-&gt;as_Initialize();
 610       AllocateNode*  st_alloc = st_init-&gt;allocation();
 611       if (st_alloc == NULL)
 612         break;              // something degenerated
 613       bool known_identical = false;
 614       bool known_independent = false;
 615       if (alloc == st_alloc)
 616         known_identical = true;
 617       else if (alloc != NULL)
 618         known_independent = true;
 619       else if (all_controls_dominate(this, st_alloc))
 620         known_independent = true;
 621 
 622       if (known_independent) {
 623         // The bases are provably independent: Either they are
 624         // manifestly distinct allocations, or else the control
 625         // of this load dominates the store's allocation.
 626         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 627         if (alias_idx == Compile::AliasIdxRaw) {
 628           mem = st_alloc-&gt;in(TypeFunc::Memory);
 629         } else {
 630           mem = st_init-&gt;memory(alias_idx);
 631         }
 632         continue;           // (a) advance through independent store memory
 633       }
 634 
 635       // (b) at this point, if we are not looking at a store initializing
 636       // the same allocation we are loading from, we lose.
 637       if (known_identical) {
 638         // From caller, can_see_stored_value will consult find_captured_store.
 639         return mem;         // let caller handle steps (c), (d)
 640       }
 641 
 642     } else if (find_previous_arraycopy(phase, alloc, mem, false) != NULL) {
 643       if (prev != mem) {
 644         // Found an arraycopy but it doesn't affect that load
 645         continue;
 646       }
 647       // Found an arraycopy that may affect that load
 648       return mem;
 649     } else if (addr_t != NULL &amp;&amp; addr_t-&gt;is_known_instance_field()) {
 650       // Can't use optimize_simple_memory_chain() since it needs PhaseGVN.
 651       if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Call()) {
 652         // ArrayCopyNodes processed here as well.
 653         CallNode *call = mem-&gt;in(0)-&gt;as_Call();
 654         if (!call-&gt;may_modify(addr_t, phase)) {
 655           mem = call-&gt;in(TypeFunc::Memory);
 656           continue;         // (a) advance through independent call memory
 657         }
 658       } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_MemBar()) {
 659         if (ArrayCopyNode::may_modify(addr_t, mem-&gt;in(0)-&gt;as_MemBar(), phase)) {
 660           break;
 661         }
 662         mem = mem-&gt;in(0)-&gt;in(TypeFunc::Memory);
 663         continue;           // (a) advance through independent MemBar memory
 664       } else if (mem-&gt;is_ClearArray()) {
 665         if (ClearArrayNode::step_through(&amp;mem, (uint)addr_t-&gt;instance_id(), phase)) {
 666           // (the call updated 'mem' value)
 667           continue;         // (a) advance through independent allocation memory
 668         } else {
 669           // Can not bypass initialization of the instance
 670           // we are looking for.
 671           return mem;
 672         }
 673       } else if (mem-&gt;is_MergeMem()) {
 674         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 675         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 676         continue;           // (a) advance through independent MergeMem memory
 677       }
 678     }
 679 
 680     // Unless there is an explicit 'continue', we must bail out here,
 681     // because 'mem' is an inscrutable memory state (e.g., a call).
 682     break;
 683   }
 684 
 685   return NULL;              // bail out
 686 }
 687 
 688 //----------------------calculate_adr_type-------------------------------------
 689 // Helper function.  Notices when the given type of address hits top or bottom.
 690 // Also, asserts a cross-check of the type against the expected address type.
 691 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 692   if (t == Type::TOP)  return NULL; // does not touch memory any more?
 693   #ifdef PRODUCT
 694   cross_check = NULL;
 695   #else
 696   if (!VerifyAliases || is_error_reported() || Node::in_dump())  cross_check = NULL;
 697   #endif
 698   const TypePtr* tp = t-&gt;isa_ptr();
 699   if (tp == NULL) {
 700     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, "expected memory type must be wide");
 701     return TypePtr::BOTTOM;           // touches lots of memory
 702   } else {
 703     #ifdef ASSERT
 704     // %%%% [phh] We don't check the alias index if cross_check is
 705     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 706     if (cross_check != NULL &amp;&amp;
 707         cross_check != TypePtr::BOTTOM &amp;&amp;
 708         cross_check != TypeRawPtr::BOTTOM) {
 709       // Recheck the alias index, to see if it has changed (due to a bug).
 710       Compile* C = Compile::current();
 711       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 712              "must stay in the original alias category");
 713       // The type of the address must be contained in the adr_type,
 714       // disregarding "null"-ness.
 715       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 716       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 717       assert(cross_check-&gt;meet(tp_notnull) == cross_check-&gt;remove_speculative(),
 718              "real address must not escape from expected memory type");
 719     }
 720     #endif
 721     return tp;
 722   }
 723 }
 724 
 725 //=============================================================================
 726 // Should LoadNode::Ideal() attempt to remove control edges?
 727 bool LoadNode::can_remove_control() const {
 728   return true;
 729 }
 730 uint LoadNode::size_of() const { return sizeof(*this); }
 731 uint LoadNode::cmp( const Node &amp;n ) const
 732 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 733 const Type *LoadNode::bottom_type() const { return _type; }
 734 uint LoadNode::ideal_reg() const {
 735   return _type-&gt;ideal_reg();
 736 }
 737 
 738 #ifndef PRODUCT
 739 void LoadNode::dump_spec(outputStream *st) const {
 740   MemNode::dump_spec(st);
 741   if( !Verbose &amp;&amp; !WizardMode ) {
 742     // standard dump does this in Verbose and WizardMode
 743     st-&gt;print(" #"); _type-&gt;dump_on(st);
 744   }
 745   if (!_depends_only_on_test) {
 746     st-&gt;print(" (does not depend only on test)");
 747   }
 748 }
 749 #endif
 750 
 751 #ifdef ASSERT
 752 //----------------------------is_immutable_value-------------------------------
 753 // Helper function to allow a raw load without control edge for some cases
 754 bool LoadNode::is_immutable_value(Node* adr) {
 755   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 756           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 757           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 758            in_bytes(JavaThread::osthread_offset())));
 759 }
 760 #endif
 761 
 762 //----------------------------LoadNode::make-----------------------------------
 763 // Polymorphic factory method:
 764 Node *LoadNode::make(PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt, MemOrd mo,
 765                      ControlDependency control_dependency, bool unaligned, bool mismatched) {
 766   Compile* C = gvn.C;
 767 
 768   // sanity check the alias category against the created node type
 769   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 770            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 771          "use LoadKlassNode instead");
 772   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 773            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 774          "use LoadRangeNode instead");
 775   // Check control edge of raw loads
 776   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 777           // oop will be recorded in oop map if load crosses safepoint
 778           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 779           "raw memory operations should have control edge");
 780   LoadNode* load = NULL;
 781   switch (bt) {
 782   case T_BOOLEAN: load = new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 783   case T_BYTE:    load = new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 784   case T_INT:     load = new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 785   case T_CHAR:    load = new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 786   case T_SHORT:   load = new LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 787   case T_LONG:    load = new LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency); break;
 788   case T_FLOAT:   load = new LoadFNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 789   case T_DOUBLE:  load = new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 790   case T_ADDRESS: load = new LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo, control_dependency); break;
 791   case T_OBJECT:
 792 #ifdef _LP64
 793     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 794       load = new LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo, control_dependency);
 795     } else
 796 #endif
 797     {
 798       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), "should have got back a narrow oop");
 799       load = new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_oopptr(), mo, control_dependency);
 800     }
 801     break;
 802   }
 803   assert(load != NULL, "LoadNode should have been created");
 804   if (unaligned) {
 805     load-&gt;set_unaligned_access();
 806   }
 807   if (mismatched) {
 808     load-&gt;set_mismatched_access();
 809   }
 810   if (load-&gt;Opcode() == Op_LoadN) {
 811     Node* ld = gvn.transform(load);
 812     return new DecodeNNode(ld, ld-&gt;bottom_type()-&gt;make_ptr());
 813   }
 814 
 815   return load;
 816 }
 817 
 818 LoadLNode* LoadLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,
 819                                   ControlDependency control_dependency, bool unaligned, bool mismatched) {
 820   bool require_atomic = true;
 821   LoadLNode* load = new LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency, require_atomic);
 822   if (unaligned) {
 823     load-&gt;set_unaligned_access();
 824   }
 825   if (mismatched) {
 826     load-&gt;set_mismatched_access();
 827   }
 828   return load;
 829 }
 830 
 831 LoadDNode* LoadDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,
 832                                   ControlDependency control_dependency, bool unaligned, bool mismatched) {
 833   bool require_atomic = true;
 834   LoadDNode* load = new LoadDNode(ctl, mem, adr, adr_type, rt, mo, control_dependency, require_atomic);
 835   if (unaligned) {
 836     load-&gt;set_unaligned_access();
 837   }
 838   if (mismatched) {
 839     load-&gt;set_mismatched_access();
 840   }
 841   return load;
 842 }
 843 
 844 
 845 
 846 //------------------------------hash-------------------------------------------
 847 uint LoadNode::hash() const {
 848   // unroll addition of interesting fields
 849   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 850 }
 851 
 852 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 853   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 854     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 855     bool is_stable_ary = FoldStableValues &amp;&amp;
 856                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 857                          tp-&gt;isa_aryptr()-&gt;is_stable();
 858 
 859     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 860   }
 861 
 862   return false;
 863 }
 864 
 865 // Is the value loaded previously stored by an arraycopy? If so return
 866 // a load node that reads from the source array so we may be able to
 867 // optimize out the ArrayCopy node later.
 868 Node* LoadNode::can_see_arraycopy_value(Node* st, PhaseTransform* phase) const {
 869   Node* ld_adr = in(MemNode::Address);
 870   intptr_t ld_off = 0;
 871   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 872   Node* ac = find_previous_arraycopy(phase, ld_alloc, st, true);
 873   if (ac != NULL) {
 874     assert(ac-&gt;is_ArrayCopy(), "what kind of node can this be?");
 875 
 876     Node* ld = clone();
 877     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 878       assert(ld_alloc != NULL, "need an alloc");
 879       Node* addp = in(MemNode::Address)-&gt;clone();
 880       assert(addp-&gt;is_AddP(), "address must be addp");
 881       assert(addp-&gt;in(AddPNode::Base) == ac-&gt;in(ArrayCopyNode::Dest)-&gt;in(AddPNode::Base), "strange pattern");
 882       assert(addp-&gt;in(AddPNode::Address) == ac-&gt;in(ArrayCopyNode::Dest)-&gt;in(AddPNode::Address), "strange pattern");
 883       addp-&gt;set_req(AddPNode::Base, ac-&gt;in(ArrayCopyNode::Src)-&gt;in(AddPNode::Base));
 884       addp-&gt;set_req(AddPNode::Address, ac-&gt;in(ArrayCopyNode::Src)-&gt;in(AddPNode::Address));
 885       ld-&gt;set_req(MemNode::Address, phase-&gt;transform(addp));
 886       if (in(0) != NULL) {
 887         assert(ld_alloc-&gt;in(0) != NULL, "alloc must have control");
 888         ld-&gt;set_req(0, ld_alloc-&gt;in(0));
 889       }
 890     } else {
 891       Node* addp = in(MemNode::Address)-&gt;clone();
 892       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), "should be");
 893       addp-&gt;set_req(AddPNode::Base, ac-&gt;in(ArrayCopyNode::Src));
 894       addp-&gt;set_req(AddPNode::Address, ac-&gt;in(ArrayCopyNode::Src));
 895 
 896       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
 897       BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
 898       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 899       uint shift  = exact_log2(type2aelembytes(ary_elem));
 900 
 901       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 902 #ifdef _LP64
 903       diff = phase-&gt;transform(new ConvI2LNode(diff));
 904 #endif
 905       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 906 
 907       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 908       addp-&gt;set_req(AddPNode::Offset, offset);
 909       ld-&gt;set_req(MemNode::Address, phase-&gt;transform(addp));
 910 
 911       if (in(0) != NULL) {
 912         assert(ac-&gt;in(0) != NULL, "alloc must have control");
 913         ld-&gt;set_req(0, ac-&gt;in(0));
 914       }
 915     }
 916     // load depends on the tests that validate the arraycopy
 917     ld-&gt;as_Load()-&gt;_depends_only_on_test = Pinned;
 918     return ld;
 919   }
 920   return NULL;
 921 }
 922 
 923 
 924 //---------------------------can_see_stored_value------------------------------
 925 // This routine exists to make sure this set of tests is done the same
 926 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 927 // will change the graph shape in a way which makes memory alive twice at the
 928 // same time (uses the Oracle model of aliasing), then some
 929 // LoadXNode::Identity will fold things back to the equivalence-class model
 930 // of aliasing.
 931 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
 932   Node* ld_adr = in(MemNode::Address);
 933   intptr_t ld_off = 0;
 934   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 935   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
 936   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
 937   // This is more general than load from boxing objects.
 938   if (skip_through_membars(atp, tp, phase-&gt;C-&gt;eliminate_boxing())) {
 939     uint alias_idx = atp-&gt;index();
 940     bool final = !atp-&gt;is_rewritable();
 941     Node* result = NULL;
 942     Node* current = st;
 943     // Skip through chains of MemBarNodes checking the MergeMems for
 944     // new states for the slice of this load.  Stop once any other
 945     // kind of node is encountered.  Loads from final memory can skip
 946     // through any kind of MemBar but normal loads shouldn't skip
 947     // through MemBarAcquire since the could allow them to move out of
 948     // a synchronized region.
 949     while (current-&gt;is_Proj()) {
 950       int opc = current-&gt;in(0)-&gt;Opcode();
 951       if ((final &amp;&amp; (opc == Op_MemBarAcquire ||
 952                      opc == Op_MemBarAcquireLock ||
 953                      opc == Op_LoadFence)) ||
 954           opc == Op_MemBarRelease ||
 955           opc == Op_StoreFence ||
 956           opc == Op_MemBarReleaseLock ||
 957           opc == Op_MemBarStoreStore ||
 958           opc == Op_MemBarCPUOrder) {
 959         Node* mem = current-&gt;in(0)-&gt;in(TypeFunc::Memory);
 960         if (mem-&gt;is_MergeMem()) {
 961           MergeMemNode* merge = mem-&gt;as_MergeMem();
 962           Node* new_st = merge-&gt;memory_at(alias_idx);
 963           if (new_st == merge-&gt;base_memory()) {
 964             // Keep searching
 965             current = new_st;
 966             continue;
 967           }
 968           // Save the new memory state for the slice and fall through
 969           // to exit.
 970           result = new_st;
 971         }
 972       }
 973       break;
 974     }
 975     if (result != NULL) {
 976       st = result;
 977     }
 978   }
 979 
 980   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
 981   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
 982   for (int trip = 0; trip &lt;= 1; trip++) {
 983 
 984     if (st-&gt;is_Store()) {
 985       Node* st_adr = st-&gt;in(MemNode::Address);
 986       if (!phase-&gt;eqv(st_adr, ld_adr)) {
 987         // Try harder before giving up...  Match raw and non-raw pointers.
 988         intptr_t st_off = 0;
 989         AllocateNode* alloc = AllocateNode::Ideal_allocation(st_adr, phase, st_off);
 990         if (alloc == NULL)       return NULL;
 991         if (alloc != ld_alloc)   return NULL;
 992         if (ld_off != st_off)    return NULL;
 993         // At this point we have proven something like this setup:
 994         //  A = Allocate(...)
 995         //  L = LoadQ(,  AddP(CastPP(, A.Parm),, #Off))
 996         //  S = StoreQ(, AddP(,        A.Parm  , #Off), V)
 997         // (Actually, we haven't yet proven the Q's are the same.)
 998         // In other words, we are loading from a casted version of
 999         // the same pointer-and-offset that we stored to.
1000         // Thus, we are able to replace L by V.
1001       }
1002       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1003       if (store_Opcode() != st-&gt;Opcode())
1004         return NULL;
1005       return st-&gt;in(MemNode::ValueIn);
1006     }
1007 
1008     // A load from a freshly-created object always returns zero.
1009     // (This can happen after LoadNode::Ideal resets the load's memory input
1010     // to find_captured_store, which returned InitializeNode::zero_memory.)
1011     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1012         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1013         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1014       // return a zero value for the load's basic type
1015       // (This is one of the few places where a generic PhaseTransform
1016       // can create new nodes.  Think of it as lazily manifesting
1017       // virtually pre-existing constants.)
1018       return phase-&gt;zerocon(memory_type());
1019     }
1020 
1021     // A load from an initialization barrier can match a captured store.
1022     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1023       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1024       AllocateNode* alloc = init-&gt;allocation();
1025       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1026         // examine a captured store value
1027         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1028         if (st != NULL) {
1029           continue;             // take one more trip around
1030         }
1031       }
1032     }
1033 
1034     // Load boxed value from result of valueOf() call is input parameter.
1035     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1036         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1037       intptr_t ignore = 0;
1038       Node* base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ignore);
1039       if (base != NULL &amp;&amp; base-&gt;is_Proj() &amp;&amp;
1040           base-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp;
1041           base-&gt;in(0)-&gt;is_CallStaticJava() &amp;&amp;
1042           base-&gt;in(0)-&gt;as_CallStaticJava()-&gt;is_boxing_method()) {
1043         return base-&gt;in(0)-&gt;in(TypeFunc::Parms);
1044       }
1045     }
1046 
1047     break;
1048   }
1049 
1050   return NULL;
1051 }
1052 
1053 //----------------------is_instance_field_load_with_local_phi------------------
1054 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1055   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1056       in(Address)-&gt;is_AddP() ) {
1057     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1058     // Only instances and boxed values.
1059     if( t_oop != NULL &amp;&amp;
1060         (t_oop-&gt;is_ptr_to_boxed_value() ||
1061          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1062         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1063         t_oop-&gt;offset() != Type::OffsetTop) {
1064       return true;
1065     }
1066   }
1067   return false;
1068 }
1069 
1070 //------------------------------Identity---------------------------------------
1071 // Loads are identity if previous store is to same address
1072 Node *LoadNode::Identity( PhaseTransform *phase ) {
1073   // If the previous store-maker is the right kind of Store, and the store is
1074   // to the same address, then we are equal to the value stored.
1075   Node* mem = in(Memory);
1076   Node* value = can_see_stored_value(mem, phase);
1077   if( value ) {
1078     // byte, short &amp; char stores truncate naturally.
1079     // A load has to load the truncated value which requires
1080     // some sort of masking operation and that requires an
1081     // Ideal call instead of an Identity call.
1082     if (memory_size() &lt; BytesPerInt) {
1083       // If the input to the store does not fit with the load's result type,
1084       // it must be truncated via an Ideal call.
1085       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1086         return this;
1087     }
1088     // (This works even when value is a Con, but LoadNode::Value
1089     // usually runs first, producing the singleton type of the Con.)
1090     return value;
1091   }
1092 
1093   // Search for an existing data phi which was generated before for the same
1094   // instance's field to avoid infinite generation of phis in a loop.
1095   Node *region = mem-&gt;in(0);
1096   if (is_instance_field_load_with_local_phi(region)) {
1097     const TypeOopPtr *addr_t = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1098     int this_index  = phase-&gt;C-&gt;get_alias_index(addr_t);
1099     int this_offset = addr_t-&gt;offset();
1100     int this_iid    = addr_t-&gt;instance_id();
1101     if (!addr_t-&gt;is_known_instance() &amp;&amp;
1102          addr_t-&gt;is_ptr_to_boxed_value()) {
1103       // Use _idx of address base (could be Phi node) for boxed values.
1104       intptr_t   ignore = 0;
1105       Node*      base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1106       this_iid = base-&gt;_idx;
1107     }
1108     const Type* this_type = bottom_type();
1109     for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1110       Node* phi = region-&gt;fast_out(i);
1111       if (phi-&gt;is_Phi() &amp;&amp; phi != mem &amp;&amp;
1112           phi-&gt;as_Phi()-&gt;is_same_inst_field(this_type, this_iid, this_index, this_offset)) {
1113         return phi;
1114       }
1115     }
1116   }
1117 
1118   return this;
1119 }
1120 
1121 // We're loading from an object which has autobox behaviour.
1122 // If this object is result of a valueOf call we'll have a phi
1123 // merging a newly allocated object and a load from the cache.
1124 // We want to replace this load with the original incoming
1125 // argument to the valueOf call.
1126 Node* LoadNode::eliminate_autobox(PhaseGVN* phase) {
1127   assert(phase-&gt;C-&gt;eliminate_boxing(), "sanity");
1128   intptr_t ignore = 0;
1129   Node* base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1130   if ((base == NULL) || base-&gt;is_Phi()) {
1131     // Push the loads from the phi that comes from valueOf up
1132     // through it to allow elimination of the loads and the recovery
1133     // of the original value. It is done in split_through_phi().
1134     return NULL;
1135   } else if (base-&gt;is_Load() ||
1136              base-&gt;is_DecodeN() &amp;&amp; base-&gt;in(1)-&gt;is_Load()) {
1137     // Eliminate the load of boxed value for integer types from the cache
1138     // array by deriving the value from the index into the array.
1139     // Capture the offset of the load and then reverse the computation.
1140 
1141     // Get LoadN node which loads a boxing object from 'cache' array.
1142     if (base-&gt;is_DecodeN()) {
1143       base = base-&gt;in(1);
1144     }
1145     if (!base-&gt;in(Address)-&gt;is_AddP()) {
1146       return NULL; // Complex address
1147     }
1148     AddPNode* address = base-&gt;in(Address)-&gt;as_AddP();
1149     Node* cache_base = address-&gt;in(AddPNode::Base);
1150     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_DecodeN()) {
1151       // Get ConP node which is static 'cache' field.
1152       cache_base = cache_base-&gt;in(1);
1153     }
1154     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_Con()) {
1155       const TypeAryPtr* base_type = cache_base-&gt;bottom_type()-&gt;isa_aryptr();
1156       if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1157         Node* elements[4];
1158         int shift = exact_log2(type2aelembytes(T_OBJECT));
1159         int count = address-&gt;unpack_offsets(elements, ARRAY_SIZE(elements));
1160         if ((count &gt;  0) &amp;&amp; elements[0]-&gt;is_Con() &amp;&amp;
1161             ((count == 1) ||
1162              (count == 2) &amp;&amp; elements[1]-&gt;Opcode() == Op_LShiftX &amp;&amp;
1163                              elements[1]-&gt;in(2) == phase-&gt;intcon(shift))) {
1164           ciObjArray* array = base_type-&gt;const_oop()-&gt;as_obj_array();
1165           // Fetch the box object cache[0] at the base of the array and get its value
1166           ciInstance* box = array-&gt;obj_at(0)-&gt;as_instance();
1167           ciInstanceKlass* ik = box-&gt;klass()-&gt;as_instance_klass();
1168           assert(ik-&gt;is_box_klass(), "sanity");
1169           assert(ik-&gt;nof_nonstatic_fields() == 1, "change following code");
1170           if (ik-&gt;nof_nonstatic_fields() == 1) {
1171             // This should be true nonstatic_field_at requires calling
1172             // nof_nonstatic_fields so check it anyway
1173             ciConstant c = box-&gt;field_value(ik-&gt;nonstatic_field_at(0));
1174             BasicType bt = c.basic_type();
1175             // Only integer types have boxing cache.
1176             assert(bt == T_BOOLEAN || bt == T_CHAR  ||
1177                    bt == T_BYTE    || bt == T_SHORT ||
1178                    bt == T_INT     || bt == T_LONG, "wrong type = %s", type2name(bt));
1179             jlong cache_low = (bt == T_LONG) ? c.as_long() : c.as_int();
1180             if (cache_low != (int)cache_low) {
1181               return NULL; // should not happen since cache is array indexed by value
1182             }
1183             jlong offset = arrayOopDesc::base_offset_in_bytes(T_OBJECT) - (cache_low &lt;&lt; shift);
1184             if (offset != (int)offset) {
1185               return NULL; // should not happen since cache is array indexed by value
1186             }
1187            // Add up all the offsets making of the address of the load
1188             Node* result = elements[0];
1189             for (int i = 1; i &lt; count; i++) {
1190               result = phase-&gt;transform(new AddXNode(result, elements[i]));
1191             }
1192             // Remove the constant offset from the address and then
1193             result = phase-&gt;transform(new AddXNode(result, phase-&gt;MakeConX(-(int)offset)));
1194             // remove the scaling of the offset to recover the original index.
1195             if (result-&gt;Opcode() == Op_LShiftX &amp;&amp; result-&gt;in(2) == phase-&gt;intcon(shift)) {
1196               // Peel the shift off directly but wrap it in a dummy node
1197               // since Ideal can't return existing nodes
1198               result = new RShiftXNode(result-&gt;in(1), phase-&gt;intcon(0));
1199             } else if (result-&gt;is_Add() &amp;&amp; result-&gt;in(2)-&gt;is_Con() &amp;&amp;
1200                        result-&gt;in(1)-&gt;Opcode() == Op_LShiftX &amp;&amp;
1201                        result-&gt;in(1)-&gt;in(2) == phase-&gt;intcon(shift)) {
1202               // We can't do general optimization: ((X&lt;&lt;Z) + Y) &gt;&gt; Z ==&gt; X + (Y&gt;&gt;Z)
1203               // but for boxing cache access we know that X&lt;&lt;Z will not overflow
1204               // (there is range check) so we do this optimizatrion by hand here.
1205               Node* add_con = new RShiftXNode(result-&gt;in(2), phase-&gt;intcon(shift));
1206               result = new AddXNode(result-&gt;in(1)-&gt;in(1), phase-&gt;transform(add_con));
1207             } else {
1208               result = new RShiftXNode(result, phase-&gt;intcon(shift));
1209             }
1210 #ifdef _LP64
1211             if (bt != T_LONG) {
1212               result = new ConvL2INode(phase-&gt;transform(result));
1213             }
1214 #else
1215             if (bt == T_LONG) {
1216               result = new ConvI2LNode(phase-&gt;transform(result));
1217             }
1218 #endif
1219             // Boxing/unboxing can be done from signed &amp; unsigned loads (e.g. LoadUB -&gt; ... -&gt; LoadB pair).
1220             // Need to preserve unboxing load type if it is unsigned.
1221             switch(this-&gt;Opcode()) {
1222               case Op_LoadUB:
1223                 result = new AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFF));
1224                 break;
1225               case Op_LoadUS:
1226                 result = new AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFFFF));
1227                 break;
1228             }
1229             return result;
1230           }
1231         }
1232       }
1233     }
1234   }
1235   return NULL;
1236 }
1237 
1238 static bool stable_phi(PhiNode* phi, PhaseGVN *phase) {
1239   Node* region = phi-&gt;in(0);
1240   if (region == NULL) {
1241     return false; // Wait stable graph
1242   }
1243   uint cnt = phi-&gt;req();
1244   for (uint i = 1; i &lt; cnt; i++) {
1245     Node* rc = region-&gt;in(i);
1246     if (rc == NULL || phase-&gt;type(rc) == Type::TOP)
1247       return false; // Wait stable graph
1248     Node* in = phi-&gt;in(i);
1249     if (in == NULL || phase-&gt;type(in) == Type::TOP)
1250       return false; // Wait stable graph
1251   }
1252   return true;
1253 }
1254 //------------------------------split_through_phi------------------------------
1255 // Split instance or boxed field load through Phi.
1256 Node *LoadNode::split_through_phi(PhaseGVN *phase) {
1257   Node* mem     = in(Memory);
1258   Node* address = in(Address);
1259   const TypeOopPtr *t_oop = phase-&gt;type(address)-&gt;isa_oopptr();
1260 
1261   assert((t_oop != NULL) &amp;&amp;
1262          (t_oop-&gt;is_known_instance_field() ||
1263           t_oop-&gt;is_ptr_to_boxed_value()), "invalide conditions");
1264 
1265   Compile* C = phase-&gt;C;
1266   intptr_t ignore = 0;
1267   Node*    base = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1268   bool base_is_phi = (base != NULL) &amp;&amp; base-&gt;is_Phi();
1269   bool load_boxed_values = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp; C-&gt;aggressive_unboxing() &amp;&amp;
1270                            (base != NULL) &amp;&amp; (base == address-&gt;in(AddPNode::Base)) &amp;&amp;
1271                            phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL);
1272 
1273   if (!((mem-&gt;is_Phi() || base_is_phi) &amp;&amp;
1274         (load_boxed_values || t_oop-&gt;is_known_instance_field()))) {
1275     return NULL; // memory is not Phi
1276   }
1277 
1278   if (mem-&gt;is_Phi()) {
1279     if (!stable_phi(mem-&gt;as_Phi(), phase)) {
1280       return NULL; // Wait stable graph
1281     }
1282     uint cnt = mem-&gt;req();
1283     // Check for loop invariant memory.
1284     if (cnt == 3) {
1285       for (uint i = 1; i &lt; cnt; i++) {
1286         Node* in = mem-&gt;in(i);
1287         Node*  m = optimize_memory_chain(in, t_oop, this, phase);
1288         if (m == mem) {
1289           set_req(Memory, mem-&gt;in(cnt - i));
1290           return this; // made change
1291         }
1292       }
1293     }
1294   }
1295   if (base_is_phi) {
1296     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1297       return NULL; // Wait stable graph
1298     }
1299     uint cnt = base-&gt;req();
1300     // Check for loop invariant memory.
1301     if (cnt == 3) {
1302       for (uint i = 1; i &lt; cnt; i++) {
1303         if (base-&gt;in(i) == base) {
1304           return NULL; // Wait stable graph
1305         }
1306       }
1307     }
1308   }
1309 
1310   bool load_boxed_phi = load_boxed_values &amp;&amp; base_is_phi &amp;&amp; (base-&gt;in(0) == mem-&gt;in(0));
1311 
1312   // Split through Phi (see original code in loopopts.cpp).
1313   assert(C-&gt;have_alias_type(t_oop), "instance should have alias type");
1314 
1315   // Do nothing here if Identity will find a value
1316   // (to avoid infinite chain of value phis generation).
1317   if (!phase-&gt;eqv(this, this-&gt;Identity(phase)))
1318     return NULL;
1319 
1320   // Select Region to split through.
1321   Node* region;
1322   if (!base_is_phi) {
1323     assert(mem-&gt;is_Phi(), "sanity");
1324     region = mem-&gt;in(0);
1325     // Skip if the region dominates some control edge of the address.
1326     if (!MemNode::all_controls_dominate(address, region))
1327       return NULL;
1328   } else if (!mem-&gt;is_Phi()) {
1329     assert(base_is_phi, "sanity");
1330     region = base-&gt;in(0);
1331     // Skip if the region dominates some control edge of the memory.
1332     if (!MemNode::all_controls_dominate(mem, region))
1333       return NULL;
1334   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1335     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), "sanity");
1336     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1337       region = base-&gt;in(0);
1338     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
1339       region = mem-&gt;in(0);
1340     } else {
1341       return NULL; // complex graph
1342     }
1343   } else {
1344     assert(base-&gt;in(0) == mem-&gt;in(0), "sanity");
1345     region = mem-&gt;in(0);
1346   }
1347 
1348   const Type* this_type = this-&gt;bottom_type();
1349   int this_index  = C-&gt;get_alias_index(t_oop);
1350   int this_offset = t_oop-&gt;offset();
1351   int this_iid    = t_oop-&gt;instance_id();
1352   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1353     // Use _idx of address base for boxed values.
1354     this_iid = base-&gt;_idx;
1355   }
1356   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1357   Node* phi = new PhiNode(region, this_type, NULL, this_iid, this_index, this_offset);
1358   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1359     Node* x;
1360     Node* the_clone = NULL;
1361     if (region-&gt;in(i) == C-&gt;top()) {
1362       x = C-&gt;top();      // Dead path?  Use a dead data op
1363     } else {
1364       x = this-&gt;clone();        // Else clone up the data op
1365       the_clone = x;            // Remember for possible deletion.
1366       // Alter data node to use pre-phi inputs
1367       if (this-&gt;in(0) == region) {
1368         x-&gt;set_req(0, region-&gt;in(i));
1369       } else {
1370         x-&gt;set_req(0, NULL);
1371       }
1372       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1373         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1374       }
1375       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1376         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1377       }
1378       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1379         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1380         Node* adr_x = phase-&gt;transform(new AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1381         x-&gt;set_req(Address, adr_x);
1382       }
1383     }
1384     // Check for a 'win' on some paths
1385     const Type *t = x-&gt;Value(igvn);
1386 
1387     bool singleton = t-&gt;singleton();
1388 
1389     // See comments in PhaseIdealLoop::split_thru_phi().
1390     if (singleton &amp;&amp; t == Type::TOP) {
1391       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1392     }
1393 
1394     if (singleton) {
1395       x = igvn-&gt;makecon(t);
1396     } else {
1397       // We now call Identity to try to simplify the cloned node.
1398       // Note that some Identity methods call phase-&gt;type(this).
1399       // Make sure that the type array is big enough for
1400       // our new node, even though we may throw the node away.
1401       // (This tweaking with igvn only works because x is a new node.)
1402       igvn-&gt;set_type(x, t);
1403       // If x is a TypeNode, capture any more-precise type permanently into Node
1404       // otherwise it will be not updated during igvn-&gt;transform since
1405       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1406       x-&gt;raise_bottom_type(t);
1407       Node *y = x-&gt;Identity(igvn);
1408       if (y != x) {
1409         x = y;
1410       } else {
1411         y = igvn-&gt;hash_find_insert(x);
1412         if (y) {
1413           x = y;
1414         } else {
1415           // Else x is a new node we are keeping
1416           // We do not need register_new_node_with_optimizer
1417           // because set_type has already been called.
1418           igvn-&gt;_worklist.push(x);
1419         }
1420       }
1421     }
1422     if (x != the_clone &amp;&amp; the_clone != NULL) {
1423       igvn-&gt;remove_dead_node(the_clone);
1424     }
1425     phi-&gt;set_req(i, x);
1426   }
1427   // Record Phi
1428   igvn-&gt;register_new_node_with_optimizer(phi);
1429   return phi;
1430 }
1431 
1432 //------------------------------Ideal------------------------------------------
1433 // If the load is from Field memory and the pointer is non-null, it might be possible to
1434 // zero out the control input.
1435 // If the offset is constant and the base is an object allocation,
1436 // try to hook me up to the exact initializing store.
1437 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1438   Node* p = MemNode::Ideal_common(phase, can_reshape);
1439   if (p)  return (p == NodeSentinel) ? NULL : p;
1440 
1441   Node* ctrl    = in(MemNode::Control);
1442   Node* address = in(MemNode::Address);
1443   bool progress = false;
1444 
1445   // Skip up past a SafePoint control.  Cannot do this for Stores because
1446   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1447   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1448       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw ) {
1449     ctrl = ctrl-&gt;in(0);
1450     set_req(MemNode::Control,ctrl);
1451     progress = true;
1452   }
1453 
1454   intptr_t ignore = 0;
1455   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1456   if (base != NULL
1457       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1458     // Check for useless control edge in some common special cases
1459     if (in(MemNode::Control) != NULL
1460         &amp;&amp; can_remove_control()
1461         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1462         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1463       // A method-invariant, non-null address (constant or 'this' argument).
1464       set_req(MemNode::Control, NULL);
1465       progress = true;
1466     }
1467   }
1468 
1469   Node* mem = in(MemNode::Memory);
1470   const TypePtr *addr_t = phase-&gt;type(address)-&gt;isa_ptr();
1471 
1472   if (can_reshape &amp;&amp; (addr_t != NULL)) {
1473     // try to optimize our memory input
1474     Node* opt_mem = MemNode::optimize_memory_chain(mem, addr_t, this, phase);
1475     if (opt_mem != mem) {
1476       set_req(MemNode::Memory, opt_mem);
1477       if (phase-&gt;type( opt_mem ) == Type::TOP) return NULL;
1478       return this;
1479     }
1480     const TypeOopPtr *t_oop = addr_t-&gt;isa_oopptr();
1481     if ((t_oop != NULL) &amp;&amp;
1482         (t_oop-&gt;is_known_instance_field() ||
1483          t_oop-&gt;is_ptr_to_boxed_value())) {
1484       PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
1485       if (igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(opt_mem)) {
1486         // Delay this transformation until memory Phi is processed.
1487         phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
1488         return NULL;
1489       }
1490       // Split instance field load through Phi.
1491       Node* result = split_through_phi(phase);
1492       if (result != NULL) return result;
1493 
1494       if (t_oop-&gt;is_ptr_to_boxed_value()) {
1495         Node* result = eliminate_autobox(phase);
1496         if (result != NULL) return result;
1497       }
1498     }
1499   }
1500 
1501   // Is there a dominating load that loads the same value?  Leave
1502   // anything that is not a load of a field/array element (like
1503   // barriers etc.) alone
1504   if (in(0) != NULL &amp;&amp; adr_type() != TypeRawPtr::BOTTOM &amp;&amp; can_reshape) {
1505     for (DUIterator_Fast imax, i = mem-&gt;fast_outs(imax); i &lt; imax; i++) {
1506       Node *use = mem-&gt;fast_out(i);
1507       if (use != this &amp;&amp;
1508           use-&gt;Opcode() == Opcode() &amp;&amp;
1509           use-&gt;in(0) != NULL &amp;&amp;
1510           use-&gt;in(0) != in(0) &amp;&amp;
1511           use-&gt;in(Address) == in(Address)) {
1512         Node* ctl = in(0);
1513         for (int i = 0; i &lt; 10 &amp;&amp; ctl != NULL; i++) {
1514           ctl = IfNode::up_one_dom(ctl);
1515           if (ctl == use-&gt;in(0)) {
1516             set_req(0, use-&gt;in(0));
1517             return this;
1518           }
1519         }
1520       }
1521     }
1522   }
1523 
1524   // Check for prior store with a different base or offset; make Load
1525   // independent.  Skip through any number of them.  Bail out if the stores
1526   // are in an endless dead cycle and report no progress.  This is a key
1527   // transform for Reflection.  However, if after skipping through the Stores
1528   // we can't then fold up against a prior store do NOT do the transform as
1529   // this amounts to using the 'Oracle' model of aliasing.  It leaves the same
1530   // array memory alive twice: once for the hoisted Load and again after the
1531   // bypassed Store.  This situation only works if EVERYBODY who does
1532   // anti-dependence work knows how to bypass.  I.e. we need all
1533   // anti-dependence checks to ask the same Oracle.  Right now, that Oracle is
1534   // the alias index stuff.  So instead, peek through Stores and IFF we can
1535   // fold up, do so.
1536   Node* prev_mem = find_previous_store(phase);
1537   if (prev_mem != NULL) {
1538     Node* value = can_see_arraycopy_value(prev_mem, phase);
1539     if (value != NULL) {
1540       return value;
1541     }
1542   }
1543   // Steps (a), (b):  Walk past independent stores to find an exact match.
1544   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1545     // (c) See if we can fold up on the spot, but don't fold up here.
1546     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1547     // just return a prior value, which is done by Identity calls.
1548     if (can_see_stored_value(prev_mem, phase)) {
1549       // Make ready for step (d):
1550       set_req(MemNode::Memory, prev_mem);
1551       return this;
1552     }
1553   }
1554 
1555   return progress ? this : NULL;
1556 }
1557 
1558 // Helper to recognize certain Klass fields which are invariant across
1559 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1560 const Type*
1561 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1562                                  ciKlass* klass) const {
1563   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1564     // The field is Klass::_modifier_flags.  Return its (constant) value.
1565     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1566     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _modifier_flags");
1567     return TypeInt::make(klass-&gt;modifier_flags());
1568   }
1569   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1570     // The field is Klass::_access_flags.  Return its (constant) value.
1571     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1572     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _access_flags");
1573     return TypeInt::make(klass-&gt;access_flags());
1574   }
1575   if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())) {
1576     // The field is Klass::_layout_helper.  Return its constant value if known.
1577     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _layout_helper");
1578     return TypeInt::make(klass-&gt;layout_helper());
1579   }
1580 
1581   // No match.
1582   return NULL;
1583 }
1584 
1585 // Try to constant-fold a stable array element.
1586 static const Type* fold_stable_ary_elem(const TypeAryPtr* ary, int off, BasicType loadbt) {
1587   assert(ary-&gt;const_oop(), "array should be constant");
1588   assert(ary-&gt;is_stable(), "array should be stable");
1589 
1590   // Decode the results of GraphKit::array_element_address.
1591   ciArray* aobj = ary-&gt;const_oop()-&gt;as_array();
1592   ciConstant con = aobj-&gt;element_value_by_offset(off);
1593 
1594   if (con.basic_type() != T_ILLEGAL &amp;&amp; !con.is_null_or_zero()) {
1595     const Type* con_type = Type::make_from_constant(con);
1596     if (con_type != NULL) {
1597       if (con_type-&gt;isa_aryptr()) {
1598         // Join with the array element type, in case it is also stable.
1599         int dim = ary-&gt;stable_dimension();
1600         con_type = con_type-&gt;is_aryptr()-&gt;cast_to_stable(true, dim-1);
1601       }
1602       if (loadbt == T_NARROWOOP &amp;&amp; con_type-&gt;isa_oopptr()) {
1603         con_type = con_type-&gt;make_narrowoop();
1604       }
1605 #ifndef PRODUCT
1606       if (TraceIterativeGVN) {
1607         tty-&gt;print("FoldStableValues: array element [off=%d]: con_type=", off);
1608         con_type-&gt;dump(); tty-&gt;cr();
1609       }
1610 #endif //PRODUCT
1611       return con_type;
1612     }
1613   }
1614   return NULL;
1615 }
1616 
1617 //------------------------------Value-----------------------------------------
1618 const Type *LoadNode::Value( PhaseTransform *phase ) const {
1619   // Either input is TOP ==&gt; the result is TOP
1620   Node* mem = in(MemNode::Memory);
1621   const Type *t1 = phase-&gt;type(mem);
1622   if (t1 == Type::TOP)  return Type::TOP;
1623   Node* adr = in(MemNode::Address);
1624   const TypePtr* tp = phase-&gt;type(adr)-&gt;isa_ptr();
1625   if (tp == NULL || tp-&gt;empty())  return Type::TOP;
1626   int off = tp-&gt;offset();
1627   assert(off != Type::OffsetTop, "case covered by TypePtr::empty");
1628   Compile* C = phase-&gt;C;
1629 
1630   // Try to guess loaded type from pointer type
1631   if (tp-&gt;isa_aryptr()) {
1632     const TypeAryPtr* ary = tp-&gt;is_aryptr();
1633     const Type* t = ary-&gt;elem();
1634 
1635     // Determine whether the reference is beyond the header or not, by comparing
1636     // the offset against the offset of the start of the array's data.
1637     // Different array types begin at slightly different offsets (12 vs. 16).
1638     // We choose T_BYTE as an example base type that is least restrictive
1639     // as to alignment, which will therefore produce the smallest
1640     // possible base offset.
1641     const int min_base_off = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1642     const bool off_beyond_header = ((uint)off &gt;= (uint)min_base_off);
1643 
1644     // Try to constant-fold a stable array element.
1645     if (FoldStableValues &amp;&amp; ary-&gt;is_stable() &amp;&amp; ary-&gt;const_oop() != NULL) {
1646       // Make sure the reference is not into the header and the offset is constant
1647       if (off_beyond_header &amp;&amp; adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1648         const Type* con_type = fold_stable_ary_elem(ary, off, memory_type());
1649         if (con_type != NULL) {
1650           return con_type;
1651         }
1652       }
1653     }
1654 
1655     // Don't do this for integer types. There is only potential profit if
1656     // the element type t is lower than _type; that is, for int types, if _type is
1657     // more restrictive than t.  This only happens here if one is short and the other
1658     // char (both 16 bits), and in those cases we've made an intentional decision
1659     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1660     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1661     //
1662     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1663     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1664     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1665     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1666     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1667     // In fact, that could have been the original type of p1, and p1 could have
1668     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1669     // expression (LShiftL quux 3) independently optimized to the constant 8.
1670     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1671         &amp;&amp; (_type-&gt;isa_vect() == NULL)
1672         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1673       // t might actually be lower than _type, if _type is a unique
1674       // concrete subclass of abstract class t.
1675       if (off_beyond_header) {  // is the offset beyond the header?
1676         const Type* jt = t-&gt;join_speculative(_type);
1677         // In any case, do not allow the join, per se, to empty out the type.
1678         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1679           // This can happen if a interface-typed array narrows to a class type.
1680           jt = _type;
1681         }
1682 #ifdef ASSERT
1683         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1684           // The pointers in the autobox arrays are always non-null
1685           Node* base = adr-&gt;in(AddPNode::Base);
1686           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1687             // Get LoadN node which loads IntegerCache.cache field
1688             base = base-&gt;in(1);
1689           }
1690           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1691             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1692             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1693               // It could be narrow oop
1694               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,"sanity");
1695             }
1696           }
1697         }
1698 #endif
1699         return jt;
1700       }
1701     }
1702   } else if (tp-&gt;base() == Type::InstPtr) {
1703     ciEnv* env = C-&gt;env();
1704     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1705     ciKlass* klass = tinst-&gt;klass();
1706     assert( off != Type::OffsetBot ||
1707             // arrays can be cast to Objects
1708             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1709             // unsafe field access may not have a constant offset
1710             C-&gt;has_unsafe_access(),
1711             "Field accesses must be precise" );
1712     // For oop loads, we expect the _type to be precise
1713     if (klass == env-&gt;String_klass() &amp;&amp;
1714         adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1715       // For constant Strings treat the final fields as compile time constants.
1716       // While we can list what field types java.lang.String has, it is more
1717       // future-proof to handle all possible field types, anticipating future
1718       // changes and experiments in String code.
1719       Node* base = adr-&gt;in(AddPNode::Base);
1720       const TypeOopPtr* t = phase-&gt;type(base)-&gt;isa_oopptr();
1721       if (t != NULL &amp;&amp; t-&gt;singleton()) {
1722         ciField* field = env-&gt;String_klass()-&gt;get_field_by_offset(off, false);
1723         if (field != NULL &amp;&amp; field-&gt;is_final()) {
1724           ciObject* string = t-&gt;const_oop();
1725           ciConstant constant = string-&gt;as_instance()-&gt;field_value(field);
1726           // Type::make_from_constant does not handle narrow oops, so handle it here.
1727           // Everything else can use the factory method.
1728           if ((constant.basic_type() == T_ARRAY || constant.basic_type() == T_OBJECT)
1729                   &amp;&amp; adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1730             return TypeNarrowOop::make_from_constant(constant.as_object(), true);
1731           } else {
1732             return Type::make_from_constant(constant, true);
1733           }
1734         }
1735       }
1736     }
1737     // Optimizations for constant objects
1738     ciObject* const_oop = tinst-&gt;const_oop();
1739     if (const_oop != NULL) {
1740       // For constant Boxed value treat the target field as a compile time constant.
1741       if (tinst-&gt;is_ptr_to_boxed_value()) {
1742         return tinst-&gt;get_const_boxed_value();
1743       } else
1744       // For constant CallSites treat the target field as a compile time constant.
1745       if (const_oop-&gt;is_call_site()) {
1746         ciCallSite* call_site = const_oop-&gt;as_call_site();
1747         ciField* field = call_site-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_offset(off, /*is_static=*/ false);
1748         if (field != NULL &amp;&amp; field-&gt;is_call_site_target()) {
1749           ciMethodHandle* target = call_site-&gt;get_target();
1750           if (target != NULL) {  // just in case
1751             ciConstant constant(T_OBJECT, target);
1752             const Type* t;
1753             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1754               t = TypeNarrowOop::make_from_constant(constant.as_object(), true);
1755             } else {
1756               t = TypeOopPtr::make_from_constant(constant.as_object(), true);
1757             }
1758             // Add a dependence for invalidation of the optimization.
1759             if (!call_site-&gt;is_constant_call_site()) {
1760               C-&gt;dependencies()-&gt;assert_call_site_target_value(call_site, target);
1761             }
1762             return t;
1763           }
1764         }
1765       }
1766     }
1767   } else if (tp-&gt;base() == Type::KlassPtr) {
1768     assert( off != Type::OffsetBot ||
1769             // arrays can be cast to Objects
1770             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1771             // also allow array-loading from the primary supertype
1772             // array during subtype checks
1773             Opcode() == Op_LoadKlass,
1774             "Field accesses must be precise" );
1775     // For klass/static loads, we expect the _type to be precise
1776   }
1777 
1778   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1779   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1780     ciKlass* klass = tkls-&gt;klass();
1781     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1782       // We are loading a field from a Klass metaobject whose identity
1783       // is known at compile time (the type is "exact" or "precise").
1784       // Check for fields we know are maintained as constants by the VM.
1785       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1786         // The field is Klass::_super_check_offset.  Return its (constant) value.
1787         // (Folds up type checking code.)
1788         assert(Opcode() == Op_LoadI, "must load an int from _super_check_offset");
1789         return TypeInt::make(klass-&gt;super_check_offset());
1790       }
1791       // Compute index into primary_supers array
1792       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1793       // Check for overflowing; use unsigned compare to handle the negative case.
1794       if( depth &lt; ciKlass::primary_super_limit() ) {
1795         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1796         // (Folds up type checking code.)
1797         assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1798         ciKlass *ss = klass-&gt;super_of_depth(depth);
1799         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1800       }
1801       const Type* aift = load_array_final_field(tkls, klass);
1802       if (aift != NULL)  return aift;
1803       if (tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1804         // The field is Klass::_java_mirror.  Return its (constant) value.
1805         // (Folds up the 2nd indirection in anObjConstant.getClass().)
1806         assert(Opcode() == Op_LoadP, "must load an oop from _java_mirror");
1807         return TypeInstPtr::make(klass-&gt;java_mirror());
1808       }
1809     }
1810 
1811     // We can still check if we are loading from the primary_supers array at a
1812     // shallow enough depth.  Even though the klass is not exact, entries less
1813     // than or equal to its super depth are correct.
1814     if (klass-&gt;is_loaded() ) {
1815       ciType *inner = klass;
1816       while( inner-&gt;is_obj_array_klass() )
1817         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1818       if( inner-&gt;is_instance_klass() &amp;&amp;
1819           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1820         // Compute index into primary_supers array
1821         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1822         // Check for overflowing; use unsigned compare to handle the negative case.
1823         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1824             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1825           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1826           // (Folds up type checking code.)
1827           assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1828           ciKlass *ss = klass-&gt;super_of_depth(depth);
1829           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1830         }
1831       }
1832     }
1833 
1834     // If the type is enough to determine that the thing is not an array,
1835     // we can give the layout_helper a positive interval type.
1836     // This will help short-circuit some reflective code.
1837     if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())
1838         &amp;&amp; !klass-&gt;is_array_klass() // not directly typed as an array
1839         &amp;&amp; !klass-&gt;is_interface()  // specifically not Serializable &amp; Cloneable
1840         &amp;&amp; !klass-&gt;is_java_lang_Object()   // not the supertype of all T[]
1841         ) {
1842       // Note:  When interfaces are reliable, we can narrow the interface
1843       // test to (klass != Serializable &amp;&amp; klass != Cloneable).
1844       assert(Opcode() == Op_LoadI, "must load an int from _layout_helper");
1845       jint min_size = Klass::instance_layout_helper(oopDesc::header_size(), false);
1846       // The key property of this type is that it folds up tests
1847       // for array-ness, since it proves that the layout_helper is positive.
1848       // Thus, a generic value like the basic object layout helper works fine.
1849       return TypeInt::make(min_size, max_jint, Type::WidenMin);
1850     }
1851   }
1852 
1853   // If we are loading from a freshly-allocated object, produce a zero,
1854   // if the load is provably beyond the header of the object.
1855   // (Also allow a variable load from a fresh array to produce zero.)
1856   const TypeOopPtr *tinst = tp-&gt;isa_oopptr();
1857   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1858   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1859   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1860     Node* value = can_see_stored_value(mem,phase);
1861     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1862       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),"sanity");
1863       return value-&gt;bottom_type();
1864     }
1865   }
1866 
1867   if (is_instance) {
1868     // If we have an instance type and our memory input is the
1869     // programs's initial memory state, there is no matching store,
1870     // so just return a zero of the appropriate type
1871     Node *mem = in(MemNode::Memory);
1872     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1873       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, "must be memory Parm");
1874       return Type::get_zero_type(_type-&gt;basic_type());
1875     }
1876   }
1877   return _type;
1878 }
1879 
1880 //------------------------------match_edge-------------------------------------
1881 // Do we Match on this edge index or not?  Match only the address.
1882 uint LoadNode::match_edge(uint idx) const {
1883   return idx == MemNode::Address;
1884 }
1885 
1886 //--------------------------LoadBNode::Ideal--------------------------------------
1887 //
1888 //  If the previous store is to the same address as this load,
1889 //  and the value stored was larger than a byte, replace this load
1890 //  with the value stored truncated to a byte.  If no truncation is
1891 //  needed, the replacement is done in LoadNode::Identity().
1892 //
1893 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1894   Node* mem = in(MemNode::Memory);
1895   Node* value = can_see_stored_value(mem,phase);
1896   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
1897     Node *result = phase-&gt;transform( new LShiftINode(value, phase-&gt;intcon(24)) );
1898     return new RShiftINode(result, phase-&gt;intcon(24));
1899   }
1900   // Identity call will handle the case where truncation is not needed.
1901   return LoadNode::Ideal(phase, can_reshape);
1902 }
1903 
1904 const Type* LoadBNode::Value(PhaseTransform *phase) const {
1905   Node* mem = in(MemNode::Memory);
1906   Node* value = can_see_stored_value(mem,phase);
1907   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1908       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1909     // If the input to the store does not fit with the load's result type,
1910     // it must be truncated. We can't delay until Ideal call since
1911     // a singleton Value is needed for split_thru_phi optimization.
1912     int con = value-&gt;get_int();
1913     return TypeInt::make((con &lt;&lt; 24) &gt;&gt; 24);
1914   }
1915   return LoadNode::Value(phase);
1916 }
1917 
1918 //--------------------------LoadUBNode::Ideal-------------------------------------
1919 //
1920 //  If the previous store is to the same address as this load,
1921 //  and the value stored was larger than a byte, replace this load
1922 //  with the value stored truncated to a byte.  If no truncation is
1923 //  needed, the replacement is done in LoadNode::Identity().
1924 //
1925 Node* LoadUBNode::Ideal(PhaseGVN* phase, bool can_reshape) {
1926   Node* mem = in(MemNode::Memory);
1927   Node* value = can_see_stored_value(mem, phase);
1928   if (value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal(_type))
1929     return new AndINode(value, phase-&gt;intcon(0xFF));
1930   // Identity call will handle the case where truncation is not needed.
1931   return LoadNode::Ideal(phase, can_reshape);
1932 }
1933 
1934 const Type* LoadUBNode::Value(PhaseTransform *phase) const {
1935   Node* mem = in(MemNode::Memory);
1936   Node* value = can_see_stored_value(mem,phase);
1937   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1938       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1939     // If the input to the store does not fit with the load's result type,
1940     // it must be truncated. We can't delay until Ideal call since
1941     // a singleton Value is needed for split_thru_phi optimization.
1942     int con = value-&gt;get_int();
1943     return TypeInt::make(con &amp; 0xFF);
1944   }
1945   return LoadNode::Value(phase);
1946 }
1947 
1948 //--------------------------LoadUSNode::Ideal-------------------------------------
1949 //
1950 //  If the previous store is to the same address as this load,
1951 //  and the value stored was larger than a char, replace this load
1952 //  with the value stored truncated to a char.  If no truncation is
1953 //  needed, the replacement is done in LoadNode::Identity().
1954 //
1955 Node *LoadUSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1956   Node* mem = in(MemNode::Memory);
1957   Node* value = can_see_stored_value(mem,phase);
1958   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) )
1959     return new AndINode(value,phase-&gt;intcon(0xFFFF));
1960   // Identity call will handle the case where truncation is not needed.
1961   return LoadNode::Ideal(phase, can_reshape);
1962 }
1963 
1964 const Type* LoadUSNode::Value(PhaseTransform *phase) const {
1965   Node* mem = in(MemNode::Memory);
1966   Node* value = can_see_stored_value(mem,phase);
1967   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1968       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1969     // If the input to the store does not fit with the load's result type,
1970     // it must be truncated. We can't delay until Ideal call since
1971     // a singleton Value is needed for split_thru_phi optimization.
1972     int con = value-&gt;get_int();
1973     return TypeInt::make(con &amp; 0xFFFF);
1974   }
1975   return LoadNode::Value(phase);
1976 }
1977 
1978 //--------------------------LoadSNode::Ideal--------------------------------------
1979 //
1980 //  If the previous store is to the same address as this load,
1981 //  and the value stored was larger than a short, replace this load
1982 //  with the value stored truncated to a short.  If no truncation is
1983 //  needed, the replacement is done in LoadNode::Identity().
1984 //
1985 Node *LoadSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1986   Node* mem = in(MemNode::Memory);
1987   Node* value = can_see_stored_value(mem,phase);
1988   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
1989     Node *result = phase-&gt;transform( new LShiftINode(value, phase-&gt;intcon(16)) );
1990     return new RShiftINode(result, phase-&gt;intcon(16));
1991   }
1992   // Identity call will handle the case where truncation is not needed.
1993   return LoadNode::Ideal(phase, can_reshape);
1994 }
1995 
1996 const Type* LoadSNode::Value(PhaseTransform *phase) const {
1997   Node* mem = in(MemNode::Memory);
1998   Node* value = can_see_stored_value(mem,phase);
1999   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2000       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2001     // If the input to the store does not fit with the load's result type,
2002     // it must be truncated. We can't delay until Ideal call since
2003     // a singleton Value is needed for split_thru_phi optimization.
2004     int con = value-&gt;get_int();
2005     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2006   }
2007   return LoadNode::Value(phase);
2008 }
2009 
2010 //=============================================================================
2011 //----------------------------LoadKlassNode::make------------------------------
2012 // Polymorphic factory method:
2013 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {
2014   // sanity check the alias category against the created node type
2015   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2016   assert(adr_type != NULL, "expecting TypeKlassPtr");
2017 #ifdef _LP64
2018   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2019     assert(UseCompressedClassPointers, "no compressed klasses");
2020     Node* load_klass = gvn.transform(new LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2021     return new DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2022   }
2023 #endif
2024   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), "should have got back a narrow oop");
2025   return new LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2026 }
2027 
2028 //------------------------------Value------------------------------------------
2029 const Type *LoadKlassNode::Value( PhaseTransform *phase ) const {
2030   return klass_value_common(phase);
2031 }
2032 
2033 // In most cases, LoadKlassNode does not have the control input set. If the control
2034 // input is set, it must not be removed (by LoadNode::Ideal()).
2035 bool LoadKlassNode::can_remove_control() const {
2036   return false;
2037 }
2038 
2039 const Type *LoadNode::klass_value_common( PhaseTransform *phase ) const {
2040   // Either input is TOP ==&gt; the result is TOP
2041   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2042   if (t1 == Type::TOP)  return Type::TOP;
2043   Node *adr = in(MemNode::Address);
2044   const Type *t2 = phase-&gt;type( adr );
2045   if (t2 == Type::TOP)  return Type::TOP;
2046   const TypePtr *tp = t2-&gt;is_ptr();
2047   if (TypePtr::above_centerline(tp-&gt;ptr()) ||
2048       tp-&gt;ptr() == TypePtr::Null)  return Type::TOP;
2049 
2050   // Return a more precise klass, if possible
2051   const TypeInstPtr *tinst = tp-&gt;isa_instptr();
2052   if (tinst != NULL) {
2053     ciInstanceKlass* ik = tinst-&gt;klass()-&gt;as_instance_klass();
2054     int offset = tinst-&gt;offset();
2055     if (ik == phase-&gt;C-&gt;env()-&gt;Class_klass()
2056         &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2057             offset == java_lang_Class::array_klass_offset_in_bytes())) {
2058       // We are loading a special hidden field from a Class mirror object,
2059       // the field which points to the VM's Klass metaobject.
2060       ciType* t = tinst-&gt;java_mirror_type();
2061       // java_mirror_type returns non-null for compile-time Class constants.
2062       if (t != NULL) {
2063         // constant oop =&gt; constant klass
2064         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2065           if (t-&gt;is_void()) {
2066             // We cannot create a void array.  Since void is a primitive type return null
2067             // klass.  Users of this result need to do a null check on the returned klass.
2068             return TypePtr::NULL_PTR;
2069           }
2070           return TypeKlassPtr::make(ciArrayKlass::make(t));
2071         }
2072         if (!t-&gt;is_klass()) {
2073           // a primitive Class (e.g., int.class) has NULL for a klass field
2074           return TypePtr::NULL_PTR;
2075         }
2076         // (Folds up the 1st indirection in aClassConstant.getModifiers().)
2077         return TypeKlassPtr::make(t-&gt;as_klass());
2078       }
2079       // non-constant mirror, so we can't tell what's going on
2080     }
2081     if( !ik-&gt;is_loaded() )
2082       return _type;             // Bail out if not loaded
2083     if (offset == oopDesc::klass_offset_in_bytes()) {
2084       if (tinst-&gt;klass_is_exact()) {
2085         return TypeKlassPtr::make(ik);
2086       }
2087       // See if we can become precise: no subklasses and no interface
2088       // (Note:  We need to support verified interfaces.)
2089       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2090         //assert(!UseExactTypes, "this code should be useless with exact types");
2091         // Add a dependence; if any subclass added we need to recompile
2092         if (!ik-&gt;is_final()) {
2093           // %%% should use stronger assert_unique_concrete_subtype instead
2094           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2095         }
2096         // Return precise klass
2097         return TypeKlassPtr::make(ik);
2098       }
2099 
2100       // Return root of possible klass
2101       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);
2102     }
2103   }
2104 
2105   // Check for loading klass from an array
2106   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
2107   if( tary != NULL ) {
2108     ciKlass *tary_klass = tary-&gt;klass();
2109     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2110         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2111       if (tary-&gt;klass_is_exact()) {
2112         return TypeKlassPtr::make(tary_klass);
2113       }
2114       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();
2115       // If the klass is an object array, we defer the question to the
2116       // array component klass.
2117       if( ak-&gt;is_obj_array_klass() ) {
2118         assert( ak-&gt;is_loaded(), "" );
2119         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
2120         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {
2121           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();
2122           // See if we can become precise: no subklasses and no interface
2123           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2124             //assert(!UseExactTypes, "this code should be useless with exact types");
2125             // Add a dependence; if any subclass added we need to recompile
2126             if (!ik-&gt;is_final()) {
2127               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2128             }
2129             // Return precise array klass
2130             return TypeKlassPtr::make(ak);
2131           }
2132         }
2133         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
2134       } else {                  // Found a type-array?
2135         //assert(!UseExactTypes, "this code should be useless with exact types");
2136         assert( ak-&gt;is_type_array_klass(), "" );
2137         return TypeKlassPtr::make(ak); // These are always precise
2138       }
2139     }
2140   }
2141 
2142   // Check for loading klass from an array klass
2143   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2144   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2145     ciKlass* klass = tkls-&gt;klass();
2146     if( !klass-&gt;is_loaded() )
2147       return _type;             // Bail out if not loaded
2148     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2149         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2150       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2151       // // Always returning precise element type is incorrect,
2152       // // e.g., element type could be object and array may contain strings
2153       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2154 
2155       // The array's TypeKlassPtr was declared 'precise' or 'not precise'
2156       // according to the element type's subclassing.
2157       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);
2158     }
2159     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2160         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2161       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2162       // The field is Klass::_super.  Return its (constant) value.
2163       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2164       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2165     }
2166   }
2167 
2168   // Bailout case
2169   return LoadNode::Value(phase);
2170 }
2171 
2172 //------------------------------Identity---------------------------------------
2173 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2174 // Also feed through the klass in Allocate(...klass...)._klass.
2175 Node* LoadKlassNode::Identity( PhaseTransform *phase ) {
2176   return klass_identity_common(phase);
2177 }
2178 
2179 Node* LoadNode::klass_identity_common(PhaseTransform *phase ) {
2180   Node* x = LoadNode::Identity(phase);
2181   if (x != this)  return x;
2182 
2183   // Take apart the address into an oop and and offset.
2184   // Return 'this' if we cannot.
2185   Node*    adr    = in(MemNode::Address);
2186   intptr_t offset = 0;
2187   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2188   if (base == NULL)     return this;
2189   const TypeOopPtr* toop = phase-&gt;type(adr)-&gt;isa_oopptr();
2190   if (toop == NULL)     return this;
2191 
2192   // We can fetch the klass directly through an AllocateNode.
2193   // This works even if the klass is not constant (clone or newArray).
2194   if (offset == oopDesc::klass_offset_in_bytes()) {
2195     Node* allocated_klass = AllocateNode::Ideal_klass(base, phase);
2196     if (allocated_klass != NULL) {
2197       return allocated_klass;
2198     }
2199   }
2200 
2201   // Simplify k.java_mirror.as_klass to plain k, where k is a Klass*.
2202   // See inline_native_Class_query for occurrences of these patterns.
2203   // Java Example:  x.getClass().isAssignableFrom(y)
2204   //
2205   // This improves reflective code, often making the Class
2206   // mirror go completely dead.  (Current exception:  Class
2207   // mirrors may appear in debug info, but we could clean them out by
2208   // introducing a new debug info operator for Klass*.java_mirror).
2209   if (toop-&gt;isa_instptr() &amp;&amp; toop-&gt;klass() == phase-&gt;C-&gt;env()-&gt;Class_klass()
2210       &amp;&amp; offset == java_lang_Class::klass_offset_in_bytes()) {
2211     // We are loading a special hidden field from a Class mirror,
2212     // the field which points to its Klass or ArrayKlass metaobject.
2213     if (base-&gt;is_Load()) {
2214       Node* adr2 = base-&gt;in(MemNode::Address);
2215       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
2216       if (tkls != NULL &amp;&amp; !tkls-&gt;empty()
2217           &amp;&amp; (tkls-&gt;klass()-&gt;is_instance_klass() ||
2218               tkls-&gt;klass()-&gt;is_array_klass())
2219           &amp;&amp; adr2-&gt;is_AddP()
2220           ) {
2221         int mirror_field = in_bytes(Klass::java_mirror_offset());
2222         if (tkls-&gt;offset() == mirror_field) {
2223           return adr2-&gt;in(AddPNode::Base);
2224         }
2225       }
2226     }
2227   }
2228 
2229   return this;
2230 }
2231 
2232 
2233 //------------------------------Value------------------------------------------
2234 const Type *LoadNKlassNode::Value( PhaseTransform *phase ) const {
2235   const Type *t = klass_value_common(phase);
2236   if (t == Type::TOP)
2237     return t;
2238 
2239   return t-&gt;make_narrowklass();
2240 }
2241 
2242 //------------------------------Identity---------------------------------------
2243 // To clean up reflective code, simplify k.java_mirror.as_klass to narrow k.
2244 // Also feed through the klass in Allocate(...klass...)._klass.
2245 Node* LoadNKlassNode::Identity( PhaseTransform *phase ) {
2246   Node *x = klass_identity_common(phase);
2247 
2248   const Type *t = phase-&gt;type( x );
2249   if( t == Type::TOP ) return x;
2250   if( t-&gt;isa_narrowklass()) return x;
2251   assert (!t-&gt;isa_narrowoop(), "no narrow oop here");
2252 
2253   return phase-&gt;transform(new EncodePKlassNode(x, t-&gt;make_narrowklass()));
2254 }
2255 
2256 //------------------------------Value-----------------------------------------
2257 const Type *LoadRangeNode::Value( PhaseTransform *phase ) const {
2258   // Either input is TOP ==&gt; the result is TOP
2259   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2260   if( t1 == Type::TOP ) return Type::TOP;
2261   Node *adr = in(MemNode::Address);
2262   const Type *t2 = phase-&gt;type( adr );
2263   if( t2 == Type::TOP ) return Type::TOP;
2264   const TypePtr *tp = t2-&gt;is_ptr();
2265   if (TypePtr::above_centerline(tp-&gt;ptr()))  return Type::TOP;
2266   const TypeAryPtr *tap = tp-&gt;isa_aryptr();
2267   if( !tap ) return _type;
2268   return tap-&gt;size();
2269 }
2270 
2271 //-------------------------------Ideal---------------------------------------
2272 // Feed through the length in AllocateArray(...length...)._length.
2273 Node *LoadRangeNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2274   Node* p = MemNode::Ideal_common(phase, can_reshape);
2275   if (p)  return (p == NodeSentinel) ? NULL : p;
2276 
2277   // Take apart the address into an oop and and offset.
2278   // Return 'this' if we cannot.
2279   Node*    adr    = in(MemNode::Address);
2280   intptr_t offset = 0;
2281   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase,  offset);
2282   if (base == NULL)     return NULL;
2283   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2284   if (tary == NULL)     return NULL;
2285 
2286   // We can fetch the length directly through an AllocateArrayNode.
2287   // This works even if the length is not constant (clone or newArray).
2288   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2289     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2290     if (alloc != NULL) {
2291       Node* allocated_length = alloc-&gt;Ideal_length();
2292       Node* len = alloc-&gt;make_ideal_length(tary, phase);
2293       if (allocated_length != len) {
2294         // New CastII improves on this.
2295         return len;
2296       }
2297     }
2298   }
2299 
2300   return NULL;
2301 }
2302 
2303 //------------------------------Identity---------------------------------------
2304 // Feed through the length in AllocateArray(...length...)._length.
2305 Node* LoadRangeNode::Identity( PhaseTransform *phase ) {
2306   Node* x = LoadINode::Identity(phase);
2307   if (x != this)  return x;
2308 
2309   // Take apart the address into an oop and and offset.
2310   // Return 'this' if we cannot.
2311   Node*    adr    = in(MemNode::Address);
2312   intptr_t offset = 0;
2313   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2314   if (base == NULL)     return this;
2315   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2316   if (tary == NULL)     return this;
2317 
2318   // We can fetch the length directly through an AllocateArrayNode.
2319   // This works even if the length is not constant (clone or newArray).
2320   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2321     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2322     if (alloc != NULL) {
2323       Node* allocated_length = alloc-&gt;Ideal_length();
2324       // Do not allow make_ideal_length to allocate a CastII node.
2325       Node* len = alloc-&gt;make_ideal_length(tary, phase, false);
2326       if (allocated_length == len) {
2327         // Return allocated_length only if it would not be improved by a CastII.
2328         return allocated_length;
2329       }
2330     }
2331   }
2332 
2333   return this;
2334 
2335 }
2336 
2337 //=============================================================================
2338 //---------------------------StoreNode::make-----------------------------------
2339 // Polymorphic factory method:
2340 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2341   assert((mo == unordered || mo == release), "unexpected");
2342   Compile* C = gvn.C;
2343   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2344          ctl != NULL, "raw memory operations should have control edge");
2345 
2346   switch (bt) {
2347   case T_BOOLEAN:
2348   case T_BYTE:    return new StoreBNode(ctl, mem, adr, adr_type, val, mo);
2349   case T_INT:     return new StoreINode(ctl, mem, adr, adr_type, val, mo);
2350   case T_CHAR:
2351   case T_SHORT:   return new StoreCNode(ctl, mem, adr, adr_type, val, mo);
2352   case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo);
2353   case T_FLOAT:   return new StoreFNode(ctl, mem, adr, adr_type, val, mo);
2354   case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo);
2355   case T_METADATA:
2356   case T_ADDRESS:
2357   case T_OBJECT:
2358 #ifdef _LP64
2359     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2360       val = gvn.transform(new EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2361       return new StoreNNode(ctl, mem, adr, adr_type, val, mo);
2362     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2363                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2364                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2365       val = gvn.transform(new EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2366       return new StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2367     }
2368 #endif
2369     {
2370       return new StorePNode(ctl, mem, adr, adr_type, val, mo);
2371     }
2372   }
2373   ShouldNotReachHere();
2374   return (StoreNode*)NULL;
2375 }
2376 
2377 StoreLNode* StoreLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {
2378   bool require_atomic = true;
2379   return new StoreLNode(ctl, mem, adr, adr_type, val, mo, require_atomic);
2380 }
2381 
2382 StoreDNode* StoreDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {
2383   bool require_atomic = true;
2384   return new StoreDNode(ctl, mem, adr, adr_type, val, mo, require_atomic);
2385 }
2386 
2387 
2388 //--------------------------bottom_type----------------------------------------
2389 const Type *StoreNode::bottom_type() const {
2390   return Type::MEMORY;
2391 }
2392 
2393 //------------------------------hash-------------------------------------------
2394 uint StoreNode::hash() const {
2395   // unroll addition of interesting fields
2396   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2397 
2398   // Since they are not commoned, do not hash them:
2399   return NO_HASH;
2400 }
2401 
2402 //------------------------------Ideal------------------------------------------
2403 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2404 // When a store immediately follows a relevant allocation/initialization,
2405 // try to capture it into the initialization, or hoist it above.
2406 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2407   Node* p = MemNode::Ideal_common(phase, can_reshape);
2408   if (p)  return (p == NodeSentinel) ? NULL : p;
2409 
2410   Node* mem     = in(MemNode::Memory);
2411   Node* address = in(MemNode::Address);
2412   // Back-to-back stores to same address?  Fold em up.  Generally
2413   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2414   // since they must follow each StoreP operation.  Redundant StoreCMs
2415   // are eliminated just before matching in final_graph_reshape.
2416   {
2417     Node* st = mem;
2418     // If Store 'st' has more than one use, we cannot fold 'st' away.
2419     // For example, 'st' might be the final state at a conditional
2420     // return.  Or, 'st' might be used by some node which is live at
2421     // the same time 'st' is live, which might be unschedulable.  So,
2422     // require exactly ONE user until such time as we clone 'mem' for
2423     // each of 'mem's uses (thus making the exactly-1-user-rule hold
2424     // true).
2425     while (st-&gt;is_Store() &amp;&amp; st-&gt;outcnt() == 1 &amp;&amp; st-&gt;Opcode() != Op_StoreCM) {
2426       // Looking at a dead closed cycle of memory?
2427       assert(st != st-&gt;in(MemNode::Memory), "dead loop in StoreNode::Ideal");
2428       assert(Opcode() == st-&gt;Opcode() ||
2429              st-&gt;Opcode() == Op_StoreVector ||
2430              Opcode() == Op_StoreVector ||
2431              phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw ||
2432              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreI) || // expanded ClearArrayNode
2433              (is_mismatched_access() || st-&gt;as_Store()-&gt;is_mismatched_access()),
2434              "no mismatched stores, except on raw memory: %s %s", NodeClassNames[Opcode()], NodeClassNames[st-&gt;Opcode()]);
2435 
2436       if (st-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2437           st-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2438         Node* use = st-&gt;raw_out(0);
2439         phase-&gt;igvn_rehash_node_delayed(use);
2440         if (can_reshape) {
2441           use-&gt;set_req_X(MemNode::Memory, st-&gt;in(MemNode::Memory), phase-&gt;is_IterGVN());
2442         } else {
2443           // It's OK to do this in the parser, since DU info is always accurate,
2444           // and the parser always refers to nodes via SafePointNode maps.
2445           use-&gt;set_req(MemNode::Memory, st-&gt;in(MemNode::Memory));
2446         }
2447         return this;
2448       }
2449       st = st-&gt;in(MemNode::Memory);
2450     }
2451   }
2452 
2453 
2454   // Capture an unaliased, unconditional, simple store into an initializer.
2455   // Or, if it is independent of the allocation, hoist it above the allocation.
2456   if (ReduceFieldZeroing &amp;&amp; /*can_reshape &amp;&amp;*/
2457       mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
2458     InitializeNode* init = mem-&gt;in(0)-&gt;as_Initialize();
2459     intptr_t offset = init-&gt;can_capture_store(this, phase, can_reshape);
2460     if (offset &gt; 0) {
2461       Node* moved = init-&gt;capture_store(this, offset, phase, can_reshape);
2462       // If the InitializeNode captured me, it made a raw copy of me,
2463       // and I need to disappear.
2464       if (moved != NULL) {
2465         // %%% hack to ensure that Ideal returns a new node:
2466         mem = MergeMemNode::make(mem);
2467         return mem;             // fold me away
2468       }
2469     }
2470   }
2471 
2472   return NULL;                  // No further progress
2473 }
2474 
2475 //------------------------------Value-----------------------------------------
2476 const Type *StoreNode::Value( PhaseTransform *phase ) const {
2477   // Either input is TOP ==&gt; the result is TOP
2478   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2479   if( t1 == Type::TOP ) return Type::TOP;
2480   const Type *t2 = phase-&gt;type( in(MemNode::Address) );
2481   if( t2 == Type::TOP ) return Type::TOP;
2482   const Type *t3 = phase-&gt;type( in(MemNode::ValueIn) );
2483   if( t3 == Type::TOP ) return Type::TOP;
2484   return Type::MEMORY;
2485 }
2486 
2487 //------------------------------Identity---------------------------------------
2488 // Remove redundant stores:
2489 //   Store(m, p, Load(m, p)) changes to m.
2490 //   Store(, p, x) -&gt; Store(m, p, x) changes to Store(m, p, x).
2491 Node *StoreNode::Identity( PhaseTransform *phase ) {
2492   Node* mem = in(MemNode::Memory);
2493   Node* adr = in(MemNode::Address);
2494   Node* val = in(MemNode::ValueIn);
2495 
2496   // Load then Store?  Then the Store is useless
2497   if (val-&gt;is_Load() &amp;&amp;
2498       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2499       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2500       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2501     return mem;
2502   }
2503 
2504   // Two stores in a row of the same value?
2505   if (mem-&gt;is_Store() &amp;&amp;
2506       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2507       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2508       mem-&gt;Opcode() == Opcode()) {
2509     return mem;
2510   }
2511 
2512   // Store of zero anywhere into a freshly-allocated object?
2513   // Then the store is useless.
2514   // (It must already have been captured by the InitializeNode.)
2515   if (ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {
2516     // a newly allocated object is already all-zeroes everywhere
2517     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {
2518       return mem;
2519     }
2520 
2521     // the store may also apply to zero-bits in an earlier object
2522     Node* prev_mem = find_previous_store(phase);
2523     // Steps (a), (b):  Walk past independent stores to find an exact match.
2524     if (prev_mem != NULL) {
2525       Node* prev_val = can_see_stored_value(prev_mem, phase);
2526       if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2527         // prev_val and val might differ by a cast; it would be good
2528         // to keep the more informative of the two.
2529         return mem;
2530       }
2531     }
2532   }
2533 
2534   return this;
2535 }
2536 
2537 //------------------------------match_edge-------------------------------------
2538 // Do we Match on this edge index or not?  Match only memory &amp; value
2539 uint StoreNode::match_edge(uint idx) const {
2540   return idx == MemNode::Address || idx == MemNode::ValueIn;
2541 }
2542 
2543 //------------------------------cmp--------------------------------------------
2544 // Do not common stores up together.  They generally have to be split
2545 // back up anyways, so do not bother.
2546 uint StoreNode::cmp( const Node &amp;n ) const {
2547   return (&amp;n == this);          // Always fail except on self
2548 }
2549 
2550 //------------------------------Ideal_masked_input-----------------------------
2551 // Check for a useless mask before a partial-word store
2552 // (StoreB ... (AndI valIn conIa) )
2553 // If (conIa &amp; mask == mask) this simplifies to
2554 // (StoreB ... (valIn) )
2555 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2556   Node *val = in(MemNode::ValueIn);
2557   if( val-&gt;Opcode() == Op_AndI ) {
2558     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2559     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2560       set_req(MemNode::ValueIn, val-&gt;in(1));
2561       return this;
2562     }
2563   }
2564   return NULL;
2565 }
2566 
2567 
2568 //------------------------------Ideal_sign_extended_input----------------------
2569 // Check for useless sign-extension before a partial-word store
2570 // (StoreB ... (RShiftI _ (LShiftI _ valIn conIL ) conIR) )
2571 // If (conIL == conIR &amp;&amp; conIR &lt;= num_bits)  this simplifies to
2572 // (StoreB ... (valIn) )
2573 Node *StoreNode::Ideal_sign_extended_input(PhaseGVN *phase, int num_bits) {
2574   Node *val = in(MemNode::ValueIn);
2575   if( val-&gt;Opcode() == Op_RShiftI ) {
2576     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2577     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &lt;= num_bits) ) {
2578       Node *shl = val-&gt;in(1);
2579       if( shl-&gt;Opcode() == Op_LShiftI ) {
2580         const TypeInt *t2 = phase-&gt;type( shl-&gt;in(2) )-&gt;isa_int();
2581         if( t2 &amp;&amp; t2-&gt;is_con() &amp;&amp; (t2-&gt;get_con() == t-&gt;get_con()) ) {
2582           set_req(MemNode::ValueIn, shl-&gt;in(1));
2583           return this;
2584         }
2585       }
2586     }
2587   }
2588   return NULL;
2589 }
2590 
2591 //------------------------------value_never_loaded-----------------------------------
2592 // Determine whether there are any possible loads of the value stored.
2593 // For simplicity, we actually check if there are any loads from the
2594 // address stored to, not just for loads of the value stored by this node.
2595 //
2596 bool StoreNode::value_never_loaded( PhaseTransform *phase) const {
2597   Node *adr = in(Address);
2598   const TypeOopPtr *adr_oop = phase-&gt;type(adr)-&gt;isa_oopptr();
2599   if (adr_oop == NULL)
2600     return false;
2601   if (!adr_oop-&gt;is_known_instance_field())
2602     return false; // if not a distinct instance, there may be aliases of the address
2603   for (DUIterator_Fast imax, i = adr-&gt;fast_outs(imax); i &lt; imax; i++) {
2604     Node *use = adr-&gt;fast_out(i);
2605     if (use-&gt;is_Load() || use-&gt;is_LoadStore()) {
2606       return false;
2607     }
2608   }
2609   return true;
2610 }
2611 
2612 //=============================================================================
2613 //------------------------------Ideal------------------------------------------
2614 // If the store is from an AND mask that leaves the low bits untouched, then
2615 // we can skip the AND operation.  If the store is from a sign-extension
2616 // (a left shift, then right shift) we can skip both.
2617 Node *StoreBNode::Ideal(PhaseGVN *phase, bool can_reshape){
2618   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFF);
2619   if( progress != NULL ) return progress;
2620 
2621   progress = StoreNode::Ideal_sign_extended_input(phase, 24);
2622   if( progress != NULL ) return progress;
2623 
2624   // Finally check the default case
2625   return StoreNode::Ideal(phase, can_reshape);
2626 }
2627 
2628 //=============================================================================
2629 //------------------------------Ideal------------------------------------------
2630 // If the store is from an AND mask that leaves the low bits untouched, then
2631 // we can skip the AND operation
2632 Node *StoreCNode::Ideal(PhaseGVN *phase, bool can_reshape){
2633   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFFFF);
2634   if( progress != NULL ) return progress;
2635 
2636   progress = StoreNode::Ideal_sign_extended_input(phase, 16);
2637   if( progress != NULL ) return progress;
2638 
2639   // Finally check the default case
2640   return StoreNode::Ideal(phase, can_reshape);
2641 }
2642 
2643 //=============================================================================
2644 //------------------------------Identity---------------------------------------
2645 Node *StoreCMNode::Identity( PhaseTransform *phase ) {
2646   // No need to card mark when storing a null ptr
2647   Node* my_store = in(MemNode::OopStore);
2648   if (my_store-&gt;is_Store()) {
2649     const Type *t1 = phase-&gt;type( my_store-&gt;in(MemNode::ValueIn) );
2650     if( t1 == TypePtr::NULL_PTR ) {
2651       return in(MemNode::Memory);
2652     }
2653   }
2654   return this;
2655 }
2656 
2657 //=============================================================================
2658 //------------------------------Ideal---------------------------------------
2659 Node *StoreCMNode::Ideal(PhaseGVN *phase, bool can_reshape){
2660   Node* progress = StoreNode::Ideal(phase, can_reshape);
2661   if (progress != NULL) return progress;
2662 
2663   Node* my_store = in(MemNode::OopStore);
2664   if (my_store-&gt;is_MergeMem()) {
2665     Node* mem = my_store-&gt;as_MergeMem()-&gt;memory_at(oop_alias_idx());
2666     set_req(MemNode::OopStore, mem);
2667     return this;
2668   }
2669 
2670   return NULL;
2671 }
2672 
2673 //------------------------------Value-----------------------------------------
2674 const Type *StoreCMNode::Value( PhaseTransform *phase ) const {
2675   // Either input is TOP ==&gt; the result is TOP
2676   const Type *t = phase-&gt;type( in(MemNode::Memory) );
2677   if( t == Type::TOP ) return Type::TOP;
2678   t = phase-&gt;type( in(MemNode::Address) );
2679   if( t == Type::TOP ) return Type::TOP;
2680   t = phase-&gt;type( in(MemNode::ValueIn) );
2681   if( t == Type::TOP ) return Type::TOP;
2682   // If extra input is TOP ==&gt; the result is TOP
2683   t = phase-&gt;type( in(MemNode::OopStore) );
2684   if( t == Type::TOP ) return Type::TOP;
2685 
2686   return StoreNode::Value( phase );
2687 }
2688 
2689 
2690 //=============================================================================
2691 //----------------------------------SCMemProjNode------------------------------
2692 const Type * SCMemProjNode::Value( PhaseTransform *phase ) const
2693 {
2694   return bottom_type();
2695 }
2696 
2697 //=============================================================================
2698 //----------------------------------LoadStoreNode------------------------------
2699 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2700   : Node(required),
2701     _type(rt),
2702     _adr_type(at)
2703 {
2704   init_req(MemNode::Control, c  );
2705   init_req(MemNode::Memory , mem);
2706   init_req(MemNode::Address, adr);
2707   init_req(MemNode::ValueIn, val);
2708   init_class_id(Class_LoadStore);
2709 }
2710 
2711 uint LoadStoreNode::ideal_reg() const {
2712   return _type-&gt;ideal_reg();
2713 }
2714 
2715 bool LoadStoreNode::result_not_used() const {
2716   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2717     Node *x = fast_out(i);
2718     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2719     return false;
2720   }
2721   return true;
2722 }
2723 
2724 uint LoadStoreNode::size_of() const { return sizeof(*this); }
2725 
2726 //=============================================================================
2727 //----------------------------------LoadStoreConditionalNode--------------------
2728 LoadStoreConditionalNode::LoadStoreConditionalNode( Node *c, Node *mem, Node *adr, Node *val, Node *ex ) : LoadStoreNode(c, mem, adr, val, NULL, TypeInt::BOOL, 5) {
2729   init_req(ExpectedIn, ex );
2730 }
2731 
2732 //=============================================================================
2733 //-------------------------------adr_type--------------------------------------
2734 const TypePtr* ClearArrayNode::adr_type() const {
2735   Node *adr = in(3);
2736   if (adr == NULL)  return NULL; // node is dead
2737   return MemNode::calculate_adr_type(adr-&gt;bottom_type());
2738 }
2739 
2740 //------------------------------match_edge-------------------------------------
2741 // Do we Match on this edge index or not?  Do not match memory
2742 uint ClearArrayNode::match_edge(uint idx) const {
2743   return idx &gt; 1;
2744 }
2745 
2746 //------------------------------Identity---------------------------------------
2747 // Clearing a zero length array does nothing
2748 Node *ClearArrayNode::Identity( PhaseTransform *phase ) {
2749   return phase-&gt;type(in(2))-&gt;higher_equal(TypeX::ZERO)  ? in(1) : this;
2750 }
2751 
2752 //------------------------------Idealize---------------------------------------
2753 // Clearing a short array is faster with stores
2754 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape){
2755   const int unit = BytesPerLong;
2756   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2757   if (!t)  return NULL;
2758   if (!t-&gt;is_con())  return NULL;
2759   intptr_t raw_count = t-&gt;get_con();
2760   intptr_t size = raw_count;
2761   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2762   // Clearing nothing uses the Identity call.
2763   // Negative clears are possible on dead ClearArrays
2764   // (see jck test stmt114.stmt11402.val).
2765   if (size &lt;= 0 || size % unit != 0)  return NULL;
2766   intptr_t count = size / unit;
2767   // Length too long; use fast hardware clear
2768   if (size &gt; Matcher::init_array_short_size)  return NULL;
2769   Node *mem = in(1);
2770   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2771   Node *adr = in(3);
2772   const Type* at = phase-&gt;type(adr);
2773   if( at==Type::TOP ) return NULL;
2774   const TypePtr* atp = at-&gt;isa_ptr();
2775   // adjust atp to be the correct array element address type
2776   if (atp == NULL)  atp = TypePtr::BOTTOM;
2777   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2778   // Get base for derived pointer purposes
2779   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2780   Node *base = adr-&gt;in(1);
2781 
2782   Node *zero = phase-&gt;makecon(TypeLong::ZERO);
2783   Node *off  = phase-&gt;MakeConX(BytesPerLong);
2784   mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2785   count--;
2786   while( count-- ) {
2787     mem = phase-&gt;transform(mem);
2788     adr = phase-&gt;transform(new AddPNode(base,adr,off));
2789     mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2790   }
2791   return mem;
2792 }
2793 
2794 //----------------------------step_through----------------------------------
2795 // Return allocation input memory edge if it is different instance
2796 // or itself if it is the one we are looking for.
2797 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2798   Node* n = *np;
2799   assert(n-&gt;is_ClearArray(), "sanity");
2800   intptr_t offset;
2801   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2802   // This method is called only before Allocate nodes are expanded
2803   // during macro nodes expansion. Before that ClearArray nodes are
2804   // only generated in PhaseMacroExpand::generate_arraycopy() (before
2805   // Allocate nodes are expanded) which follows allocations.
2806   assert(alloc != NULL, "should have allocation");
2807   if (alloc-&gt;_idx == instance_id) {
2808     // Can not bypass initialization of the instance we are looking for.
2809     return false;
2810   }
2811   // Otherwise skip it.
2812   InitializeNode* init = alloc-&gt;initialization();
2813   if (init != NULL)
2814     *np = init-&gt;in(TypeFunc::Memory);
2815   else
2816     *np = alloc-&gt;in(TypeFunc::Memory);
2817   return true;
2818 }
2819 
2820 //----------------------------clear_memory-------------------------------------
2821 // Generate code to initialize object storage to zero.
2822 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2823                                    intptr_t start_offset,
2824                                    Node* end_offset,
2825                                    PhaseGVN* phase) {
2826   intptr_t offset = start_offset;
2827 
2828   int unit = BytesPerLong;
2829   if ((offset % unit) != 0) {
2830     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(offset));
2831     adr = phase-&gt;transform(adr);
2832     const TypePtr* atp = TypeRawPtr::BOTTOM;
2833     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2834     mem = phase-&gt;transform(mem);
2835     offset += BytesPerInt;
2836   }
2837   assert((offset % unit) == 0, "");
2838 
2839   // Initialize the remaining stuff, if any, with a ClearArray.
2840   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);
2841 }
2842 
2843 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2844                                    Node* start_offset,
2845                                    Node* end_offset,
2846                                    PhaseGVN* phase) {
2847   if (start_offset == end_offset) {
2848     // nothing to do
2849     return mem;
2850   }
2851 
2852   int unit = BytesPerLong;
2853   Node* zbase = start_offset;
2854   Node* zend  = end_offset;
2855 
2856   // Scale to the unit required by the CPU:
2857   if (!Matcher::init_array_count_is_in_bytes) {
2858     Node* shift = phase-&gt;intcon(exact_log2(unit));
2859     zbase = phase-&gt;transform(new URShiftXNode(zbase, shift) );
2860     zend  = phase-&gt;transform(new URShiftXNode(zend,  shift) );
2861   }
2862 
2863   // Bulk clear double-words
2864   Node* zsize = phase-&gt;transform(new SubXNode(zend, zbase) );
2865   Node* adr = phase-&gt;transform(new AddPNode(dest, dest, start_offset) );
2866   mem = new ClearArrayNode(ctl, mem, zsize, adr);
2867   return phase-&gt;transform(mem);
2868 }
2869 
2870 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2871                                    intptr_t start_offset,
2872                                    intptr_t end_offset,
2873                                    PhaseGVN* phase) {
2874   if (start_offset == end_offset) {
2875     // nothing to do
2876     return mem;
2877   }
2878 
2879   assert((end_offset % BytesPerInt) == 0, "odd end offset");
2880   intptr_t done_offset = end_offset;
2881   if ((done_offset % BytesPerLong) != 0) {
2882     done_offset -= BytesPerInt;
2883   }
2884   if (done_offset &gt; start_offset) {
2885     mem = clear_memory(ctl, mem, dest,
2886                        start_offset, phase-&gt;MakeConX(done_offset), phase);
2887   }
2888   if (done_offset &lt; end_offset) { // emit the final 32-bit store
2889     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
2890     adr = phase-&gt;transform(adr);
2891     const TypePtr* atp = TypeRawPtr::BOTTOM;
2892     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2893     mem = phase-&gt;transform(mem);
2894     done_offset += BytesPerInt;
2895   }
2896   assert(done_offset == end_offset, "");
2897   return mem;
2898 }
2899 
2900 //=============================================================================
2901 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
2902   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
2903     _adr_type(C-&gt;get_adr_type(alias_idx))
2904 {
2905   init_class_id(Class_MemBar);
2906   Node* top = C-&gt;top();
2907   init_req(TypeFunc::I_O,top);
2908   init_req(TypeFunc::FramePtr,top);
2909   init_req(TypeFunc::ReturnAdr,top);
2910   if (precedent != NULL)
2911     init_req(TypeFunc::Parms, precedent);
2912 }
2913 
2914 //------------------------------cmp--------------------------------------------
2915 uint MemBarNode::hash() const { return NO_HASH; }
2916 uint MemBarNode::cmp( const Node &amp;n ) const {
2917   return (&amp;n == this);          // Always fail except on self
2918 }
2919 
2920 //------------------------------make-------------------------------------------
2921 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
2922   switch (opcode) {
2923   case Op_MemBarAcquire:     return new MemBarAcquireNode(C, atp, pn);
2924   case Op_LoadFence:         return new LoadFenceNode(C, atp, pn);
2925   case Op_MemBarRelease:     return new MemBarReleaseNode(C, atp, pn);
2926   case Op_StoreFence:        return new StoreFenceNode(C, atp, pn);
2927   case Op_MemBarAcquireLock: return new MemBarAcquireLockNode(C, atp, pn);
2928   case Op_MemBarReleaseLock: return new MemBarReleaseLockNode(C, atp, pn);
2929   case Op_MemBarVolatile:    return new MemBarVolatileNode(C, atp, pn);
2930   case Op_MemBarCPUOrder:    return new MemBarCPUOrderNode(C, atp, pn);
2931   case Op_OnSpinWait:        return new OnSpinWaitNode(C, atp, pn);
2932   case Op_Initialize:        return new InitializeNode(C, atp, pn);
2933   case Op_MemBarStoreStore:  return new MemBarStoreStoreNode(C, atp, pn);
2934   default: ShouldNotReachHere(); return NULL;
2935   }
2936 }
2937 
2938 //------------------------------Ideal------------------------------------------
2939 // Return a node which is more "ideal" than the current node.  Strip out
2940 // control copies
2941 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2942   if (remove_dead_region(phase, can_reshape)) return this;
2943   // Don't bother trying to transform a dead node
2944   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
2945     return NULL;
2946   }
2947 
2948   bool progress = false;
2949   // Eliminate volatile MemBars for scalar replaced objects.
2950   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
2951     bool eliminate = false;
2952     int opc = Opcode();
2953     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
2954       // Volatile field loads and stores.
2955       Node* my_mem = in(MemBarNode::Precedent);
2956       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
2957       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
2958         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
2959         // replace this Precedent (decodeN) with the Load instead.
2960         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
2961           Node* load_node = my_mem-&gt;in(1);
2962           set_req(MemBarNode::Precedent, load_node);
2963           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
2964           my_mem = load_node;
2965         } else {
2966           assert(my_mem-&gt;unique_out() == this, "sanity");
2967           del_req(Precedent);
2968           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem); // remove dead node later
2969           my_mem = NULL;
2970         }
2971         progress = true;
2972       }
2973       if (my_mem != NULL &amp;&amp; my_mem-&gt;is_Mem()) {
2974         const TypeOopPtr* t_oop = my_mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_oopptr();
2975         // Check for scalar replaced object reference.
2976         if( t_oop != NULL &amp;&amp; t_oop-&gt;is_known_instance_field() &amp;&amp;
2977             t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
2978             t_oop-&gt;offset() != Type::OffsetTop) {
2979           eliminate = true;
2980         }
2981       }
2982     } else if (opc == Op_MemBarRelease) {
2983       // Final field stores.
2984       Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent), phase);
2985       if ((alloc != NULL) &amp;&amp; alloc-&gt;is_Allocate() &amp;&amp;
2986           alloc-&gt;as_Allocate()-&gt;does_not_escape_thread()) {
2987         // The allocated object does not escape.
2988         eliminate = true;
2989       }
2990     }
2991     if (eliminate) {
2992       // Replace MemBar projections by its inputs.
2993       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
2994       igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
2995       igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
2996       // Must return either the original node (now dead) or a new node
2997       // (Do not return a top here, since that would break the uniqueness of top.)
2998       return new ConINode(TypeInt::ZERO);
2999     }
3000   }
3001   return progress ? this : NULL;
3002 }
3003 
3004 //------------------------------Value------------------------------------------
3005 const Type *MemBarNode::Value( PhaseTransform *phase ) const {
3006   if( !in(0) ) return Type::TOP;
3007   if( phase-&gt;type(in(0)) == Type::TOP )
3008     return Type::TOP;
3009   return TypeTuple::MEMBAR;
3010 }
3011 
3012 //------------------------------match------------------------------------------
3013 // Construct projections for memory.
3014 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {
3015   switch (proj-&gt;_con) {
3016   case TypeFunc::Control:
3017   case TypeFunc::Memory:
3018     return new MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3019   }
3020   ShouldNotReachHere();
3021   return NULL;
3022 }
3023 
3024 //===========================InitializeNode====================================
3025 // SUMMARY:
3026 // This node acts as a memory barrier on raw memory, after some raw stores.
3027 // The 'cooked' oop value feeds from the Initialize, not the Allocation.
3028 // The Initialize can 'capture' suitably constrained stores as raw inits.
3029 // It can coalesce related raw stores into larger units (called 'tiles').
3030 // It can avoid zeroing new storage for memory units which have raw inits.
3031 // At macro-expansion, it is marked 'complete', and does not optimize further.
3032 //
3033 // EXAMPLE:
3034 // The object 'new short[2]' occupies 16 bytes in a 32-bit machine.
3035 //   ctl = incoming control; mem* = incoming memory
3036 // (Note:  A star * on a memory edge denotes I/O and other standard edges.)
3037 // First allocate uninitialized memory and fill in the header:
3038 //   alloc = (Allocate ctl mem* 16 #short[].klass ...)
3039 //   ctl := alloc.Control; mem* := alloc.Memory*
3040 //   rawmem = alloc.Memory; rawoop = alloc.RawAddress
3041 // Then initialize to zero the non-header parts of the raw memory block:
3042 //   init = (Initialize alloc.Control alloc.Memory* alloc.RawAddress)
3043 //   ctl := init.Control; mem.SLICE(#short[*]) := init.Memory
3044 // After the initialize node executes, the object is ready for service:
3045 //   oop := (CheckCastPP init.Control alloc.RawAddress #short[])
3046 // Suppose its body is immediately initialized as {1,2}:
3047 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3048 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3049 //   mem.SLICE(#short[*]) := store2
3050 //
3051 // DETAILS:
3052 // An InitializeNode collects and isolates object initialization after
3053 // an AllocateNode and before the next possible safepoint.  As a
3054 // memory barrier (MemBarNode), it keeps critical stores from drifting
3055 // down past any safepoint or any publication of the allocation.
3056 // Before this barrier, a newly-allocated object may have uninitialized bits.
3057 // After this barrier, it may be treated as a real oop, and GC is allowed.
3058 //
3059 // The semantics of the InitializeNode include an implicit zeroing of
3060 // the new object from object header to the end of the object.
3061 // (The object header and end are determined by the AllocateNode.)
3062 //
3063 // Certain stores may be added as direct inputs to the InitializeNode.
3064 // These stores must update raw memory, and they must be to addresses
3065 // derived from the raw address produced by AllocateNode, and with
3066 // a constant offset.  They must be ordered by increasing offset.
3067 // The first one is at in(RawStores), the last at in(req()-1).
3068 // Unlike most memory operations, they are not linked in a chain,
3069 // but are displayed in parallel as users of the rawmem output of
3070 // the allocation.
3071 //
3072 // (See comments in InitializeNode::capture_store, which continue
3073 // the example given above.)
3074 //
3075 // When the associated Allocate is macro-expanded, the InitializeNode
3076 // may be rewritten to optimize collected stores.  A ClearArrayNode
3077 // may also be created at that point to represent any required zeroing.
3078 // The InitializeNode is then marked 'complete', prohibiting further
3079 // capturing of nearby memory operations.
3080 //
3081 // During macro-expansion, all captured initializations which store
3082 // constant values of 32 bits or smaller are coalesced (if advantageous)
3083 // into larger 'tiles' 32 or 64 bits.  This allows an object to be
3084 // initialized in fewer memory operations.  Memory words which are
3085 // covered by neither tiles nor non-constant stores are pre-zeroed
3086 // by explicit stores of zero.  (The code shape happens to do all
3087 // zeroing first, then all other stores, with both sequences occurring
3088 // in order of ascending offsets.)
3089 //
3090 // Alternatively, code may be inserted between an AllocateNode and its
3091 // InitializeNode, to perform arbitrary initialization of the new object.
3092 // E.g., the object copying intrinsics insert complex data transfers here.
3093 // The initialization must then be marked as 'complete' disable the
3094 // built-in zeroing semantics and the collection of initializing stores.
3095 //
3096 // While an InitializeNode is incomplete, reads from the memory state
3097 // produced by it are optimizable if they match the control edge and
3098 // new oop address associated with the allocation/initialization.
3099 // They return a stored value (if the offset matches) or else zero.
3100 // A write to the memory state, if it matches control and address,
3101 // and if it is to a constant offset, may be 'captured' by the
3102 // InitializeNode.  It is cloned as a raw memory operation and rewired
3103 // inside the initialization, to the raw oop produced by the allocation.
3104 // Operations on addresses which are provably distinct (e.g., to
3105 // other AllocateNodes) are allowed to bypass the initialization.
3106 //
3107 // The effect of all this is to consolidate object initialization
3108 // (both arrays and non-arrays, both piecewise and bulk) into a
3109 // single location, where it can be optimized as a unit.
3110 //
3111 // Only stores with an offset less than TrackedInitializationLimit words
3112 // will be considered for capture by an InitializeNode.  This puts a
3113 // reasonable limit on the complexity of optimized initializations.
3114 
3115 //---------------------------InitializeNode------------------------------------
3116 InitializeNode::InitializeNode(Compile* C, int adr_type, Node* rawoop)
3117   : _is_complete(Incomplete), _does_not_escape(false),
3118     MemBarNode(C, adr_type, rawoop)
3119 {
3120   init_class_id(Class_Initialize);
3121 
3122   assert(adr_type == Compile::AliasIdxRaw, "only valid atp");
3123   assert(in(RawAddress) == rawoop, "proper init");
3124   // Note:  allocation() can be NULL, for secondary initialization barriers
3125 }
3126 
3127 // Since this node is not matched, it will be processed by the
3128 // register allocator.  Declare that there are no constraints
3129 // on the allocation of the RawAddress edge.
3130 const RegMask &amp;InitializeNode::in_RegMask(uint idx) const {
3131   // This edge should be set to top, by the set_complete.  But be conservative.
3132   if (idx == InitializeNode::RawAddress)
3133     return *(Compile::current()-&gt;matcher()-&gt;idealreg2spillmask[in(idx)-&gt;ideal_reg()]);
3134   return RegMask::Empty;
3135 }
3136 
3137 Node* InitializeNode::memory(uint alias_idx) {
3138   Node* mem = in(Memory);
3139   if (mem-&gt;is_MergeMem()) {
3140     return mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
3141   } else {
3142     // incoming raw memory is not split
3143     return mem;
3144   }
3145 }
3146 
3147 bool InitializeNode::is_non_zero() {
3148   if (is_complete())  return false;
3149   remove_extra_zeroes();
3150   return (req() &gt; RawStores);
3151 }
3152 
3153 void InitializeNode::set_complete(PhaseGVN* phase) {
3154   assert(!is_complete(), "caller responsibility");
3155   _is_complete = Complete;
3156 
3157   // After this node is complete, it contains a bunch of
3158   // raw-memory initializations.  There is no need for
3159   // it to have anything to do with non-raw memory effects.
3160   // Therefore, tell all non-raw users to re-optimize themselves,
3161   // after skipping the memory effects of this initialization.
3162   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3163   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3164 }
3165 
3166 // convenience function
3167 // return false if the init contains any stores already
3168 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3169   InitializeNode* init = initialization();
3170   if (init == NULL || init-&gt;is_complete())  return false;
3171   init-&gt;remove_extra_zeroes();
3172   // for now, if this allocation has already collected any inits, bail:
3173   if (init-&gt;is_non_zero())  return false;
3174   init-&gt;set_complete(phase);
3175   return true;
3176 }
3177 
3178 void InitializeNode::remove_extra_zeroes() {
3179   if (req() == RawStores)  return;
3180   Node* zmem = zero_memory();
3181   uint fill = RawStores;
3182   for (uint i = fill; i &lt; req(); i++) {
3183     Node* n = in(i);
3184     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3185     if (fill &lt; i)  set_req(fill, n);          // compact
3186     ++fill;
3187   }
3188   // delete any empty spaces created:
3189   while (fill &lt; req()) {
3190     del_req(fill);
3191   }
3192 }
3193 
3194 // Helper for remembering which stores go with which offsets.
3195 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3196   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3197   intptr_t offset = -1;
3198   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3199                                                phase, offset);
3200   if (base == NULL)     return -1;  // something is dead,
3201   if (offset &lt; 0)       return -1;  //        dead, dead
3202   return offset;
3203 }
3204 
3205 // Helper for proving that an initialization expression is
3206 // "simple enough" to be folded into an object initialization.
3207 // Attempts to prove that a store's initial value 'n' can be captured
3208 // within the initialization without creating a vicious cycle, such as:
3209 //     { Foo p = new Foo(); p.next = p; }
3210 // True for constants and parameters and small combinations thereof.
3211 bool InitializeNode::detect_init_independence(Node* n, int&amp; count) {
3212   if (n == NULL)      return true;   // (can this really happen?)
3213   if (n-&gt;is_Proj())   n = n-&gt;in(0);
3214   if (n == this)      return false;  // found a cycle
3215   if (n-&gt;is_Con())    return true;
3216   if (n-&gt;is_Start())  return true;   // params, etc., are OK
3217   if (n-&gt;is_Root())   return true;   // even better
3218 
3219   Node* ctl = n-&gt;in(0);
3220   if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {
3221     if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);
3222     if (ctl == this)  return false;
3223 
3224     // If we already know that the enclosing memory op is pinned right after
3225     // the init, then any control flow that the store has picked up
3226     // must have preceded the init, or else be equal to the init.
3227     // Even after loop optimizations (which might change control edges)
3228     // a store is never pinned *before* the availability of its inputs.
3229     if (!MemNode::all_controls_dominate(n, this))
3230       return false;                  // failed to prove a good control
3231   }
3232 
3233   // Check data edges for possible dependencies on 'this'.
3234   if ((count += 1) &gt; 20)  return false;  // complexity limit
3235   for (uint i = 1; i &lt; n-&gt;req(); i++) {
3236     Node* m = n-&gt;in(i);
3237     if (m == NULL || m == n || m-&gt;is_top())  continue;
3238     uint first_i = n-&gt;find_edge(m);
3239     if (i != first_i)  continue;  // process duplicate edge just once
3240     if (!detect_init_independence(m, count)) {
3241       return false;
3242     }
3243   }
3244 
3245   return true;
3246 }
3247 
3248 // Here are all the checks a Store must pass before it can be moved into
3249 // an initialization.  Returns zero if a check fails.
3250 // On success, returns the (constant) offset to which the store applies,
3251 // within the initialized memory.
3252 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape) {
3253   const int FAIL = 0;
3254   if (st-&gt;is_unaligned_access()) {
3255     return FAIL;
3256   }
3257   if (st-&gt;req() != MemNode::ValueIn + 1)
3258     return FAIL;                // an inscrutable StoreNode (card mark?)
3259   Node* ctl = st-&gt;in(MemNode::Control);
3260   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3261     return FAIL;                // must be unconditional after the initialization
3262   Node* mem = st-&gt;in(MemNode::Memory);
3263   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3264     return FAIL;                // must not be preceded by other stores
3265   Node* adr = st-&gt;in(MemNode::Address);
3266   intptr_t offset;
3267   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3268   if (alloc == NULL)
3269     return FAIL;                // inscrutable address
3270   if (alloc != allocation())
3271     return FAIL;                // wrong allocation!  (store needs to float up)
3272   Node* val = st-&gt;in(MemNode::ValueIn);
3273   int complexity_count = 0;
3274   if (!detect_init_independence(val, complexity_count))
3275     return FAIL;                // stored value must be 'simple enough'
3276 
3277   // The Store can be captured only if nothing after the allocation
3278   // and before the Store is using the memory location that the store
3279   // overwrites.
3280   bool failed = false;
3281   // If is_complete_with_arraycopy() is true the shape of the graph is
3282   // well defined and is safe so no need for extra checks.
3283   if (!is_complete_with_arraycopy()) {
3284     // We are going to look at each use of the memory state following
3285     // the allocation to make sure nothing reads the memory that the
3286     // Store writes.
3287     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3288     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3289     ResourceMark rm;
3290     Unique_Node_List mems;
3291     mems.push(mem);
3292     Node* unique_merge = NULL;
3293     for (uint next = 0; next &lt; mems.size(); ++next) {
3294       Node *m  = mems.at(next);
3295       for (DUIterator_Fast jmax, j = m-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3296         Node *n = m-&gt;fast_out(j);
3297         if (n-&gt;outcnt() == 0) {
3298           continue;
3299         }
3300         if (n == st) {
3301           continue;
3302         } else if (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0) != ctl) {
3303           // If the control of this use is different from the control
3304           // of the Store which is right after the InitializeNode then
3305           // this node cannot be between the InitializeNode and the
3306           // Store.
3307           continue;
3308         } else if (n-&gt;is_MergeMem()) {
3309           if (n-&gt;as_MergeMem()-&gt;memory_at(alias_idx) == m) {
3310             // We can hit a MergeMemNode (that will likely go away
3311             // later) that is a direct use of the memory state
3312             // following the InitializeNode on the same slice as the
3313             // store node that we'd like to capture. We need to check
3314             // the uses of the MergeMemNode.
3315             mems.push(n);
3316           }
3317         } else if (n-&gt;is_Mem()) {
3318           Node* other_adr = n-&gt;in(MemNode::Address);
3319           if (other_adr == adr) {
3320             failed = true;
3321             break;
3322           } else {
3323             const TypePtr* other_t_adr = phase-&gt;type(other_adr)-&gt;isa_ptr();
3324             if (other_t_adr != NULL) {
3325               int other_alias_idx = phase-&gt;C-&gt;get_alias_index(other_t_adr);
3326               if (other_alias_idx == alias_idx) {
3327                 // A load from the same memory slice as the store right
3328                 // after the InitializeNode. We check the control of the
3329                 // object/array that is loaded from. If it's the same as
3330                 // the store control then we cannot capture the store.
3331                 assert(!n-&gt;is_Store(), "2 stores to same slice on same control?");
3332                 Node* base = other_adr;
3333                 assert(base-&gt;is_AddP(), "should be addp but is %s", base-&gt;Name());
3334                 base = base-&gt;in(AddPNode::Base);
3335                 if (base != NULL) {
3336                   base = base-&gt;uncast();
3337                   if (base-&gt;is_Proj() &amp;&amp; base-&gt;in(0) == alloc) {
3338                     failed = true;
3339                     break;
3340                   }
3341                 }
3342               }
3343             }
3344           }
3345         } else {
3346           failed = true;
3347           break;
3348         }
3349       }
3350     }
3351   }
3352   if (failed) {
3353     if (!can_reshape) {
3354       // We decided we couldn't capture the store during parsing. We
3355       // should try again during the next IGVN once the graph is
3356       // cleaner.
3357       phase-&gt;C-&gt;record_for_igvn(st);
3358     }
3359     return FAIL;
3360   }
3361 
3362   return offset;                // success
3363 }
3364 
3365 // Find the captured store in(i) which corresponds to the range
3366 // [start..start+size) in the initialized object.
3367 // If there is one, return its index i.  If there isn't, return the
3368 // negative of the index where it should be inserted.
3369 // Return 0 if the queried range overlaps an initialization boundary
3370 // or if dead code is encountered.
3371 // If size_in_bytes is zero, do not bother with overlap checks.
3372 int InitializeNode::captured_store_insertion_point(intptr_t start,
3373                                                    int size_in_bytes,
3374                                                    PhaseTransform* phase) {
3375   const int FAIL = 0, MAX_STORE = BytesPerLong;
3376 
3377   if (is_complete())
3378     return FAIL;                // arraycopy got here first; punt
3379 
3380   assert(allocation() != NULL, "must be present");
3381 
3382   // no negatives, no header fields:
3383   if (start &lt; (intptr_t) allocation()-&gt;minimum_header_size())  return FAIL;
3384 
3385   // after a certain size, we bail out on tracking all the stores:
3386   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3387   if (start &gt;= ti_limit)  return FAIL;
3388 
3389   for (uint i = InitializeNode::RawStores, limit = req(); ; ) {
3390     if (i &gt;= limit)  return -(int)i; // not found; here is where to put it
3391 
3392     Node*    st     = in(i);
3393     intptr_t st_off = get_store_offset(st, phase);
3394     if (st_off &lt; 0) {
3395       if (st != zero_memory()) {
3396         return FAIL;            // bail out if there is dead garbage
3397       }
3398     } else if (st_off &gt; start) {
3399       // ...we are done, since stores are ordered
3400       if (st_off &lt; start + size_in_bytes) {
3401         return FAIL;            // the next store overlaps
3402       }
3403       return -(int)i;           // not found; here is where to put it
3404     } else if (st_off &lt; start) {
3405       if (size_in_bytes != 0 &amp;&amp;
3406           start &lt; st_off + MAX_STORE &amp;&amp;
3407           start &lt; st_off + st-&gt;as_Store()-&gt;memory_size()) {
3408         return FAIL;            // the previous store overlaps
3409       }
3410     } else {
3411       if (size_in_bytes != 0 &amp;&amp;
3412           st-&gt;as_Store()-&gt;memory_size() != size_in_bytes) {
3413         return FAIL;            // mismatched store size
3414       }
3415       return i;
3416     }
3417 
3418     ++i;
3419   }
3420 }
3421 
3422 // Look for a captured store which initializes at the offset 'start'
3423 // with the given size.  If there is no such store, and no other
3424 // initialization interferes, then return zero_memory (the memory
3425 // projection of the AllocateNode).
3426 Node* InitializeNode::find_captured_store(intptr_t start, int size_in_bytes,
3427                                           PhaseTransform* phase) {
3428   assert(stores_are_sane(phase), "");
3429   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3430   if (i == 0) {
3431     return NULL;                // something is dead
3432   } else if (i &lt; 0) {
3433     return zero_memory();       // just primordial zero bits here
3434   } else {
3435     Node* st = in(i);           // here is the store at this position
3436     assert(get_store_offset(st-&gt;as_Store(), phase) == start, "sanity");
3437     return st;
3438   }
3439 }
3440 
3441 // Create, as a raw pointer, an address within my new object at 'offset'.
3442 Node* InitializeNode::make_raw_address(intptr_t offset,
3443                                        PhaseTransform* phase) {
3444   Node* addr = in(RawAddress);
3445   if (offset != 0) {
3446     Compile* C = phase-&gt;C;
3447     addr = phase-&gt;transform( new AddPNode(C-&gt;top(), addr,
3448                                                  phase-&gt;MakeConX(offset)) );
3449   }
3450   return addr;
3451 }
3452 
3453 // Clone the given store, converting it into a raw store
3454 // initializing a field or element of my new object.
3455 // Caller is responsible for retiring the original store,
3456 // with subsume_node or the like.
3457 //
3458 // From the example above InitializeNode::InitializeNode,
3459 // here are the old stores to be captured:
3460 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3461 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3462 //
3463 // Here is the changed code; note the extra edges on init:
3464 //   alloc = (Allocate ...)
3465 //   rawoop = alloc.RawAddress
3466 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3467 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3468 //   init = (Initialize alloc.Control alloc.Memory rawoop
3469 //                      rawstore1 rawstore2)
3470 //
3471 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
3472                                     PhaseTransform* phase, bool can_reshape) {
3473   assert(stores_are_sane(phase), "");
3474 
3475   if (start &lt; 0)  return NULL;
3476   assert(can_capture_store(st, phase, can_reshape) == start, "sanity");
3477 
3478   Compile* C = phase-&gt;C;
3479   int size_in_bytes = st-&gt;memory_size();
3480   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3481   if (i == 0)  return NULL;     // bail out
3482   Node* prev_mem = NULL;        // raw memory for the captured store
3483   if (i &gt; 0) {
3484     prev_mem = in(i);           // there is a pre-existing store under this one
3485     set_req(i, C-&gt;top());       // temporarily disconnect it
3486     // See StoreNode::Ideal 'st-&gt;outcnt() == 1' for the reason to disconnect.
3487   } else {
3488     i = -i;                     // no pre-existing store
3489     prev_mem = zero_memory();   // a slice of the newly allocated object
3490     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3491       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3492     else
3493       ins_req(i, C-&gt;top());     // build a new edge
3494   }
3495   Node* new_st = st-&gt;clone();
3496   new_st-&gt;set_req(MemNode::Control, in(Control));
3497   new_st-&gt;set_req(MemNode::Memory,  prev_mem);
3498   new_st-&gt;set_req(MemNode::Address, make_raw_address(start, phase));
3499   new_st = phase-&gt;transform(new_st);
3500 
3501   // At this point, new_st might have swallowed a pre-existing store
3502   // at the same offset, or perhaps new_st might have disappeared,
3503   // if it redundantly stored the same value (or zero to fresh memory).
3504 
3505   // In any case, wire it in:
3506   phase-&gt;igvn_rehash_node_delayed(this);
3507   set_req(i, new_st);
3508 
3509   // The caller may now kill the old guy.
3510   DEBUG_ONLY(Node* check_st = find_captured_store(start, size_in_bytes, phase));
3511   assert(check_st == new_st || check_st == NULL, "must be findable");
3512   assert(!is_complete(), "");
3513   return new_st;
3514 }
3515 
3516 static bool store_constant(jlong* tiles, int num_tiles,
3517                            intptr_t st_off, int st_size,
3518                            jlong con) {
3519   if ((st_off &amp; (st_size-1)) != 0)
3520     return false;               // strange store offset (assume size==2**N)
3521   address addr = (address)tiles + st_off;
3522   assert(st_off &gt;= 0 &amp;&amp; addr+st_size &lt;= (address)&amp;tiles[num_tiles], "oob");
3523   switch (st_size) {
3524   case sizeof(jbyte):  *(jbyte*) addr = (jbyte) con; break;
3525   case sizeof(jchar):  *(jchar*) addr = (jchar) con; break;
3526   case sizeof(jint):   *(jint*)  addr = (jint)  con; break;
3527   case sizeof(jlong):  *(jlong*) addr = (jlong) con; break;
3528   default: return false;        // strange store size (detect size!=2**N here)
3529   }
3530   return true;                  // return success to caller
3531 }
3532 
3533 // Coalesce subword constants into int constants and possibly
3534 // into long constants.  The goal, if the CPU permits,
3535 // is to initialize the object with a small number of 64-bit tiles.
3536 // Also, convert floating-point constants to bit patterns.
3537 // Non-constants are not relevant to this pass.
3538 //
3539 // In terms of the running example on InitializeNode::InitializeNode
3540 // and InitializeNode::capture_store, here is the transformation
3541 // of rawstore1 and rawstore2 into rawstore12:
3542 //   alloc = (Allocate ...)
3543 //   rawoop = alloc.RawAddress
3544 //   tile12 = 0x00010002
3545 //   rawstore12 = (StoreI alloc.Control alloc.Memory (+ rawoop 12) tile12)
3546 //   init = (Initialize alloc.Control alloc.Memory rawoop rawstore12)
3547 //
3548 void
3549 InitializeNode::coalesce_subword_stores(intptr_t header_size,
3550                                         Node* size_in_bytes,
3551                                         PhaseGVN* phase) {
3552   Compile* C = phase-&gt;C;
3553 
3554   assert(stores_are_sane(phase), "");
3555   // Note:  After this pass, they are not completely sane,
3556   // since there may be some overlaps.
3557 
3558   int old_subword = 0, old_long = 0, new_int = 0, new_long = 0;
3559 
3560   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3561   intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, ti_limit);
3562   size_limit = MIN2(size_limit, ti_limit);
3563   size_limit = align_size_up(size_limit, BytesPerLong);
3564   int num_tiles = size_limit / BytesPerLong;
3565 
3566   // allocate space for the tile map:
3567   const int small_len = DEBUG_ONLY(true ? 3 :) 30; // keep stack frames small
3568   jlong  tiles_buf[small_len];
3569   Node*  nodes_buf[small_len];
3570   jlong  inits_buf[small_len];
3571   jlong* tiles = ((num_tiles &lt;= small_len) ? &amp;tiles_buf[0]
3572                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3573   Node** nodes = ((num_tiles &lt;= small_len) ? &amp;nodes_buf[0]
3574                   : NEW_RESOURCE_ARRAY(Node*, num_tiles));
3575   jlong* inits = ((num_tiles &lt;= small_len) ? &amp;inits_buf[0]
3576                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3577   // tiles: exact bitwise model of all primitive constants
3578   // nodes: last constant-storing node subsumed into the tiles model
3579   // inits: which bytes (in each tile) are touched by any initializations
3580 
3581   //// Pass A: Fill in the tile model with any relevant stores.
3582 
3583   Copy::zero_to_bytes(tiles, sizeof(tiles[0]) * num_tiles);
3584   Copy::zero_to_bytes(nodes, sizeof(nodes[0]) * num_tiles);
3585   Copy::zero_to_bytes(inits, sizeof(inits[0]) * num_tiles);
3586   Node* zmem = zero_memory(); // initially zero memory state
3587   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3588     Node* st = in(i);
3589     intptr_t st_off = get_store_offset(st, phase);
3590 
3591     // Figure out the store's offset and constant value:
3592     if (st_off &lt; header_size)             continue; //skip (ignore header)
3593     if (st-&gt;in(MemNode::Memory) != zmem)  continue; //skip (odd store chain)
3594     int st_size = st-&gt;as_Store()-&gt;memory_size();
3595     if (st_off + st_size &gt; size_limit)    break;
3596 
3597     // Record which bytes are touched, whether by constant or not.
3598     if (!store_constant(inits, num_tiles, st_off, st_size, (jlong) -1))
3599       continue;                 // skip (strange store size)
3600 
3601     const Type* val = phase-&gt;type(st-&gt;in(MemNode::ValueIn));
3602     if (!val-&gt;singleton())                continue; //skip (non-con store)
3603     BasicType type = val-&gt;basic_type();
3604 
3605     jlong con = 0;
3606     switch (type) {
3607     case T_INT:    con = val-&gt;is_int()-&gt;get_con();  break;
3608     case T_LONG:   con = val-&gt;is_long()-&gt;get_con(); break;
3609     case T_FLOAT:  con = jint_cast(val-&gt;getf());    break;
3610     case T_DOUBLE: con = jlong_cast(val-&gt;getd());   break;
3611     default:                              continue; //skip (odd store type)
3612     }
3613 
3614     if (type == T_LONG &amp;&amp; Matcher::isSimpleConstant64(con) &amp;&amp;
3615         st-&gt;Opcode() == Op_StoreL) {
3616       continue;                 // This StoreL is already optimal.
3617     }
3618 
3619     // Store down the constant.
3620     store_constant(tiles, num_tiles, st_off, st_size, con);
3621 
3622     intptr_t j = st_off &gt;&gt; LogBytesPerLong;
3623 
3624     if (type == T_INT &amp;&amp; st_size == BytesPerInt
3625         &amp;&amp; (st_off &amp; BytesPerInt) == BytesPerInt) {
3626       jlong lcon = tiles[j];
3627       if (!Matcher::isSimpleConstant64(lcon) &amp;&amp;
3628           st-&gt;Opcode() == Op_StoreI) {
3629         // This StoreI is already optimal by itself.
3630         jint* intcon = (jint*) &amp;tiles[j];
3631         intcon[1] = 0;  // undo the store_constant()
3632 
3633         // If the previous store is also optimal by itself, back up and
3634         // undo the action of the previous loop iteration... if we can.
3635         // But if we can't, just let the previous half take care of itself.
3636         st = nodes[j];
3637         st_off -= BytesPerInt;
3638         con = intcon[0];
3639         if (con != 0 &amp;&amp; st != NULL &amp;&amp; st-&gt;Opcode() == Op_StoreI) {
3640           assert(st_off &gt;= header_size, "still ignoring header");
3641           assert(get_store_offset(st, phase) == st_off, "must be");
3642           assert(in(i-1) == zmem, "must be");
3643           DEBUG_ONLY(const Type* tcon = phase-&gt;type(st-&gt;in(MemNode::ValueIn)));
3644           assert(con == tcon-&gt;is_int()-&gt;get_con(), "must be");
3645           // Undo the effects of the previous loop trip, which swallowed st:
3646           intcon[0] = 0;        // undo store_constant()
3647           set_req(i-1, st);     // undo set_req(i, zmem)
3648           nodes[j] = NULL;      // undo nodes[j] = st
3649           --old_subword;        // undo ++old_subword
3650         }
3651         continue;               // This StoreI is already optimal.
3652       }
3653     }
3654 
3655     // This store is not needed.
3656     set_req(i, zmem);
3657     nodes[j] = st;              // record for the moment
3658     if (st_size &lt; BytesPerLong) // something has changed
3659           ++old_subword;        // includes int/float, but who's counting...
3660     else  ++old_long;
3661   }
3662 
3663   if ((old_subword + old_long) == 0)
3664     return;                     // nothing more to do
3665 
3666   //// Pass B: Convert any non-zero tiles into optimal constant stores.
3667   // Be sure to insert them before overlapping non-constant stores.
3668   // (E.g., byte[] x = { 1,2,y,4 }  =&gt;  x[int 0] = 0x01020004, x[2]=y.)
3669   for (int j = 0; j &lt; num_tiles; j++) {
3670     jlong con  = tiles[j];
3671     jlong init = inits[j];
3672     if (con == 0)  continue;
3673     jint con0,  con1;           // split the constant, address-wise
3674     jint init0, init1;          // split the init map, address-wise
3675     { union { jlong con; jint intcon[2]; } u;
3676       u.con = con;
3677       con0  = u.intcon[0];
3678       con1  = u.intcon[1];
3679       u.con = init;
3680       init0 = u.intcon[0];
3681       init1 = u.intcon[1];
3682     }
3683 
3684     Node* old = nodes[j];
3685     assert(old != NULL, "need the prior store");
3686     intptr_t offset = (j * BytesPerLong);
3687 
3688     bool split = !Matcher::isSimpleConstant64(con);
3689 
3690     if (offset &lt; header_size) {
3691       assert(offset + BytesPerInt &gt;= header_size, "second int counts");
3692       assert(*(jint*)&amp;tiles[j] == 0, "junk in header");
3693       split = true;             // only the second word counts
3694       // Example:  int a[] = { 42 ... }
3695     } else if (con0 == 0 &amp;&amp; init0 == -1) {
3696       split = true;             // first word is covered by full inits
3697       // Example:  int a[] = { ... foo(), 42 ... }
3698     } else if (con1 == 0 &amp;&amp; init1 == -1) {
3699       split = true;             // second word is covered by full inits
3700       // Example:  int a[] = { ... 42, foo() ... }
3701     }
3702 
3703     // Here's a case where init0 is neither 0 nor -1:
3704     //   byte a[] = { ... 0,0,foo(),0,  0,0,0,42 ... }
3705     // Assuming big-endian memory, init0, init1 are 0x0000FF00, 0x000000FF.
3706     // In this case the tile is not split; it is (jlong)42.
3707     // The big tile is stored down, and then the foo() value is inserted.
3708     // (If there were foo(),foo() instead of foo(),0, init0 would be -1.)
3709 
3710     Node* ctl = old-&gt;in(MemNode::Control);
3711     Node* adr = make_raw_address(offset, phase);
3712     const TypePtr* atp = TypeRawPtr::BOTTOM;
3713 
3714     // One or two coalesced stores to plop down.
3715     Node*    st[2];
3716     intptr_t off[2];
3717     int  nst = 0;
3718     if (!split) {
3719       ++new_long;
3720       off[nst] = offset;
3721       st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3722                                   phase-&gt;longcon(con), T_LONG, MemNode::unordered);
3723     } else {
3724       // Omit either if it is a zero.
3725       if (con0 != 0) {
3726         ++new_int;
3727         off[nst]  = offset;
3728         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3729                                     phase-&gt;intcon(con0), T_INT, MemNode::unordered);
3730       }
3731       if (con1 != 0) {
3732         ++new_int;
3733         offset += BytesPerInt;
3734         adr = make_raw_address(offset, phase);
3735         off[nst]  = offset;
3736         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3737                                     phase-&gt;intcon(con1), T_INT, MemNode::unordered);
3738       }
3739     }
3740 
3741     // Insert second store first, then the first before the second.
3742     // Insert each one just before any overlapping non-constant stores.
3743     while (nst &gt; 0) {
3744       Node* st1 = st[--nst];
3745       C-&gt;copy_node_notes_to(st1, old);
3746       st1 = phase-&gt;transform(st1);
3747       offset = off[nst];
3748       assert(offset &gt;= header_size, "do not smash header");
3749       int ins_idx = captured_store_insertion_point(offset, /*size:*/0, phase);
3750       guarantee(ins_idx != 0, "must re-insert constant store");
3751       if (ins_idx &lt; 0)  ins_idx = -ins_idx;  // never overlap
3752       if (ins_idx &gt; InitializeNode::RawStores &amp;&amp; in(ins_idx-1) == zmem)
3753         set_req(--ins_idx, st1);
3754       else
3755         ins_req(ins_idx, st1);
3756     }
3757   }
3758 
3759   if (PrintCompilation &amp;&amp; WizardMode)
3760     tty-&gt;print_cr("Changed %d/%d subword/long constants into %d/%d int/long",
3761                   old_subword, old_long, new_int, new_long);
3762   if (C-&gt;log() != NULL)
3763     C-&gt;log()-&gt;elem("comment that='%d/%d subword/long to %d/%d int/long'",
3764                    old_subword, old_long, new_int, new_long);
3765 
3766   // Clean up any remaining occurrences of zmem:
3767   remove_extra_zeroes();
3768 }
3769 
3770 // Explore forward from in(start) to find the first fully initialized
3771 // word, and return its offset.  Skip groups of subword stores which
3772 // together initialize full words.  If in(start) is itself part of a
3773 // fully initialized word, return the offset of in(start).  If there
3774 // are no following full-word stores, or if something is fishy, return
3775 // a negative value.
3776 intptr_t InitializeNode::find_next_fullword_store(uint start, PhaseGVN* phase) {
3777   int       int_map = 0;
3778   intptr_t  int_map_off = 0;
3779   const int FULL_MAP = right_n_bits(BytesPerInt);  // the int_map we hope for
3780 
3781   for (uint i = start, limit = req(); i &lt; limit; i++) {
3782     Node* st = in(i);
3783 
3784     intptr_t st_off = get_store_offset(st, phase);
3785     if (st_off &lt; 0)  break;  // return conservative answer
3786 
3787     int st_size = st-&gt;as_Store()-&gt;memory_size();
3788     if (st_size &gt;= BytesPerInt &amp;&amp; (st_off % BytesPerInt) == 0) {
3789       return st_off;            // we found a complete word init
3790     }
3791 
3792     // update the map:
3793 
3794     intptr_t this_int_off = align_size_down(st_off, BytesPerInt);
3795     if (this_int_off != int_map_off) {
3796       // reset the map:
3797       int_map = 0;
3798       int_map_off = this_int_off;
3799     }
3800 
3801     int subword_off = st_off - this_int_off;
3802     int_map |= right_n_bits(st_size) &lt;&lt; subword_off;
3803     if ((int_map &amp; FULL_MAP) == FULL_MAP) {
3804       return this_int_off;      // we found a complete word init
3805     }
3806 
3807     // Did this store hit or cross the word boundary?
3808     intptr_t next_int_off = align_size_down(st_off + st_size, BytesPerInt);
3809     if (next_int_off == this_int_off + BytesPerInt) {
3810       // We passed the current int, without fully initializing it.
3811       int_map_off = next_int_off;
3812       int_map &gt;&gt;= BytesPerInt;
3813     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
3814       // We passed the current and next int.
3815       return this_int_off + BytesPerInt;
3816     }
3817   }
3818 
3819   return -1;
3820 }
3821 
3822 
3823 // Called when the associated AllocateNode is expanded into CFG.
3824 // At this point, we may perform additional optimizations.
3825 // Linearize the stores by ascending offset, to make memory
3826 // activity as coherent as possible.
3827 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
3828                                       intptr_t header_size,
3829                                       Node* size_in_bytes,
3830                                       PhaseGVN* phase) {
3831   assert(!is_complete(), "not already complete");
3832   assert(stores_are_sane(phase), "");
3833   assert(allocation() != NULL, "must be present");
3834 
3835   remove_extra_zeroes();
3836 
3837   if (ReduceFieldZeroing || ReduceBulkZeroing)
3838     // reduce instruction count for common initialization patterns
3839     coalesce_subword_stores(header_size, size_in_bytes, phase);
3840 
3841   Node* zmem = zero_memory();   // initially zero memory state
3842   Node* inits = zmem;           // accumulating a linearized chain of inits
3843   #ifdef ASSERT
3844   intptr_t first_offset = allocation()-&gt;minimum_header_size();
3845   intptr_t last_init_off = first_offset;  // previous init offset
3846   intptr_t last_init_end = first_offset;  // previous init offset+size
3847   intptr_t last_tile_end = first_offset;  // previous tile offset+size
3848   #endif
3849   intptr_t zeroes_done = header_size;
3850 
3851   bool do_zeroing = true;       // we might give up if inits are very sparse
3852   int  big_init_gaps = 0;       // how many large gaps have we seen?
3853 
3854   if (ZeroTLAB)  do_zeroing = false;
3855   if (!ReduceFieldZeroing &amp;&amp; !ReduceBulkZeroing)  do_zeroing = false;
3856 
3857   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3858     Node* st = in(i);
3859     intptr_t st_off = get_store_offset(st, phase);
3860     if (st_off &lt; 0)
3861       break;                    // unknown junk in the inits
3862     if (st-&gt;in(MemNode::Memory) != zmem)
3863       break;                    // complicated store chains somehow in list
3864 
3865     int st_size = st-&gt;as_Store()-&gt;memory_size();
3866     intptr_t next_init_off = st_off + st_size;
3867 
3868     if (do_zeroing &amp;&amp; zeroes_done &lt; next_init_off) {
3869       // See if this store needs a zero before it or under it.
3870       intptr_t zeroes_needed = st_off;
3871 
3872       if (st_size &lt; BytesPerInt) {
3873         // Look for subword stores which only partially initialize words.
3874         // If we find some, we must lay down some word-level zeroes first,
3875         // underneath the subword stores.
3876         //
3877         // Examples:
3878         //   byte[] a = { p,q,r,s }  =&gt;  a[0]=p,a[1]=q,a[2]=r,a[3]=s
3879         //   byte[] a = { x,y,0,0 }  =&gt;  a[0..3] = 0, a[0]=x,a[1]=y
3880         //   byte[] a = { 0,0,z,0 }  =&gt;  a[0..3] = 0, a[2]=z
3881         //
3882         // Note:  coalesce_subword_stores may have already done this,
3883         // if it was prompted by constant non-zero subword initializers.
3884         // But this case can still arise with non-constant stores.
3885 
3886         intptr_t next_full_store = find_next_fullword_store(i, phase);
3887 
3888         // In the examples above:
3889         //   in(i)          p   q   r   s     x   y     z
3890         //   st_off        12  13  14  15    12  13    14
3891         //   st_size        1   1   1   1     1   1     1
3892         //   next_full_s.  12  16  16  16    16  16    16
3893         //   z's_done      12  16  16  16    12  16    12
3894         //   z's_needed    12  16  16  16    16  16    16
3895         //   zsize          0   0   0   0     4   0     4
3896         if (next_full_store &lt; 0) {
3897           // Conservative tack:  Zero to end of current word.
3898           zeroes_needed = align_size_up(zeroes_needed, BytesPerInt);
3899         } else {
3900           // Zero to beginning of next fully initialized word.
3901           // Or, don't zero at all, if we are already in that word.
3902           assert(next_full_store &gt;= zeroes_needed, "must go forward");
3903           assert((next_full_store &amp; (BytesPerInt-1)) == 0, "even boundary");
3904           zeroes_needed = next_full_store;
3905         }
3906       }
3907 
3908       if (zeroes_needed &gt; zeroes_done) {
3909         intptr_t zsize = zeroes_needed - zeroes_done;
3910         // Do some incremental zeroing on rawmem, in parallel with inits.
3911         zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3912         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3913                                               zeroes_done, zeroes_needed,
3914                                               phase);
3915         zeroes_done = zeroes_needed;
3916         if (zsize &gt; Matcher::init_array_short_size &amp;&amp; ++big_init_gaps &gt; 2)
3917           do_zeroing = false;   // leave the hole, next time
3918       }
3919     }
3920 
3921     // Collect the store and move on:
3922     st-&gt;set_req(MemNode::Memory, inits);
3923     inits = st;                 // put it on the linearized chain
3924     set_req(i, zmem);           // unhook from previous position
3925 
3926     if (zeroes_done == st_off)
3927       zeroes_done = next_init_off;
3928 
3929     assert(!do_zeroing || zeroes_done &gt;= next_init_off, "don't miss any");
3930 
3931     #ifdef ASSERT
3932     // Various order invariants.  Weaker than stores_are_sane because
3933     // a large constant tile can be filled in by smaller non-constant stores.
3934     assert(st_off &gt;= last_init_off, "inits do not reverse");
3935     last_init_off = st_off;
3936     const Type* val = NULL;
3937     if (st_size &gt;= BytesPerInt &amp;&amp;
3938         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
3939         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
3940       assert(st_off &gt;= last_tile_end, "tiles do not overlap");
3941       assert(st_off &gt;= last_init_end, "tiles do not overwrite inits");
3942       last_tile_end = MAX2(last_tile_end, next_init_off);
3943     } else {
3944       intptr_t st_tile_end = align_size_up(next_init_off, BytesPerLong);
3945       assert(st_tile_end &gt;= last_tile_end, "inits stay with tiles");
3946       assert(st_off      &gt;= last_init_end, "inits do not overlap");
3947       last_init_end = next_init_off;  // it's a non-tile
3948     }
3949     #endif //ASSERT
3950   }
3951 
3952   remove_extra_zeroes();        // clear out all the zmems left over
3953   add_req(inits);
3954 
3955   if (!ZeroTLAB) {
3956     // If anything remains to be zeroed, zero it all now.
3957     zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3958     // if it is the last unused 4 bytes of an instance, forget about it
3959     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
3960     if (zeroes_done + BytesPerLong &gt;= size_limit) {
3961       assert(allocation() != NULL, "");
3962       if (allocation()-&gt;Opcode() == Op_Allocate) {
3963         Node* klass_node = allocation()-&gt;in(AllocateNode::KlassNode);
3964         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
3965         if (zeroes_done == k-&gt;layout_helper())
3966           zeroes_done = size_limit;
3967       }
3968     }
3969     if (zeroes_done &lt; size_limit) {
3970       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3971                                             zeroes_done, size_in_bytes, phase);
3972     }
3973   }
3974 
3975   set_complete(phase);
3976   return rawmem;
3977 }
3978 
3979 
3980 #ifdef ASSERT
3981 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
3982   if (is_complete())
3983     return true;                // stores could be anything at this point
3984   assert(allocation() != NULL, "must be present");
3985   intptr_t last_off = allocation()-&gt;minimum_header_size();
3986   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
3987     Node* st = in(i);
3988     intptr_t st_off = get_store_offset(st, phase);
3989     if (st_off &lt; 0)  continue;  // ignore dead garbage
3990     if (last_off &gt; st_off) {
3991       tty-&gt;print_cr("*** bad store offset at %d: " INTX_FORMAT " &gt; " INTX_FORMAT, i, last_off, st_off);
3992       this-&gt;dump(2);
3993       assert(false, "ascending store offsets");
3994       return false;
3995     }
3996     last_off = st_off + st-&gt;as_Store()-&gt;memory_size();
3997   }
3998   return true;
3999 }
4000 #endif //ASSERT
4001 
4002 
4003 
4004 
4005 //============================MergeMemNode=====================================
4006 //
4007 // SEMANTICS OF MEMORY MERGES:  A MergeMem is a memory state assembled from several
4008 // contributing store or call operations.  Each contributor provides the memory
4009 // state for a particular "alias type" (see Compile::alias_type).  For example,
4010 // if a MergeMem has an input X for alias category #6, then any memory reference
4011 // to alias category #6 may use X as its memory state input, as an exact equivalent
4012 // to using the MergeMem as a whole.
4013 //   Load&lt;6&gt;( MergeMem(&lt;6&gt;: X, ...), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4014 //
4015 // (Here, the &lt;N&gt; notation gives the index of the relevant adr_type.)
4016 //
4017 // In one special case (and more cases in the future), alias categories overlap.
4018 // The special alias category "Bot" (Compile::AliasIdxBot) includes all memory
4019 // states.  Therefore, if a MergeMem has only one contributing input W for Bot,
4020 // it is exactly equivalent to that state W:
4021 //   MergeMem(&lt;Bot&gt;: W) &lt;==&gt; W
4022 //
4023 // Usually, the merge has more than one input.  In that case, where inputs
4024 // overlap (i.e., one is Bot), the narrower alias type determines the memory
4025 // state for that type, and the wider alias type (Bot) fills in everywhere else:
4026 //   Load&lt;5&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;5&gt;(W,p)
4027 //   Load&lt;6&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4028 //
4029 // A merge can take a "wide" memory state as one of its narrow inputs.
4030 // This simply means that the merge observes out only the relevant parts of
4031 // the wide input.  That is, wide memory states arriving at narrow merge inputs
4032 // are implicitly "filtered" or "sliced" as necessary.  (This is rare.)
4033 //
4034 // These rules imply that MergeMem nodes may cascade (via their &lt;Bot&gt; links),
4035 // and that memory slices "leak through":
4036 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)) &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)
4037 //
4038 // But, in such a cascade, repeated memory slices can "block the leak":
4039 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y), &lt;7&gt;: Y') &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y')
4040 //
4041 // In the last example, Y is not part of the combined memory state of the
4042 // outermost MergeMem.  The system must, of course, prevent unschedulable
4043 // memory states from arising, so you can be sure that the state Y is somehow
4044 // a precursor to state Y'.
4045 //
4046 //
4047 // REPRESENTATION OF MEMORY MERGES: The indexes used to address the Node::in array
4048 // of each MergeMemNode array are exactly the numerical alias indexes, including
4049 // but not limited to AliasIdxTop, AliasIdxBot, and AliasIdxRaw.  The functions
4050 // Compile::alias_type (and kin) produce and manage these indexes.
4051 //
4052 // By convention, the value of in(AliasIdxTop) (i.e., in(1)) is always the top node.
4053 // (Note that this provides quick access to the top node inside MergeMem methods,
4054 // without the need to reach out via TLS to Compile::current.)
4055 //
4056 // As a consequence of what was just described, a MergeMem that represents a full
4057 // memory state has an edge in(AliasIdxBot) which is a "wide" memory state,
4058 // containing all alias categories.
4059 //
4060 // MergeMem nodes never (?) have control inputs, so in(0) is NULL.
4061 //
4062 // All other edges in(N) (including in(AliasIdxRaw), which is in(3)) are either
4063 // a memory state for the alias type &lt;N&gt;, or else the top node, meaning that
4064 // there is no particular input for that alias type.  Note that the length of
4065 // a MergeMem is variable, and may be extended at any time to accommodate new
4066 // memory states at larger alias indexes.  When merges grow, they are of course
4067 // filled with "top" in the unused in() positions.
4068 //
4069 // This use of top is named "empty_memory()", or "empty_mem" (no-memory) as a variable.
4070 // (Top was chosen because it works smoothly with passes like GCM.)
4071 //
4072 // For convenience, we hardwire the alias index for TypeRawPtr::BOTTOM.  (It is
4073 // the type of random VM bits like TLS references.)  Since it is always the
4074 // first non-Bot memory slice, some low-level loops use it to initialize an
4075 // index variable:  for (i = AliasIdxRaw; i &lt; req(); i++).
4076 //
4077 //
4078 // ACCESSORS:  There is a special accessor MergeMemNode::base_memory which returns
4079 // the distinguished "wide" state.  The accessor MergeMemNode::memory_at(N) returns
4080 // the memory state for alias type &lt;N&gt;, or (if there is no particular slice at &lt;N&gt;,
4081 // it returns the base memory.  To prevent bugs, memory_at does not accept &lt;Top&gt;
4082 // or &lt;Bot&gt; indexes.  The iterator MergeMemStream provides robust iteration over
4083 // MergeMem nodes or pairs of such nodes, ensuring that the non-top edges are visited.
4084 //
4085 // %%%% We may get rid of base_memory as a separate accessor at some point; it isn't
4086 // really that different from the other memory inputs.  An abbreviation called
4087 // "bot_memory()" for "memory_at(AliasIdxBot)" would keep code tidy.
4088 //
4089 //
4090 // PARTIAL MEMORY STATES:  During optimization, MergeMem nodes may arise that represent
4091 // partial memory states.  When a Phi splits through a MergeMem, the copy of the Phi
4092 // that "emerges though" the base memory will be marked as excluding the alias types
4093 // of the other (narrow-memory) copies which "emerged through" the narrow edges:
4094 //
4095 //   Phi&lt;Bot&gt;(U, MergeMem(&lt;Bot&gt;: W, &lt;8&gt;: Y))
4096 //     ==Ideal=&gt;  MergeMem(&lt;Bot&gt;: Phi&lt;Bot-8&gt;(U, W), Phi&lt;8&gt;(U, Y))
4097 //
4098 // This strange "subtraction" effect is necessary to ensure IGVN convergence.
4099 // (It is currently unimplemented.)  As you can see, the resulting merge is
4100 // actually a disjoint union of memory states, rather than an overlay.
4101 //
4102 
4103 //------------------------------MergeMemNode-----------------------------------
4104 Node* MergeMemNode::make_empty_memory() {
4105   Node* empty_memory = (Node*) Compile::current()-&gt;top();
4106   assert(empty_memory-&gt;is_top(), "correct sentinel identity");
4107   return empty_memory;
4108 }
4109 
4110 MergeMemNode::MergeMemNode(Node *new_base) : Node(1+Compile::AliasIdxRaw) {
4111   init_class_id(Class_MergeMem);
4112   // all inputs are nullified in Node::Node(int)
4113   // set_input(0, NULL);  // no control input
4114 
4115   // Initialize the edges uniformly to top, for starters.
4116   Node* empty_mem = make_empty_memory();
4117   for (uint i = Compile::AliasIdxTop; i &lt; req(); i++) {
4118     init_req(i,empty_mem);
4119   }
4120   assert(empty_memory() == empty_mem, "");
4121 
4122   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4123     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4124     assert(mdef-&gt;empty_memory() == empty_mem, "consistent sentinels");
4125     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4126       mms.set_memory(mms.memory2());
4127     }
4128     assert(base_memory() == mdef-&gt;base_memory(), "");
4129   } else {
4130     set_base_memory(new_base);
4131   }
4132 }
4133 
4134 // Make a new, untransformed MergeMem with the same base as 'mem'.
4135 // If mem is itself a MergeMem, populate the result with the same edges.
4136 MergeMemNode* MergeMemNode::make(Node* mem) {
4137   return new MergeMemNode(mem);
4138 }
4139 
4140 //------------------------------cmp--------------------------------------------
4141 uint MergeMemNode::hash() const { return NO_HASH; }
4142 uint MergeMemNode::cmp( const Node &amp;n ) const {
4143   return (&amp;n == this);          // Always fail except on self
4144 }
4145 
4146 //------------------------------Identity---------------------------------------
4147 Node* MergeMemNode::Identity(PhaseTransform *phase) {
4148   // Identity if this merge point does not record any interesting memory
4149   // disambiguations.
4150   Node* base_mem = base_memory();
4151   Node* empty_mem = empty_memory();
4152   if (base_mem != empty_mem) {  // Memory path is not dead?
4153     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4154       Node* mem = in(i);
4155       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4156         return this;            // Many memory splits; no change
4157       }
4158     }
4159   }
4160   return base_mem;              // No memory splits; ID on the one true input
4161 }
4162 
4163 //------------------------------Ideal------------------------------------------
4164 // This method is invoked recursively on chains of MergeMem nodes
4165 Node *MergeMemNode::Ideal(PhaseGVN *phase, bool can_reshape) {
4166   // Remove chain'd MergeMems
4167   //
4168   // This is delicate, because the each "in(i)" (i &gt;= Raw) is interpreted
4169   // relative to the "in(Bot)".  Since we are patching both at the same time,
4170   // we have to be careful to read each "in(i)" relative to the old "in(Bot)",
4171   // but rewrite each "in(i)" relative to the new "in(Bot)".
4172   Node *progress = NULL;
4173 
4174 
4175   Node* old_base = base_memory();
4176   Node* empty_mem = empty_memory();
4177   if (old_base == empty_mem)
4178     return NULL; // Dead memory path.
4179 
4180   MergeMemNode* old_mbase;
4181   if (old_base != NULL &amp;&amp; old_base-&gt;is_MergeMem())
4182     old_mbase = old_base-&gt;as_MergeMem();
4183   else
4184     old_mbase = NULL;
4185   Node* new_base = old_base;
4186 
4187   // simplify stacked MergeMems in base memory
4188   if (old_mbase)  new_base = old_mbase-&gt;base_memory();
4189 
4190   // the base memory might contribute new slices beyond my req()
4191   if (old_mbase)  grow_to_match(old_mbase);
4192 
4193   // Look carefully at the base node if it is a phi.
4194   PhiNode* phi_base;
4195   if (new_base != NULL &amp;&amp; new_base-&gt;is_Phi())
4196     phi_base = new_base-&gt;as_Phi();
4197   else
4198     phi_base = NULL;
4199 
4200   Node*    phi_reg = NULL;
4201   uint     phi_len = (uint)-1;
4202   if (phi_base != NULL &amp;&amp; !phi_base-&gt;is_copy()) {
4203     // do not examine phi if degraded to a copy
4204     phi_reg = phi_base-&gt;region();
4205     phi_len = phi_base-&gt;req();
4206     // see if the phi is unfinished
4207     for (uint i = 1; i &lt; phi_len; i++) {
4208       if (phi_base-&gt;in(i) == NULL) {
4209         // incomplete phi; do not look at it yet!
4210         phi_reg = NULL;
4211         phi_len = (uint)-1;
4212         break;
4213       }
4214     }
4215   }
4216 
4217   // Note:  We do not call verify_sparse on entry, because inputs
4218   // can normalize to the base_memory via subsume_node or similar
4219   // mechanisms.  This method repairs that damage.
4220 
4221   assert(!old_mbase || old_mbase-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4222 
4223   // Look at each slice.
4224   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4225     Node* old_in = in(i);
4226     // calculate the old memory value
4227     Node* old_mem = old_in;
4228     if (old_mem == empty_mem)  old_mem = old_base;
4229     assert(old_mem == memory_at(i), "");
4230 
4231     // maybe update (reslice) the old memory value
4232 
4233     // simplify stacked MergeMems
4234     Node* new_mem = old_mem;
4235     MergeMemNode* old_mmem;
4236     if (old_mem != NULL &amp;&amp; old_mem-&gt;is_MergeMem())
4237       old_mmem = old_mem-&gt;as_MergeMem();
4238     else
4239       old_mmem = NULL;
4240     if (old_mmem == this) {
4241       // This can happen if loops break up and safepoints disappear.
4242       // A merge of BotPtr (default) with a RawPtr memory derived from a
4243       // safepoint can be rewritten to a merge of the same BotPtr with
4244       // the BotPtr phi coming into the loop.  If that phi disappears
4245       // also, we can end up with a self-loop of the mergemem.
4246       // In general, if loops degenerate and memory effects disappear,
4247       // a mergemem can be left looking at itself.  This simply means
4248       // that the mergemem's default should be used, since there is
4249       // no longer any apparent effect on this slice.
4250       // Note: If a memory slice is a MergeMem cycle, it is unreachable
4251       //       from start.  Update the input to TOP.
4252       new_mem = (new_base == this || new_base == empty_mem)? empty_mem : new_base;
4253     }
4254     else if (old_mmem != NULL) {
4255       new_mem = old_mmem-&gt;memory_at(i);
4256     }
4257     // else preceding memory was not a MergeMem
4258 
4259     // replace equivalent phis (unfortunately, they do not GVN together)
4260     if (new_mem != NULL &amp;&amp; new_mem != new_base &amp;&amp;
4261         new_mem-&gt;req() == phi_len &amp;&amp; new_mem-&gt;in(0) == phi_reg) {
4262       if (new_mem-&gt;is_Phi()) {
4263         PhiNode* phi_mem = new_mem-&gt;as_Phi();
4264         for (uint i = 1; i &lt; phi_len; i++) {
4265           if (phi_base-&gt;in(i) != phi_mem-&gt;in(i)) {
4266             phi_mem = NULL;
4267             break;
4268           }
4269         }
4270         if (phi_mem != NULL) {
4271           // equivalent phi nodes; revert to the def
4272           new_mem = new_base;
4273         }
4274       }
4275     }
4276 
4277     // maybe store down a new value
4278     Node* new_in = new_mem;
4279     if (new_in == new_base)  new_in = empty_mem;
4280 
4281     if (new_in != old_in) {
4282       // Warning:  Do not combine this "if" with the previous "if"
4283       // A memory slice might have be be rewritten even if it is semantically
4284       // unchanged, if the base_memory value has changed.
4285       set_req(i, new_in);
4286       progress = this;          // Report progress
4287     }
4288   }
4289 
4290   if (new_base != old_base) {
4291     set_req(Compile::AliasIdxBot, new_base);
4292     // Don't use set_base_memory(new_base), because we need to update du.
4293     assert(base_memory() == new_base, "");
4294     progress = this;
4295   }
4296 
4297   if( base_memory() == this ) {
4298     // a self cycle indicates this memory path is dead
4299     set_req(Compile::AliasIdxBot, empty_mem);
4300   }
4301 
4302   // Resolve external cycles by calling Ideal on a MergeMem base_memory
4303   // Recursion must occur after the self cycle check above
4304   if( base_memory()-&gt;is_MergeMem() ) {
4305     MergeMemNode *new_mbase = base_memory()-&gt;as_MergeMem();
4306     Node *m = phase-&gt;transform(new_mbase);  // Rollup any cycles
4307     if( m != NULL &amp;&amp; (m-&gt;is_top() ||
4308         m-&gt;is_MergeMem() &amp;&amp; m-&gt;as_MergeMem()-&gt;base_memory() == empty_mem) ) {
4309       // propagate rollup of dead cycle to self
4310       set_req(Compile::AliasIdxBot, empty_mem);
4311     }
4312   }
4313 
4314   if( base_memory() == empty_mem ) {
4315     progress = this;
4316     // Cut inputs during Parse phase only.
4317     // During Optimize phase a dead MergeMem node will be subsumed by Top.
4318     if( !can_reshape ) {
4319       for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4320         if( in(i) != empty_mem ) { set_req(i, empty_mem); }
4321       }
4322     }
4323   }
4324 
4325   if( !progress &amp;&amp; base_memory()-&gt;is_Phi() &amp;&amp; can_reshape ) {
4326     // Check if PhiNode::Ideal's "Split phis through memory merges"
4327     // transform should be attempted. Look for this-&gt;phi-&gt;this cycle.
4328     uint merge_width = req();
4329     if (merge_width &gt; Compile::AliasIdxRaw) {
4330       PhiNode* phi = base_memory()-&gt;as_Phi();
4331       for( uint i = 1; i &lt; phi-&gt;req(); ++i ) {// For all paths in
4332         if (phi-&gt;in(i) == this) {
4333           phase-&gt;is_IterGVN()-&gt;_worklist.push(phi);
4334           break;
4335         }
4336       }
4337     }
4338   }
4339 
4340   assert(progress || verify_sparse(), "please, no dups of base");
4341   return progress;
4342 }
4343 
4344 //-------------------------set_base_memory-------------------------------------
4345 void MergeMemNode::set_base_memory(Node *new_base) {
4346   Node* empty_mem = empty_memory();
4347   set_req(Compile::AliasIdxBot, new_base);
4348   assert(memory_at(req()) == new_base, "must set default memory");
4349   // Clear out other occurrences of new_base:
4350   if (new_base != empty_mem) {
4351     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4352       if (in(i) == new_base)  set_req(i, empty_mem);
4353     }
4354   }
4355 }
4356 
4357 //------------------------------out_RegMask------------------------------------
4358 const RegMask &amp;MergeMemNode::out_RegMask() const {
4359   return RegMask::Empty;
4360 }
4361 
4362 //------------------------------dump_spec--------------------------------------
4363 #ifndef PRODUCT
4364 void MergeMemNode::dump_spec(outputStream *st) const {
4365   st-&gt;print(" {");
4366   Node* base_mem = base_memory();
4367   for( uint i = Compile::AliasIdxRaw; i &lt; req(); i++ ) {
4368     Node* mem = (in(i) != NULL) ? memory_at(i) : base_mem;
4369     if (mem == base_mem) { st-&gt;print(" -"); continue; }
4370     st-&gt;print( " N%d:", mem-&gt;_idx );
4371     Compile::current()-&gt;get_adr_type(i)-&gt;dump_on(st);
4372   }
4373   st-&gt;print(" }");
4374 }
4375 #endif // !PRODUCT
4376 
4377 
4378 #ifdef ASSERT
4379 static bool might_be_same(Node* a, Node* b) {
4380   if (a == b)  return true;
4381   if (!(a-&gt;is_Phi() || b-&gt;is_Phi()))  return false;
4382   // phis shift around during optimization
4383   return true;  // pretty stupid...
4384 }
4385 
4386 // verify a narrow slice (either incoming or outgoing)
4387 static void verify_memory_slice(const MergeMemNode* m, int alias_idx, Node* n) {
4388   if (!VerifyAliases)       return;  // don't bother to verify unless requested
4389   if (is_error_reported())  return;  // muzzle asserts when debugging an error
4390   if (Node::in_dump())      return;  // muzzle asserts when printing
4391   assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not disturb base_memory or sentinel");
4392   assert(n != NULL, "");
4393   // Elide intervening MergeMem's
4394   while (n-&gt;is_MergeMem()) {
4395     n = n-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
4396   }
4397   Compile* C = Compile::current();
4398   const TypePtr* n_adr_type = n-&gt;adr_type();
4399   if (n == m-&gt;empty_memory()) {
4400     // Implicit copy of base_memory()
4401   } else if (n_adr_type != TypePtr::BOTTOM) {
4402     assert(n_adr_type != NULL, "new memory must have a well-defined adr_type");
4403     assert(C-&gt;must_alias(n_adr_type, alias_idx), "new memory must match selected slice");
4404   } else {
4405     // A few places like make_runtime_call "know" that VM calls are narrow,
4406     // and can be used to update only the VM bits stored as TypeRawPtr::BOTTOM.
4407     bool expected_wide_mem = false;
4408     if (n == m-&gt;base_memory()) {
4409       expected_wide_mem = true;
4410     } else if (alias_idx == Compile::AliasIdxRaw ||
4411                n == m-&gt;memory_at(Compile::AliasIdxRaw)) {
4412       expected_wide_mem = true;
4413     } else if (!C-&gt;alias_type(alias_idx)-&gt;is_rewritable()) {
4414       // memory can "leak through" calls on channels that
4415       // are write-once.  Allow this also.
4416       expected_wide_mem = true;
4417     }
4418     assert(expected_wide_mem, "expected narrow slice replacement");
4419   }
4420 }
4421 #else // !ASSERT
4422 #define verify_memory_slice(m,i,n) (void)(0)  // PRODUCT version is no-op
4423 #endif
4424 
4425 
4426 //-----------------------------memory_at---------------------------------------
4427 Node* MergeMemNode::memory_at(uint alias_idx) const {
4428   assert(alias_idx &gt;= Compile::AliasIdxRaw ||
4429          alias_idx == Compile::AliasIdxBot &amp;&amp; Compile::current()-&gt;AliasLevel() == 0,
4430          "must avoid base_memory and AliasIdxTop");
4431 
4432   // Otherwise, it is a narrow slice.
4433   Node* n = alias_idx &lt; req() ? in(alias_idx) : empty_memory();
4434   Compile *C = Compile::current();
4435   if (is_empty_memory(n)) {
4436     // the array is sparse; empty slots are the "top" node
4437     n = base_memory();
4438     assert(Node::in_dump()
4439            || n == NULL || n-&gt;bottom_type() == Type::TOP
4440            || n-&gt;adr_type() == NULL // address is TOP
4441            || n-&gt;adr_type() == TypePtr::BOTTOM
4442            || n-&gt;adr_type() == TypeRawPtr::BOTTOM
4443            || Compile::current()-&gt;AliasLevel() == 0,
4444            "must be a wide memory");
4445     // AliasLevel == 0 if we are organizing the memory states manually.
4446     // See verify_memory_slice for comments on TypeRawPtr::BOTTOM.
4447   } else {
4448     // make sure the stored slice is sane
4449     #ifdef ASSERT
4450     if (is_error_reported() || Node::in_dump()) {
4451     } else if (might_be_same(n, base_memory())) {
4452       // Give it a pass:  It is a mostly harmless repetition of the base.
4453       // This can arise normally from node subsumption during optimization.
4454     } else {
4455       verify_memory_slice(this, alias_idx, n);
4456     }
4457     #endif
4458   }
4459   return n;
4460 }
4461 
4462 //---------------------------set_memory_at-------------------------------------
4463 void MergeMemNode::set_memory_at(uint alias_idx, Node *n) {
4464   verify_memory_slice(this, alias_idx, n);
4465   Node* empty_mem = empty_memory();
4466   if (n == base_memory())  n = empty_mem;  // collapse default
4467   uint need_req = alias_idx+1;
4468   if (req() &lt; need_req) {
4469     if (n == empty_mem)  return;  // already the default, so do not grow me
4470     // grow the sparse array
4471     do {
4472       add_req(empty_mem);
4473     } while (req() &lt; need_req);
4474   }
4475   set_req( alias_idx, n );
4476 }
4477 
4478 
4479 
4480 //--------------------------iteration_setup------------------------------------
4481 void MergeMemNode::iteration_setup(const MergeMemNode* other) {
4482   if (other != NULL) {
4483     grow_to_match(other);
4484     // invariant:  the finite support of mm2 is within mm-&gt;req()
4485     #ifdef ASSERT
4486     for (uint i = req(); i &lt; other-&gt;req(); i++) {
4487       assert(other-&gt;is_empty_memory(other-&gt;in(i)), "slice left uncovered");
4488     }
4489     #endif
4490   }
4491   // Replace spurious copies of base_memory by top.
4492   Node* base_mem = base_memory();
4493   if (base_mem != NULL &amp;&amp; !base_mem-&gt;is_top()) {
4494     for (uint i = Compile::AliasIdxBot+1, imax = req(); i &lt; imax; i++) {
4495       if (in(i) == base_mem)
4496         set_req(i, empty_memory());
4497     }
4498   }
4499 }
4500 
4501 //---------------------------grow_to_match-------------------------------------
4502 void MergeMemNode::grow_to_match(const MergeMemNode* other) {
4503   Node* empty_mem = empty_memory();
4504   assert(other-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4505   // look for the finite support of the other memory
4506   for (uint i = other-&gt;req(); --i &gt;= req(); ) {
4507     if (other-&gt;in(i) != empty_mem) {
4508       uint new_len = i+1;
4509       while (req() &lt; new_len)  add_req(empty_mem);
4510       break;
4511     }
4512   }
4513 }
4514 
4515 //---------------------------verify_sparse-------------------------------------
4516 #ifndef PRODUCT
4517 bool MergeMemNode::verify_sparse() const {
4518   assert(is_empty_memory(make_empty_memory()), "sane sentinel");
4519   Node* base_mem = base_memory();
4520   // The following can happen in degenerate cases, since empty==top.
4521   if (is_empty_memory(base_mem))  return true;
4522   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4523     assert(in(i) != NULL, "sane slice");
4524     if (in(i) == base_mem)  return false;  // should have been the sentinel value!
4525   }
4526   return true;
4527 }
4528 
4529 bool MergeMemStream::match_memory(Node* mem, const MergeMemNode* mm, int idx) {
4530   Node* n;
4531   n = mm-&gt;in(idx);
4532   if (mem == n)  return true;  // might be empty_memory()
4533   n = (idx == Compile::AliasIdxBot)? mm-&gt;base_memory(): mm-&gt;memory_at(idx);
4534   if (mem == n)  return true;
4535   while (n-&gt;is_Phi() &amp;&amp; (n = n-&gt;as_Phi()-&gt;is_copy()) != NULL) {
4536     if (mem == n)  return true;
4537     if (n == NULL)  break;
4538   }
4539   return false;
4540 }
4541 #endif // !PRODUCT
</pre></body></html>
