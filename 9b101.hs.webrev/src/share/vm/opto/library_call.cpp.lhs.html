<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "classfile/vmSymbols.hpp"
  29 #include "compiler/compileBroker.hpp"
  30 #include "compiler/compileLog.hpp"
  31 #include "oops/objArrayKlass.hpp"
  32 #include "opto/addnode.hpp"
  33 #include "opto/arraycopynode.hpp"
  34 #include "opto/c2compiler.hpp"
  35 #include "opto/callGenerator.hpp"
  36 #include "opto/castnode.hpp"
  37 #include "opto/cfgnode.hpp"
  38 #include "opto/convertnode.hpp"
  39 #include "opto/countbitsnode.hpp"
  40 #include "opto/intrinsicnode.hpp"
  41 #include "opto/idealKit.hpp"
  42 #include "opto/mathexactnode.hpp"
  43 #include "opto/movenode.hpp"
  44 #include "opto/mulnode.hpp"
  45 #include "opto/narrowptrnode.hpp"
  46 #include "opto/opaquenode.hpp"
  47 #include "opto/parse.hpp"
  48 #include "opto/runtime.hpp"
  49 #include "opto/subnode.hpp"
  50 #include "prims/nativeLookup.hpp"
  51 #include "runtime/sharedRuntime.hpp"
  52 #include "trace/traceMacros.hpp"
  53 
  54 class LibraryIntrinsic : public InlineCallGenerator {
  55   // Extend the set of intrinsics known to the runtime:
  56  public:
  57  private:
  58   bool             _is_virtual;
  59   bool             _does_virtual_dispatch;
  60   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  61   int8_t           _last_predicate; // Last generated predicate
  62   vmIntrinsics::ID _intrinsic_id;
  63 
  64  public:
  65   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  66     : InlineCallGenerator(m),
  67       _is_virtual(is_virtual),
  68       _does_virtual_dispatch(does_virtual_dispatch),
  69       _predicates_count((int8_t)predicates_count),
  70       _last_predicate((int8_t)-1),
  71       _intrinsic_id(id)
  72   {
  73   }
  74   virtual bool is_intrinsic() const { return true; }
  75   virtual bool is_virtual()   const { return _is_virtual; }
  76   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  77   virtual int  predicates_count() const { return _predicates_count; }
  78   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  79   virtual JVMState* generate(JVMState* jvms);
  80   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  81   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  82 };
  83 
  84 
  85 // Local helper class for LibraryIntrinsic:
  86 class LibraryCallKit : public GraphKit {
  87  private:
  88   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  89   Node*             _result;        // the result node, if any
  90   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  91 
  92   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  93 
  94  public:
  95   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  96     : GraphKit(jvms),
  97       _intrinsic(intrinsic),
  98       _result(NULL)
  99   {
 100     // Check if this is a root compile.  In that case we don't have a caller.
 101     if (!jvms-&gt;has_method()) {
 102       _reexecute_sp = sp();
 103     } else {
 104       // Find out how many arguments the interpreter needs when deoptimizing
 105       // and save the stack pointer value so it can used by uncommon_trap.
 106       // We find the argument count by looking at the declared signature.
 107       bool ignored_will_link;
 108       ciSignature* declared_signature = NULL;
 109       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 110       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 111       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 112     }
 113   }
 114 
 115   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 116 
 117   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 118   int               bci()       const    { return jvms()-&gt;bci(); }
 119   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 120   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 121   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 122 
 123   bool  try_to_inline(int predicate);
 124   Node* try_to_predicate(int predicate);
 125 
 126   void push_result() {
 127     // Push the result onto the stack.
 128     if (!stopped() &amp;&amp; result() != NULL) {
 129       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 130       push_node(bt, result());
 131     }
 132   }
 133 
 134  private:
 135   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 136     fatal("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid));
 137   }
 138 
 139   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 140   void  set_result(RegionNode* region, PhiNode* value);
 141   Node*     result() { return _result; }
 142 
 143   virtual int reexecute_sp() { return _reexecute_sp; }
 144 
 145   // Helper functions to inline natives
 146   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 147   Node* generate_slow_guard(Node* test, RegionNode* region);
 148   Node* generate_fair_guard(Node* test, RegionNode* region);
 149   Node* generate_negative_guard(Node* index, RegionNode* region,
 150                                 // resulting CastII of index:
 151                                 Node* *pos_index = NULL);
 152   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 153                              Node* array_length,
 154                              RegionNode* region);
 155   void  generate_string_range_check(Node* array, Node* offset,
 156                                     Node* length, bool char_count);
 157   Node* generate_current_thread(Node* &amp;tls_output);
 158   Node* load_mirror_from_klass(Node* klass);
 159   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 160                                       RegionNode* region, int null_path,
 161                                       int offset);
 162   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 163                                RegionNode* region, int null_path) {
 164     int offset = java_lang_Class::klass_offset_in_bytes();
 165     return load_klass_from_mirror_common(mirror, never_see_null,
 166                                          region, null_path,
 167                                          offset);
 168   }
 169   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 170                                      RegionNode* region, int null_path) {
 171     int offset = java_lang_Class::array_klass_offset_in_bytes();
 172     return load_klass_from_mirror_common(mirror, never_see_null,
 173                                          region, null_path,
 174                                          offset);
 175   }
 176   Node* generate_access_flags_guard(Node* kls,
 177                                     int modifier_mask, int modifier_bits,
 178                                     RegionNode* region);
 179   Node* generate_interface_guard(Node* kls, RegionNode* region);
 180   Node* generate_array_guard(Node* kls, RegionNode* region) {
 181     return generate_array_guard_common(kls, region, false, false);
 182   }
 183   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 184     return generate_array_guard_common(kls, region, false, true);
 185   }
 186   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 187     return generate_array_guard_common(kls, region, true, false);
 188   }
 189   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 190     return generate_array_guard_common(kls, region, true, true);
 191   }
 192   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 193                                     bool obj_array, bool not_array);
 194   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 195   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 196                                      bool is_virtual = false, bool is_static = false);
 197   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 198     return generate_method_call(method_id, false, true);
 199   }
 200   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 201     return generate_method_call(method_id, true, false);
 202   }
 203   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 204 
 205   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 206   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 207   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 208   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 209   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
 210                           RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae);
 211   bool inline_string_indexOfChar();
 212   bool inline_string_equals(StrIntrinsicNode::ArgEnc ae);
 213   bool inline_string_toBytesU();
 214   bool inline_string_getCharsU();
 215   bool inline_string_copy(bool compress);
 216   bool inline_string_char_access(bool is_store);
 217   Node* round_double_node(Node* n);
 218   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 219   bool inline_math_native(vmIntrinsics::ID id);
 220   bool inline_trig(vmIntrinsics::ID id);
 221   bool inline_math(vmIntrinsics::ID id);
 222   template &lt;typename OverflowOp&gt;
 223   bool inline_math_overflow(Node* arg1, Node* arg2);
 224   void inline_math_mathExact(Node* math, Node* test);
 225   bool inline_math_addExactI(bool is_increment);
 226   bool inline_math_addExactL(bool is_increment);
 227   bool inline_math_multiplyExactI();
 228   bool inline_math_multiplyExactL();
 229   bool inline_math_negateExactI();
 230   bool inline_math_negateExactL();
 231   bool inline_math_subtractExactI(bool is_decrement);
 232   bool inline_math_subtractExactL(bool is_decrement);
 233   bool inline_pow();
 234   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 235   bool inline_min_max(vmIntrinsics::ID id);
 236   bool inline_notify(vmIntrinsics::ID id);
 237   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 238   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 239   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 240   Node* make_unsafe_address(Node* base, Node* offset);
 241   // Helper for inline_unsafe_access.
 242   // Generates the guards that check whether the result of
 243   // Unsafe.getObject should be recorded in an SATB log buffer.
 244   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 245   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile, bool is_unaligned);
 246   static bool klass_needs_init_guard(Node* kls);
 247   bool inline_unsafe_allocate();
 248   bool inline_unsafe_copyMemory();
 249   bool inline_native_currentThread();
 250 #ifdef TRACE_HAVE_INTRINSICS
 251   bool inline_native_classID();
 252   bool inline_native_threadID();
 253 #endif
 254   bool inline_native_time_funcs(address method, const char* funcName);
 255   bool inline_native_isInterrupted();
 256   bool inline_native_Class_query(vmIntrinsics::ID id);
 257   bool inline_native_subtype_check();
 258 
 259   bool inline_native_newArray();
 260   bool inline_native_getLength();
 261   bool inline_array_copyOf(bool is_copyOfRange);
 262   bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);
 263   bool inline_objects_checkIndex();
 264   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 265   bool inline_native_clone(bool is_virtual);
 266   bool inline_native_Reflection_getCallerClass();
 267   // Helper function for inlining native object hash method
 268   bool inline_native_hashcode(bool is_virtual, bool is_static);
 269   bool inline_native_getClass();
 270 
 271   // Helper functions for inlining arraycopy
 272   bool inline_arraycopy();
 273   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 274                                                 RegionNode* slow_region);
 275   JVMState* arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp);
 276   void arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp);
 277 
 278   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 279   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 280   bool inline_unsafe_ordered_store(BasicType type);
 281   bool inline_unsafe_fence(vmIntrinsics::ID id);
<a name="1" id="anc1"></a>
 282   bool inline_fp_conversions(vmIntrinsics::ID id);
 283   bool inline_number_methods(vmIntrinsics::ID id);
 284   bool inline_reference_get();
 285   bool inline_Class_cast();
 286   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 287   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 288   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 289   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 290   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 291   bool inline_ghash_processBlocks();
 292   bool inline_sha_implCompress(vmIntrinsics::ID id);
 293   bool inline_digestBase_implCompressMB(int predicate);
 294   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 295                                  bool long_state, address stubAddr, const char *stubName,
 296                                  Node* src_start, Node* ofs, Node* limit);
 297   Node* get_state_from_sha_object(Node *sha_object);
 298   Node* get_state_from_sha5_object(Node *sha_object);
 299   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 300   bool inline_encodeISOArray();
 301   bool inline_updateCRC32();
 302   bool inline_updateBytesCRC32();
 303   bool inline_updateByteBufferCRC32();
 304   Node* get_table_from_crc32c_class(ciInstanceKlass *crc32c_class);
 305   bool inline_updateBytesCRC32C();
 306   bool inline_updateDirectByteBufferCRC32C();
 307   bool inline_updateBytesAdler32();
 308   bool inline_updateByteBufferAdler32();
 309   bool inline_multiplyToLen();
 310   bool inline_hasNegatives();
 311   bool inline_squareToLen();
 312   bool inline_mulAdd();
 313   bool inline_montgomeryMultiply();
 314   bool inline_montgomerySquare();
 315 
 316   bool inline_profileBoolean();
 317   bool inline_isCompileConstant();
 318 };
 319 
 320 //---------------------------make_vm_intrinsic----------------------------
 321 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 322   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 323   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 324 
 325   if (!m-&gt;is_loaded()) {
 326     // Do not attempt to inline unloaded methods.
 327     return NULL;
 328   }
 329 
 330   C2Compiler* compiler = (C2Compiler*)CompileBroker::compiler(CompLevel_full_optimization);
 331   bool is_available = false;
 332 
 333   {
 334     // For calling is_intrinsic_supported and is_intrinsic_disabled_by_flag
 335     // the compiler must transition to '_thread_in_vm' state because both
 336     // methods access VM-internal data.
 337     VM_ENTRY_MARK;
 338     methodHandle mh(THREAD, m-&gt;get_Method());
 339     is_available = compiler-&gt;is_intrinsic_supported(mh, is_virtual) &amp;&amp;
 340                    !C-&gt;directive()-&gt;is_intrinsic_disabled(mh) &amp;&amp;
 341                    !vmIntrinsics::is_disabled_by_flags(mh);
 342 
 343   }
 344 
 345   if (is_available) {
 346     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 347     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 348     return new LibraryIntrinsic(m, is_virtual,
 349                                 vmIntrinsics::predicates_needed(id),
 350                                 vmIntrinsics::does_virtual_dispatch(id),
 351                                 (vmIntrinsics::ID) id);
 352   } else {
 353     return NULL;
 354   }
 355 }
 356 
 357 //----------------------register_library_intrinsics-----------------------
 358 // Initialize this file's data structures, for each Compile instance.
 359 void Compile::register_library_intrinsics() {
 360   // Nothing to do here.
 361 }
 362 
 363 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 364   LibraryCallKit kit(jvms, this);
 365   Compile* C = kit.C;
 366   int nodes = C-&gt;unique();
 367 #ifndef PRODUCT
 368   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 369     char buf[1000];
 370     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 371     tty-&gt;print_cr("Intrinsic %s", str);
 372   }
 373 #endif
 374   ciMethod* callee = kit.callee();
 375   const int bci    = kit.bci();
 376 
 377   // Try to inline the intrinsic.
 378   if ((CheckIntrinsics ? callee-&gt;intrinsic_candidate() : true) &amp;&amp;
 379       kit.try_to_inline(_last_predicate)) {
 380     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 381       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 382     }
 383     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 384     if (C-&gt;log()) {
 385       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 386                      vmIntrinsics::name_at(intrinsic_id()),
 387                      (is_virtual() ? " virtual='1'" : ""),
 388                      C-&gt;unique() - nodes);
 389     }
 390     // Push the result from the inlined method onto the stack.
 391     kit.push_result();
 392     C-&gt;print_inlining_update(this);
 393     return kit.transfer_exceptions_into_jvms();
 394   }
 395 
 396   // The intrinsic bailed out
 397   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 398     if (jvms-&gt;has_method()) {
 399       // Not a root compile.
 400       const char* msg;
 401       if (callee-&gt;intrinsic_candidate()) {
 402         msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 403       } else {
 404         msg = is_virtual() ? "failed to inline (intrinsic, virtual), method not annotated"
 405                            : "failed to inline (intrinsic), method not annotated";
 406       }
 407       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 408     } else {
 409       // Root compile
 410       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 411                vmIntrinsics::name_at(intrinsic_id()),
 412                (is_virtual() ? " (virtual)" : ""), bci);
 413     }
 414   }
 415   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 416   C-&gt;print_inlining_update(this);
 417   return NULL;
 418 }
 419 
 420 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 421   LibraryCallKit kit(jvms, this);
 422   Compile* C = kit.C;
 423   int nodes = C-&gt;unique();
 424   _last_predicate = predicate;
 425 #ifndef PRODUCT
 426   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 427   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 428     char buf[1000];
 429     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 430     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 431   }
 432 #endif
 433   ciMethod* callee = kit.callee();
 434   const int bci    = kit.bci();
 435 
 436   Node* slow_ctl = kit.try_to_predicate(predicate);
 437   if (!kit.failing()) {
 438     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 439       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 440     }
 441     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 442     if (C-&gt;log()) {
 443       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 444                      vmIntrinsics::name_at(intrinsic_id()),
 445                      (is_virtual() ? " virtual='1'" : ""),
 446                      C-&gt;unique() - nodes);
 447     }
 448     return slow_ctl; // Could be NULL if the check folds.
 449   }
 450 
 451   // The intrinsic bailed out
 452   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 453     if (jvms-&gt;has_method()) {
 454       // Not a root compile.
 455       const char* msg = "failed to generate predicate for intrinsic";
 456       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 457     } else {
 458       // Root compile
 459       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 460                                         vmIntrinsics::name_at(intrinsic_id()),
 461                                         (is_virtual() ? " (virtual)" : ""), bci);
 462     }
 463   }
 464   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 465   return NULL;
 466 }
 467 
 468 bool LibraryCallKit::try_to_inline(int predicate) {
 469   // Handle symbolic names for otherwise undistinguished boolean switches:
 470   const bool is_store       = true;
 471   const bool is_compress    = true;
 472   const bool is_native_ptr  = true;
 473   const bool is_static      = true;
 474   const bool is_volatile    = true;
 475 
 476   if (!jvms()-&gt;has_method()) {
 477     // Root JVMState has a null method.
 478     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 479     // Insert the memory aliasing node
 480     set_all_memory(reset_memory());
 481   }
 482   assert(merged_memory(), "");
 483 
 484 
 485   switch (intrinsic_id()) {
 486   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 487   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 488   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 489 
 490   case vmIntrinsics::_dsin:
 491   case vmIntrinsics::_dcos:
 492   case vmIntrinsics::_dtan:
 493   case vmIntrinsics::_dabs:
 494   case vmIntrinsics::_datan2:
 495   case vmIntrinsics::_dsqrt:
 496   case vmIntrinsics::_dexp:
 497   case vmIntrinsics::_dlog:
 498   case vmIntrinsics::_dlog10:
 499   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 500 
 501   case vmIntrinsics::_min:
 502   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 503 
 504   case vmIntrinsics::_notify:
 505   case vmIntrinsics::_notifyAll:
 506     if (InlineNotify) {
 507       return inline_notify(intrinsic_id());
 508     }
 509     return false;
 510 
 511   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 512   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 513   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 514   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 515   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 516   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 517   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 518   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 519   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 520   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 521   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 522   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 523 
 524   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 525 
 526   case vmIntrinsics::_compareToL:               return inline_string_compareTo(StrIntrinsicNode::LL);
 527   case vmIntrinsics::_compareToU:               return inline_string_compareTo(StrIntrinsicNode::UU);
 528   case vmIntrinsics::_compareToLU:              return inline_string_compareTo(StrIntrinsicNode::LU);
 529   case vmIntrinsics::_compareToUL:              return inline_string_compareTo(StrIntrinsicNode::UL);
 530 
 531   case vmIntrinsics::_indexOfL:                 return inline_string_indexOf(StrIntrinsicNode::LL);
 532   case vmIntrinsics::_indexOfU:                 return inline_string_indexOf(StrIntrinsicNode::UU);
 533   case vmIntrinsics::_indexOfUL:                return inline_string_indexOf(StrIntrinsicNode::UL);
 534   case vmIntrinsics::_indexOfIL:                return inline_string_indexOfI(StrIntrinsicNode::LL);
 535   case vmIntrinsics::_indexOfIU:                return inline_string_indexOfI(StrIntrinsicNode::UU);
 536   case vmIntrinsics::_indexOfIUL:               return inline_string_indexOfI(StrIntrinsicNode::UL);
 537   case vmIntrinsics::_indexOfU_char:            return inline_string_indexOfChar();
 538 
 539   case vmIntrinsics::_equalsL:                  return inline_string_equals(StrIntrinsicNode::LL);
 540   case vmIntrinsics::_equalsU:                  return inline_string_equals(StrIntrinsicNode::UU);
 541 
 542   case vmIntrinsics::_toBytesStringU:           return inline_string_toBytesU();
 543   case vmIntrinsics::_getCharsStringU:          return inline_string_getCharsU();
 544   case vmIntrinsics::_getCharStringU:           return inline_string_char_access(!is_store);
 545   case vmIntrinsics::_putCharStringU:           return inline_string_char_access( is_store);
 546 
 547   case vmIntrinsics::_compressStringC:
 548   case vmIntrinsics::_compressStringB:          return inline_string_copy( is_compress);
 549   case vmIntrinsics::_inflateStringC:
 550   case vmIntrinsics::_inflateStringB:           return inline_string_copy(!is_compress);
 551 
 552   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile, false);
 553   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile, false);
 554   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile, false);
 555   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile, false);
 556   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile, false);
 557   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile, false);
 558   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile, false);
 559   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile, false);
 560   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile, false);
 561   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile, false);
 562   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile, false);
 563   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile, false);
 564   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile, false);
 565   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile, false);
 566   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile, false);
 567   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile, false);
 568   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile, false);
 569   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile, false);
 570 
 571   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile, false);
 572   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile, false);
 573   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile, false);
 574   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile, false);
 575   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile, false);
 576   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile, false);
 577   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile, false);
 578   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile, false);
 579 
 580   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile, false);
 581   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile, false);
 582   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile, false);
 583   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile, false);
 584   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile, false);
 585   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile, false);
 586   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile, false);
 587   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile, false);
 588 
 589   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile, false);
 590   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile, false);
 591   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile, false);
 592   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile, false);
 593   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile, false);
 594   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile, false);
 595   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile, false);
 596   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile, false);
 597   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile, false);
 598 
 599   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile, false);
 600   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile, false);
 601   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile, false);
 602   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile, false);
 603   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile, false);
 604   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile, false);
 605   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile, false);
 606   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile, false);
 607   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile, false);
 608 
 609   case vmIntrinsics::_getShortUnaligned:        return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile, true);
 610   case vmIntrinsics::_getCharUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile, true);
 611   case vmIntrinsics::_getIntUnaligned:          return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile, true);
 612   case vmIntrinsics::_getLongUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile, true);
 613 
 614   case vmIntrinsics::_putShortUnaligned:        return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile, true);
 615   case vmIntrinsics::_putCharUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile, true);
 616   case vmIntrinsics::_putIntUnaligned:          return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile, true);
 617   case vmIntrinsics::_putLongUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile, true);
 618 
 619   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 620   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 621   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 622 
 623   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 624   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 625   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 626 
 627   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 628   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 629   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 630   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 631   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 632 
 633   case vmIntrinsics::_loadFence:
 634   case vmIntrinsics::_storeFence:
 635   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 636 
<a name="2" id="anc2"></a>

 637   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 638   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 639 
 640 #ifdef TRACE_HAVE_INTRINSICS
 641   case vmIntrinsics::_classID:                  return inline_native_classID();
 642   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 643   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 644 #endif
 645   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 646   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 647   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 648   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 649   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 650   case vmIntrinsics::_getLength:                return inline_native_getLength();
 651   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 652   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 653   case vmIntrinsics::_equalsB:                  return inline_array_equals(StrIntrinsicNode::LL);
 654   case vmIntrinsics::_equalsC:                  return inline_array_equals(StrIntrinsicNode::UU);
 655   case vmIntrinsics::_Objects_checkIndex:       return inline_objects_checkIndex();
 656   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 657 
 658   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 659 
 660   case vmIntrinsics::_isInstance:
 661   case vmIntrinsics::_getModifiers:
 662   case vmIntrinsics::_isInterface:
 663   case vmIntrinsics::_isArray:
 664   case vmIntrinsics::_isPrimitive:
 665   case vmIntrinsics::_getSuperclass:
 666   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 667 
 668   case vmIntrinsics::_floatToRawIntBits:
 669   case vmIntrinsics::_floatToIntBits:
 670   case vmIntrinsics::_intBitsToFloat:
 671   case vmIntrinsics::_doubleToRawLongBits:
 672   case vmIntrinsics::_doubleToLongBits:
 673   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 674 
 675   case vmIntrinsics::_numberOfLeadingZeros_i:
 676   case vmIntrinsics::_numberOfLeadingZeros_l:
 677   case vmIntrinsics::_numberOfTrailingZeros_i:
 678   case vmIntrinsics::_numberOfTrailingZeros_l:
 679   case vmIntrinsics::_bitCount_i:
 680   case vmIntrinsics::_bitCount_l:
 681   case vmIntrinsics::_reverseBytes_i:
 682   case vmIntrinsics::_reverseBytes_l:
 683   case vmIntrinsics::_reverseBytes_s:
 684   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 685 
 686   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 687 
 688   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 689 
 690   case vmIntrinsics::_Class_cast:               return inline_Class_cast();
 691 
 692   case vmIntrinsics::_aescrypt_encryptBlock:
 693   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 694 
 695   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 696   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 697     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 698 
 699   case vmIntrinsics::_sha_implCompress:
 700   case vmIntrinsics::_sha2_implCompress:
 701   case vmIntrinsics::_sha5_implCompress:
 702     return inline_sha_implCompress(intrinsic_id());
 703 
 704   case vmIntrinsics::_digestBase_implCompressMB:
 705     return inline_digestBase_implCompressMB(predicate);
 706 
 707   case vmIntrinsics::_multiplyToLen:
 708     return inline_multiplyToLen();
 709 
 710   case vmIntrinsics::_squareToLen:
 711     return inline_squareToLen();
 712 
 713   case vmIntrinsics::_mulAdd:
 714     return inline_mulAdd();
 715 
 716   case vmIntrinsics::_montgomeryMultiply:
 717     return inline_montgomeryMultiply();
 718   case vmIntrinsics::_montgomerySquare:
 719     return inline_montgomerySquare();
 720 
 721   case vmIntrinsics::_ghash_processBlocks:
 722     return inline_ghash_processBlocks();
 723 
 724   case vmIntrinsics::_encodeISOArray:
 725   case vmIntrinsics::_encodeByteISOArray:
 726     return inline_encodeISOArray();
 727 
 728   case vmIntrinsics::_updateCRC32:
 729     return inline_updateCRC32();
 730   case vmIntrinsics::_updateBytesCRC32:
 731     return inline_updateBytesCRC32();
 732   case vmIntrinsics::_updateByteBufferCRC32:
 733     return inline_updateByteBufferCRC32();
 734 
 735   case vmIntrinsics::_updateBytesCRC32C:
 736     return inline_updateBytesCRC32C();
 737   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 738     return inline_updateDirectByteBufferCRC32C();
 739 
 740   case vmIntrinsics::_updateBytesAdler32:
 741     return inline_updateBytesAdler32();
 742   case vmIntrinsics::_updateByteBufferAdler32:
 743     return inline_updateByteBufferAdler32();
 744 
 745   case vmIntrinsics::_profileBoolean:
 746     return inline_profileBoolean();
 747   case vmIntrinsics::_isCompileConstant:
 748     return inline_isCompileConstant();
 749 
 750   case vmIntrinsics::_hasNegatives:
 751     return inline_hasNegatives();
 752 
 753   default:
 754     // If you get here, it may be that someone has added a new intrinsic
 755     // to the list in vmSymbols.hpp without implementing it here.
 756 #ifndef PRODUCT
 757     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 758       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 759                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 760     }
 761 #endif
 762     return false;
 763   }
 764 }
 765 
 766 Node* LibraryCallKit::try_to_predicate(int predicate) {
 767   if (!jvms()-&gt;has_method()) {
 768     // Root JVMState has a null method.
 769     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 770     // Insert the memory aliasing node
 771     set_all_memory(reset_memory());
 772   }
 773   assert(merged_memory(), "");
 774 
 775   switch (intrinsic_id()) {
 776   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 777     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 778   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 779     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 780   case vmIntrinsics::_digestBase_implCompressMB:
 781     return inline_digestBase_implCompressMB_predicate(predicate);
 782 
 783   default:
 784     // If you get here, it may be that someone has added a new intrinsic
 785     // to the list in vmSymbols.hpp without implementing it here.
 786 #ifndef PRODUCT
 787     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 788       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 789                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 790     }
 791 #endif
 792     Node* slow_ctl = control();
 793     set_control(top()); // No fast path instrinsic
 794     return slow_ctl;
 795   }
 796 }
 797 
 798 //------------------------------set_result-------------------------------
 799 // Helper function for finishing intrinsics.
 800 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 801   record_for_igvn(region);
 802   set_control(_gvn.transform(region));
 803   set_result( _gvn.transform(value));
 804   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
 805 }
 806 
 807 //------------------------------generate_guard---------------------------
 808 // Helper function for generating guarded fast-slow graph structures.
 809 // The given 'test', if true, guards a slow path.  If the test fails
 810 // then a fast path can be taken.  (We generally hope it fails.)
 811 // In all cases, GraphKit::control() is updated to the fast path.
 812 // The returned value represents the control for the slow path.
 813 // The return value is never 'top'; it is either a valid control
 814 // or NULL if it is obvious that the slow path can never be taken.
 815 // Also, if region and the slow control are not NULL, the slow edge
 816 // is appended to the region.
 817 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
 818   if (stopped()) {
 819     // Already short circuited.
 820     return NULL;
 821   }
 822 
 823   // Build an if node and its projections.
 824   // If test is true we take the slow path, which we assume is uncommon.
 825   if (_gvn.type(test) == TypeInt::ZERO) {
 826     // The slow branch is never taken.  No need to build this guard.
 827     return NULL;
 828   }
 829 
 830   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
 831 
 832   Node* if_slow = _gvn.transform(new IfTrueNode(iff));
 833   if (if_slow == top()) {
 834     // The slow branch is never taken.  No need to build this guard.
 835     return NULL;
 836   }
 837 
 838   if (region != NULL)
 839     region-&gt;add_req(if_slow);
 840 
 841   Node* if_fast = _gvn.transform(new IfFalseNode(iff));
 842   set_control(if_fast);
 843 
 844   return if_slow;
 845 }
 846 
 847 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
 848   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
 849 }
 850 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
 851   return generate_guard(test, region, PROB_FAIR);
 852 }
 853 
 854 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
 855                                                      Node* *pos_index) {
 856   if (stopped())
 857     return NULL;                // already stopped
 858   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
 859     return NULL;                // index is already adequately typed
 860   Node* cmp_lt = _gvn.transform(new CmpINode(index, intcon(0)));
 861   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 862   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
 863   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
 864     // Emulate effect of Parse::adjust_map_after_if.
 865     Node* ccast = new CastIINode(index, TypeInt::POS);
 866     ccast-&gt;set_req(0, control());
 867     (*pos_index) = _gvn.transform(ccast);
 868   }
 869   return is_neg;
 870 }
 871 
 872 // Make sure that 'position' is a valid limit index, in [0..length].
 873 // There are two equivalent plans for checking this:
 874 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
 875 //   B. offset  &lt;=  (arrayLength - copyLength)
 876 // We require that all of the values above, except for the sum and
 877 // difference, are already known to be non-negative.
 878 // Plan A is robust in the face of overflow, if offset and copyLength
 879 // are both hugely positive.
 880 //
 881 // Plan B is less direct and intuitive, but it does not overflow at
 882 // all, since the difference of two non-negatives is always
 883 // representable.  Whenever Java methods must perform the equivalent
 884 // check they generally use Plan B instead of Plan A.
 885 // For the moment we use Plan A.
 886 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
 887                                                   Node* subseq_length,
 888                                                   Node* array_length,
 889                                                   RegionNode* region) {
 890   if (stopped())
 891     return NULL;                // already stopped
 892   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
 893   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
 894     return NULL;                // common case of whole-array copy
 895   Node* last = subseq_length;
 896   if (!zero_offset)             // last += offset
 897     last = _gvn.transform(new AddINode(last, offset));
 898   Node* cmp_lt = _gvn.transform(new CmpUNode(array_length, last));
 899   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 900   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
 901   return is_over;
 902 }
 903 
 904 // Emit range checks for the given String.value byte array
 905 void LibraryCallKit::generate_string_range_check(Node* array, Node* offset, Node* count, bool char_count) {
 906   if (stopped()) {
 907     return; // already stopped
 908   }
 909   RegionNode* bailout = new RegionNode(1);
 910   record_for_igvn(bailout);
 911   if (char_count) {
 912     // Convert char count to byte count
 913     count = _gvn.transform(new LShiftINode(count, intcon(1)));
 914   }
 915 
 916   // Offset and count must not be negative
 917   generate_negative_guard(offset, bailout);
 918   generate_negative_guard(count, bailout);
 919   // Offset + count must not exceed length of array
 920   generate_limit_guard(offset, count, load_array_length(array), bailout);
 921 
 922   if (bailout-&gt;req() &gt; 1) {
 923     PreserveJVMState pjvms(this);
 924     set_control(_gvn.transform(bailout));
 925     uncommon_trap(Deoptimization::Reason_intrinsic,
 926                   Deoptimization::Action_maybe_recompile);
 927   }
 928 }
 929 
 930 //--------------------------generate_current_thread--------------------
 931 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
 932   ciKlass*    thread_klass = env()-&gt;Thread_klass();
 933   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
 934   Node* thread = _gvn.transform(new ThreadLocalNode());
 935   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
 936   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
 937   tls_output = thread;
 938   return threadObj;
 939 }
 940 
 941 
 942 //------------------------------make_string_method_node------------------------
 943 // Helper method for String intrinsic functions. This version is called with
 944 // str1 and str2 pointing to byte[] nodes containing Latin1 or UTF16 encoded
 945 // characters (depending on 'is_byte'). cnt1 and cnt2 are pointing to Int nodes
 946 // containing the lengths of str1 and str2.
 947 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae) {
 948   Node* result = NULL;
 949   switch (opcode) {
 950   case Op_StrIndexOf:
 951     result = new StrIndexOfNode(control(), memory(TypeAryPtr::BYTES),
 952                                 str1_start, cnt1, str2_start, cnt2, ae);
 953     break;
 954   case Op_StrComp:
 955     result = new StrCompNode(control(), memory(TypeAryPtr::BYTES),
 956                              str1_start, cnt1, str2_start, cnt2, ae);
 957     break;
 958   case Op_StrEquals:
 959     result = new StrEqualsNode(control(), memory(TypeAryPtr::BYTES),
 960                                str1_start, str2_start, cnt1, ae);
 961     break;
 962   default:
 963     ShouldNotReachHere();
 964     return NULL;
 965   }
 966 
 967   // All these intrinsics have checks.
 968   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
 969 
 970   return _gvn.transform(result);
 971 }
 972 
 973 //------------------------------inline_string_compareTo------------------------
 974 bool LibraryCallKit::inline_string_compareTo(StrIntrinsicNode::ArgEnc ae) {
 975   Node* arg1 = argument(0);
 976   Node* arg2 = argument(1);
 977 
 978   // Get start addr and length of first argument
 979   Node* arg1_start  = array_element_address(arg1, intcon(0), T_BYTE);
 980   Node* arg1_cnt    = load_array_length(arg1);
 981 
 982   // Get start addr and length of second argument
 983   Node* arg2_start  = array_element_address(arg2, intcon(0), T_BYTE);
 984   Node* arg2_cnt    = load_array_length(arg2);
 985 
 986   Node* result = make_string_method_node(Op_StrComp, arg1_start, arg1_cnt, arg2_start, arg2_cnt, ae);
 987   set_result(result);
 988   return true;
 989 }
 990 
 991 //------------------------------inline_string_equals------------------------
 992 bool LibraryCallKit::inline_string_equals(StrIntrinsicNode::ArgEnc ae) {
 993   Node* arg1 = argument(0);
 994   Node* arg2 = argument(1);
 995 
 996   // paths (plus control) merge
 997   RegionNode* region = new RegionNode(3);
 998   Node* phi = new PhiNode(region, TypeInt::BOOL);
 999 
1000   if (!stopped()) {
1001     // Get start addr and length of first argument
1002     Node* arg1_start  = array_element_address(arg1, intcon(0), T_BYTE);
1003     Node* arg1_cnt    = load_array_length(arg1);
1004 
1005     // Get start addr and length of second argument
1006     Node* arg2_start  = array_element_address(arg2, intcon(0), T_BYTE);
1007     Node* arg2_cnt    = load_array_length(arg2);
1008 
1009     // Check for arg1_cnt != arg2_cnt
1010     Node* cmp = _gvn.transform(new CmpINode(arg1_cnt, arg2_cnt));
1011     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1012     Node* if_ne = generate_slow_guard(bol, NULL);
1013     if (if_ne != NULL) {
1014       phi-&gt;init_req(2, intcon(0));
1015       region-&gt;init_req(2, if_ne);
1016     }
1017 
1018     // Check for count == 0 is done by assembler code for StrEquals.
1019 
1020     if (!stopped()) {
1021       Node* equals = make_string_method_node(Op_StrEquals, arg1_start, arg1_cnt, arg2_start, arg2_cnt, ae);
1022       phi-&gt;init_req(1, equals);
1023       region-&gt;init_req(1, control());
1024     }
1025   }
1026 
1027   // post merge
1028   set_control(_gvn.transform(region));
1029   record_for_igvn(region);
1030 
1031   set_result(_gvn.transform(phi));
1032   return true;
1033 }
1034 
1035 //------------------------------inline_array_equals----------------------------
1036 bool LibraryCallKit::inline_array_equals(StrIntrinsicNode::ArgEnc ae) {
1037   assert(ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::LL, "unsupported array types");
1038   Node* arg1 = argument(0);
1039   Node* arg2 = argument(1);
1040 
1041   const TypeAryPtr* mtype = (ae == StrIntrinsicNode::UU) ? TypeAryPtr::CHARS : TypeAryPtr::BYTES;
1042   set_result(_gvn.transform(new AryEqNode(control(), memory(mtype), arg1, arg2, ae)));
1043   return true;
1044 }
1045 
1046 //------------------------------inline_hasNegatives------------------------------
1047 bool LibraryCallKit::inline_hasNegatives() {
1048   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1049     return false;
1050   }
1051 
1052   assert(callee()-&gt;signature()-&gt;size() == 3, "hasNegatives has 3 parameters");
1053   // no receiver since it is static method
1054   Node* ba         = argument(0);
1055   Node* offset     = argument(1);
1056   Node* len        = argument(2);
1057 
1058   // Range checks
1059   generate_string_range_check(ba, offset, len, false);
1060   if (stopped()) {
1061     return true;
1062   }
1063   Node* ba_start = array_element_address(ba, offset, T_BYTE);
1064   Node* result = new HasNegativesNode(control(), memory(TypeAryPtr::BYTES), ba_start, len);
1065   set_result(_gvn.transform(result));
1066   return true;
1067 }
1068 
1069 bool LibraryCallKit::inline_objects_checkIndex() {
1070   Node* index = argument(0);
1071   Node* length = argument(1);
1072   if (too_many_traps(Deoptimization::Reason_intrinsic) || too_many_traps(Deoptimization::Reason_range_check)) {
1073     return false;
1074   }
1075 
1076   Node* len_pos_cmp = _gvn.transform(new CmpINode(length, intcon(0)));
1077   Node* len_pos_bol = _gvn.transform(new BoolNode(len_pos_cmp, BoolTest::ge));
1078 
1079   {
1080     BuildCutout unless(this, len_pos_bol, PROB_MAX);
1081     uncommon_trap(Deoptimization::Reason_intrinsic,
1082                   Deoptimization::Action_make_not_entrant);
1083   }
1084 
1085   if (stopped()) {
1086     return false;
1087   }
1088 
1089   Node* rc_cmp = _gvn.transform(new CmpUNode(index, length));
1090   BoolTest::mask btest = BoolTest::lt;
1091   Node* rc_bool = _gvn.transform(new BoolNode(rc_cmp, btest));
1092   RangeCheckNode* rc = new RangeCheckNode(control(), rc_bool, PROB_MAX, COUNT_UNKNOWN);
1093   _gvn.set_type(rc, rc-&gt;Value(&amp;_gvn));
1094   if (!rc_bool-&gt;is_Con()) {
1095     record_for_igvn(rc);
1096   }
1097   set_control(_gvn.transform(new IfTrueNode(rc)));
1098   {
1099     PreserveJVMState pjvms(this);
1100     set_control(_gvn.transform(new IfFalseNode(rc)));
1101     uncommon_trap(Deoptimization::Reason_range_check,
1102                   Deoptimization::Action_make_not_entrant);
1103   }
1104 
1105   if (stopped()) {
1106     return false;
1107   }
1108 
1109   Node* result = new CastIINode(index, TypeInt::make(0, _gvn.type(length)-&gt;is_int()-&gt;_hi, Type::WidenMax));
1110   result-&gt;set_req(0, control());
1111   result = _gvn.transform(result);
1112   set_result(result);
1113   replace_in_map(index, result);
1114   return true;
1115 }
1116 
1117 //------------------------------inline_string_indexOf------------------------
1118 bool LibraryCallKit::inline_string_indexOf(StrIntrinsicNode::ArgEnc ae) {
1119   if (!Matcher::has_match_rule(Op_StrIndexOf) || !UseSSE42Intrinsics) {
1120     return false;
1121   }
1122   Node* src = argument(0);
1123   Node* tgt = argument(1);
1124 
1125   // Make the merge point
1126   RegionNode* result_rgn = new RegionNode(4);
1127   Node*       result_phi = new PhiNode(result_rgn, TypeInt::INT);
1128 
1129   // Get start addr and length of source string
1130   Node* src_start = array_element_address(src, intcon(0), T_BYTE);
1131   Node* src_count = load_array_length(src);
1132 
1133   // Get start addr and length of substring
1134   Node* tgt_start = array_element_address(tgt, intcon(0), T_BYTE);
1135   Node* tgt_count = load_array_length(tgt);
1136 
1137   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1138     // Divide src size by 2 if String is UTF16 encoded
1139     src_count = _gvn.transform(new RShiftINode(src_count, intcon(1)));
1140   }
1141   if (ae == StrIntrinsicNode::UU) {
1142     // Divide substring size by 2 if String is UTF16 encoded
1143     tgt_count = _gvn.transform(new RShiftINode(tgt_count, intcon(1)));
1144   }
1145 
1146   Node* result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, result_rgn, result_phi, ae);
1147   if (result != NULL) {
1148     result_phi-&gt;init_req(3, result);
1149     result_rgn-&gt;init_req(3, control());
1150   }
1151   set_control(_gvn.transform(result_rgn));
1152   record_for_igvn(result_rgn);
1153   set_result(_gvn.transform(result_phi));
1154 
1155   return true;
1156 }
1157 
1158 //-----------------------------inline_string_indexOf-----------------------
1159 bool LibraryCallKit::inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae) {
1160   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1161     return false;
1162   }
1163   if (!Matcher::has_match_rule(Op_StrIndexOf) || !UseSSE42Intrinsics) {
1164     return false;
1165   }
1166   assert(callee()-&gt;signature()-&gt;size() == 5, "String.indexOf() has 5 arguments");
1167   Node* src         = argument(0); // byte[]
1168   Node* src_count   = argument(1); // char count
1169   Node* tgt         = argument(2); // byte[]
1170   Node* tgt_count   = argument(3); // char count
1171   Node* from_index  = argument(4); // char index
1172 
1173   // Multiply byte array index by 2 if String is UTF16 encoded
1174   Node* src_offset = (ae == StrIntrinsicNode::LL) ? from_index : _gvn.transform(new LShiftINode(from_index, intcon(1)));
1175   src_count = _gvn.transform(new SubINode(src_count, from_index));
1176   Node* src_start = array_element_address(src, src_offset, T_BYTE);
1177   Node* tgt_start = array_element_address(tgt, intcon(0), T_BYTE);
1178 
1179   // Range checks
1180   generate_string_range_check(src, src_offset, src_count, ae != StrIntrinsicNode::LL);
1181   generate_string_range_check(tgt, intcon(0), tgt_count, ae == StrIntrinsicNode::UU);
1182   if (stopped()) {
1183     return true;
1184   }
1185 
1186   RegionNode* region = new RegionNode(5);
1187   Node* phi = new PhiNode(region, TypeInt::INT);
1188 
1189   Node* result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, region, phi, ae);
1190   if (result != NULL) {
1191     // The result is index relative to from_index if substring was found, -1 otherwise.
1192     // Generate code which will fold into cmove.
1193     Node* cmp = _gvn.transform(new CmpINode(result, intcon(0)));
1194     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::lt));
1195 
1196     Node* if_lt = generate_slow_guard(bol, NULL);
1197     if (if_lt != NULL) {
1198       // result == -1
1199       phi-&gt;init_req(3, result);
1200       region-&gt;init_req(3, if_lt);
1201     }
1202     if (!stopped()) {
1203       result = _gvn.transform(new AddINode(result, from_index));
1204       phi-&gt;init_req(4, result);
1205       region-&gt;init_req(4, control());
1206     }
1207   }
1208 
1209   set_control(_gvn.transform(region));
1210   record_for_igvn(region);
1211   set_result(_gvn.transform(phi));
1212 
1213   return true;
1214 }
1215 
1216 // Create StrIndexOfNode with fast path checks
1217 Node* LibraryCallKit::make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
1218                                         RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae) {
1219   // Check for substr count &gt; string count
1220   Node* cmp = _gvn.transform(new CmpINode(tgt_count, src_count));
1221   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::gt));
1222   Node* if_gt = generate_slow_guard(bol, NULL);
1223   if (if_gt != NULL) {
1224     phi-&gt;init_req(1, intcon(-1));
1225     region-&gt;init_req(1, if_gt);
1226   }
1227   if (!stopped()) {
1228     // Check for substr count == 0
1229     cmp = _gvn.transform(new CmpINode(tgt_count, intcon(0)));
1230     bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1231     Node* if_zero = generate_slow_guard(bol, NULL);
1232     if (if_zero != NULL) {
1233       phi-&gt;init_req(2, intcon(0));
1234       region-&gt;init_req(2, if_zero);
1235     }
1236   }
1237   if (!stopped()) {
1238     return make_string_method_node(Op_StrIndexOf, src_start, src_count, tgt_start, tgt_count, ae);
1239   }
1240   return NULL;
1241 }
1242 
1243 //-----------------------------inline_string_indexOfChar-----------------------
1244 bool LibraryCallKit::inline_string_indexOfChar() {
1245   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1246     return false;
1247   }
1248   if (!Matcher::has_match_rule(Op_StrIndexOfChar) || !(UseSSE &gt; 4)) {
1249     return false;
1250   }
1251   assert(callee()-&gt;signature()-&gt;size() == 4, "String.indexOfChar() has 4 arguments");
1252   Node* src         = argument(0); // byte[]
1253   Node* tgt         = argument(1); // tgt is int ch
1254   Node* from_index  = argument(2);
1255   Node* max         = argument(3);
1256 
1257   Node* src_offset = _gvn.transform(new LShiftINode(from_index, intcon(1)));
1258   Node* src_start = array_element_address(src, src_offset, T_BYTE);
1259   Node* src_count = _gvn.transform(new SubINode(max, from_index));
1260 
1261   // Range checks
1262   generate_string_range_check(src, src_offset, src_count, true);
1263   if (stopped()) {
1264     return true;
1265   }
1266 
1267   RegionNode* region = new RegionNode(3);
1268   Node* phi = new PhiNode(region, TypeInt::INT);
1269 
1270   Node* result = new StrIndexOfCharNode(control(), memory(TypeAryPtr::BYTES), src_start, src_count, tgt, StrIntrinsicNode::none);
1271   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1272   _gvn.transform(result);
1273 
1274   Node* cmp = _gvn.transform(new CmpINode(result, intcon(0)));
1275   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::lt));
1276 
1277   Node* if_lt = generate_slow_guard(bol, NULL);
1278   if (if_lt != NULL) {
1279     // result == -1
1280     phi-&gt;init_req(2, result);
1281     region-&gt;init_req(2, if_lt);
1282   }
1283   if (!stopped()) {
1284     result = _gvn.transform(new AddINode(result, from_index));
1285     phi-&gt;init_req(1, result);
1286     region-&gt;init_req(1, control());
1287   }
1288   set_control(_gvn.transform(region));
1289   record_for_igvn(region);
1290   set_result(_gvn.transform(phi));
1291 
1292   return true;
1293 }
1294 //---------------------------inline_string_copy---------------------
1295 // compressIt == true --&gt; generate a compressed copy operation (compress char[]/byte[] to byte[])
1296 //   int StringUTF16.compress(char[] src, int srcOff, byte[] dst, int dstOff, int len)
1297 //   int StringUTF16.compress(byte[] src, int srcOff, byte[] dst, int dstOff, int len)
1298 // compressIt == false --&gt; generate an inflated copy operation (inflate byte[] to char[]/byte[])
1299 //   void StringLatin1.inflate(byte[] src, int srcOff, char[] dst, int dstOff, int len)
1300 //   void StringLatin1.inflate(byte[] src, int srcOff, byte[] dst, int dstOff, int len)
1301 bool LibraryCallKit::inline_string_copy(bool compress) {
1302   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1303     return false;
1304   }
1305   int nargs = 5;  // 2 oops, 3 ints
1306   assert(callee()-&gt;signature()-&gt;size() == nargs, "string copy has 5 arguments");
1307 
1308   Node* src         = argument(0);
1309   Node* src_offset  = argument(1);
1310   Node* dst         = argument(2);
1311   Node* dst_offset  = argument(3);
1312   Node* length      = argument(4);
1313 
1314   // Check for allocation before we add nodes that would confuse
1315   // tightly_coupled_allocation()
1316   AllocateArrayNode* alloc = tightly_coupled_allocation(dst, NULL);
1317 
1318   // Figure out the size and type of the elements we will be copying.
1319   const Type* src_type = src-&gt;Value(&amp;_gvn);
1320   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
1321   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
1322   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
1323   assert((compress &amp;&amp; dst_elem == T_BYTE &amp;&amp; (src_elem == T_BYTE || src_elem == T_CHAR)) ||
1324          (!compress &amp;&amp; src_elem == T_BYTE &amp;&amp; (dst_elem == T_BYTE || dst_elem == T_CHAR)),
1325          "Unsupported array types for inline_string_copy");
1326 
1327   // Range checks
1328   generate_string_range_check(src, src_offset, length, compress &amp;&amp; src_elem == T_BYTE);
1329   generate_string_range_check(dst, dst_offset, length, !compress &amp;&amp; dst_elem == T_BYTE);
1330   if (stopped()) {
1331     return true;
1332   }
1333 
1334   // Convert char[] offsets to byte[] offsets
1335   if (compress &amp;&amp; src_elem == T_BYTE) {
1336     src_offset = _gvn.transform(new LShiftINode(src_offset, intcon(1)));
1337   } else if (!compress &amp;&amp; dst_elem == T_BYTE) {
1338     dst_offset = _gvn.transform(new LShiftINode(dst_offset, intcon(1)));
1339   }
1340 
1341   Node* src_start = array_element_address(src, src_offset, src_elem);
1342   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
1343   // 'src_start' points to src array + scaled offset
1344   // 'dst_start' points to dst array + scaled offset
1345   Node* count = NULL;
1346   if (compress) {
1347     count = compress_string(src_start, dst_start, length);
1348   } else {
1349     inflate_string(src_start, dst_start, length);
1350   }
1351 
1352   if (alloc != NULL) {
1353     if (alloc-&gt;maybe_set_complete(&amp;_gvn)) {
1354       // "You break it, you buy it."
1355       InitializeNode* init = alloc-&gt;initialization();
1356       assert(init-&gt;is_complete(), "we just did this");
1357       init-&gt;set_complete_with_arraycopy();
1358       assert(dst-&gt;is_CheckCastPP(), "sanity");
1359       assert(dst-&gt;in(0)-&gt;in(0) == init, "dest pinned");
1360     }
1361     // Do not let stores that initialize this object be reordered with
1362     // a subsequent store that would make this object accessible by
1363     // other threads.
1364     // Record what AllocateNode this StoreStore protects so that
1365     // escape analysis can go from the MemBarStoreStoreNode to the
1366     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
1367     // based on the escape status of the AllocateNode.
1368     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
1369   }
1370   if (compress) {
1371     set_result(_gvn.transform(count));
1372   }
1373   return true;
1374 }
1375 
1376 #ifdef _LP64
1377 #define XTOP ,top() /*additional argument*/
1378 #else  //_LP64
1379 #define XTOP        /*no additional argument*/
1380 #endif //_LP64
1381 
1382 //------------------------inline_string_toBytesU--------------------------
1383 // public static byte[] StringUTF16.toBytes(char[] value, int off, int len)
1384 bool LibraryCallKit::inline_string_toBytesU() {
1385   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1386     return false;
1387   }
1388   // Get the arguments.
1389   Node* value     = argument(0);
1390   Node* offset    = argument(1);
1391   Node* length    = argument(2);
1392 
1393   Node* newcopy = NULL;
1394 
1395   // Set the original stack and the reexecute bit for the interpreter to reexecute
1396   // the bytecode that invokes StringUTF16.toBytes() if deoptimization happens.
1397   { PreserveReexecuteState preexecs(this);
1398     jvms()-&gt;set_should_reexecute(true);
1399 
1400     // Check if a null path was taken unconditionally.
1401     value = null_check(value);
1402 
1403     RegionNode* bailout = new RegionNode(1);
1404     record_for_igvn(bailout);
1405 
1406     // Range checks
1407     generate_negative_guard(offset, bailout);
1408     generate_negative_guard(length, bailout);
1409     generate_limit_guard(offset, length, load_array_length(value), bailout);
1410     // Make sure that resulting byte[] length does not overflow Integer.MAX_VALUE
1411     generate_limit_guard(length, intcon(0), intcon(max_jint/2), bailout);
1412 
1413     if (bailout-&gt;req() &gt; 1) {
1414       PreserveJVMState pjvms(this);
1415       set_control(_gvn.transform(bailout));
1416       uncommon_trap(Deoptimization::Reason_intrinsic,
1417                     Deoptimization::Action_maybe_recompile);
1418     }
1419     if (stopped()) {
1420       return true;
1421     }
1422 
1423     Node* size = _gvn.transform(new LShiftINode(length, intcon(1)));
1424     Node* klass_node = makecon(TypeKlassPtr::make(ciTypeArrayKlass::make(T_BYTE)));
1425     newcopy = new_array(klass_node, size, 0);  // no arguments to push
1426     AllocateArrayNode* alloc = tightly_coupled_allocation(newcopy, NULL);
1427 
1428     // Calculate starting addresses.
1429     Node* src_start = array_element_address(value, offset, T_CHAR);
1430     Node* dst_start = basic_plus_adr(newcopy, arrayOopDesc::base_offset_in_bytes(T_BYTE));
1431 
1432     // Check if src array address is aligned to HeapWordSize (dst is always aligned)
1433     const TypeInt* toffset = gvn().type(offset)-&gt;is_int();
1434     bool aligned = toffset-&gt;is_con() &amp;&amp; ((toffset-&gt;get_con() * type2aelembytes(T_CHAR)) % HeapWordSize == 0);
1435 
1436     // Figure out which arraycopy runtime method to call (disjoint, uninitialized).
1437     const char* copyfunc_name = "arraycopy";
1438     address     copyfunc_addr = StubRoutines::select_arraycopy_function(T_CHAR, aligned, true, copyfunc_name, true);
1439     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
1440                       OptoRuntime::fast_arraycopy_Type(),
1441                       copyfunc_addr, copyfunc_name, TypeRawPtr::BOTTOM,
1442                       src_start, dst_start, ConvI2X(length) XTOP);
1443     // Do not let reads from the cloned object float above the arraycopy.
1444     if (alloc != NULL) {
1445       if (alloc-&gt;maybe_set_complete(&amp;_gvn)) {
1446         // "You break it, you buy it."
1447         InitializeNode* init = alloc-&gt;initialization();
1448         assert(init-&gt;is_complete(), "we just did this");
1449         init-&gt;set_complete_with_arraycopy();
1450         assert(newcopy-&gt;is_CheckCastPP(), "sanity");
1451         assert(newcopy-&gt;in(0)-&gt;in(0) == init, "dest pinned");
1452       }
1453       // Do not let stores that initialize this object be reordered with
1454       // a subsequent store that would make this object accessible by
1455       // other threads.
1456       // Record what AllocateNode this StoreStore protects so that
1457       // escape analysis can go from the MemBarStoreStoreNode to the
1458       // AllocateNode and eliminate the MemBarStoreStoreNode if possible
1459       // based on the escape status of the AllocateNode.
1460       insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
1461     } else {
1462       insert_mem_bar(Op_MemBarCPUOrder);
1463     }
1464   } // original reexecute is set back here
1465 
1466   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1467   if (!stopped()) {
1468     set_result(newcopy);
1469   }
1470   return true;
1471 }
1472 
1473 //------------------------inline_string_getCharsU--------------------------
1474 // public void StringUTF16.getChars(byte[] src, int srcBegin, int srcEnd, char dst[], int dstBegin)
1475 bool LibraryCallKit::inline_string_getCharsU() {
1476   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1477     return false;
1478   }
1479 
1480   // Get the arguments.
1481   Node* src       = argument(0);
1482   Node* src_begin = argument(1);
1483   Node* src_end   = argument(2); // exclusive offset (i &lt; src_end)
1484   Node* dst       = argument(3);
1485   Node* dst_begin = argument(4);
1486 
1487   // Check for allocation before we add nodes that would confuse
1488   // tightly_coupled_allocation()
1489   AllocateArrayNode* alloc = tightly_coupled_allocation(dst, NULL);
1490 
1491   // Check if a null path was taken unconditionally.
1492   src = null_check(src);
1493   dst = null_check(dst);
1494   if (stopped()) {
1495     return true;
1496   }
1497 
1498   // Get length and convert char[] offset to byte[] offset
1499   Node* length = _gvn.transform(new SubINode(src_end, src_begin));
1500   src_begin = _gvn.transform(new LShiftINode(src_begin, intcon(1)));
1501 
1502   // Range checks
1503   generate_string_range_check(src, src_begin, length, true);
1504   generate_string_range_check(dst, dst_begin, length, false);
1505   if (stopped()) {
1506     return true;
1507   }
1508 
1509   if (!stopped()) {
1510     // Calculate starting addresses.
1511     Node* src_start = array_element_address(src, src_begin, T_BYTE);
1512     Node* dst_start = array_element_address(dst, dst_begin, T_CHAR);
1513 
1514     // Check if array addresses are aligned to HeapWordSize
1515     const TypeInt* tsrc = gvn().type(src_begin)-&gt;is_int();
1516     const TypeInt* tdst = gvn().type(dst_begin)-&gt;is_int();
1517     bool aligned = tsrc-&gt;is_con() &amp;&amp; ((tsrc-&gt;get_con() * type2aelembytes(T_BYTE)) % HeapWordSize == 0) &amp;&amp;
1518                    tdst-&gt;is_con() &amp;&amp; ((tdst-&gt;get_con() * type2aelembytes(T_CHAR)) % HeapWordSize == 0);
1519 
1520     // Figure out which arraycopy runtime method to call (disjoint, uninitialized).
1521     const char* copyfunc_name = "arraycopy";
1522     address     copyfunc_addr = StubRoutines::select_arraycopy_function(T_CHAR, aligned, true, copyfunc_name, true);
1523     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
1524                       OptoRuntime::fast_arraycopy_Type(),
1525                       copyfunc_addr, copyfunc_name, TypeRawPtr::BOTTOM,
1526                       src_start, dst_start, ConvI2X(length) XTOP);
1527     // Do not let reads from the cloned object float above the arraycopy.
1528     if (alloc != NULL) {
1529       if (alloc-&gt;maybe_set_complete(&amp;_gvn)) {
1530         // "You break it, you buy it."
1531         InitializeNode* init = alloc-&gt;initialization();
1532         assert(init-&gt;is_complete(), "we just did this");
1533         init-&gt;set_complete_with_arraycopy();
1534         assert(dst-&gt;is_CheckCastPP(), "sanity");
1535         assert(dst-&gt;in(0)-&gt;in(0) == init, "dest pinned");
1536       }
1537       // Do not let stores that initialize this object be reordered with
1538       // a subsequent store that would make this object accessible by
1539       // other threads.
1540       // Record what AllocateNode this StoreStore protects so that
1541       // escape analysis can go from the MemBarStoreStoreNode to the
1542       // AllocateNode and eliminate the MemBarStoreStoreNode if possible
1543       // based on the escape status of the AllocateNode.
1544       insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
1545     } else {
1546       insert_mem_bar(Op_MemBarCPUOrder);
1547     }
1548   }
1549 
1550   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1551   return true;
1552 }
1553 
1554 //----------------------inline_string_char_access----------------------------
1555 // Store/Load char to/from byte[] array.
1556 // static void StringUTF16.putChar(byte[] val, int index, int c)
1557 // static char StringUTF16.getChar(byte[] val, int index)
1558 bool LibraryCallKit::inline_string_char_access(bool is_store) {
1559   Node* value  = argument(0);
1560   Node* index  = argument(1);
1561   Node* ch = is_store ? argument(2) : NULL;
1562 
1563   // This intrinsic accesses byte[] array as char[] array. Computing the offsets
1564   // correctly requires matched array shapes.
1565   assert (arrayOopDesc::base_offset_in_bytes(T_CHAR) == arrayOopDesc::base_offset_in_bytes(T_BYTE),
1566           "sanity: byte[] and char[] bases agree");
1567   assert (type2aelembytes(T_CHAR) == type2aelembytes(T_BYTE)*2,
1568           "sanity: byte[] and char[] scales agree");
1569 
1570   Node* adr = array_element_address(value, index, T_CHAR);
1571   if (is_store) {
1572     (void) store_to_memory(control(), adr, ch, T_CHAR, TypeAryPtr::BYTES, MemNode::unordered,
1573                            false, false, true /* mismatched */);
1574   } else {
1575     ch = make_load(control(), adr, TypeInt::CHAR, T_CHAR, MemNode::unordered,
1576                    LoadNode::DependsOnlyOnTest, false, false, true /* mismatched */);
1577     set_result(ch);
1578   }
1579   return true;
1580 }
1581 
1582 //--------------------------round_double_node--------------------------------
1583 // Round a double node if necessary.
1584 Node* LibraryCallKit::round_double_node(Node* n) {
1585   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1586     n = _gvn.transform(new RoundDoubleNode(0, n));
1587   return n;
1588 }
1589 
1590 //------------------------------inline_math-----------------------------------
1591 // public static double Math.abs(double)
1592 // public static double Math.sqrt(double)
1593 // public static double Math.log(double)
1594 // public static double Math.log10(double)
1595 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1596   Node* arg = round_double_node(argument(0));
1597   Node* n = NULL;
1598   switch (id) {
1599   case vmIntrinsics::_dabs:   n = new AbsDNode(                arg);  break;
1600   case vmIntrinsics::_dsqrt:  n = new SqrtDNode(C, control(),  arg);  break;
1601   case vmIntrinsics::_dlog10: n = new Log10DNode(C, control(), arg);  break;
1602   default:  fatal_unexpected_iid(id);  break;
1603   }
1604   set_result(_gvn.transform(n));
1605   return true;
1606 }
1607 
1608 //------------------------------inline_trig----------------------------------
1609 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1610 // argument reduction which will turn into a fast/slow diamond.
1611 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1612   Node* arg = round_double_node(argument(0));
1613   Node* n = NULL;
1614 
1615   switch (id) {
1616   case vmIntrinsics::_dsin:  n = new SinDNode(C, control(), arg);  break;
1617   case vmIntrinsics::_dcos:  n = new CosDNode(C, control(), arg);  break;
1618   case vmIntrinsics::_dtan:  n = new TanDNode(C, control(), arg);  break;
1619   default:  fatal_unexpected_iid(id);  break;
1620   }
1621   n = _gvn.transform(n);
1622 
1623   // Rounding required?  Check for argument reduction!
1624   if (Matcher::strict_fp_requires_explicit_rounding) {
1625     static const double     pi_4 =  0.7853981633974483;
1626     static const double neg_pi_4 = -0.7853981633974483;
1627     // pi/2 in 80-bit extended precision
1628     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1629     // -pi/2 in 80-bit extended precision
1630     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1631     // Cutoff value for using this argument reduction technique
1632     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1633     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1634 
1635     // Pseudocode for sin:
1636     // if (x &lt;= Math.PI / 4.0) {
1637     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1638     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1639     // } else {
1640     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1641     // }
1642     // return StrictMath.sin(x);
1643 
1644     // Pseudocode for cos:
1645     // if (x &lt;= Math.PI / 4.0) {
1646     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1647     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1648     // } else {
1649     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1650     // }
1651     // return StrictMath.cos(x);
1652 
1653     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1654     // requires a special machine instruction to load it.  Instead we'll try
1655     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1656     // probably do the math inside the SIN encoding.
1657 
1658     // Make the merge point
1659     RegionNode* r = new RegionNode(3);
1660     Node* phi = new PhiNode(r, Type::DOUBLE);
1661 
1662     // Flatten arg so we need only 1 test
1663     Node *abs = _gvn.transform(new AbsDNode(arg));
1664     // Node for PI/4 constant
1665     Node *pi4 = makecon(TypeD::make(pi_4));
1666     // Check PI/4 : abs(arg)
1667     Node *cmp = _gvn.transform(new CmpDNode(pi4,abs));
1668     // Check: If PI/4 &lt; abs(arg) then go slow
1669     Node *bol = _gvn.transform(new BoolNode( cmp, BoolTest::lt ));
1670     // Branch either way
1671     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1672     set_control(opt_iff(r,iff));
1673 
1674     // Set fast path result
1675     phi-&gt;init_req(2, n);
1676 
1677     // Slow path - non-blocking leaf call
1678     Node* call = NULL;
1679     switch (id) {
1680     case vmIntrinsics::_dsin:
1681       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1682                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1683                                "Sin", NULL, arg, top());
1684       break;
1685     case vmIntrinsics::_dcos:
1686       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1687                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1688                                "Cos", NULL, arg, top());
1689       break;
1690     case vmIntrinsics::_dtan:
1691       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1692                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1693                                "Tan", NULL, arg, top());
1694       break;
1695     }
1696     assert(control()-&gt;in(0) == call, "");
1697     Node* slow_result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1698     r-&gt;init_req(1, control());
1699     phi-&gt;init_req(1, slow_result);
1700 
1701     // Post-merge
1702     set_control(_gvn.transform(r));
1703     record_for_igvn(r);
1704     n = _gvn.transform(phi);
1705 
1706     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1707   }
1708   set_result(n);
1709   return true;
1710 }
1711 
1712 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1713   //-------------------
1714   //result=(result.isNaN())? funcAddr():result;
1715   // Check: If isNaN() by checking result!=result? then either trap
1716   // or go to runtime
1717   Node* cmpisnan = _gvn.transform(new CmpDNode(result, result));
1718   // Build the boolean node
1719   Node* bolisnum = _gvn.transform(new BoolNode(cmpisnan, BoolTest::eq));
1720 
1721   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1722     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1723       // The pow or exp intrinsic returned a NaN, which requires a call
1724       // to the runtime.  Recompile with the runtime call.
1725       uncommon_trap(Deoptimization::Reason_intrinsic,
1726                     Deoptimization::Action_make_not_entrant);
1727     }
1728     return result;
1729   } else {
1730     // If this inlining ever returned NaN in the past, we compile a call
1731     // to the runtime to properly handle corner cases
1732 
1733     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1734     Node* if_slow = _gvn.transform(new IfFalseNode(iff));
1735     Node* if_fast = _gvn.transform(new IfTrueNode(iff));
1736 
1737     if (!if_slow-&gt;is_top()) {
1738       RegionNode* result_region = new RegionNode(3);
1739       PhiNode*    result_val = new PhiNode(result_region, Type::DOUBLE);
1740 
1741       result_region-&gt;init_req(1, if_fast);
1742       result_val-&gt;init_req(1, result);
1743 
1744       set_control(if_slow);
1745 
1746       const TypePtr* no_memory_effects = NULL;
1747       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1748                                    no_memory_effects,
1749                                    x, top(), y, y ? top() : NULL);
1750       Node* value = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+0));
1751 #ifdef ASSERT
1752       Node* value_top = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+1));
1753       assert(value_top == top(), "second value must be top");
1754 #endif
1755 
1756       result_region-&gt;init_req(2, control());
1757       result_val-&gt;init_req(2, value);
1758       set_control(_gvn.transform(result_region));
1759       return _gvn.transform(result_val);
1760     } else {
1761       return result;
1762     }
1763   }
1764 }
1765 
1766 //------------------------------inline_pow-------------------------------------
1767 // Inline power instructions, if possible.
1768 bool LibraryCallKit::inline_pow() {
1769   // Pseudocode for pow
1770   // if (y == 2) {
1771   //   return x * x;
1772   // } else {
1773   //   if (x &lt;= 0.0) {
1774   //     long longy = (long)y;
1775   //     if ((double)longy == y) { // if y is long
1776   //       if (y + 1 == y) longy = 0; // huge number: even
1777   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1778   //     } else {
1779   //       result = NaN;
1780   //     }
1781   //   } else {
1782   //     result = DPow(x,y);
1783   //   }
1784   //   if (result != result)?  {
1785   //     result = uncommon_trap() or runtime_call();
1786   //   }
1787   //   return result;
1788   // }
1789 
1790   Node* x = round_double_node(argument(0));
1791   Node* y = round_double_node(argument(2));
1792 
1793   Node* result = NULL;
1794 
1795   Node*   const_two_node = makecon(TypeD::make(2.0));
1796   Node*   cmp_node       = _gvn.transform(new CmpDNode(y, const_two_node));
1797   Node*   bool_node      = _gvn.transform(new BoolNode(cmp_node, BoolTest::eq));
1798   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1799   Node*   if_true        = _gvn.transform(new IfTrueNode(if_node));
1800   Node*   if_false       = _gvn.transform(new IfFalseNode(if_node));
1801 
1802   RegionNode* region_node = new RegionNode(3);
1803   region_node-&gt;init_req(1, if_true);
1804 
1805   Node* phi_node = new PhiNode(region_node, Type::DOUBLE);
1806   // special case for x^y where y == 2, we can convert it to x * x
1807   phi_node-&gt;init_req(1, _gvn.transform(new MulDNode(x, x)));
1808 
1809   // set control to if_false since we will now process the false branch
1810   set_control(if_false);
1811 
1812   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1813     // Short form: skip the fancy tests and just check for NaN result.
1814     result = _gvn.transform(new PowDNode(C, control(), x, y));
1815   } else {
1816     // If this inlining ever returned NaN in the past, include all
1817     // checks + call to the runtime.
1818 
1819     // Set the merge point for If node with condition of (x &lt;= 0.0)
1820     // There are four possible paths to region node and phi node
1821     RegionNode *r = new RegionNode(4);
1822     Node *phi = new PhiNode(r, Type::DOUBLE);
1823 
1824     // Build the first if node: if (x &lt;= 0.0)
1825     // Node for 0 constant
1826     Node *zeronode = makecon(TypeD::ZERO);
1827     // Check x:0
1828     Node *cmp = _gvn.transform(new CmpDNode(x, zeronode));
1829     // Check: If (x&lt;=0) then go complex path
1830     Node *bol1 = _gvn.transform(new BoolNode( cmp, BoolTest::le ));
1831     // Branch either way
1832     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1833     // Fast path taken; set region slot 3
1834     Node *fast_taken = _gvn.transform(new IfFalseNode(if1));
1835     r-&gt;init_req(3,fast_taken); // Capture fast-control
1836 
1837     // Fast path not-taken, i.e. slow path
1838     Node *complex_path = _gvn.transform(new IfTrueNode(if1));
1839 
1840     // Set fast path result
1841     Node *fast_result = _gvn.transform(new PowDNode(C, control(), x, y));
1842     phi-&gt;init_req(3, fast_result);
1843 
1844     // Complex path
1845     // Build the second if node (if y is long)
1846     // Node for (long)y
1847     Node *longy = _gvn.transform(new ConvD2LNode(y));
1848     // Node for (double)((long) y)
1849     Node *doublelongy= _gvn.transform(new ConvL2DNode(longy));
1850     // Check (double)((long) y) : y
1851     Node *cmplongy= _gvn.transform(new CmpDNode(doublelongy, y));
1852     // Check if (y isn't long) then go to slow path
1853 
1854     Node *bol2 = _gvn.transform(new BoolNode( cmplongy, BoolTest::ne ));
1855     // Branch either way
1856     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1857     Node* ylong_path = _gvn.transform(new IfFalseNode(if2));
1858 
1859     Node *slow_path = _gvn.transform(new IfTrueNode(if2));
1860 
1861     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1862     // Node for constant 1
1863     Node *conone = longcon(1);
1864     // 1&amp; (long)y
1865     Node *signnode= _gvn.transform(new AndLNode(conone, longy));
1866 
1867     // A huge number is always even. Detect a huge number by checking
1868     // if y + 1 == y and set integer to be tested for parity to 0.
1869     // Required for corner case:
1870     // (long)9.223372036854776E18 = max_jlong
1871     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1872     // max_jlong is odd but 9.223372036854776E18 is even
1873     Node* yplus1 = _gvn.transform(new AddDNode(y, makecon(TypeD::make(1))));
1874     Node *cmpyplus1= _gvn.transform(new CmpDNode(yplus1, y));
1875     Node *bolyplus1 = _gvn.transform(new BoolNode( cmpyplus1, BoolTest::eq ));
1876     Node* correctedsign = NULL;
1877     if (ConditionalMoveLimit != 0) {
1878       correctedsign = _gvn.transform(CMoveNode::make(NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1879     } else {
1880       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1881       RegionNode *r = new RegionNode(3);
1882       Node *phi = new PhiNode(r, TypeLong::LONG);
1883       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyplus1)));
1884       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyplus1)));
1885       phi-&gt;init_req(1, signnode);
1886       phi-&gt;init_req(2, longcon(0));
1887       correctedsign = _gvn.transform(phi);
1888       ylong_path = _gvn.transform(r);
1889       record_for_igvn(r);
1890     }
1891 
1892     // zero node
1893     Node *conzero = longcon(0);
1894     // Check (1&amp;(long)y)==0?
1895     Node *cmpeq1 = _gvn.transform(new CmpLNode(correctedsign, conzero));
1896     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1897     Node *bol3 = _gvn.transform(new BoolNode( cmpeq1, BoolTest::ne ));
1898     // abs(x)
1899     Node *absx=_gvn.transform(new AbsDNode(x));
1900     // abs(x)^y
1901     Node *absxpowy = _gvn.transform(new PowDNode(C, control(), absx, y));
1902     // -abs(x)^y
1903     Node *negabsxpowy = _gvn.transform(new NegDNode (absxpowy));
1904     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1905     Node *signresult = NULL;
1906     if (ConditionalMoveLimit != 0) {
1907       signresult = _gvn.transform(CMoveNode::make(NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1908     } else {
1909       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1910       RegionNode *r = new RegionNode(3);
1911       Node *phi = new PhiNode(r, Type::DOUBLE);
1912       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyeven)));
1913       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyeven)));
1914       phi-&gt;init_req(1, absxpowy);
1915       phi-&gt;init_req(2, negabsxpowy);
1916       signresult = _gvn.transform(phi);
1917       ylong_path = _gvn.transform(r);
1918       record_for_igvn(r);
1919     }
1920     // Set complex path fast result
1921     r-&gt;init_req(2, ylong_path);
1922     phi-&gt;init_req(2, signresult);
1923 
1924     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1925     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1926     r-&gt;init_req(1,slow_path);
1927     phi-&gt;init_req(1,slow_result);
1928 
1929     // Post merge
1930     set_control(_gvn.transform(r));
1931     record_for_igvn(r);
1932     result = _gvn.transform(phi);
1933   }
1934 
1935   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1936 
1937   // control from finish_pow_exp is now input to the region node
1938   region_node-&gt;set_req(2, control());
1939   // the result from finish_pow_exp is now input to the phi node
1940   phi_node-&gt;init_req(2, result);
1941   set_control(_gvn.transform(region_node));
1942   record_for_igvn(region_node);
1943   set_result(_gvn.transform(phi_node));
1944 
1945   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1946   return true;
1947 }
1948 
1949 //------------------------------runtime_math-----------------------------
1950 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1951   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1952          "must be (DD)D or (D)D type");
1953 
1954   // Inputs
1955   Node* a = round_double_node(argument(0));
1956   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1957 
1958   const TypePtr* no_memory_effects = NULL;
1959   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1960                                  no_memory_effects,
1961                                  a, top(), b, b ? top() : NULL);
1962   Node* value = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+0));
1963 #ifdef ASSERT
1964   Node* value_top = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+1));
1965   assert(value_top == top(), "second value must be top");
1966 #endif
1967 
1968   set_result(value);
1969   return true;
1970 }
1971 
1972 //------------------------------inline_math_native-----------------------------
1973 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
1974 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
1975   switch (id) {
1976     // These intrinsics are not properly supported on all hardware
1977   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
1978     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
1979   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
1980     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
1981   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
1982     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
1983 
1984   case vmIntrinsics::_dlog:
1985     return StubRoutines::dlog() != NULL ?
1986     runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dlog(), "dlog") :
1987     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
1988   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
1989     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
1990 
1991     // These intrinsics are supported on all hardware
1992   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
1993   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
1994 
1995   case vmIntrinsics::_dexp:
1996     return StubRoutines::dexp() != NULL ?
1997       runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dexp(),  "dexp") :
1998       runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dexp),  "EXP");
1999   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
2000     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
2001 #undef FN_PTR
2002 
2003    // These intrinsics are not yet correctly implemented
2004   case vmIntrinsics::_datan2:
2005     return false;
2006 
2007   default:
2008     fatal_unexpected_iid(id);
2009     return false;
2010   }
2011 }
2012 
2013 static bool is_simple_name(Node* n) {
2014   return (n-&gt;req() == 1         // constant
2015           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
2016           || n-&gt;is_Proj()       // parameter or return value
2017           || n-&gt;is_Phi()        // local of some sort
2018           );
2019 }
2020 
2021 //----------------------------inline_notify-----------------------------------*
2022 bool LibraryCallKit::inline_notify(vmIntrinsics::ID id) {
2023   const TypeFunc* ftype = OptoRuntime::monitor_notify_Type();
2024   address func;
2025   if (id == vmIntrinsics::_notify) {
2026     func = OptoRuntime::monitor_notify_Java();
2027   } else {
2028     func = OptoRuntime::monitor_notifyAll_Java();
2029   }
2030   Node* call = make_runtime_call(RC_NO_LEAF, ftype, func, NULL, TypeRawPtr::BOTTOM, argument(0));
2031   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
2032   return true;
2033 }
2034 
2035 
2036 //----------------------------inline_min_max-----------------------------------
2037 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
2038   set_result(generate_min_max(id, argument(0), argument(1)));
2039   return true;
2040 }
2041 
2042 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
2043   Node* bol = _gvn.transform( new BoolNode(test, BoolTest::overflow) );
2044   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
2045   Node* fast_path = _gvn.transform( new IfFalseNode(check));
2046   Node* slow_path = _gvn.transform( new IfTrueNode(check) );
2047 
2048   {
2049     PreserveJVMState pjvms(this);
2050     PreserveReexecuteState preexecs(this);
2051     jvms()-&gt;set_should_reexecute(true);
2052 
2053     set_control(slow_path);
2054     set_i_o(i_o());
2055 
2056     uncommon_trap(Deoptimization::Reason_intrinsic,
2057                   Deoptimization::Action_none);
2058   }
2059 
2060   set_control(fast_path);
2061   set_result(math);
2062 }
2063 
2064 template &lt;typename OverflowOp&gt;
2065 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2066   typedef typename OverflowOp::MathOp MathOp;
2067 
2068   MathOp* mathOp = new MathOp(arg1, arg2);
2069   Node* operation = _gvn.transform( mathOp );
2070   Node* ofcheck = _gvn.transform( new OverflowOp(arg1, arg2) );
2071   inline_math_mathExact(operation, ofcheck);
2072   return true;
2073 }
2074 
2075 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2076   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2077 }
2078 
2079 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2080   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2081 }
2082 
2083 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2084   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2085 }
2086 
2087 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2088   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2089 }
2090 
2091 bool LibraryCallKit::inline_math_negateExactI() {
2092   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2093 }
2094 
2095 bool LibraryCallKit::inline_math_negateExactL() {
2096   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2097 }
2098 
2099 bool LibraryCallKit::inline_math_multiplyExactI() {
2100   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2101 }
2102 
2103 bool LibraryCallKit::inline_math_multiplyExactL() {
2104   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2105 }
2106 
2107 Node*
2108 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2109   // These are the candidate return value:
2110   Node* xvalue = x0;
2111   Node* yvalue = y0;
2112 
2113   if (xvalue == yvalue) {
2114     return xvalue;
2115   }
2116 
2117   bool want_max = (id == vmIntrinsics::_max);
2118 
2119   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2120   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2121   if (txvalue == NULL || tyvalue == NULL)  return top();
2122   // This is not really necessary, but it is consistent with a
2123   // hypothetical MaxINode::Value method:
2124   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2125 
2126   // %%% This folding logic should (ideally) be in a different place.
2127   // Some should be inside IfNode, and there to be a more reliable
2128   // transformation of ?: style patterns into cmoves.  We also want
2129   // more powerful optimizations around cmove and min/max.
2130 
2131   // Try to find a dominating comparison of these guys.
2132   // It can simplify the index computation for Arrays.copyOf
2133   // and similar uses of System.arraycopy.
2134   // First, compute the normalized version of CmpI(x, y).
2135   int   cmp_op = Op_CmpI;
2136   Node* xkey = xvalue;
2137   Node* ykey = yvalue;
2138   Node* ideal_cmpxy = _gvn.transform(new CmpINode(xkey, ykey));
2139   if (ideal_cmpxy-&gt;is_Cmp()) {
2140     // E.g., if we have CmpI(length - offset, count),
2141     // it might idealize to CmpI(length, count + offset)
2142     cmp_op = ideal_cmpxy-&gt;Opcode();
2143     xkey = ideal_cmpxy-&gt;in(1);
2144     ykey = ideal_cmpxy-&gt;in(2);
2145   }
2146 
2147   // Start by locating any relevant comparisons.
2148   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2149   Node* cmpxy = NULL;
2150   Node* cmpyx = NULL;
2151   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2152     Node* cmp = start_from-&gt;fast_out(k);
2153     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2154         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2155         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2156       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2157       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2158     }
2159   }
2160 
2161   const int NCMPS = 2;
2162   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2163   int cmpn;
2164   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2165     if (cmps[cmpn] != NULL)  break;     // find a result
2166   }
2167   if (cmpn &lt; NCMPS) {
2168     // Look for a dominating test that tells us the min and max.
2169     int depth = 0;                // Limit search depth for speed
2170     Node* dom = control();
2171     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2172       if (++depth &gt;= 100)  break;
2173       Node* ifproj = dom;
2174       if (!ifproj-&gt;is_Proj())  continue;
2175       Node* iff = ifproj-&gt;in(0);
2176       if (!iff-&gt;is_If())  continue;
2177       Node* bol = iff-&gt;in(1);
2178       if (!bol-&gt;is_Bool())  continue;
2179       Node* cmp = bol-&gt;in(1);
2180       if (cmp == NULL)  continue;
2181       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2182         if (cmps[cmpn] == cmp)  break;
2183       if (cmpn == NCMPS)  continue;
2184       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2185       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2186       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2187       // At this point, we know that 'x btest y' is true.
2188       switch (btest) {
2189       case BoolTest::eq:
2190         // They are proven equal, so we can collapse the min/max.
2191         // Either value is the answer.  Choose the simpler.
2192         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2193           return yvalue;
2194         return xvalue;
2195       case BoolTest::lt:          // x &lt; y
2196       case BoolTest::le:          // x &lt;= y
2197         return (want_max ? yvalue : xvalue);
2198       case BoolTest::gt:          // x &gt; y
2199       case BoolTest::ge:          // x &gt;= y
2200         return (want_max ? xvalue : yvalue);
2201       }
2202     }
2203   }
2204 
2205   // We failed to find a dominating test.
2206   // Let's pick a test that might GVN with prior tests.
2207   Node*          best_bol   = NULL;
2208   BoolTest::mask best_btest = BoolTest::illegal;
2209   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2210     Node* cmp = cmps[cmpn];
2211     if (cmp == NULL)  continue;
2212     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2213       Node* bol = cmp-&gt;fast_out(j);
2214       if (!bol-&gt;is_Bool())  continue;
2215       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2216       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2217       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2218       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2219         best_bol   = bol-&gt;as_Bool();
2220         best_btest = btest;
2221       }
2222     }
2223   }
2224 
2225   Node* answer_if_true  = NULL;
2226   Node* answer_if_false = NULL;
2227   switch (best_btest) {
2228   default:
2229     if (cmpxy == NULL)
2230       cmpxy = ideal_cmpxy;
2231     best_bol = _gvn.transform(new BoolNode(cmpxy, BoolTest::lt));
2232     // and fall through:
2233   case BoolTest::lt:          // x &lt; y
2234   case BoolTest::le:          // x &lt;= y
2235     answer_if_true  = (want_max ? yvalue : xvalue);
2236     answer_if_false = (want_max ? xvalue : yvalue);
2237     break;
2238   case BoolTest::gt:          // x &gt; y
2239   case BoolTest::ge:          // x &gt;= y
2240     answer_if_true  = (want_max ? xvalue : yvalue);
2241     answer_if_false = (want_max ? yvalue : xvalue);
2242     break;
2243   }
2244 
2245   jint hi, lo;
2246   if (want_max) {
2247     // We can sharpen the minimum.
2248     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2249     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2250   } else {
2251     // We can sharpen the maximum.
2252     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2253     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2254   }
2255 
2256   // Use a flow-free graph structure, to avoid creating excess control edges
2257   // which could hinder other optimizations.
2258   // Since Math.min/max is often used with arraycopy, we want
2259   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2260   Node* cmov = CMoveNode::make(NULL, best_bol,
2261                                answer_if_false, answer_if_true,
2262                                TypeInt::make(lo, hi, widen));
2263 
2264   return _gvn.transform(cmov);
2265 
2266   /*
2267   // This is not as desirable as it may seem, since Min and Max
2268   // nodes do not have a full set of optimizations.
2269   // And they would interfere, anyway, with 'if' optimizations
2270   // and with CMoveI canonical forms.
2271   switch (id) {
2272   case vmIntrinsics::_min:
2273     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2274   case vmIntrinsics::_max:
2275     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2276   default:
2277     ShouldNotReachHere();
2278   }
2279   */
2280 }
2281 
2282 inline int
2283 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2284   const TypePtr* base_type = TypePtr::NULL_PTR;
2285   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2286   if (base_type == NULL) {
2287     // Unknown type.
2288     return Type::AnyPtr;
2289   } else if (base_type == TypePtr::NULL_PTR) {
2290     // Since this is a NULL+long form, we have to switch to a rawptr.
2291     base   = _gvn.transform(new CastX2PNode(offset));
2292     offset = MakeConX(0);
2293     return Type::RawPtr;
2294   } else if (base_type-&gt;base() == Type::RawPtr) {
2295     return Type::RawPtr;
2296   } else if (base_type-&gt;isa_oopptr()) {
2297     // Base is never null =&gt; always a heap address.
2298     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2299       return Type::OopPtr;
2300     }
2301     // Offset is small =&gt; always a heap address.
2302     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2303     if (offset_type != NULL &amp;&amp;
2304         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2305         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2306         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2307       return Type::OopPtr;
2308     }
2309     // Otherwise, it might either be oop+off or NULL+addr.
2310     return Type::AnyPtr;
2311   } else {
2312     // No information:
2313     return Type::AnyPtr;
2314   }
2315 }
2316 
2317 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2318   int kind = classify_unsafe_addr(base, offset);
2319   if (kind == Type::RawPtr) {
2320     return basic_plus_adr(top(), base, offset);
2321   } else {
2322     return basic_plus_adr(base, offset);
2323   }
2324 }
2325 
2326 //--------------------------inline_number_methods-----------------------------
2327 // inline int     Integer.numberOfLeadingZeros(int)
2328 // inline int        Long.numberOfLeadingZeros(long)
2329 //
2330 // inline int     Integer.numberOfTrailingZeros(int)
2331 // inline int        Long.numberOfTrailingZeros(long)
2332 //
2333 // inline int     Integer.bitCount(int)
2334 // inline int        Long.bitCount(long)
2335 //
2336 // inline char  Character.reverseBytes(char)
2337 // inline short     Short.reverseBytes(short)
2338 // inline int     Integer.reverseBytes(int)
2339 // inline long       Long.reverseBytes(long)
2340 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2341   Node* arg = argument(0);
2342   Node* n = NULL;
2343   switch (id) {
2344   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new CountLeadingZerosINode( arg);  break;
2345   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new CountLeadingZerosLNode( arg);  break;
2346   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new CountTrailingZerosINode(arg);  break;
2347   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new CountTrailingZerosLNode(arg);  break;
2348   case vmIntrinsics::_bitCount_i:               n = new PopCountINode(          arg);  break;
2349   case vmIntrinsics::_bitCount_l:               n = new PopCountLNode(          arg);  break;
2350   case vmIntrinsics::_reverseBytes_c:           n = new ReverseBytesUSNode(0,   arg);  break;
2351   case vmIntrinsics::_reverseBytes_s:           n = new ReverseBytesSNode( 0,   arg);  break;
2352   case vmIntrinsics::_reverseBytes_i:           n = new ReverseBytesINode( 0,   arg);  break;
2353   case vmIntrinsics::_reverseBytes_l:           n = new ReverseBytesLNode( 0,   arg);  break;
2354   default:  fatal_unexpected_iid(id);  break;
2355   }
2356   set_result(_gvn.transform(n));
2357   return true;
2358 }
2359 
2360 //----------------------------inline_unsafe_access----------------------------
2361 
2362 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2363 
2364 // Helper that guards and inserts a pre-barrier.
2365 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2366                                         Node* pre_val, bool need_mem_bar) {
2367   // We could be accessing the referent field of a reference object. If so, when G1
2368   // is enabled, we need to log the value in the referent field in an SATB buffer.
2369   // This routine performs some compile time filters and generates suitable
2370   // runtime filters that guard the pre-barrier code.
2371   // Also add memory barrier for non volatile load from the referent field
2372   // to prevent commoning of loads across safepoint.
2373   if (!UseG1GC &amp;&amp; !need_mem_bar)
2374     return;
2375 
2376   // Some compile time checks.
2377 
2378   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2379   const TypeX* otype = offset-&gt;find_intptr_t_type();
2380   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2381       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2382     // Constant offset but not the reference_offset so just return
2383     return;
2384   }
2385 
2386   // We only need to generate the runtime guards for instances.
2387   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2388   if (btype != NULL) {
2389     if (btype-&gt;isa_aryptr()) {
2390       // Array type so nothing to do
2391       return;
2392     }
2393 
2394     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2395     if (itype != NULL) {
2396       // Can the klass of base_oop be statically determined to be
2397       // _not_ a sub-class of Reference and _not_ Object?
2398       ciKlass* klass = itype-&gt;klass();
2399       if ( klass-&gt;is_loaded() &amp;&amp;
2400           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2401           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2402         return;
2403       }
2404     }
2405   }
2406 
2407   // The compile time filters did not reject base_oop/offset so
2408   // we need to generate the following runtime filters
2409   //
2410   // if (offset == java_lang_ref_Reference::_reference_offset) {
2411   //   if (instance_of(base, java.lang.ref.Reference)) {
2412   //     pre_barrier(_, pre_val, ...);
2413   //   }
2414   // }
2415 
2416   float likely   = PROB_LIKELY(  0.999);
2417   float unlikely = PROB_UNLIKELY(0.999);
2418 
2419   IdealKit ideal(this);
2420 #define __ ideal.
2421 
2422   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2423 
2424   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2425       // Update graphKit memory and control from IdealKit.
2426       sync_kit(ideal);
2427 
2428       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2429       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2430 
2431       // Update IdealKit memory and control from graphKit.
2432       __ sync_kit(this);
2433 
2434       Node* one = __ ConI(1);
2435       // is_instof == 0 if base_oop == NULL
2436       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2437 
2438         // Update graphKit from IdeakKit.
2439         sync_kit(ideal);
2440 
2441         // Use the pre-barrier to record the value in the referent field
2442         pre_barrier(false /* do_load */,
2443                     __ ctrl(),
2444                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2445                     pre_val /* pre_val */,
2446                     T_OBJECT);
2447         if (need_mem_bar) {
2448           // Add memory barrier to prevent commoning reads from this field
2449           // across safepoint since GC can change its value.
2450           insert_mem_bar(Op_MemBarCPUOrder);
2451         }
2452         // Update IdealKit from graphKit.
2453         __ sync_kit(this);
2454 
2455       } __ end_if(); // _ref_type != ref_none
2456   } __ end_if(); // offset == referent_offset
2457 
2458   // Final sync IdealKit and GraphKit.
2459   final_sync(ideal);
2460 #undef __
2461 }
2462 
2463 
2464 // Interpret Unsafe.fieldOffset cookies correctly:
2465 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2466 
2467 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2468   // Attempt to infer a sharper value type from the offset and base type.
2469   ciKlass* sharpened_klass = NULL;
2470 
2471   // See if it is an instance field, with an object type.
2472   if (alias_type-&gt;field() != NULL) {
2473     assert(!is_native_ptr, "native pointer op cannot use a java address");
2474     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2475       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2476     }
2477   }
2478 
2479   // See if it is a narrow oop array.
2480   if (adr_type-&gt;isa_aryptr()) {
2481     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2482       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2483       if (elem_type != NULL) {
2484         sharpened_klass = elem_type-&gt;klass();
2485       }
2486     }
2487   }
2488 
2489   // The sharpened class might be unloaded if there is no class loader
2490   // contraint in place.
2491   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2492     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2493 
2494 #ifndef PRODUCT
2495     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2496       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2497       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2498     }
2499 #endif
2500     // Sharpen the value type.
2501     return tjp;
2502   }
2503   return NULL;
2504 }
2505 
2506 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile, bool unaligned) {
2507   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2508 
2509 #ifndef PRODUCT
2510   {
2511     ResourceMark rm;
2512     // Check the signatures.
2513     ciSignature* sig = callee()-&gt;signature();
2514 #ifdef ASSERT
2515     if (!is_store) {
2516       // Object getObject(Object base, int/long offset), etc.
2517       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2518       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2519           rtype = T_ADDRESS;  // it is really a C void*
2520       assert(rtype == type, "getter must return the expected value");
2521       if (!is_native_ptr) {
2522         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2523         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2524         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2525       } else {
2526         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2527         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2528       }
2529     } else {
2530       // void putObject(Object base, int/long offset, Object x), etc.
2531       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2532       if (!is_native_ptr) {
2533         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2534         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2535         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2536       } else {
2537         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2538         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2539       }
2540       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2541       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2542         vtype = T_ADDRESS;  // it is really a C void*
2543       assert(vtype == type, "putter must accept the expected value");
2544     }
2545 #endif // ASSERT
2546  }
2547 #endif //PRODUCT
2548 
2549   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2550 
2551   Node* receiver = argument(0);  // type: oop
2552 
2553   // Build address expression.
2554   Node* adr;
2555   Node* heap_base_oop = top();
2556   Node* offset = top();
2557   Node* val;
2558 
2559   if (!is_native_ptr) {
2560     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2561     Node* base = argument(1);  // type: oop
2562     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2563     offset = argument(2);  // type: long
2564     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2565     // to be plain byte offsets, which are also the same as those accepted
2566     // by oopDesc::field_base.
2567     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2568            "fieldOffset must be byte-scaled");
2569     // 32-bit machines ignore the high half!
2570     offset = ConvL2X(offset);
2571     adr = make_unsafe_address(base, offset);
2572     heap_base_oop = base;
2573     val = is_store ? argument(4) : NULL;
2574   } else {
2575     Node* ptr = argument(1);  // type: long
2576     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2577     adr = make_unsafe_address(NULL, ptr);
2578     val = is_store ? argument(3) : NULL;
2579   }
2580 
2581   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2582 
2583   // First guess at the value type.
2584   const Type *value_type = Type::get_const_basic_type(type);
2585 
2586   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2587   // there was not enough information to nail it down.
2588   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2589   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2590 
2591   // We will need memory barriers unless we can determine a unique
2592   // alias category for this reference.  (Note:  If for some reason
2593   // the barriers get omitted and the unsafe reference begins to "pollute"
2594   // the alias analysis of the rest of the graph, either Compile::can_alias
2595   // or Compile::must_alias will throw a diagnostic assert.)
2596   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2597 
2598   // If we are reading the value of the referent field of a Reference
2599   // object (either by using Unsafe directly or through reflection)
2600   // then, if G1 is enabled, we need to record the referent in an
2601   // SATB log buffer using the pre-barrier mechanism.
2602   // Also we need to add memory barrier to prevent commoning reads
2603   // from this field across safepoint since GC can change its value.
2604   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2605                            offset != top() &amp;&amp; heap_base_oop != top();
2606 
2607   if (!is_store &amp;&amp; type == T_OBJECT) {
2608     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2609     if (tjp != NULL) {
2610       value_type = tjp;
2611     }
2612   }
2613 
2614   receiver = null_check(receiver);
2615   if (stopped()) {
2616     return true;
2617   }
2618   // Heap pointers get a null-check from the interpreter,
2619   // as a courtesy.  However, this is not guaranteed by Unsafe,
2620   // and it is not possible to fully distinguish unintended nulls
2621   // from intended ones in this API.
2622 
2623   if (is_volatile) {
2624     // We need to emit leading and trailing CPU membars (see below) in
2625     // addition to memory membars when is_volatile. This is a little
2626     // too strong, but avoids the need to insert per-alias-type
2627     // volatile membars (for stores; compare Parse::do_put_xxx), which
2628     // we cannot do effectively here because we probably only have a
2629     // rough approximation of type.
2630     need_mem_bar = true;
2631     // For Stores, place a memory ordering barrier now.
2632     if (is_store) {
2633       insert_mem_bar(Op_MemBarRelease);
2634     } else {
2635       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2636         insert_mem_bar(Op_MemBarVolatile);
2637       }
2638     }
2639   }
2640 
2641   // Memory barrier to prevent normal and 'unsafe' accesses from
2642   // bypassing each other.  Happens after null checks, so the
2643   // exception paths do not take memory state from the memory barrier,
2644   // so there's no problems making a strong assert about mixing users
2645   // of safe &amp; unsafe memory.
2646   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2647 
2648   assert(alias_type-&gt;adr_type() == TypeRawPtr::BOTTOM || alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM ||
2649          alias_type-&gt;field() != NULL || alias_type-&gt;element() != NULL, "field, array element or unknown");
2650   bool mismatched = false;
2651   if (alias_type-&gt;element() != NULL || alias_type-&gt;field() != NULL) {
2652     BasicType bt;
2653     if (alias_type-&gt;element() != NULL) {
2654       const Type* element = alias_type-&gt;element();
2655       bt = element-&gt;isa_narrowoop() ? T_OBJECT : element-&gt;array_element_basic_type();
2656     } else {
2657       bt = alias_type-&gt;field()-&gt;type()-&gt;basic_type();
2658     }
2659     if (bt == T_ARRAY) {
2660       // accessing an array field with getObject is not a mismatch
2661       bt = T_OBJECT;
2662     }
2663     if (bt != type) {
2664       mismatched = true;
2665     }
2666   }
2667   assert(type != T_OBJECT || !unaligned, "unaligned access not supported with object type");
2668 
2669   if (!is_store) {
2670     Node* p = NULL;
2671     // Try to constant fold a load from a constant field
2672     ciField* field = alias_type-&gt;field();
2673     if (heap_base_oop != top() &amp;&amp;
2674         field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; field-&gt;layout_type() == type) {
2675       // final or stable field
2676       const Type* con_type = Type::make_constant(alias_type-&gt;field(), heap_base_oop);
2677       if (con_type != NULL) {
2678         p = makecon(con_type);
2679       }
2680     }
2681     if (p == NULL) {
2682       MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2683       // To be valid, unsafe loads may depend on other conditions than
2684       // the one that guards them: pin the Load node
2685       p = make_load(control(), adr, value_type, type, adr_type, mo, LoadNode::Pinned, is_volatile, unaligned, mismatched);
2686       // load value
2687       switch (type) {
2688       case T_BOOLEAN:
2689       case T_CHAR:
2690       case T_BYTE:
2691       case T_SHORT:
2692       case T_INT:
2693       case T_LONG:
2694       case T_FLOAT:
2695       case T_DOUBLE:
2696         break;
2697       case T_OBJECT:
2698         if (need_read_barrier) {
2699           insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2700         }
2701         break;
2702       case T_ADDRESS:
2703         // Cast to an int type.
2704         p = _gvn.transform(new CastP2XNode(NULL, p));
2705         p = ConvX2UL(p);
2706         break;
2707       default:
2708         fatal("unexpected type %d: %s", type, type2name(type));
2709         break;
2710       }
2711     }
2712     // The load node has the control of the preceding MemBarCPUOrder.  All
2713     // following nodes will have the control of the MemBarCPUOrder inserted at
2714     // the end of this method.  So, pushing the load onto the stack at a later
2715     // point is fine.
2716     set_result(p);
2717   } else {
2718     // place effect of store into memory
2719     switch (type) {
2720     case T_DOUBLE:
2721       val = dstore_rounding(val);
2722       break;
2723     case T_ADDRESS:
2724       // Repackage the long as a pointer.
2725       val = ConvL2X(val);
2726       val = _gvn.transform(new CastX2PNode(val));
2727       break;
2728     }
2729 
2730     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2731     if (type != T_OBJECT ) {
2732       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile, unaligned, mismatched);
2733     } else {
2734       // Possibly an oop being stored to Java heap or native memory
2735       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2736         // oop to Java heap.
2737         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo, mismatched);
2738       } else {
2739         // We can't tell at compile time if we are storing in the Java heap or outside
2740         // of it. So we need to emit code to conditionally do the proper type of
2741         // store.
2742 
2743         IdealKit ideal(this);
2744 #define __ ideal.
2745         // QQQ who knows what probability is here??
2746         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2747           // Sync IdealKit and graphKit.
2748           sync_kit(ideal);
2749           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo, mismatched);
2750           // Update IdealKit memory.
2751           __ sync_kit(this);
2752         } __ else_(); {
2753           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile, mismatched);
2754         } __ end_if();
2755         // Final sync IdealKit and GraphKit.
2756         final_sync(ideal);
2757 #undef __
2758       }
2759     }
2760   }
2761 
2762   if (is_volatile) {
2763     if (!is_store) {
2764       insert_mem_bar(Op_MemBarAcquire);
2765     } else {
2766       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2767         insert_mem_bar(Op_MemBarVolatile);
2768       }
2769     }
2770   }
2771 
2772   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2773 
2774   return true;
2775 }
2776 
2777 //----------------------------inline_unsafe_load_store----------------------------
2778 // This method serves a couple of different customers (depending on LoadStoreKind):
2779 //
2780 // LS_cmpxchg:
2781 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2782 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2783 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2784 //
2785 // LS_xadd:
2786 //   public int  getAndAddInt( Object o, long offset, int  delta)
2787 //   public long getAndAddLong(Object o, long offset, long delta)
2788 //
2789 // LS_xchg:
2790 //   int    getAndSet(Object o, long offset, int    newValue)
2791 //   long   getAndSet(Object o, long offset, long   newValue)
2792 //   Object getAndSet(Object o, long offset, Object newValue)
2793 //
2794 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2795   // This basic scheme here is the same as inline_unsafe_access, but
2796   // differs in enough details that combining them would make the code
2797   // overly confusing.  (This is a true fact! I originally combined
2798   // them, but even I was confused by it!) As much code/comments as
2799   // possible are retained from inline_unsafe_access though to make
2800   // the correspondences clearer. - dl
2801 
2802   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2803 
2804 #ifndef PRODUCT
2805   BasicType rtype;
2806   {
2807     ResourceMark rm;
2808     // Check the signatures.
2809     ciSignature* sig = callee()-&gt;signature();
2810     rtype = sig-&gt;return_type()-&gt;basic_type();
2811     if (kind == LS_xadd || kind == LS_xchg) {
2812       // Check the signatures.
2813 #ifdef ASSERT
2814       assert(rtype == type, "get and set must return the expected type");
2815       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2816       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2817       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2818       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2819 #endif // ASSERT
2820     } else if (kind == LS_cmpxchg) {
2821       // Check the signatures.
2822 #ifdef ASSERT
2823       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2824       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2825       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2826       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2827 #endif // ASSERT
2828     } else {
2829       ShouldNotReachHere();
2830     }
2831   }
2832 #endif //PRODUCT
2833 
2834   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2835 
2836   // Get arguments:
2837   Node* receiver = NULL;
2838   Node* base     = NULL;
2839   Node* offset   = NULL;
2840   Node* oldval   = NULL;
2841   Node* newval   = NULL;
2842   if (kind == LS_cmpxchg) {
2843     const bool two_slot_type = type2size[type] == 2;
2844     receiver = argument(0);  // type: oop
2845     base     = argument(1);  // type: oop
2846     offset   = argument(2);  // type: long
2847     oldval   = argument(4);  // type: oop, int, or long
2848     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2849   } else if (kind == LS_xadd || kind == LS_xchg){
2850     receiver = argument(0);  // type: oop
2851     base     = argument(1);  // type: oop
2852     offset   = argument(2);  // type: long
2853     oldval   = NULL;
2854     newval   = argument(4);  // type: oop, int, or long
2855   }
2856 
2857   // Null check receiver.
2858   receiver = null_check(receiver);
2859   if (stopped()) {
2860     return true;
2861   }
2862 
2863   // Build field offset expression.
2864   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2865   // to be plain byte offsets, which are also the same as those accepted
2866   // by oopDesc::field_base.
2867   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2868   // 32-bit machines ignore the high half of long offsets
2869   offset = ConvL2X(offset);
2870   Node* adr = make_unsafe_address(base, offset);
2871   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2872 
2873   // For CAS, unlike inline_unsafe_access, there seems no point in
2874   // trying to refine types. Just use the coarse types here.
2875   const Type *value_type = Type::get_const_basic_type(type);
2876   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2877   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2878 
2879   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2880     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2881     if (tjp != NULL) {
2882       value_type = tjp;
2883     }
2884   }
2885 
2886   int alias_idx = C-&gt;get_alias_index(adr_type);
2887 
2888   // Memory-model-wise, a LoadStore acts like a little synchronized
2889   // block, so needs barriers on each side.  These don't translate
2890   // into actual barriers on most machines, but we still need rest of
2891   // compiler to respect ordering.
2892 
2893   insert_mem_bar(Op_MemBarRelease);
2894   insert_mem_bar(Op_MemBarCPUOrder);
2895 
2896   // 4984716: MemBars must be inserted before this
2897   //          memory node in order to avoid a false
2898   //          dependency which will confuse the scheduler.
2899   Node *mem = memory(alias_idx);
2900 
2901   // For now, we handle only those cases that actually exist: ints,
2902   // longs, and Object. Adding others should be straightforward.
2903   Node* load_store = NULL;
2904   switch(type) {
2905   case T_INT:
2906     if (kind == LS_xadd) {
2907       load_store = _gvn.transform(new GetAndAddINode(control(), mem, adr, newval, adr_type));
2908     } else if (kind == LS_xchg) {
2909       load_store = _gvn.transform(new GetAndSetINode(control(), mem, adr, newval, adr_type));
2910     } else if (kind == LS_cmpxchg) {
2911       load_store = _gvn.transform(new CompareAndSwapINode(control(), mem, adr, newval, oldval));
2912     } else {
2913       ShouldNotReachHere();
2914     }
2915     break;
2916   case T_LONG:
2917     if (kind == LS_xadd) {
2918       load_store = _gvn.transform(new GetAndAddLNode(control(), mem, adr, newval, adr_type));
2919     } else if (kind == LS_xchg) {
2920       load_store = _gvn.transform(new GetAndSetLNode(control(), mem, adr, newval, adr_type));
2921     } else if (kind == LS_cmpxchg) {
2922       load_store = _gvn.transform(new CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2923     } else {
2924       ShouldNotReachHere();
2925     }
2926     break;
2927   case T_OBJECT:
2928     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2929     // could be delayed during Parse (for example, in adjust_map_after_if()).
2930     // Execute transformation here to avoid barrier generation in such case.
2931     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2932       newval = _gvn.makecon(TypePtr::NULL_PTR);
2933 
2934     // Reference stores need a store barrier.
2935     if (kind == LS_xchg) {
2936       // If pre-barrier must execute before the oop store, old value will require do_load here.
2937       if (!can_move_pre_barrier()) {
2938         pre_barrier(true /* do_load*/,
2939                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2940                     NULL /* pre_val*/,
2941                     T_OBJECT);
2942       } // Else move pre_barrier to use load_store value, see below.
2943     } else if (kind == LS_cmpxchg) {
2944       // Same as for newval above:
2945       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2946         oldval = _gvn.makecon(TypePtr::NULL_PTR);
2947       }
2948       // The only known value which might get overwritten is oldval.
2949       pre_barrier(false /* do_load */,
2950                   control(), NULL, NULL, max_juint, NULL, NULL,
2951                   oldval /* pre_val */,
2952                   T_OBJECT);
2953     } else {
2954       ShouldNotReachHere();
2955     }
2956 
2957 #ifdef _LP64
2958     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2959       Node *newval_enc = _gvn.transform(new EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2960       if (kind == LS_xchg) {
2961         load_store = _gvn.transform(new GetAndSetNNode(control(), mem, adr,
2962                                                        newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2963       } else {
2964         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2965         Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2966         load_store = _gvn.transform(new CompareAndSwapNNode(control(), mem, adr,
2967                                                                 newval_enc, oldval_enc));
2968       }
2969     } else
2970 #endif
2971     {
2972       if (kind == LS_xchg) {
2973         load_store = _gvn.transform(new GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2974       } else {
2975         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2976         load_store = _gvn.transform(new CompareAndSwapPNode(control(), mem, adr, newval, oldval));
2977       }
2978     }
2979     if (kind == LS_cmpxchg) {
2980       // Emit the post barrier only when the actual store happened.
2981       // This makes sense to check only for compareAndSet that can fail to set the value.
2982       // CAS success path is marked more likely since we anticipate this is a performance
2983       // critical path, while CAS failure path can use the penalty for going through unlikely
2984       // path as backoff. Which is still better than doing a store barrier there.
2985       IdealKit ideal(this);
2986       ideal.if_then(load_store, BoolTest::ne, ideal.ConI(0), PROB_STATIC_FREQUENT); {
2987         sync_kit(ideal);
2988         post_barrier(ideal.ctrl(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2989         ideal.sync_kit(this);
2990       } ideal.end_if();
2991       final_sync(ideal);
2992     } else {
2993       post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2994     }
2995     break;
2996   default:
2997     fatal("unexpected type %d: %s", type, type2name(type));
2998     break;
2999   }
3000 
3001   // SCMemProjNodes represent the memory state of a LoadStore. Their
3002   // main role is to prevent LoadStore nodes from being optimized away
3003   // when their results aren't used.
3004   Node* proj = _gvn.transform(new SCMemProjNode(load_store));
3005   set_memory(proj, alias_idx);
3006 
3007   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
3008 #ifdef _LP64
3009     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3010       load_store = _gvn.transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
3011     }
3012 #endif
3013     if (can_move_pre_barrier()) {
3014       // Don't need to load pre_val. The old value is returned by load_store.
3015       // The pre_barrier can execute after the xchg as long as no safepoint
3016       // gets inserted between them.
3017       pre_barrier(false /* do_load */,
3018                   control(), NULL, NULL, max_juint, NULL, NULL,
3019                   load_store /* pre_val */,
3020                   T_OBJECT);
3021     }
3022   }
3023 
3024   // Add the trailing membar surrounding the access
3025   insert_mem_bar(Op_MemBarCPUOrder);
3026   insert_mem_bar(Op_MemBarAcquire);
3027 
3028   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
3029   set_result(load_store);
3030   return true;
3031 }
3032 
3033 //----------------------------inline_unsafe_ordered_store----------------------
3034 // public native void Unsafe.putOrderedObject(Object o, long offset, Object x);
3035 // public native void Unsafe.putOrderedInt(Object o, long offset, int x);
3036 // public native void Unsafe.putOrderedLong(Object o, long offset, long x);
3037 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
3038   // This is another variant of inline_unsafe_access, differing in
3039   // that it always issues store-store ("release") barrier and ensures
3040   // store-atomicity (which only matters for "long").
3041 
3042   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3043 
3044 #ifndef PRODUCT
3045   {
3046     ResourceMark rm;
3047     // Check the signatures.
3048     ciSignature* sig = callee()-&gt;signature();
3049 #ifdef ASSERT
3050     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3051     assert(rtype == T_VOID, "must return void");
3052     assert(sig-&gt;count() == 3, "has 3 arguments");
3053     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3054     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3055 #endif // ASSERT
3056   }
3057 #endif //PRODUCT
3058 
3059   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3060 
3061   // Get arguments:
3062   Node* receiver = argument(0);  // type: oop
3063   Node* base     = argument(1);  // type: oop
3064   Node* offset   = argument(2);  // type: long
3065   Node* val      = argument(4);  // type: oop, int, or long
3066 
3067   // Null check receiver.
3068   receiver = null_check(receiver);
3069   if (stopped()) {
3070     return true;
3071   }
3072 
3073   // Build field offset expression.
3074   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3075   // 32-bit machines ignore the high half of long offsets
3076   offset = ConvL2X(offset);
3077   Node* adr = make_unsafe_address(base, offset);
3078   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3079   const Type *value_type = Type::get_const_basic_type(type);
3080   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3081 
3082   insert_mem_bar(Op_MemBarRelease);
3083   insert_mem_bar(Op_MemBarCPUOrder);
3084   // Ensure that the store is atomic for longs:
3085   const bool require_atomic_access = true;
3086   Node* store;
3087   if (type == T_OBJECT) // reference stores need a store barrier.
3088     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3089   else {
3090     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3091   }
3092   insert_mem_bar(Op_MemBarCPUOrder);
3093   return true;
3094 }
3095 
3096 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3097   // Regardless of form, don't allow previous ld/st to move down,
3098   // then issue acquire, release, or volatile mem_bar.
3099   insert_mem_bar(Op_MemBarCPUOrder);
3100   switch(id) {
3101     case vmIntrinsics::_loadFence:
3102       insert_mem_bar(Op_LoadFence);
3103       return true;
3104     case vmIntrinsics::_storeFence:
3105       insert_mem_bar(Op_StoreFence);
3106       return true;
3107     case vmIntrinsics::_fullFence:
3108       insert_mem_bar(Op_MemBarVolatile);
3109       return true;
3110     default:
3111       fatal_unexpected_iid(id);
3112       return false;
3113   }
3114 }
3115 
<a name="3" id="anc3"></a>




3116 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3117   if (!kls-&gt;is_Con()) {
3118     return true;
3119   }
3120   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3121   if (klsptr == NULL) {
3122     return true;
3123   }
3124   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3125   // don't need a guard for a klass that is already initialized
3126   return !ik-&gt;is_initialized();
3127 }
3128 
3129 //----------------------------inline_unsafe_allocate---------------------------
3130 // public native Object Unsafe.allocateInstance(Class&lt;?&gt; cls);
3131 bool LibraryCallKit::inline_unsafe_allocate() {
3132   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3133 
3134   null_check_receiver();  // null-check, then ignore
3135   Node* cls = null_check(argument(1));
3136   if (stopped())  return true;
3137 
3138   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3139   kls = null_check(kls);
3140   if (stopped())  return true;  // argument was like int.class
3141 
3142   Node* test = NULL;
3143   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3144     // Note:  The argument might still be an illegal value like
3145     // Serializable.class or Object[].class.   The runtime will handle it.
3146     // But we must make an explicit check for initialization.
3147     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3148     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3149     // can generate code to load it as unsigned byte.
3150     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3151     Node* bits = intcon(InstanceKlass::fully_initialized);
3152     test = _gvn.transform(new SubINode(inst, bits));
3153     // The 'test' is non-zero if we need to take a slow path.
3154   }
3155 
3156   Node* obj = new_instance(kls, test);
3157   set_result(obj);
3158   return true;
3159 }
3160 
3161 #ifdef TRACE_HAVE_INTRINSICS
3162 /*
3163  * oop -&gt; myklass
3164  * myklass-&gt;trace_id |= USED
3165  * return myklass-&gt;trace_id &amp; ~0x3
3166  */
3167 bool LibraryCallKit::inline_native_classID() {
3168   null_check_receiver();  // null-check, then ignore
3169   Node* cls = null_check(argument(1), T_OBJECT);
3170   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3171   kls = null_check(kls, T_OBJECT);
3172   ByteSize offset = TRACE_ID_OFFSET;
3173   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3174   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3175   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3176   Node* andl = _gvn.transform(new AndLNode(tvalue, bits));
3177   Node* clsused = longcon(0x01l); // set the class bit
3178   Node* orl = _gvn.transform(new OrLNode(tvalue, clsused));
3179 
3180   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3181   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3182   set_result(andl);
3183   return true;
3184 }
3185 
3186 bool LibraryCallKit::inline_native_threadID() {
3187   Node* tls_ptr = NULL;
3188   Node* cur_thr = generate_current_thread(tls_ptr);
3189   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3190   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3191   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3192 
3193   Node* threadid = NULL;
3194   size_t thread_id_size = OSThread::thread_id_size();
3195   if (thread_id_size == (size_t) BytesPerLong) {
3196     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3197   } else if (thread_id_size == (size_t) BytesPerInt) {
3198     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3199   } else {
3200     ShouldNotReachHere();
3201   }
3202   set_result(threadid);
3203   return true;
3204 }
3205 #endif
3206 
3207 //------------------------inline_native_time_funcs--------------
3208 // inline code for System.currentTimeMillis() and System.nanoTime()
3209 // these have the same type and signature
3210 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3211   const TypeFunc* tf = OptoRuntime::void_long_Type();
3212   const TypePtr* no_memory_effects = NULL;
3213   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3214   Node* value = _gvn.transform(new ProjNode(time, TypeFunc::Parms+0));
3215 #ifdef ASSERT
3216   Node* value_top = _gvn.transform(new ProjNode(time, TypeFunc::Parms+1));
3217   assert(value_top == top(), "second value must be top");
3218 #endif
3219   set_result(value);
3220   return true;
3221 }
3222 
3223 //------------------------inline_native_currentThread------------------
3224 bool LibraryCallKit::inline_native_currentThread() {
3225   Node* junk = NULL;
3226   set_result(generate_current_thread(junk));
3227   return true;
3228 }
3229 
3230 //------------------------inline_native_isInterrupted------------------
3231 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3232 bool LibraryCallKit::inline_native_isInterrupted() {
3233   // Add a fast path to t.isInterrupted(clear_int):
3234   //   (t == Thread.current() &amp;&amp;
3235   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3236   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3237   // So, in the common case that the interrupt bit is false,
3238   // we avoid making a call into the VM.  Even if the interrupt bit
3239   // is true, if the clear_int argument is false, we avoid the VM call.
3240   // However, if the receiver is not currentThread, we must call the VM,
3241   // because there must be some locking done around the operation.
3242 
3243   // We only go to the fast case code if we pass two guards.
3244   // Paths which do not pass are accumulated in the slow_region.
3245 
3246   enum {
3247     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3248     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3249     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3250     PATH_LIMIT
3251   };
3252 
3253   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3254   // out of the function.
3255   insert_mem_bar(Op_MemBarCPUOrder);
3256 
3257   RegionNode* result_rgn = new RegionNode(PATH_LIMIT);
3258   PhiNode*    result_val = new PhiNode(result_rgn, TypeInt::BOOL);
3259 
3260   RegionNode* slow_region = new RegionNode(1);
3261   record_for_igvn(slow_region);
3262 
3263   // (a) Receiving thread must be the current thread.
3264   Node* rec_thr = argument(0);
3265   Node* tls_ptr = NULL;
3266   Node* cur_thr = generate_current_thread(tls_ptr);
3267   Node* cmp_thr = _gvn.transform(new CmpPNode(cur_thr, rec_thr));
3268   Node* bol_thr = _gvn.transform(new BoolNode(cmp_thr, BoolTest::ne));
3269 
3270   generate_slow_guard(bol_thr, slow_region);
3271 
3272   // (b) Interrupt bit on TLS must be false.
3273   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3274   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3275   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3276 
3277   // Set the control input on the field _interrupted read to prevent it floating up.
3278   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3279   Node* cmp_bit = _gvn.transform(new CmpINode(int_bit, intcon(0)));
3280   Node* bol_bit = _gvn.transform(new BoolNode(cmp_bit, BoolTest::ne));
3281 
3282   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3283 
3284   // First fast path:  if (!TLS._interrupted) return false;
3285   Node* false_bit = _gvn.transform(new IfFalseNode(iff_bit));
3286   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3287   result_val-&gt;init_req(no_int_result_path, intcon(0));
3288 
3289   // drop through to next case
3290   set_control( _gvn.transform(new IfTrueNode(iff_bit)));
3291 
3292 #ifndef TARGET_OS_FAMILY_windows
3293   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3294   Node* clr_arg = argument(1);
3295   Node* cmp_arg = _gvn.transform(new CmpINode(clr_arg, intcon(0)));
3296   Node* bol_arg = _gvn.transform(new BoolNode(cmp_arg, BoolTest::ne));
3297   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3298 
3299   // Second fast path:  ... else if (!clear_int) return true;
3300   Node* false_arg = _gvn.transform(new IfFalseNode(iff_arg));
3301   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3302   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3303 
3304   // drop through to next case
3305   set_control( _gvn.transform(new IfTrueNode(iff_arg)));
3306 #else
3307   // To return true on Windows you must read the _interrupted field
3308   // and check the the event state i.e. take the slow path.
3309 #endif // TARGET_OS_FAMILY_windows
3310 
3311   // (d) Otherwise, go to the slow path.
3312   slow_region-&gt;add_req(control());
3313   set_control( _gvn.transform(slow_region));
3314 
3315   if (stopped()) {
3316     // There is no slow path.
3317     result_rgn-&gt;init_req(slow_result_path, top());
3318     result_val-&gt;init_req(slow_result_path, top());
3319   } else {
3320     // non-virtual because it is a private non-static
3321     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3322 
3323     Node* slow_val = set_results_for_java_call(slow_call);
3324     // this-&gt;control() comes from set_results_for_java_call
3325 
3326     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3327     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3328 
3329     // These two phis are pre-filled with copies of of the fast IO and Memory
3330     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3331     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3332 
3333     result_rgn-&gt;init_req(slow_result_path, control());
3334     result_io -&gt;init_req(slow_result_path, i_o());
3335     result_mem-&gt;init_req(slow_result_path, reset_memory());
3336     result_val-&gt;init_req(slow_result_path, slow_val);
3337 
3338     set_all_memory(_gvn.transform(result_mem));
3339     set_i_o(       _gvn.transform(result_io));
3340   }
3341 
3342   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3343   set_result(result_rgn, result_val);
3344   return true;
3345 }
3346 
3347 //---------------------------load_mirror_from_klass----------------------------
3348 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3349 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3350   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3351   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3352 }
3353 
3354 //-----------------------load_klass_from_mirror_common-------------------------
3355 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3356 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3357 // and branch to the given path on the region.
3358 // If never_see_null, take an uncommon trap on null, so we can optimistically
3359 // compile for the non-null case.
3360 // If the region is NULL, force never_see_null = true.
3361 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3362                                                     bool never_see_null,
3363                                                     RegionNode* region,
3364                                                     int null_path,
3365                                                     int offset) {
3366   if (region == NULL)  never_see_null = true;
3367   Node* p = basic_plus_adr(mirror, offset);
3368   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3369   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3370   Node* null_ctl = top();
3371   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3372   if (region != NULL) {
3373     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3374     region-&gt;init_req(null_path, null_ctl);
3375   } else {
3376     assert(null_ctl == top(), "no loose ends");
3377   }
3378   return kls;
3379 }
3380 
3381 //--------------------(inline_native_Class_query helpers)---------------------
3382 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3383 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3384 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3385   // Branch around if the given klass has the given modifier bit set.
3386   // Like generate_guard, adds a new path onto the region.
3387   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3388   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3389   Node* mask = intcon(modifier_mask);
3390   Node* bits = intcon(modifier_bits);
3391   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3392   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3393   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3394   return generate_fair_guard(bol, region);
3395 }
3396 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3397   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3398 }
3399 
3400 //-------------------------inline_native_Class_query-------------------
3401 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3402   const Type* return_type = TypeInt::BOOL;
3403   Node* prim_return_value = top();  // what happens if it's a primitive class?
3404   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3405   bool expect_prim = false;     // most of these guys expect to work on refs
3406 
3407   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3408 
3409   Node* mirror = argument(0);
3410   Node* obj    = top();
3411 
3412   switch (id) {
3413   case vmIntrinsics::_isInstance:
3414     // nothing is an instance of a primitive type
3415     prim_return_value = intcon(0);
3416     obj = argument(1);
3417     break;
3418   case vmIntrinsics::_getModifiers:
3419     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3420     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3421     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3422     break;
3423   case vmIntrinsics::_isInterface:
3424     prim_return_value = intcon(0);
3425     break;
3426   case vmIntrinsics::_isArray:
3427     prim_return_value = intcon(0);
3428     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3429     break;
3430   case vmIntrinsics::_isPrimitive:
3431     prim_return_value = intcon(1);
3432     expect_prim = true;  // obviously
3433     break;
3434   case vmIntrinsics::_getSuperclass:
3435     prim_return_value = null();
3436     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3437     break;
3438   case vmIntrinsics::_getClassAccessFlags:
3439     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3440     return_type = TypeInt::INT;  // not bool!  6297094
3441     break;
3442   default:
3443     fatal_unexpected_iid(id);
3444     break;
3445   }
3446 
3447   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3448   if (mirror_con == NULL)  return false;  // cannot happen?
3449 
3450 #ifndef PRODUCT
3451   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3452     ciType* k = mirror_con-&gt;java_mirror_type();
3453     if (k) {
3454       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3455       k-&gt;print_name();
3456       tty-&gt;cr();
3457     }
3458   }
3459 #endif
3460 
3461   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3462   RegionNode* region = new RegionNode(PATH_LIMIT);
3463   record_for_igvn(region);
3464   PhiNode* phi = new PhiNode(region, return_type);
3465 
3466   // The mirror will never be null of Reflection.getClassAccessFlags, however
3467   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3468   // if it is. See bug 4774291.
3469 
3470   // For Reflection.getClassAccessFlags(), the null check occurs in
3471   // the wrong place; see inline_unsafe_access(), above, for a similar
3472   // situation.
3473   mirror = null_check(mirror);
3474   // If mirror or obj is dead, only null-path is taken.
3475   if (stopped())  return true;
3476 
3477   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3478 
3479   // Now load the mirror's klass metaobject, and null-check it.
3480   // Side-effects region with the control path if the klass is null.
3481   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3482   // If kls is null, we have a primitive mirror.
3483   phi-&gt;init_req(_prim_path, prim_return_value);
3484   if (stopped()) { set_result(region, phi); return true; }
3485   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3486 
3487   Node* p;  // handy temp
3488   Node* null_ctl;
3489 
3490   // Now that we have the non-null klass, we can perform the real query.
3491   // For constant classes, the query will constant-fold in LoadNode::Value.
3492   Node* query_value = top();
3493   switch (id) {
3494   case vmIntrinsics::_isInstance:
3495     // nothing is an instance of a primitive type
3496     query_value = gen_instanceof(obj, kls, safe_for_replace);
3497     break;
3498 
<a name="4" id="anc4"></a>


3499   case vmIntrinsics::_getModifiers:
3500     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3501     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3502     break;
3503 
3504   case vmIntrinsics::_isInterface:
3505     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3506     if (generate_interface_guard(kls, region) != NULL)
3507       // A guard was added.  If the guard is taken, it was an interface.
3508       phi-&gt;add_req(intcon(1));
3509     // If we fall through, it's a plain class.
3510     query_value = intcon(0);
3511     break;
3512 
3513   case vmIntrinsics::_isArray:
3514     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3515     if (generate_array_guard(kls, region) != NULL)
3516       // A guard was added.  If the guard is taken, it was an array.
3517       phi-&gt;add_req(intcon(1));
3518     // If we fall through, it's a plain class.
3519     query_value = intcon(0);
3520     break;
3521 
3522   case vmIntrinsics::_isPrimitive:
3523     query_value = intcon(0); // "normal" path produces false
3524     break;
3525 
3526   case vmIntrinsics::_getSuperclass:
3527     // The rules here are somewhat unfortunate, but we can still do better
3528     // with random logic than with a JNI call.
3529     // Interfaces store null or Object as _super, but must report null.
3530     // Arrays store an intermediate super as _super, but must report Object.
3531     // Other types can report the actual _super.
3532     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3533     if (generate_interface_guard(kls, region) != NULL)
3534       // A guard was added.  If the guard is taken, it was an interface.
3535       phi-&gt;add_req(null());
3536     if (generate_array_guard(kls, region) != NULL)
3537       // A guard was added.  If the guard is taken, it was an array.
3538       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3539     // If we fall through, it's a plain class.  Get its _super.
3540     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3541     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3542     null_ctl = top();
3543     kls = null_check_oop(kls, &amp;null_ctl);
3544     if (null_ctl != top()) {
3545       // If the guard is taken, Object.superClass is null (both klass and mirror).
3546       region-&gt;add_req(null_ctl);
3547       phi   -&gt;add_req(null());
3548     }
3549     if (!stopped()) {
3550       query_value = load_mirror_from_klass(kls);
3551     }
3552     break;
3553 
3554   case vmIntrinsics::_getClassAccessFlags:
3555     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3556     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3557     break;
3558 
3559   default:
3560     fatal_unexpected_iid(id);
3561     break;
3562   }
3563 
3564   // Fall-through is the normal case of a query to a real class.
3565   phi-&gt;init_req(1, query_value);
3566   region-&gt;init_req(1, control());
3567 
3568   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3569   set_result(region, phi);
3570   return true;
3571 }
3572 
3573 //-------------------------inline_Class_cast-------------------
3574 bool LibraryCallKit::inline_Class_cast() {
3575   Node* mirror = argument(0); // Class
3576   Node* obj    = argument(1);
3577   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3578   if (mirror_con == NULL) {
3579     return false;  // dead path (mirror-&gt;is_top()).
3580   }
3581   if (obj == NULL || obj-&gt;is_top()) {
3582     return false;  // dead path
3583   }
3584   const TypeOopPtr* tp = _gvn.type(obj)-&gt;isa_oopptr();
3585 
3586   // First, see if Class.cast() can be folded statically.
3587   // java_mirror_type() returns non-null for compile-time Class constants.
3588   ciType* tm = mirror_con-&gt;java_mirror_type();
3589   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp;
3590       tp != NULL &amp;&amp; tp-&gt;klass() != NULL) {
3591     if (!tp-&gt;klass()-&gt;is_loaded()) {
3592       // Don't use intrinsic when class is not loaded.
3593       return false;
3594     } else {
3595       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), tp-&gt;klass());
3596       if (static_res == Compile::SSC_always_true) {
3597         // isInstance() is true - fold the code.
3598         set_result(obj);
3599         return true;
3600       } else if (static_res == Compile::SSC_always_false) {
3601         // Don't use intrinsic, have to throw ClassCastException.
3602         // If the reference is null, the non-intrinsic bytecode will
3603         // be optimized appropriately.
3604         return false;
3605       }
3606     }
3607   }
3608 
3609   // Bailout intrinsic and do normal inlining if exception path is frequent.
3610   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3611     return false;
3612   }
3613 
3614   // Generate dynamic checks.
3615   // Class.cast() is java implementation of _checkcast bytecode.
3616   // Do checkcast (Parse::do_checkcast()) optimizations here.
3617 
3618   mirror = null_check(mirror);
3619   // If mirror is dead, only null-path is taken.
3620   if (stopped()) {
3621     return true;
3622   }
3623 
3624   // Not-subtype or the mirror's klass ptr is NULL (in case it is a primitive).
3625   enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };
3626   RegionNode* region = new RegionNode(PATH_LIMIT);
3627   record_for_igvn(region);
3628 
3629   // Now load the mirror's klass metaobject, and null-check it.
3630   // If kls is null, we have a primitive mirror and
3631   // nothing is an instance of a primitive type.
3632   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3633 
3634   Node* res = top();
3635   if (!stopped()) {
3636     Node* bad_type_ctrl = top();
3637     // Do checkcast optimizations.
3638     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3639     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3640   }
3641   if (region-&gt;in(_prim_path) != top() ||
3642       region-&gt;in(_bad_type_path) != top()) {
3643     // Let Interpreter throw ClassCastException.
3644     PreserveJVMState pjvms(this);
3645     set_control(_gvn.transform(region));
3646     uncommon_trap(Deoptimization::Reason_intrinsic,
3647                   Deoptimization::Action_maybe_recompile);
3648   }
3649   if (!stopped()) {
3650     set_result(res);
3651   }
3652   return true;
3653 }
3654 
3655 
3656 //--------------------------inline_native_subtype_check------------------------
3657 // This intrinsic takes the JNI calls out of the heart of
3658 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3659 bool LibraryCallKit::inline_native_subtype_check() {
3660   // Pull both arguments off the stack.
3661   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3662   args[0] = argument(0);
3663   args[1] = argument(1);
3664   Node* klasses[2];             // corresponding Klasses: superk, subk
3665   klasses[0] = klasses[1] = top();
3666 
3667   enum {
3668     // A full decision tree on {superc is prim, subc is prim}:
3669     _prim_0_path = 1,           // {P,N} =&gt; false
3670                                 // {P,P} &amp; superc!=subc =&gt; false
3671     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3672     _prim_1_path,               // {N,P} =&gt; false
3673     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3674     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3675     PATH_LIMIT
3676   };
3677 
3678   RegionNode* region = new RegionNode(PATH_LIMIT);
3679   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3680   record_for_igvn(region);
3681 
3682   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3683   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3684   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3685 
3686   // First null-check both mirrors and load each mirror's klass metaobject.
3687   int which_arg;
3688   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3689     Node* arg = args[which_arg];
3690     arg = null_check(arg);
3691     if (stopped())  break;
3692     args[which_arg] = arg;
3693 
3694     Node* p = basic_plus_adr(arg, class_klass_offset);
3695     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3696     klasses[which_arg] = _gvn.transform(kls);
3697   }
3698 
3699   // Having loaded both klasses, test each for null.
3700   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3701   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3702     Node* kls = klasses[which_arg];
3703     Node* null_ctl = top();
3704     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3705     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3706     region-&gt;init_req(prim_path, null_ctl);
3707     if (stopped())  break;
3708     klasses[which_arg] = kls;
3709   }
3710 
3711   if (!stopped()) {
3712     // now we have two reference types, in klasses[0..1]
3713     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3714     Node* superk = klasses[0];  // the receiver
3715     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3716     // now we have a successful reference subtype check
3717     region-&gt;set_req(_ref_subtype_path, control());
3718   }
3719 
3720   // If both operands are primitive (both klasses null), then
3721   // we must return true when they are identical primitives.
3722   // It is convenient to test this after the first null klass check.
3723   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3724   if (!stopped()) {
3725     // Since superc is primitive, make a guard for the superc==subc case.
3726     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3727     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
3728     generate_guard(bol_eq, region, PROB_FAIR);
3729     if (region-&gt;req() == PATH_LIMIT+1) {
3730       // A guard was added.  If the added guard is taken, superc==subc.
3731       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3732       region-&gt;del_req(PATH_LIMIT);
3733     }
3734     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3735   }
3736 
3737   // these are the only paths that produce 'true':
3738   phi-&gt;set_req(_prim_same_path,   intcon(1));
3739   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3740 
3741   // pull together the cases:
3742   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3743   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3744     Node* ctl = region-&gt;in(i);
3745     if (ctl == NULL || ctl == top()) {
3746       region-&gt;set_req(i, top());
3747       phi   -&gt;set_req(i, top());
3748     } else if (phi-&gt;in(i) == NULL) {
3749       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3750     }
3751   }
3752 
3753   set_control(_gvn.transform(region));
3754   set_result(_gvn.transform(phi));
3755   return true;
3756 }
3757 
3758 //---------------------generate_array_guard_common------------------------
3759 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3760                                                   bool obj_array, bool not_array) {
3761 
3762   if (stopped()) {
3763     return NULL;
3764   }
3765 
3766   // If obj_array/non_array==false/false:
3767   // Branch around if the given klass is in fact an array (either obj or prim).
3768   // If obj_array/non_array==false/true:
3769   // Branch around if the given klass is not an array klass of any kind.
3770   // If obj_array/non_array==true/true:
3771   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3772   // If obj_array/non_array==true/false:
3773   // Branch around if the kls is an oop array (Object[] or subtype)
3774   //
3775   // Like generate_guard, adds a new path onto the region.
3776   jint  layout_con = 0;
3777   Node* layout_val = get_layout_helper(kls, layout_con);
3778   if (layout_val == NULL) {
3779     bool query = (obj_array
3780                   ? Klass::layout_helper_is_objArray(layout_con)
3781                   : Klass::layout_helper_is_array(layout_con));
3782     if (query == not_array) {
3783       return NULL;                       // never a branch
3784     } else {                             // always a branch
3785       Node* always_branch = control();
3786       if (region != NULL)
3787         region-&gt;add_req(always_branch);
3788       set_control(top());
3789       return always_branch;
3790     }
3791   }
3792   // Now test the correct condition.
3793   jint  nval = (obj_array
3794                 ? ((jint)Klass::_lh_array_tag_type_value
3795                    &lt;&lt;    Klass::_lh_array_tag_shift)
3796                 : Klass::_lh_neutral_value);
3797   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3798   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3799   // invert the test if we are looking for a non-array
3800   if (not_array)  btest = BoolTest(btest).negate();
3801   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3802   return generate_fair_guard(bol, region);
3803 }
3804 
3805 
3806 //-----------------------inline_native_newArray--------------------------
3807 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3808 bool LibraryCallKit::inline_native_newArray() {
3809   Node* mirror    = argument(0);
3810   Node* count_val = argument(1);
3811 
3812   mirror = null_check(mirror);
3813   // If mirror or obj is dead, only null-path is taken.
3814   if (stopped())  return true;
3815 
3816   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3817   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3818   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3819   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3820   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3821 
3822   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3823   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3824                                                   result_reg, _slow_path);
3825   Node* normal_ctl   = control();
3826   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3827 
3828   // Generate code for the slow case.  We make a call to newArray().
3829   set_control(no_array_ctl);
3830   if (!stopped()) {
3831     // Either the input type is void.class, or else the
3832     // array klass has not yet been cached.  Either the
3833     // ensuing call will throw an exception, or else it
3834     // will cache the array klass for next time.
3835     PreserveJVMState pjvms(this);
3836     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3837     Node* slow_result = set_results_for_java_call(slow_call);
3838     // this-&gt;control() comes from set_results_for_java_call
3839     result_reg-&gt;set_req(_slow_path, control());
3840     result_val-&gt;set_req(_slow_path, slow_result);
3841     result_io -&gt;set_req(_slow_path, i_o());
3842     result_mem-&gt;set_req(_slow_path, reset_memory());
3843   }
3844 
3845   set_control(normal_ctl);
3846   if (!stopped()) {
3847     // Normal case:  The array type has been cached in the java.lang.Class.
3848     // The following call works fine even if the array type is polymorphic.
3849     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3850     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3851     result_reg-&gt;init_req(_normal_path, control());
3852     result_val-&gt;init_req(_normal_path, obj);
3853     result_io -&gt;init_req(_normal_path, i_o());
3854     result_mem-&gt;init_req(_normal_path, reset_memory());
3855   }
3856 
3857   // Return the combined state.
3858   set_i_o(        _gvn.transform(result_io)  );
3859   set_all_memory( _gvn.transform(result_mem));
3860 
3861   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3862   set_result(result_reg, result_val);
3863   return true;
3864 }
3865 
3866 //----------------------inline_native_getLength--------------------------
3867 // public static native int java.lang.reflect.Array.getLength(Object array);
3868 bool LibraryCallKit::inline_native_getLength() {
3869   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3870 
3871   Node* array = null_check(argument(0));
3872   // If array is dead, only null-path is taken.
3873   if (stopped())  return true;
3874 
3875   // Deoptimize if it is a non-array.
3876   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3877 
3878   if (non_array != NULL) {
3879     PreserveJVMState pjvms(this);
3880     set_control(non_array);
3881     uncommon_trap(Deoptimization::Reason_intrinsic,
3882                   Deoptimization::Action_maybe_recompile);
3883   }
3884 
3885   // If control is dead, only non-array-path is taken.
3886   if (stopped())  return true;
3887 
3888   // The works fine even if the array type is polymorphic.
3889   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3890   Node* result = load_array_length(array);
3891 
3892   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3893   set_result(result);
3894   return true;
3895 }
3896 
3897 //------------------------inline_array_copyOf----------------------------
3898 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3899 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3900 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3901   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3902 
3903   // Get the arguments.
3904   Node* original          = argument(0);
3905   Node* start             = is_copyOfRange? argument(1): intcon(0);
3906   Node* end               = is_copyOfRange? argument(2): argument(1);
3907   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3908 
3909   Node* newcopy = NULL;
3910 
3911   // Set the original stack and the reexecute bit for the interpreter to reexecute
3912   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3913   { PreserveReexecuteState preexecs(this);
3914     jvms()-&gt;set_should_reexecute(true);
3915 
3916     array_type_mirror = null_check(array_type_mirror);
3917     original          = null_check(original);
3918 
3919     // Check if a null path was taken unconditionally.
3920     if (stopped())  return true;
3921 
3922     Node* orig_length = load_array_length(original);
3923 
3924     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3925     klass_node = null_check(klass_node);
3926 
3927     RegionNode* bailout = new RegionNode(1);
3928     record_for_igvn(bailout);
3929 
3930     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3931     // Bail out if that is so.
3932     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3933     if (not_objArray != NULL) {
3934       // Improve the klass node's type from the new optimistic assumption:
3935       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3936       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3937       Node* cast = new CastPPNode(klass_node, akls);
3938       cast-&gt;init_req(0, control());
3939       klass_node = _gvn.transform(cast);
3940     }
3941 
3942     // Bail out if either start or end is negative.
3943     generate_negative_guard(start, bailout, &amp;start);
3944     generate_negative_guard(end,   bailout, &amp;end);
3945 
3946     Node* length = end;
3947     if (_gvn.type(start) != TypeInt::ZERO) {
3948       length = _gvn.transform(new SubINode(end, start));
3949     }
3950 
3951     // Bail out if length is negative.
3952     // Without this the new_array would throw
3953     // NegativeArraySizeException but IllegalArgumentException is what
3954     // should be thrown
3955     generate_negative_guard(length, bailout, &amp;length);
3956 
3957     if (bailout-&gt;req() &gt; 1) {
3958       PreserveJVMState pjvms(this);
3959       set_control(_gvn.transform(bailout));
3960       uncommon_trap(Deoptimization::Reason_intrinsic,
3961                     Deoptimization::Action_maybe_recompile);
3962     }
3963 
3964     if (!stopped()) {
3965       // How many elements will we copy from the original?
3966       // The answer is MinI(orig_length - start, length).
3967       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
3968       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3969 
3970       // Generate a direct call to the right arraycopy function(s).
3971       // We know the copy is disjoint but we might not know if the
3972       // oop stores need checking.
3973       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3974       // This will fail a store-check if x contains any non-nulls.
3975 
3976       // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3977       // loads/stores but it is legal only if we're sure the
3978       // Arrays.copyOf would succeed. So we need all input arguments
3979       // to the copyOf to be validated, including that the copy to the
3980       // new array won't trigger an ArrayStoreException. That subtype
3981       // check can be optimized if we know something on the type of
3982       // the input array from type speculation.
3983       if (_gvn.type(klass_node)-&gt;singleton()) {
3984         ciKlass* subk   = _gvn.type(load_object_klass(original))-&gt;is_klassptr()-&gt;klass();
3985         ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3986 
3987         int test = C-&gt;static_subtype_check(superk, subk);
3988         if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3989           const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3990           if (t_original-&gt;speculative_type() != NULL) {
3991             original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3992           }
3993         }
3994       }
3995 
3996       bool validated = false;
3997       // Reason_class_check rather than Reason_intrinsic because we
3998       // want to intrinsify even if this traps.
3999       if (!too_many_traps(Deoptimization::Reason_class_check)) {
4000         Node* not_subtype_ctrl = gen_subtype_check(load_object_klass(original),
4001                                                    klass_node);
4002 
4003         if (not_subtype_ctrl != top()) {
4004           PreserveJVMState pjvms(this);
4005           set_control(not_subtype_ctrl);
4006           uncommon_trap(Deoptimization::Reason_class_check,
4007                         Deoptimization::Action_make_not_entrant);
4008           assert(stopped(), "Should be stopped");
4009         }
4010         validated = true;
4011       }
4012 
4013       if (!stopped()) {
4014         newcopy = new_array(klass_node, length, 0);  // no arguments to push
4015 
4016         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true,
4017                                                 load_object_klass(original), klass_node);
4018         if (!is_copyOfRange) {
4019           ac-&gt;set_copyof(validated);
4020         } else {
4021           ac-&gt;set_copyofrange(validated);
4022         }
4023         Node* n = _gvn.transform(ac);
4024         if (n == ac) {
4025           ac-&gt;connect_outputs(this);
4026         } else {
4027           assert(validated, "shouldn't transform if all arguments not validated");
4028           set_all_memory(n);
4029         }
4030       }
4031     }
4032   } // original reexecute is set back here
4033 
4034   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4035   if (!stopped()) {
4036     set_result(newcopy);
4037   }
4038   return true;
4039 }
4040 
4041 
4042 //----------------------generate_virtual_guard---------------------------
4043 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
4044 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
4045                                              RegionNode* slow_region) {
4046   ciMethod* method = callee();
4047   int vtable_index = method-&gt;vtable_index();
4048   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4049          "bad index %d", vtable_index);
4050   // Get the Method* out of the appropriate vtable entry.
4051   int entry_offset  = (InstanceKlass::vtable_start_offset() +
4052                      vtable_index*vtableEntry::size()) * wordSize +
4053                      vtableEntry::method_offset_in_bytes();
4054   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
4055   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4056 
4057   // Compare the target method with the expected method (e.g., Object.hashCode).
4058   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
4059 
4060   Node* native_call = makecon(native_call_addr);
4061   Node* chk_native  = _gvn.transform(new CmpPNode(target_call, native_call));
4062   Node* test_native = _gvn.transform(new BoolNode(chk_native, BoolTest::ne));
4063 
4064   return generate_slow_guard(test_native, slow_region);
4065 }
4066 
4067 //-----------------------generate_method_call----------------------------
4068 // Use generate_method_call to make a slow-call to the real
4069 // method if the fast path fails.  An alternative would be to
4070 // use a stub like OptoRuntime::slow_arraycopy_Java.
4071 // This only works for expanding the current library call,
4072 // not another intrinsic.  (E.g., don't use this for making an
4073 // arraycopy call inside of the copyOf intrinsic.)
4074 CallJavaNode*
4075 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
4076   // When compiling the intrinsic method itself, do not use this technique.
4077   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
4078 
4079   ciMethod* method = callee();
4080   // ensure the JVMS we have will be correct for this call
4081   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
4082 
4083   const TypeFunc* tf = TypeFunc::make(method);
4084   CallJavaNode* slow_call;
4085   if (is_static) {
4086     assert(!is_virtual, "");
4087     slow_call = new CallStaticJavaNode(C, tf,
4088                            SharedRuntime::get_resolve_static_call_stub(),
4089                            method, bci());
4090   } else if (is_virtual) {
4091     null_check_receiver();
4092     int vtable_index = Method::invalid_vtable_index;
4093     if (UseInlineCaches) {
4094       // Suppress the vtable call
4095     } else {
4096       // hashCode and clone are not a miranda methods,
4097       // so the vtable index is fixed.
4098       // No need to use the linkResolver to get it.
4099        vtable_index = method-&gt;vtable_index();
4100        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4101               "bad index %d", vtable_index);
4102     }
4103     slow_call = new CallDynamicJavaNode(tf,
4104                           SharedRuntime::get_resolve_virtual_call_stub(),
4105                           method, vtable_index, bci());
4106   } else {  // neither virtual nor static:  opt_virtual
4107     null_check_receiver();
4108     slow_call = new CallStaticJavaNode(C, tf,
4109                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
4110                                 method, bci());
4111     slow_call-&gt;set_optimized_virtual(true);
4112   }
4113   set_arguments_for_java_call(slow_call);
4114   set_edges_for_java_call(slow_call);
4115   return slow_call;
4116 }
4117 
4118 
4119 /**
4120  * Build special case code for calls to hashCode on an object. This call may
4121  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4122  * slightly different code.
4123  */
4124 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4125   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
4126   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
4127 
4128   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4129 
4130   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4131   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
4132   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
4133   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4134   Node* obj = NULL;
4135   if (!is_static) {
4136     // Check for hashing null object
4137     obj = null_check_receiver();
4138     if (stopped())  return true;        // unconditionally null
4139     result_reg-&gt;init_req(_null_path, top());
4140     result_val-&gt;init_req(_null_path, top());
4141   } else {
4142     // Do a null check, and return zero if null.
4143     // System.identityHashCode(null) == 0
4144     obj = argument(0);
4145     Node* null_ctl = top();
4146     obj = null_check_oop(obj, &amp;null_ctl);
4147     result_reg-&gt;init_req(_null_path, null_ctl);
4148     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4149   }
4150 
4151   // Unconditionally null?  Then return right away.
4152   if (stopped()) {
4153     set_control( result_reg-&gt;in(_null_path));
4154     if (!stopped())
4155       set_result(result_val-&gt;in(_null_path));
4156     return true;
4157   }
4158 
4159   // We only go to the fast case code if we pass a number of guards.  The
4160   // paths which do not pass are accumulated in the slow_region.
4161   RegionNode* slow_region = new RegionNode(1);
4162   record_for_igvn(slow_region);
4163 
4164   // If this is a virtual call, we generate a funny guard.  We pull out
4165   // the vtable entry corresponding to hashCode() from the target object.
4166   // If the target method which we are calling happens to be the native
4167   // Object hashCode() method, we pass the guard.  We do not need this
4168   // guard for non-virtual calls -- the caller is known to be the native
4169   // Object hashCode().
4170   if (is_virtual) {
4171     // After null check, get the object's klass.
4172     Node* obj_klass = load_object_klass(obj);
4173     generate_virtual_guard(obj_klass, slow_region);
4174   }
4175 
4176   // Get the header out of the object, use LoadMarkNode when available
4177   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4178   // The control of the load must be NULL. Otherwise, the load can move before
4179   // the null check after castPP removal.
4180   Node* no_ctrl = NULL;
4181   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4182 
4183   // Test the header to see if it is unlocked.
4184   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4185   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
4186   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4187   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
4188   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
4189 
4190   generate_slow_guard(test_unlocked, slow_region);
4191 
4192   // Get the hash value and check to see that it has been properly assigned.
4193   // We depend on hash_mask being at most 32 bits and avoid the use of
4194   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4195   // vm: see markOop.hpp.
4196   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4197   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4198   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
4199   // This hack lets the hash bits live anywhere in the mark object now, as long
4200   // as the shift drops the relevant bits into the low 32 bits.  Note that
4201   // Java spec says that HashCode is an int so there's no point in capturing
4202   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4203   hshifted_header      = ConvX2I(hshifted_header);
4204   Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));
4205 
4206   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4207   Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));
4208   Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));
4209 
4210   generate_slow_guard(test_assigned, slow_region);
4211 
4212   Node* init_mem = reset_memory();
4213   // fill in the rest of the null path:
4214   result_io -&gt;init_req(_null_path, i_o());
4215   result_mem-&gt;init_req(_null_path, init_mem);
4216 
4217   result_val-&gt;init_req(_fast_path, hash_val);
4218   result_reg-&gt;init_req(_fast_path, control());
4219   result_io -&gt;init_req(_fast_path, i_o());
4220   result_mem-&gt;init_req(_fast_path, init_mem);
4221 
4222   // Generate code for the slow case.  We make a call to hashCode().
4223   set_control(_gvn.transform(slow_region));
4224   if (!stopped()) {
4225     // No need for PreserveJVMState, because we're using up the present state.
4226     set_all_memory(init_mem);
4227     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4228     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4229     Node* slow_result = set_results_for_java_call(slow_call);
4230     // this-&gt;control() comes from set_results_for_java_call
4231     result_reg-&gt;init_req(_slow_path, control());
4232     result_val-&gt;init_req(_slow_path, slow_result);
4233     result_io  -&gt;set_req(_slow_path, i_o());
4234     result_mem -&gt;set_req(_slow_path, reset_memory());
4235   }
4236 
4237   // Return the combined state.
4238   set_i_o(        _gvn.transform(result_io)  );
4239   set_all_memory( _gvn.transform(result_mem));
4240 
4241   set_result(result_reg, result_val);
4242   return true;
4243 }
4244 
4245 //---------------------------inline_native_getClass----------------------------
4246 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4247 //
4248 // Build special case code for calls to getClass on an object.
4249 bool LibraryCallKit::inline_native_getClass() {
4250   Node* obj = null_check_receiver();
4251   if (stopped())  return true;
4252   set_result(load_mirror_from_klass(load_object_klass(obj)));
4253   return true;
4254 }
4255 
4256 //-----------------inline_native_Reflection_getCallerClass---------------------
4257 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4258 //
4259 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4260 //
4261 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4262 // in that it must skip particular security frames and checks for
4263 // caller sensitive methods.
4264 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4265 #ifndef PRODUCT
4266   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4267     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4268   }
4269 #endif
4270 
4271   if (!jvms()-&gt;has_method()) {
4272 #ifndef PRODUCT
4273     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4274       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4275     }
4276 #endif
4277     return false;
4278   }
4279 
4280   // Walk back up the JVM state to find the caller at the required
4281   // depth.
4282   JVMState* caller_jvms = jvms();
4283 
4284   // Cf. JVM_GetCallerClass
4285   // NOTE: Start the loop at depth 1 because the current JVM state does
4286   // not include the Reflection.getCallerClass() frame.
4287   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4288     ciMethod* m = caller_jvms-&gt;method();
4289     switch (n) {
4290     case 0:
4291       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4292       break;
4293     case 1:
4294       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4295       if (!m-&gt;caller_sensitive()) {
4296 #ifndef PRODUCT
4297         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4298           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4299         }
4300 #endif
4301         return false;  // bail-out; let JVM_GetCallerClass do the work
4302       }
4303       break;
4304     default:
4305       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4306         // We have reached the desired frame; return the holder class.
4307         // Acquire method holder as java.lang.Class and push as constant.
4308         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4309         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4310         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4311 
4312 #ifndef PRODUCT
4313         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4314           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4315           tty-&gt;print_cr("  JVM state at this point:");
4316           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4317             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4318             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4319           }
4320         }
4321 #endif
4322         return true;
4323       }
4324       break;
4325     }
4326   }
4327 
4328 #ifndef PRODUCT
4329   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4330     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4331     tty-&gt;print_cr("  JVM state at this point:");
4332     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4333       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4334       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4335     }
4336   }
4337 #endif
4338 
4339   return false;  // bail-out; let JVM_GetCallerClass do the work
4340 }
4341 
4342 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4343   Node* arg = argument(0);
4344   Node* result = NULL;
4345 
4346   switch (id) {
4347   case vmIntrinsics::_floatToRawIntBits:    result = new MoveF2INode(arg);  break;
4348   case vmIntrinsics::_intBitsToFloat:       result = new MoveI2FNode(arg);  break;
4349   case vmIntrinsics::_doubleToRawLongBits:  result = new MoveD2LNode(arg);  break;
4350   case vmIntrinsics::_longBitsToDouble:     result = new MoveL2DNode(arg);  break;
4351 
4352   case vmIntrinsics::_doubleToLongBits: {
4353     // two paths (plus control) merge in a wood
4354     RegionNode *r = new RegionNode(3);
4355     Node *phi = new PhiNode(r, TypeLong::LONG);
4356 
4357     Node *cmpisnan = _gvn.transform(new CmpDNode(arg, arg));
4358     // Build the boolean node
4359     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4360 
4361     // Branch either way.
4362     // NaN case is less traveled, which makes all the difference.
4363     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4364     Node *opt_isnan = _gvn.transform(ifisnan);
4365     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4366     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4367     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4368 
4369     set_control(iftrue);
4370 
4371     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4372     Node *slow_result = longcon(nan_bits); // return NaN
4373     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4374     r-&gt;init_req(1, iftrue);
4375 
4376     // Else fall through
4377     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4378     set_control(iffalse);
4379 
4380     phi-&gt;init_req(2, _gvn.transform(new MoveD2LNode(arg)));
4381     r-&gt;init_req(2, iffalse);
4382 
4383     // Post merge
4384     set_control(_gvn.transform(r));
4385     record_for_igvn(r);
4386 
4387     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4388     result = phi;
4389     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4390     break;
4391   }
4392 
4393   case vmIntrinsics::_floatToIntBits: {
4394     // two paths (plus control) merge in a wood
4395     RegionNode *r = new RegionNode(3);
4396     Node *phi = new PhiNode(r, TypeInt::INT);
4397 
4398     Node *cmpisnan = _gvn.transform(new CmpFNode(arg, arg));
4399     // Build the boolean node
4400     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4401 
4402     // Branch either way.
4403     // NaN case is less traveled, which makes all the difference.
4404     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4405     Node *opt_isnan = _gvn.transform(ifisnan);
4406     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4407     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4408     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4409 
4410     set_control(iftrue);
4411 
4412     static const jint nan_bits = 0x7fc00000;
4413     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4414     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4415     r-&gt;init_req(1, iftrue);
4416 
4417     // Else fall through
4418     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4419     set_control(iffalse);
4420 
4421     phi-&gt;init_req(2, _gvn.transform(new MoveF2INode(arg)));
4422     r-&gt;init_req(2, iffalse);
4423 
4424     // Post merge
4425     set_control(_gvn.transform(r));
4426     record_for_igvn(r);
4427 
4428     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4429     result = phi;
4430     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4431     break;
4432   }
4433 
4434   default:
4435     fatal_unexpected_iid(id);
4436     break;
4437   }
4438   set_result(_gvn.transform(result));
4439   return true;
4440 }
4441 
4442 //----------------------inline_unsafe_copyMemory-------------------------
4443 // public native void Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4444 bool LibraryCallKit::inline_unsafe_copyMemory() {
4445   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4446   null_check_receiver();  // null-check receiver
4447   if (stopped())  return true;
4448 
4449   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4450 
4451   Node* src_ptr =         argument(1);   // type: oop
4452   Node* src_off = ConvL2X(argument(2));  // type: long
4453   Node* dst_ptr =         argument(4);   // type: oop
4454   Node* dst_off = ConvL2X(argument(5));  // type: long
4455   Node* size    = ConvL2X(argument(7));  // type: long
4456 
4457   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4458          "fieldOffset must be byte-scaled");
4459 
4460   Node* src = make_unsafe_address(src_ptr, src_off);
4461   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4462 
4463   // Conservatively insert a memory barrier on all memory slices.
4464   // Do not let writes of the copy source or destination float below the copy.
4465   insert_mem_bar(Op_MemBarCPUOrder);
4466 
4467   // Call it.  Note that the length argument is not scaled.
4468   make_runtime_call(RC_LEAF|RC_NO_FP,
4469                     OptoRuntime::fast_arraycopy_Type(),
4470                     StubRoutines::unsafe_arraycopy(),
4471                     "unsafe_arraycopy",
4472                     TypeRawPtr::BOTTOM,
4473                     src, dst, size XTOP);
4474 
4475   // Do not let reads of the copy destination float above the copy.
4476   insert_mem_bar(Op_MemBarCPUOrder);
4477 
4478   return true;
4479 }
4480 
4481 //------------------------clone_coping-----------------------------------
4482 // Helper function for inline_native_clone.
4483 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4484   assert(obj_size != NULL, "");
4485   Node* raw_obj = alloc_obj-&gt;in(1);
4486   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4487 
4488   AllocateNode* alloc = NULL;
4489   if (ReduceBulkZeroing) {
4490     // We will be completely responsible for initializing this object -
4491     // mark Initialize node as complete.
4492     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4493     // The object was just allocated - there should be no any stores!
4494     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4495     // Mark as complete_with_arraycopy so that on AllocateNode
4496     // expansion, we know this AllocateNode is initialized by an array
4497     // copy and a StoreStore barrier exists after the array copy.
4498     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4499   }
4500 
4501   // Copy the fastest available way.
4502   // TODO: generate fields copies for small objects instead.
4503   Node* src  = obj;
4504   Node* dest = alloc_obj;
4505   Node* size = _gvn.transform(obj_size);
4506 
4507   // Exclude the header but include array length to copy by 8 bytes words.
4508   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4509   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4510                             instanceOopDesc::base_offset_in_bytes();
4511   // base_off:
4512   // 8  - 32-bit VM
4513   // 12 - 64-bit VM, compressed klass
4514   // 16 - 64-bit VM, normal klass
4515   if (base_off % BytesPerLong != 0) {
4516     assert(UseCompressedClassPointers, "");
4517     if (is_array) {
4518       // Exclude length to copy by 8 bytes words.
4519       base_off += sizeof(int);
4520     } else {
4521       // Include klass to copy by 8 bytes words.
4522       base_off = instanceOopDesc::klass_offset_in_bytes();
4523     }
4524     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4525   }
4526   src  = basic_plus_adr(src,  base_off);
4527   dest = basic_plus_adr(dest, base_off);
4528 
4529   // Compute the length also, if needed:
4530   Node* countx = size;
4531   countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
4532   countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong) ));
4533 
4534   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4535 
4536   ArrayCopyNode* ac = ArrayCopyNode::make(this, false, src, NULL, dest, NULL, countx, false);
4537   ac-&gt;set_clonebasic();
4538   Node* n = _gvn.transform(ac);
4539   if (n == ac) {
4540     set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
4541   } else {
4542     set_all_memory(n);
4543   }
4544 
4545   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4546   if (card_mark) {
4547     assert(!is_array, "");
4548     // Put in store barrier for any and all oops we are sticking
4549     // into this object.  (We could avoid this if we could prove
4550     // that the object type contains no oop fields at all.)
4551     Node* no_particular_value = NULL;
4552     Node* no_particular_field = NULL;
4553     int raw_adr_idx = Compile::AliasIdxRaw;
4554     post_barrier(control(),
4555                  memory(raw_adr_type),
4556                  alloc_obj,
4557                  no_particular_field,
4558                  raw_adr_idx,
4559                  no_particular_value,
4560                  T_OBJECT,
4561                  false);
4562   }
4563 
4564   // Do not let reads from the cloned object float above the arraycopy.
4565   if (alloc != NULL) {
4566     // Do not let stores that initialize this object be reordered with
4567     // a subsequent store that would make this object accessible by
4568     // other threads.
4569     // Record what AllocateNode this StoreStore protects so that
4570     // escape analysis can go from the MemBarStoreStoreNode to the
4571     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4572     // based on the escape status of the AllocateNode.
4573     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4574   } else {
4575     insert_mem_bar(Op_MemBarCPUOrder);
4576   }
4577 }
4578 
4579 //------------------------inline_native_clone----------------------------
4580 // protected native Object java.lang.Object.clone();
4581 //
4582 // Here are the simple edge cases:
4583 //  null receiver =&gt; normal trap
4584 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4585 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4586 //
4587 // The general case has two steps, allocation and copying.
4588 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4589 //
4590 // Copying also has two cases, oop arrays and everything else.
4591 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4592 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4593 //
4594 // These steps fold up nicely if and when the cloned object's klass
4595 // can be sharply typed as an object array, a type array, or an instance.
4596 //
4597 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4598   PhiNode* result_val;
4599 
4600   // Set the reexecute bit for the interpreter to reexecute
4601   // the bytecode that invokes Object.clone if deoptimization happens.
4602   { PreserveReexecuteState preexecs(this);
4603     jvms()-&gt;set_should_reexecute(true);
4604 
4605     Node* obj = null_check_receiver();
4606     if (stopped())  return true;
4607 
4608     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4609 
4610     // If we are going to clone an instance, we need its exact type to
4611     // know the number and types of fields to convert the clone to
4612     // loads/stores. Maybe a speculative type can help us.
4613     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4614         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4615         obj_type-&gt;speculative_type()-&gt;is_instance_klass()) {
4616       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4617       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4618           !spec_ik-&gt;has_injected_fields()) {
4619         ciKlass* k = obj_type-&gt;klass();
4620         if (!k-&gt;is_instance_klass() ||
4621             k-&gt;as_instance_klass()-&gt;is_interface() ||
4622             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4623           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4624         }
4625       }
4626     }
4627 
4628     Node* obj_klass = load_object_klass(obj);
4629     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4630     const TypeOopPtr*   toop   = ((tklass != NULL)
4631                                 ? tklass-&gt;as_instance_type()
4632                                 : TypeInstPtr::NOTNULL);
4633 
4634     // Conservatively insert a memory barrier on all memory slices.
4635     // Do not let writes into the original float below the clone.
4636     insert_mem_bar(Op_MemBarCPUOrder);
4637 
4638     // paths into result_reg:
4639     enum {
4640       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4641       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4642       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4643       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4644       PATH_LIMIT
4645     };
4646     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4647     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4648     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4649     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4650     record_for_igvn(result_reg);
4651 
4652     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4653     int raw_adr_idx = Compile::AliasIdxRaw;
4654 
4655     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4656     if (array_ctl != NULL) {
4657       // It's an array.
4658       PreserveJVMState pjvms(this);
4659       set_control(array_ctl);
4660       Node* obj_length = load_array_length(obj);
4661       Node* obj_size  = NULL;
4662       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4663 
4664       if (!use_ReduceInitialCardMarks()) {
4665         // If it is an oop array, it requires very special treatment,
4666         // because card marking is required on each card of the array.
4667         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4668         if (is_obja != NULL) {
4669           PreserveJVMState pjvms2(this);
4670           set_control(is_obja);
4671           // Generate a direct call to the right arraycopy function(s).
4672           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4673           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL);
4674           ac-&gt;set_cloneoop();
4675           Node* n = _gvn.transform(ac);
4676           assert(n == ac, "cannot disappear");
4677           ac-&gt;connect_outputs(this);
4678 
4679           result_reg-&gt;init_req(_objArray_path, control());
4680           result_val-&gt;init_req(_objArray_path, alloc_obj);
4681           result_i_o -&gt;set_req(_objArray_path, i_o());
4682           result_mem -&gt;set_req(_objArray_path, reset_memory());
4683         }
4684       }
4685       // Otherwise, there are no card marks to worry about.
4686       // (We can dispense with card marks if we know the allocation
4687       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4688       //  causes the non-eden paths to take compensating steps to
4689       //  simulate a fresh allocation, so that no further
4690       //  card marks are required in compiled code to initialize
4691       //  the object.)
4692 
4693       if (!stopped()) {
4694         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4695 
4696         // Present the results of the copy.
4697         result_reg-&gt;init_req(_array_path, control());
4698         result_val-&gt;init_req(_array_path, alloc_obj);
4699         result_i_o -&gt;set_req(_array_path, i_o());
4700         result_mem -&gt;set_req(_array_path, reset_memory());
4701       }
4702     }
4703 
4704     // We only go to the instance fast case code if we pass a number of guards.
4705     // The paths which do not pass are accumulated in the slow_region.
4706     RegionNode* slow_region = new RegionNode(1);
4707     record_for_igvn(slow_region);
4708     if (!stopped()) {
4709       // It's an instance (we did array above).  Make the slow-path tests.
4710       // If this is a virtual call, we generate a funny guard.  We grab
4711       // the vtable entry corresponding to clone() from the target object.
4712       // If the target method which we are calling happens to be the
4713       // Object clone() method, we pass the guard.  We do not need this
4714       // guard for non-virtual calls; the caller is known to be the native
4715       // Object clone().
4716       if (is_virtual) {
4717         generate_virtual_guard(obj_klass, slow_region);
4718       }
4719 
4720       // The object must be cloneable and must not have a finalizer.
4721       // Both of these conditions may be checked in a single test.
4722       // We could optimize the cloneable test further, but we don't care.
4723       generate_access_flags_guard(obj_klass,
4724                                   // Test both conditions:
4725                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4726                                   // Must be cloneable but not finalizer:
4727                                   JVM_ACC_IS_CLONEABLE,
4728                                   slow_region);
4729     }
4730 
4731     if (!stopped()) {
4732       // It's an instance, and it passed the slow-path tests.
4733       PreserveJVMState pjvms(this);
4734       Node* obj_size  = NULL;
4735       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4736       // is reexecuted if deoptimization occurs and there could be problems when merging
4737       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4738       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4739 
4740       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4741 
4742       // Present the results of the slow call.
4743       result_reg-&gt;init_req(_instance_path, control());
4744       result_val-&gt;init_req(_instance_path, alloc_obj);
4745       result_i_o -&gt;set_req(_instance_path, i_o());
4746       result_mem -&gt;set_req(_instance_path, reset_memory());
4747     }
4748 
4749     // Generate code for the slow case.  We make a call to clone().
4750     set_control(_gvn.transform(slow_region));
4751     if (!stopped()) {
4752       PreserveJVMState pjvms(this);
4753       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4754       Node* slow_result = set_results_for_java_call(slow_call);
4755       // this-&gt;control() comes from set_results_for_java_call
4756       result_reg-&gt;init_req(_slow_path, control());
4757       result_val-&gt;init_req(_slow_path, slow_result);
4758       result_i_o -&gt;set_req(_slow_path, i_o());
4759       result_mem -&gt;set_req(_slow_path, reset_memory());
4760     }
4761 
4762     // Return the combined state.
4763     set_control(    _gvn.transform(result_reg));
4764     set_i_o(        _gvn.transform(result_i_o));
4765     set_all_memory( _gvn.transform(result_mem));
4766   } // original reexecute is set back here
4767 
4768   set_result(_gvn.transform(result_val));
4769   return true;
4770 }
4771 
4772 // If we have a tighly coupled allocation, the arraycopy may take care
4773 // of the array initialization. If one of the guards we insert between
4774 // the allocation and the arraycopy causes a deoptimization, an
4775 // unitialized array will escape the compiled method. To prevent that
4776 // we set the JVM state for uncommon traps between the allocation and
4777 // the arraycopy to the state before the allocation so, in case of
4778 // deoptimization, we'll reexecute the allocation and the
4779 // initialization.
4780 JVMState* LibraryCallKit::arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp) {
4781   if (alloc != NULL) {
4782     ciMethod* trap_method = alloc-&gt;jvms()-&gt;method();
4783     int trap_bci = alloc-&gt;jvms()-&gt;bci();
4784 
4785     if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;
4786           !C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
4787       // Make sure there's no store between the allocation and the
4788       // arraycopy otherwise visible side effects could be rexecuted
4789       // in case of deoptimization and cause incorrect execution.
4790       bool no_interfering_store = true;
4791       Node* mem = alloc-&gt;in(TypeFunc::Memory);
4792       if (mem-&gt;is_MergeMem()) {
4793         for (MergeMemStream mms(merged_memory(), mem-&gt;as_MergeMem()); mms.next_non_empty2(); ) {
4794           Node* n = mms.memory();
4795           if (n != mms.memory2() &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4796             assert(n-&gt;is_Store(), "what else?");
4797             no_interfering_store = false;
4798             break;
4799           }
4800         }
4801       } else {
4802         for (MergeMemStream mms(merged_memory()); mms.next_non_empty(); ) {
4803           Node* n = mms.memory();
4804           if (n != mem &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4805             assert(n-&gt;is_Store(), "what else?");
4806             no_interfering_store = false;
4807             break;
4808           }
4809         }
4810       }
4811 
4812       if (no_interfering_store) {
4813         JVMState* old_jvms = alloc-&gt;jvms()-&gt;clone_shallow(C);
4814         uint size = alloc-&gt;req();
4815         SafePointNode* sfpt = new SafePointNode(size, old_jvms);
4816         old_jvms-&gt;set_map(sfpt);
4817         for (uint i = 0; i &lt; size; i++) {
4818           sfpt-&gt;init_req(i, alloc-&gt;in(i));
4819         }
4820         // re-push array length for deoptimization
4821         sfpt-&gt;ins_req(old_jvms-&gt;stkoff() + old_jvms-&gt;sp(), alloc-&gt;in(AllocateNode::ALength));
4822         old_jvms-&gt;set_sp(old_jvms-&gt;sp()+1);
4823         old_jvms-&gt;set_monoff(old_jvms-&gt;monoff()+1);
4824         old_jvms-&gt;set_scloff(old_jvms-&gt;scloff()+1);
4825         old_jvms-&gt;set_endoff(old_jvms-&gt;endoff()+1);
4826         old_jvms-&gt;set_should_reexecute(true);
4827 
4828         sfpt-&gt;set_i_o(map()-&gt;i_o());
4829         sfpt-&gt;set_memory(map()-&gt;memory());
4830         sfpt-&gt;set_control(map()-&gt;control());
4831 
4832         JVMState* saved_jvms = jvms();
4833         saved_reexecute_sp = _reexecute_sp;
4834 
4835         set_jvms(sfpt-&gt;jvms());
4836         _reexecute_sp = jvms()-&gt;sp();
4837 
4838         return saved_jvms;
4839       }
4840     }
4841   }
4842   return NULL;
4843 }
4844 
4845 // In case of a deoptimization, we restart execution at the
4846 // allocation, allocating a new array. We would leave an uninitialized
4847 // array in the heap that GCs wouldn't expect. Move the allocation
4848 // after the traps so we don't allocate the array if we
4849 // deoptimize. This is possible because tightly_coupled_allocation()
4850 // guarantees there's no observer of the allocated array at this point
4851 // and the control flow is simple enough.
4852 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp) {
4853   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4854     assert(alloc != NULL, "only with a tightly coupled allocation");
4855     // restore JVM state to the state at the arraycopy
4856     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4857     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), "memory state changed?");
4858     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), "IO state changed?");
4859     // If we've improved the types of some nodes (null check) while
4860     // emitting the guards, propagate them to the current state
4861     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map());
4862     set_jvms(saved_jvms);
4863     _reexecute_sp = saved_reexecute_sp;
4864 
4865     // Remove the allocation from above the guards
4866     CallProjections callprojs;
4867     alloc-&gt;extract_projections(&amp;callprojs, true);
4868     InitializeNode* init = alloc-&gt;initialization();
4869     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
4870     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));
4871     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4872     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4873 
4874     // move the allocation here (after the guards)
4875     _gvn.hash_delete(alloc);
4876     alloc-&gt;set_req(TypeFunc::Control, control());
4877     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4878     Node *mem = reset_memory();
4879     set_all_memory(mem);
4880     alloc-&gt;set_req(TypeFunc::Memory, mem);
4881     set_control(init-&gt;proj_out(TypeFunc::Control));
4882     set_i_o(callprojs.fallthrough_ioproj);
4883 
4884     // Update memory as done in GraphKit::set_output_for_allocation()
4885     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4886     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4887     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4888       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4889     }
4890     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4891     int            elemidx  = C-&gt;get_alias_index(telemref);
4892     set_memory(init-&gt;proj_out(TypeFunc::Memory), Compile::AliasIdxRaw);
4893     set_memory(init-&gt;proj_out(TypeFunc::Memory), elemidx);
4894 
4895     Node* allocx = _gvn.transform(alloc);
4896     assert(allocx == alloc, "where has the allocation gone?");
4897     assert(dest-&gt;is_CheckCastPP(), "not an allocation result?");
4898 
4899     _gvn.hash_delete(dest);
4900     dest-&gt;set_req(0, control());
4901     Node* destx = _gvn.transform(dest);
4902     assert(destx == dest, "where has the allocation result gone?");
4903   }
4904 }
4905 
4906 
4907 //------------------------------inline_arraycopy-----------------------
4908 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4909 //                                                      Object dest, int destPos,
4910 //                                                      int length);
4911 bool LibraryCallKit::inline_arraycopy() {
4912   // Get the arguments.
4913   Node* src         = argument(0);  // type: oop
4914   Node* src_offset  = argument(1);  // type: int
4915   Node* dest        = argument(2);  // type: oop
4916   Node* dest_offset = argument(3);  // type: int
4917   Node* length      = argument(4);  // type: int
4918 
4919 
4920   // Check for allocation before we add nodes that would confuse
4921   // tightly_coupled_allocation()
4922   AllocateArrayNode* alloc = tightly_coupled_allocation(dest, NULL);
4923 
4924   int saved_reexecute_sp = -1;
4925   JVMState* saved_jvms = arraycopy_restore_alloc_state(alloc, saved_reexecute_sp);
4926   // See arraycopy_restore_alloc_state() comment
4927   // if alloc == NULL we don't have to worry about a tightly coupled allocation so we can emit all needed guards
4928   // if saved_jvms != NULL (then alloc != NULL) then we can handle guards and a tightly coupled allocation
4929   // if saved_jvms == NULL and alloc != NULL, we cant emit any guards
4930   bool can_emit_guards = (alloc == NULL || saved_jvms != NULL);
4931 
4932   // The following tests must be performed
4933   // (1) src and dest are arrays.
4934   // (2) src and dest arrays must have elements of the same BasicType
4935   // (3) src and dest must not be null.
4936   // (4) src_offset must not be negative.
4937   // (5) dest_offset must not be negative.
4938   // (6) length must not be negative.
4939   // (7) src_offset + length must not exceed length of src.
4940   // (8) dest_offset + length must not exceed length of dest.
4941   // (9) each element of an oop array must be assignable
4942 
4943   // (3) src and dest must not be null.
4944   // always do this here because we need the JVM state for uncommon traps
4945   Node* null_ctl = top();
4946   src  = saved_jvms != NULL ? null_check_oop(src, &amp;null_ctl, true, true) : null_check(src,  T_ARRAY);
4947   assert(null_ctl-&gt;is_top(), "no null control here");
4948   dest = null_check(dest, T_ARRAY);
4949 
4950   if (!can_emit_guards) {
4951     // if saved_jvms == NULL and alloc != NULL, we don't emit any
4952     // guards but the arraycopy node could still take advantage of a
4953     // tightly allocated allocation. tightly_coupled_allocation() is
4954     // called again to make sure it takes the null check above into
4955     // account: the null check is mandatory and if it caused an
4956     // uncommon trap to be emitted then the allocation can't be
4957     // considered tightly coupled in this context.
4958     alloc = tightly_coupled_allocation(dest, NULL);
4959   }
4960 
4961   bool validated = false;
4962 
4963   const Type* src_type  = _gvn.type(src);
4964   const Type* dest_type = _gvn.type(dest);
4965   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4966   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4967 
4968   // Do we have the type of src?
4969   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4970   // Do we have the type of dest?
4971   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4972   // Is the type for src from speculation?
4973   bool src_spec = false;
4974   // Is the type for dest from speculation?
4975   bool dest_spec = false;
4976 
4977   if ((!has_src || !has_dest) &amp;&amp; can_emit_guards) {
4978     // We don't have sufficient type information, let's see if
4979     // speculative types can help. We need to have types for both src
4980     // and dest so that it pays off.
4981 
4982     // Do we already have or could we have type information for src
4983     bool could_have_src = has_src;
4984     // Do we already have or could we have type information for dest
4985     bool could_have_dest = has_dest;
4986 
4987     ciKlass* src_k = NULL;
4988     if (!has_src) {
4989       src_k = src_type-&gt;speculative_type_not_null();
4990       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4991         could_have_src = true;
4992       }
4993     }
4994 
4995     ciKlass* dest_k = NULL;
4996     if (!has_dest) {
4997       dest_k = dest_type-&gt;speculative_type_not_null();
4998       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4999         could_have_dest = true;
5000       }
5001     }
5002 
5003     if (could_have_src &amp;&amp; could_have_dest) {
5004       // This is going to pay off so emit the required guards
5005       if (!has_src) {
5006         src = maybe_cast_profiled_obj(src, src_k, true);
5007         src_type  = _gvn.type(src);
5008         top_src  = src_type-&gt;isa_aryptr();
5009         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
5010         src_spec = true;
5011       }
5012       if (!has_dest) {
5013         dest = maybe_cast_profiled_obj(dest, dest_k, true);
5014         dest_type  = _gvn.type(dest);
5015         top_dest  = dest_type-&gt;isa_aryptr();
5016         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
5017         dest_spec = true;
5018       }
5019     }
5020   }
5021 
5022   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
5023     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5024     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5025     if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
5026     if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
5027 
5028     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
5029       // If both arrays are object arrays then having the exact types
5030       // for both will remove the need for a subtype check at runtime
5031       // before the call and may make it possible to pick a faster copy
5032       // routine (without a subtype check on every element)
5033       // Do we have the exact type of src?
5034       bool could_have_src = src_spec;
5035       // Do we have the exact type of dest?
5036       bool could_have_dest = dest_spec;
5037       ciKlass* src_k = top_src-&gt;klass();
5038       ciKlass* dest_k = top_dest-&gt;klass();
5039       if (!src_spec) {
5040         src_k = src_type-&gt;speculative_type_not_null();
5041         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
5042           could_have_src = true;
5043         }
5044       }
5045       if (!dest_spec) {
5046         dest_k = dest_type-&gt;speculative_type_not_null();
5047         if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
5048           could_have_dest = true;
5049         }
5050       }
5051       if (could_have_src &amp;&amp; could_have_dest) {
5052         // If we can have both exact types, emit the missing guards
5053         if (could_have_src &amp;&amp; !src_spec) {
5054           src = maybe_cast_profiled_obj(src, src_k, true);
5055         }
5056         if (could_have_dest &amp;&amp; !dest_spec) {
5057           dest = maybe_cast_profiled_obj(dest, dest_k, true);
5058         }
5059       }
5060     }
5061   }
5062 
5063   ciMethod* trap_method = method();
5064   int trap_bci = bci();
5065   if (saved_jvms != NULL) {
5066     trap_method = alloc-&gt;jvms()-&gt;method();
5067     trap_bci = alloc-&gt;jvms()-&gt;bci();
5068   }
5069 
5070   if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;&amp;
5071       can_emit_guards &amp;&amp;
5072       !src-&gt;is_top() &amp;&amp; !dest-&gt;is_top()) {
5073     // validate arguments: enables transformation the ArrayCopyNode
5074     validated = true;
5075 
5076     RegionNode* slow_region = new RegionNode(1);
5077     record_for_igvn(slow_region);
5078 
5079     // (1) src and dest are arrays.
5080     generate_non_array_guard(load_object_klass(src), slow_region);
5081     generate_non_array_guard(load_object_klass(dest), slow_region);
5082 
5083     // (2) src and dest arrays must have elements of the same BasicType
5084     // done at macro expansion or at Ideal transformation time
5085 
5086     // (4) src_offset must not be negative.
5087     generate_negative_guard(src_offset, slow_region);
5088 
5089     // (5) dest_offset must not be negative.
5090     generate_negative_guard(dest_offset, slow_region);
5091 
5092     // (7) src_offset + length must not exceed length of src.
5093     generate_limit_guard(src_offset, length,
5094                          load_array_length(src),
5095                          slow_region);
5096 
5097     // (8) dest_offset + length must not exceed length of dest.
5098     generate_limit_guard(dest_offset, length,
5099                          load_array_length(dest),
5100                          slow_region);
5101 
5102     // (9) each element of an oop array must be assignable
5103     Node* src_klass  = load_object_klass(src);
5104     Node* dest_klass = load_object_klass(dest);
5105     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5106 
5107     if (not_subtype_ctrl != top()) {
5108       PreserveJVMState pjvms(this);
5109       set_control(not_subtype_ctrl);
5110       uncommon_trap(Deoptimization::Reason_intrinsic,
5111                     Deoptimization::Action_make_not_entrant);
5112       assert(stopped(), "Should be stopped");
5113     }
5114     {
5115       PreserveJVMState pjvms(this);
5116       set_control(_gvn.transform(slow_region));
5117       uncommon_trap(Deoptimization::Reason_intrinsic,
5118                     Deoptimization::Action_make_not_entrant);
5119       assert(stopped(), "Should be stopped");
5120     }
5121   }
5122 
5123   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp);
5124 
5125   if (stopped()) {
5126     return true;
5127   }
5128 
5129   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL,
5130                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
5131                                           // so the compiler has a chance to eliminate them: during macro expansion,
5132                                           // we have to set their control (CastPP nodes are eliminated).
5133                                           load_object_klass(src), load_object_klass(dest),
5134                                           load_array_length(src), load_array_length(dest));
5135 
5136   ac-&gt;set_arraycopy(validated);
5137 
5138   Node* n = _gvn.transform(ac);
5139   if (n == ac) {
5140     ac-&gt;connect_outputs(this);
5141   } else {
5142     assert(validated, "shouldn't transform if all arguments not validated");
5143     set_all_memory(n);
5144   }
5145 
5146   return true;
5147 }
5148 
5149 
5150 // Helper function which determines if an arraycopy immediately follows
5151 // an allocation, with no intervening tests or other escapes for the object.
5152 AllocateArrayNode*
5153 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5154                                            RegionNode* slow_region) {
5155   if (stopped())             return NULL;  // no fast path
5156   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5157 
5158   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5159   if (alloc == NULL)  return NULL;
5160 
5161   Node* rawmem = memory(Compile::AliasIdxRaw);
5162   // Is the allocation's memory state untouched?
5163   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5164     // Bail out if there have been raw-memory effects since the allocation.
5165     // (Example:  There might have been a call or safepoint.)
5166     return NULL;
5167   }
5168   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5169   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5170     return NULL;
5171   }
5172 
5173   // There must be no unexpected observers of this allocation.
5174   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5175     Node* obs = ptr-&gt;fast_out(i);
5176     if (obs != this-&gt;map()) {
5177       return NULL;
5178     }
5179   }
5180 
5181   // This arraycopy must unconditionally follow the allocation of the ptr.
5182   Node* alloc_ctl = ptr-&gt;in(0);
5183   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5184 
5185   Node* ctl = control();
5186   while (ctl != alloc_ctl) {
5187     // There may be guards which feed into the slow_region.
5188     // Any other control flow means that we might not get a chance
5189     // to finish initializing the allocated object.
5190     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5191       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5192       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5193       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5194       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5195         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5196         continue;
5197       }
5198       // One more try:  Various low-level checks bottom out in
5199       // uncommon traps.  If the debug-info of the trap omits
5200       // any reference to the allocation, as we've already
5201       // observed, then there can be no objection to the trap.
5202       bool found_trap = false;
5203       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5204         Node* obs = not_ctl-&gt;fast_out(j);
5205         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5206             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5207           found_trap = true; break;
5208         }
5209       }
5210       if (found_trap) {
5211         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5212         continue;
5213       }
5214     }
5215     return NULL;
5216   }
5217 
5218   // If we get this far, we have an allocation which immediately
5219   // precedes the arraycopy, and we can take over zeroing the new object.
5220   // The arraycopy will finish the initialization, and provide
5221   // a new control state to which we will anchor the destination pointer.
5222 
5223   return alloc;
5224 }
5225 
5226 //-------------inline_encodeISOArray-----------------------------------
5227 // encode char[] to byte[] in ISO_8859_1
5228 bool LibraryCallKit::inline_encodeISOArray() {
5229   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5230   // no receiver since it is static method
5231   Node *src         = argument(0);
5232   Node *src_offset  = argument(1);
5233   Node *dst         = argument(2);
5234   Node *dst_offset  = argument(3);
5235   Node *length      = argument(4);
5236 
5237   const Type* src_type = src-&gt;Value(&amp;_gvn);
5238   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5239   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5240   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5241   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5242       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5243     // failed array check
5244     return false;
5245   }
5246 
5247   // Figure out the size and type of the elements we will be copying.
5248   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5249   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5250   if (!((src_elem == T_CHAR) || (src_elem== T_BYTE)) || dst_elem != T_BYTE) {
5251     return false;
5252   }
5253 
5254   Node* src_start = array_element_address(src, src_offset, T_CHAR);
5255   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5256   // 'src_start' points to src array + scaled offset
5257   // 'dst_start' points to dst array + scaled offset
5258 
5259   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5260   Node* enc = new EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5261   enc = _gvn.transform(enc);
5262   Node* res_mem = _gvn.transform(new SCMemProjNode(enc));
5263   set_memory(res_mem, mtype);
5264   set_result(enc);
5265   return true;
5266 }
5267 
5268 //-------------inline_multiplyToLen-----------------------------------
5269 bool LibraryCallKit::inline_multiplyToLen() {
5270   assert(UseMultiplyToLenIntrinsic, "not implemented on this platform");
5271 
5272   address stubAddr = StubRoutines::multiplyToLen();
5273   if (stubAddr == NULL) {
5274     return false; // Intrinsic's stub is not implemented on this platform
5275   }
5276   const char* stubName = "multiplyToLen";
5277 
5278   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5279 
5280   // no receiver because it is a static method
5281   Node* x    = argument(0);
5282   Node* xlen = argument(1);
5283   Node* y    = argument(2);
5284   Node* ylen = argument(3);
5285   Node* z    = argument(4);
5286 
5287   const Type* x_type = x-&gt;Value(&amp;_gvn);
5288   const Type* y_type = y-&gt;Value(&amp;_gvn);
5289   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5290   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5291   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5292       top_y == NULL || top_y-&gt;klass() == NULL) {
5293     // failed array check
5294     return false;
5295   }
5296 
5297   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5298   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5299   if (x_elem != T_INT || y_elem != T_INT) {
5300     return false;
5301   }
5302 
5303   // Set the original stack and the reexecute bit for the interpreter to reexecute
5304   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5305   // on the return from z array allocation in runtime.
5306   { PreserveReexecuteState preexecs(this);
5307     jvms()-&gt;set_should_reexecute(true);
5308 
5309     Node* x_start = array_element_address(x, intcon(0), x_elem);
5310     Node* y_start = array_element_address(y, intcon(0), y_elem);
5311     // 'x_start' points to x array + scaled xlen
5312     // 'y_start' points to y array + scaled ylen
5313 
5314     // Allocate the result array
5315     Node* zlen = _gvn.transform(new AddINode(xlen, ylen));
5316     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5317     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5318 
5319     IdealKit ideal(this);
5320 
5321 #define __ ideal.
5322      Node* one = __ ConI(1);
5323      Node* zero = __ ConI(0);
5324      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5325      __ set(need_alloc, zero);
5326      __ set(z_alloc, z);
5327      __ if_then(z, BoolTest::eq, null()); {
5328        __ increment (need_alloc, one);
5329      } __ else_(); {
5330        // Update graphKit memory and control from IdealKit.
5331        sync_kit(ideal);
5332        Node* zlen_arg = load_array_length(z);
5333        // Update IdealKit memory and control from graphKit.
5334        __ sync_kit(this);
5335        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5336          __ increment (need_alloc, one);
5337        } __ end_if();
5338      } __ end_if();
5339 
5340      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5341        // Update graphKit memory and control from IdealKit.
5342        sync_kit(ideal);
5343        Node * narr = new_array(klass_node, zlen, 1);
5344        // Update IdealKit memory and control from graphKit.
5345        __ sync_kit(this);
5346        __ set(z_alloc, narr);
5347      } __ end_if();
5348 
5349      sync_kit(ideal);
5350      z = __ value(z_alloc);
5351      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5352      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5353      // Final sync IdealKit and GraphKit.
5354      final_sync(ideal);
5355 #undef __
5356 
5357     Node* z_start = array_element_address(z, intcon(0), T_INT);
5358 
5359     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5360                                    OptoRuntime::multiplyToLen_Type(),
5361                                    stubAddr, stubName, TypePtr::BOTTOM,
5362                                    x_start, xlen, y_start, ylen, z_start, zlen);
5363   } // original reexecute is set back here
5364 
5365   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5366   set_result(z);
5367   return true;
5368 }
5369 
5370 //-------------inline_squareToLen------------------------------------
5371 bool LibraryCallKit::inline_squareToLen() {
5372   assert(UseSquareToLenIntrinsic, "not implemented on this platform");
5373 
5374   address stubAddr = StubRoutines::squareToLen();
5375   if (stubAddr == NULL) {
5376     return false; // Intrinsic's stub is not implemented on this platform
5377   }
5378   const char* stubName = "squareToLen";
5379 
5380   assert(callee()-&gt;signature()-&gt;size() == 4, "implSquareToLen has 4 parameters");
5381 
5382   Node* x    = argument(0);
5383   Node* len  = argument(1);
5384   Node* z    = argument(2);
5385   Node* zlen = argument(3);
5386 
5387   const Type* x_type = x-&gt;Value(&amp;_gvn);
5388   const Type* z_type = z-&gt;Value(&amp;_gvn);
5389   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5390   const TypeAryPtr* top_z = z_type-&gt;isa_aryptr();
5391   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5392       top_z  == NULL || top_z-&gt;klass()  == NULL) {
5393     // failed array check
5394     return false;
5395   }
5396 
5397   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5398   BasicType z_elem = z_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5399   if (x_elem != T_INT || z_elem != T_INT) {
5400     return false;
5401   }
5402 
5403 
5404   Node* x_start = array_element_address(x, intcon(0), x_elem);
5405   Node* z_start = array_element_address(z, intcon(0), z_elem);
5406 
5407   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5408                                   OptoRuntime::squareToLen_Type(),
5409                                   stubAddr, stubName, TypePtr::BOTTOM,
5410                                   x_start, len, z_start, zlen);
5411 
5412   set_result(z);
5413   return true;
5414 }
5415 
5416 //-------------inline_mulAdd------------------------------------------
5417 bool LibraryCallKit::inline_mulAdd() {
5418   assert(UseMulAddIntrinsic, "not implemented on this platform");
5419 
5420   address stubAddr = StubRoutines::mulAdd();
5421   if (stubAddr == NULL) {
5422     return false; // Intrinsic's stub is not implemented on this platform
5423   }
5424   const char* stubName = "mulAdd";
5425 
5426   assert(callee()-&gt;signature()-&gt;size() == 5, "mulAdd has 5 parameters");
5427 
5428   Node* out      = argument(0);
5429   Node* in       = argument(1);
5430   Node* offset   = argument(2);
5431   Node* len      = argument(3);
5432   Node* k        = argument(4);
5433 
5434   const Type* out_type = out-&gt;Value(&amp;_gvn);
5435   const Type* in_type = in-&gt;Value(&amp;_gvn);
5436   const TypeAryPtr* top_out = out_type-&gt;isa_aryptr();
5437   const TypeAryPtr* top_in = in_type-&gt;isa_aryptr();
5438   if (top_out  == NULL || top_out-&gt;klass()  == NULL ||
5439       top_in == NULL || top_in-&gt;klass() == NULL) {
5440     // failed array check
5441     return false;
5442   }
5443 
5444   BasicType out_elem = out_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5445   BasicType in_elem = in_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5446   if (out_elem != T_INT || in_elem != T_INT) {
5447     return false;
5448   }
5449 
5450   Node* outlen = load_array_length(out);
5451   Node* new_offset = _gvn.transform(new SubINode(outlen, offset));
5452   Node* out_start = array_element_address(out, intcon(0), out_elem);
5453   Node* in_start = array_element_address(in, intcon(0), in_elem);
5454 
5455   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5456                                   OptoRuntime::mulAdd_Type(),
5457                                   stubAddr, stubName, TypePtr::BOTTOM,
5458                                   out_start,in_start, new_offset, len, k);
5459   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5460   set_result(result);
5461   return true;
5462 }
5463 
5464 //-------------inline_montgomeryMultiply-----------------------------------
5465 bool LibraryCallKit::inline_montgomeryMultiply() {
5466   address stubAddr = StubRoutines::montgomeryMultiply();
5467   if (stubAddr == NULL) {
5468     return false; // Intrinsic's stub is not implemented on this platform
5469   }
5470 
5471   assert(UseMontgomeryMultiplyIntrinsic, "not implemented on this platform");
5472   const char* stubName = "montgomery_square";
5473 
5474   assert(callee()-&gt;signature()-&gt;size() == 7, "montgomeryMultiply has 7 parameters");
5475 
5476   Node* a    = argument(0);
5477   Node* b    = argument(1);
5478   Node* n    = argument(2);
5479   Node* len  = argument(3);
5480   Node* inv  = argument(4);
5481   Node* m    = argument(6);
5482 
5483   const Type* a_type = a-&gt;Value(&amp;_gvn);
5484   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5485   const Type* b_type = b-&gt;Value(&amp;_gvn);
5486   const TypeAryPtr* top_b = b_type-&gt;isa_aryptr();
5487   const Type* n_type = a-&gt;Value(&amp;_gvn);
5488   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5489   const Type* m_type = a-&gt;Value(&amp;_gvn);
5490   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5491   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5492       top_b == NULL || top_b-&gt;klass()  == NULL ||
5493       top_n == NULL || top_n-&gt;klass()  == NULL ||
5494       top_m == NULL || top_m-&gt;klass()  == NULL) {
5495     // failed array check
5496     return false;
5497   }
5498 
5499   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5500   BasicType b_elem = b_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5501   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5502   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5503   if (a_elem != T_INT || b_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5504     return false;
5505   }
5506 
5507   // Make the call
5508   {
5509     Node* a_start = array_element_address(a, intcon(0), a_elem);
5510     Node* b_start = array_element_address(b, intcon(0), b_elem);
5511     Node* n_start = array_element_address(n, intcon(0), n_elem);
5512     Node* m_start = array_element_address(m, intcon(0), m_elem);
5513 
5514     Node* call = make_runtime_call(RC_LEAF,
5515                                    OptoRuntime::montgomeryMultiply_Type(),
5516                                    stubAddr, stubName, TypePtr::BOTTOM,
5517                                    a_start, b_start, n_start, len, inv, top(),
5518                                    m_start);
5519     set_result(m);
5520   }
5521 
5522   return true;
5523 }
5524 
5525 bool LibraryCallKit::inline_montgomerySquare() {
5526   address stubAddr = StubRoutines::montgomerySquare();
5527   if (stubAddr == NULL) {
5528     return false; // Intrinsic's stub is not implemented on this platform
5529   }
5530 
5531   assert(UseMontgomerySquareIntrinsic, "not implemented on this platform");
5532   const char* stubName = "montgomery_square";
5533 
5534   assert(callee()-&gt;signature()-&gt;size() == 6, "montgomerySquare has 6 parameters");
5535 
5536   Node* a    = argument(0);
5537   Node* n    = argument(1);
5538   Node* len  = argument(2);
5539   Node* inv  = argument(3);
5540   Node* m    = argument(5);
5541 
5542   const Type* a_type = a-&gt;Value(&amp;_gvn);
5543   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5544   const Type* n_type = a-&gt;Value(&amp;_gvn);
5545   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5546   const Type* m_type = a-&gt;Value(&amp;_gvn);
5547   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5548   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5549       top_n == NULL || top_n-&gt;klass()  == NULL ||
5550       top_m == NULL || top_m-&gt;klass()  == NULL) {
5551     // failed array check
5552     return false;
5553   }
5554 
5555   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5556   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5557   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5558   if (a_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5559     return false;
5560   }
5561 
5562   // Make the call
5563   {
5564     Node* a_start = array_element_address(a, intcon(0), a_elem);
5565     Node* n_start = array_element_address(n, intcon(0), n_elem);
5566     Node* m_start = array_element_address(m, intcon(0), m_elem);
5567 
5568     Node* call = make_runtime_call(RC_LEAF,
5569                                    OptoRuntime::montgomerySquare_Type(),
5570                                    stubAddr, stubName, TypePtr::BOTTOM,
5571                                    a_start, n_start, len, inv, top(),
5572                                    m_start);
5573     set_result(m);
5574   }
5575 
5576   return true;
5577 }
5578 
5579 
5580 /**
5581  * Calculate CRC32 for byte.
5582  * int java.util.zip.CRC32.update(int crc, int b)
5583  */
5584 bool LibraryCallKit::inline_updateCRC32() {
5585   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5586   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5587   // no receiver since it is static method
5588   Node* crc  = argument(0); // type: int
5589   Node* b    = argument(1); // type: int
5590 
5591   /*
5592    *    int c = ~ crc;
5593    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5594    *    b = b ^ (c &gt;&gt;&gt; 8);
5595    *    crc = ~b;
5596    */
5597 
5598   Node* M1 = intcon(-1);
5599   crc = _gvn.transform(new XorINode(crc, M1));
5600   Node* result = _gvn.transform(new XorINode(crc, b));
5601   result = _gvn.transform(new AndINode(result, intcon(0xFF)));
5602 
5603   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5604   Node* offset = _gvn.transform(new LShiftINode(result, intcon(0x2)));
5605   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5606   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5607 
5608   crc = _gvn.transform(new URShiftINode(crc, intcon(8)));
5609   result = _gvn.transform(new XorINode(crc, result));
5610   result = _gvn.transform(new XorINode(result, M1));
5611   set_result(result);
5612   return true;
5613 }
5614 
5615 /**
5616  * Calculate CRC32 for byte[] array.
5617  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5618  */
5619 bool LibraryCallKit::inline_updateBytesCRC32() {
5620   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5621   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5622   // no receiver since it is static method
5623   Node* crc     = argument(0); // type: int
5624   Node* src     = argument(1); // type: oop
5625   Node* offset  = argument(2); // type: int
5626   Node* length  = argument(3); // type: int
5627 
5628   const Type* src_type = src-&gt;Value(&amp;_gvn);
5629   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5630   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5631     // failed array check
5632     return false;
5633   }
5634 
5635   // Figure out the size and type of the elements we will be copying.
5636   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5637   if (src_elem != T_BYTE) {
5638     return false;
5639   }
5640 
5641   // 'src_start' points to src array + scaled offset
5642   Node* src_start = array_element_address(src, offset, src_elem);
5643 
5644   // We assume that range check is done by caller.
5645   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5646 
5647   // Call the stub.
5648   address stubAddr = StubRoutines::updateBytesCRC32();
5649   const char *stubName = "updateBytesCRC32";
5650 
5651   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5652                                  stubAddr, stubName, TypePtr::BOTTOM,
5653                                  crc, src_start, length);
5654   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5655   set_result(result);
5656   return true;
5657 }
5658 
5659 /**
5660  * Calculate CRC32 for ByteBuffer.
5661  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5662  */
5663 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5664   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5665   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5666   // no receiver since it is static method
5667   Node* crc     = argument(0); // type: int
5668   Node* src     = argument(1); // type: long
5669   Node* offset  = argument(3); // type: int
5670   Node* length  = argument(4); // type: int
5671 
5672   src = ConvL2X(src);  // adjust Java long to machine word
5673   Node* base = _gvn.transform(new CastX2PNode(src));
5674   offset = ConvI2X(offset);
5675 
5676   // 'src_start' points to src array + scaled offset
5677   Node* src_start = basic_plus_adr(top(), base, offset);
5678 
5679   // Call the stub.
5680   address stubAddr = StubRoutines::updateBytesCRC32();
5681   const char *stubName = "updateBytesCRC32";
5682 
5683   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5684                                  stubAddr, stubName, TypePtr::BOTTOM,
5685                                  crc, src_start, length);
5686   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5687   set_result(result);
5688   return true;
5689 }
5690 
5691 //------------------------------get_table_from_crc32c_class-----------------------
5692 Node * LibraryCallKit::get_table_from_crc32c_class(ciInstanceKlass *crc32c_class) {
5693   Node* table = load_field_from_object(NULL, "byteTable", "[I", /*is_exact*/ false, /*is_static*/ true, crc32c_class);
5694   assert (table != NULL, "wrong version of java.util.zip.CRC32C");
5695 
5696   return table;
5697 }
5698 
5699 //------------------------------inline_updateBytesCRC32C-----------------------
5700 //
5701 // Calculate CRC32C for byte[] array.
5702 // int java.util.zip.CRC32C.updateBytes(int crc, byte[] buf, int off, int end)
5703 //
5704 bool LibraryCallKit::inline_updateBytesCRC32C() {
5705   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5706   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5707   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5708   // no receiver since it is a static method
5709   Node* crc     = argument(0); // type: int
5710   Node* src     = argument(1); // type: oop
5711   Node* offset  = argument(2); // type: int
5712   Node* end     = argument(3); // type: int
5713 
5714   Node* length = _gvn.transform(new SubINode(end, offset));
5715 
5716   const Type* src_type = src-&gt;Value(&amp;_gvn);
5717   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5718   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5719     // failed array check
5720     return false;
5721   }
5722 
5723   // Figure out the size and type of the elements we will be copying.
5724   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5725   if (src_elem != T_BYTE) {
5726     return false;
5727   }
5728 
5729   // 'src_start' points to src array + scaled offset
5730   Node* src_start = array_element_address(src, offset, src_elem);
5731 
5732   // static final int[] byteTable in class CRC32C
5733   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5734   Node* table_start = array_element_address(table, intcon(0), T_INT);
5735 
5736   // We assume that range check is done by caller.
5737   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5738 
5739   // Call the stub.
5740   address stubAddr = StubRoutines::updateBytesCRC32C();
5741   const char *stubName = "updateBytesCRC32C";
5742 
5743   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5744                                  stubAddr, stubName, TypePtr::BOTTOM,
5745                                  crc, src_start, length, table_start);
5746   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5747   set_result(result);
5748   return true;
5749 }
5750 
5751 //------------------------------inline_updateDirectByteBufferCRC32C-----------------------
5752 //
5753 // Calculate CRC32C for DirectByteBuffer.
5754 // int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)
5755 //
5756 bool LibraryCallKit::inline_updateDirectByteBufferCRC32C() {
5757   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5758   assert(callee()-&gt;signature()-&gt;size() == 5, "updateDirectByteBuffer has 4 parameters and one is long");
5759   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5760   // no receiver since it is a static method
5761   Node* crc     = argument(0); // type: int
5762   Node* src     = argument(1); // type: long
5763   Node* offset  = argument(3); // type: int
5764   Node* end     = argument(4); // type: int
5765 
5766   Node* length = _gvn.transform(new SubINode(end, offset));
5767 
5768   src = ConvL2X(src);  // adjust Java long to machine word
5769   Node* base = _gvn.transform(new CastX2PNode(src));
5770   offset = ConvI2X(offset);
5771 
5772   // 'src_start' points to src array + scaled offset
5773   Node* src_start = basic_plus_adr(top(), base, offset);
5774 
5775   // static final int[] byteTable in class CRC32C
5776   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5777   Node* table_start = array_element_address(table, intcon(0), T_INT);
5778 
5779   // Call the stub.
5780   address stubAddr = StubRoutines::updateBytesCRC32C();
5781   const char *stubName = "updateBytesCRC32C";
5782 
5783   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5784                                  stubAddr, stubName, TypePtr::BOTTOM,
5785                                  crc, src_start, length, table_start);
5786   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5787   set_result(result);
5788   return true;
5789 }
5790 
5791 //------------------------------inline_updateBytesAdler32----------------------
5792 //
5793 // Calculate Adler32 checksum for byte[] array.
5794 // int java.util.zip.Adler32.updateBytes(int crc, byte[] buf, int off, int len)
5795 //
5796 bool LibraryCallKit::inline_updateBytesAdler32() {
5797   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5798   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5799   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5800   // no receiver since it is static method
5801   Node* crc     = argument(0); // type: int
5802   Node* src     = argument(1); // type: oop
5803   Node* offset  = argument(2); // type: int
5804   Node* length  = argument(3); // type: int
5805 
5806   const Type* src_type = src-&gt;Value(&amp;_gvn);
5807   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5808   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5809     // failed array check
5810     return false;
5811   }
5812 
5813   // Figure out the size and type of the elements we will be copying.
5814   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5815   if (src_elem != T_BYTE) {
5816     return false;
5817   }
5818 
5819   // 'src_start' points to src array + scaled offset
5820   Node* src_start = array_element_address(src, offset, src_elem);
5821 
5822   // We assume that range check is done by caller.
5823   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5824 
5825   // Call the stub.
5826   address stubAddr = StubRoutines::updateBytesAdler32();
5827   const char *stubName = "updateBytesAdler32";
5828 
5829   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5830                                  stubAddr, stubName, TypePtr::BOTTOM,
5831                                  crc, src_start, length);
5832   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5833   set_result(result);
5834   return true;
5835 }
5836 
5837 //------------------------------inline_updateByteBufferAdler32---------------
5838 //
5839 // Calculate Adler32 checksum for DirectByteBuffer.
5840 // int java.util.zip.Adler32.updateByteBuffer(int crc, long buf, int off, int len)
5841 //
5842 bool LibraryCallKit::inline_updateByteBufferAdler32() {
5843   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5844   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5845   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5846   // no receiver since it is static method
5847   Node* crc     = argument(0); // type: int
5848   Node* src     = argument(1); // type: long
5849   Node* offset  = argument(3); // type: int
5850   Node* length  = argument(4); // type: int
5851 
5852   src = ConvL2X(src);  // adjust Java long to machine word
5853   Node* base = _gvn.transform(new CastX2PNode(src));
5854   offset = ConvI2X(offset);
5855 
5856   // 'src_start' points to src array + scaled offset
5857   Node* src_start = basic_plus_adr(top(), base, offset);
5858 
5859   // Call the stub.
5860   address stubAddr = StubRoutines::updateBytesAdler32();
5861   const char *stubName = "updateBytesAdler32";
5862 
5863   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5864                                  stubAddr, stubName, TypePtr::BOTTOM,
5865                                  crc, src_start, length);
5866 
5867   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5868   set_result(result);
5869   return true;
5870 }
5871 
5872 //----------------------------inline_reference_get----------------------------
5873 // public T java.lang.ref.Reference.get();
5874 bool LibraryCallKit::inline_reference_get() {
5875   const int referent_offset = java_lang_ref_Reference::referent_offset;
5876   guarantee(referent_offset &gt; 0, "should have already been set");
5877 
5878   // Get the argument:
5879   Node* reference_obj = null_check_receiver();
5880   if (stopped()) return true;
5881 
5882   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5883 
5884   ciInstanceKlass* klass = env()-&gt;Object_klass();
5885   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5886 
5887   Node* no_ctrl = NULL;
5888   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5889 
5890   // Use the pre-barrier to record the value in the referent field
5891   pre_barrier(false /* do_load */,
5892               control(),
5893               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5894               result /* pre_val */,
5895               T_OBJECT);
5896 
5897   // Add memory barrier to prevent commoning reads from this field
5898   // across safepoint since GC can change its value.
5899   insert_mem_bar(Op_MemBarCPUOrder);
5900 
5901   set_result(result);
5902   return true;
5903 }
5904 
5905 
5906 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5907                                               bool is_exact=true, bool is_static=false,
5908                                               ciInstanceKlass * fromKls=NULL) {
5909   if (fromKls == NULL) {
5910     const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5911     assert(tinst != NULL, "obj is null");
5912     assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5913     assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5914     fromKls = tinst-&gt;klass()-&gt;as_instance_klass();
5915   } else {
5916     assert(is_static, "only for static field access");
5917   }
5918   ciField* field = fromKls-&gt;get_field_by_name(ciSymbol::make(fieldName),
5919                                               ciSymbol::make(fieldTypeString),
5920                                               is_static);
5921 
5922   assert (field != NULL, "undefined field");
5923   if (field == NULL) return (Node *) NULL;
5924 
5925   if (is_static) {
5926     const TypeInstPtr* tip = TypeInstPtr::make(fromKls-&gt;java_mirror());
5927     fromObj = makecon(tip);
5928   }
5929 
5930   // Next code  copied from Parse::do_get_xxx():
5931 
5932   // Compute address and memory type.
5933   int offset  = field-&gt;offset_in_bytes();
5934   bool is_vol = field-&gt;is_volatile();
5935   ciType* field_klass = field-&gt;type();
5936   assert(field_klass-&gt;is_loaded(), "should be loaded");
5937   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5938   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5939   BasicType bt = field-&gt;layout_type();
5940 
5941   // Build the resultant type of the load
5942   const Type *type;
5943   if (bt == T_OBJECT) {
5944     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5945   } else {
5946     type = Type::get_const_basic_type(bt);
5947   }
5948 
5949   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
5950     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
5951   }
5952   // Build the load.
5953   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
5954   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, LoadNode::DependsOnlyOnTest, is_vol);
5955   // If reference is volatile, prevent following memory ops from
5956   // floating up past the volatile read.  Also prevents commoning
5957   // another volatile read.
5958   if (is_vol) {
5959     // Memory barrier includes bogus read of value to force load BEFORE membar
5960     insert_mem_bar(Op_MemBarAcquire, loadedField);
5961   }
5962   return loadedField;
5963 }
5964 
5965 
5966 //------------------------------inline_aescrypt_Block-----------------------
5967 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
5968   address stubAddr = NULL;
5969   const char *stubName;
5970   assert(UseAES, "need AES instruction support");
5971 
5972   switch(id) {
5973   case vmIntrinsics::_aescrypt_encryptBlock:
5974     stubAddr = StubRoutines::aescrypt_encryptBlock();
5975     stubName = "aescrypt_encryptBlock";
5976     break;
5977   case vmIntrinsics::_aescrypt_decryptBlock:
5978     stubAddr = StubRoutines::aescrypt_decryptBlock();
5979     stubName = "aescrypt_decryptBlock";
5980     break;
5981   }
5982   if (stubAddr == NULL) return false;
5983 
5984   Node* aescrypt_object = argument(0);
5985   Node* src             = argument(1);
5986   Node* src_offset      = argument(2);
5987   Node* dest            = argument(3);
5988   Node* dest_offset     = argument(4);
5989 
5990   // (1) src and dest are arrays.
5991   const Type* src_type = src-&gt;Value(&amp;_gvn);
5992   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5993   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5994   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5995   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5996 
5997   // for the quick and dirty code we will skip all the checks.
5998   // we are just trying to get the call to be generated.
5999   Node* src_start  = src;
6000   Node* dest_start = dest;
6001   if (src_offset != NULL || dest_offset != NULL) {
6002     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6003     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6004     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6005   }
6006 
6007   // now need to get the start of its expanded key array
6008   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6009   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6010   if (k_start == NULL) return false;
6011 
6012   if (Matcher::pass_original_key_for_aes()) {
6013     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6014     // compatibility issues between Java key expansion and SPARC crypto instructions
6015     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6016     if (original_k_start == NULL) return false;
6017 
6018     // Call the stub.
6019     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6020                       stubAddr, stubName, TypePtr::BOTTOM,
6021                       src_start, dest_start, k_start, original_k_start);
6022   } else {
6023     // Call the stub.
6024     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6025                       stubAddr, stubName, TypePtr::BOTTOM,
6026                       src_start, dest_start, k_start);
6027   }
6028 
6029   return true;
6030 }
6031 
6032 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
6033 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
6034   address stubAddr = NULL;
6035   const char *stubName = NULL;
6036 
6037   assert(UseAES, "need AES instruction support");
6038 
6039   switch(id) {
6040   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
6041     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
6042     stubName = "cipherBlockChaining_encryptAESCrypt";
6043     break;
6044   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
6045     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
6046     stubName = "cipherBlockChaining_decryptAESCrypt";
6047     break;
6048   }
6049   if (stubAddr == NULL) return false;
6050 
6051   Node* cipherBlockChaining_object = argument(0);
6052   Node* src                        = argument(1);
6053   Node* src_offset                 = argument(2);
6054   Node* len                        = argument(3);
6055   Node* dest                       = argument(4);
6056   Node* dest_offset                = argument(5);
6057 
6058   // (1) src and dest are arrays.
6059   const Type* src_type = src-&gt;Value(&amp;_gvn);
6060   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6061   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6062   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6063   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
6064           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6065 
6066   // checks are the responsibility of the caller
6067   Node* src_start  = src;
6068   Node* dest_start = dest;
6069   if (src_offset != NULL || dest_offset != NULL) {
6070     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6071     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6072     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6073   }
6074 
6075   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
6076   // (because of the predicated logic executed earlier).
6077   // so we cast it here safely.
6078   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6079 
6080   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6081   if (embeddedCipherObj == NULL) return false;
6082 
6083   // cast it to what we know it will be at runtime
6084   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
6085   assert(tinst != NULL, "CBC obj is null");
6086   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
6087   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6088   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
6089 
6090   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6091   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
6092   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6093   Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
6094   aescrypt_object = _gvn.transform(aescrypt_object);
6095 
6096   // we need to get the start of the aescrypt_object's expanded key array
6097   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6098   if (k_start == NULL) return false;
6099 
6100   // similarly, get the start address of the r vector
6101   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
6102   if (objRvec == NULL) return false;
6103   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
6104 
6105   Node* cbcCrypt;
6106   if (Matcher::pass_original_key_for_aes()) {
6107     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6108     // compatibility issues between Java key expansion and SPARC crypto instructions
6109     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6110     if (original_k_start == NULL) return false;
6111 
6112     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
6113     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6114                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6115                                  stubAddr, stubName, TypePtr::BOTTOM,
6116                                  src_start, dest_start, k_start, r_start, len, original_k_start);
6117   } else {
6118     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6119     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6120                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6121                                  stubAddr, stubName, TypePtr::BOTTOM,
6122                                  src_start, dest_start, k_start, r_start, len);
6123   }
6124 
6125   // return cipher length (int)
6126   Node* retvalue = _gvn.transform(new ProjNode(cbcCrypt, TypeFunc::Parms));
6127   set_result(retvalue);
6128   return true;
6129 }
6130 
6131 //------------------------------get_key_start_from_aescrypt_object-----------------------
6132 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
6133   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
6134   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6135   if (objAESCryptKey == NULL) return (Node *) NULL;
6136 
6137   // now have the array, need to get the start address of the K array
6138   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
6139   return k_start;
6140 }
6141 
6142 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
6143 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
6144   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
6145   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6146   if (objAESCryptKey == NULL) return (Node *) NULL;
6147 
6148   // now have the array, need to get the start address of the lastKey array
6149   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
6150   return original_k_start;
6151 }
6152 
6153 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
6154 // Return node representing slow path of predicate check.
6155 // the pseudo code we want to emulate with this predicate is:
6156 // for encryption:
6157 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6158 // for decryption:
6159 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6160 //    note cipher==plain is more conservative than the original java code but that's OK
6161 //
6162 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
6163   // The receiver was checked for NULL already.
6164   Node* objCBC = argument(0);
6165 
6166   // Load embeddedCipher field of CipherBlockChaining object.
6167   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6168 
6169   // get AESCrypt klass for instanceOf check
6170   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6171   // will have same classloader as CipherBlockChaining object
6172   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
6173   assert(tinst != NULL, "CBCobj is null");
6174   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
6175 
6176   // we want to do an instanceof comparison against the AESCrypt class
6177   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6178   if (!klass_AESCrypt-&gt;is_loaded()) {
6179     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6180     Node* ctrl = control();
6181     set_control(top()); // no regular fast path
6182     return ctrl;
6183   }
6184   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6185 
6186   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6187   Node* cmp_instof  = _gvn.transform(new CmpINode(instof, intcon(1)));
6188   Node* bool_instof  = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6189 
6190   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6191 
6192   // for encryption, we are done
6193   if (!decrypting)
6194     return instof_false;  // even if it is NULL
6195 
6196   // for decryption, we need to add a further check to avoid
6197   // taking the intrinsic path when cipher and plain are the same
6198   // see the original java code for why.
6199   RegionNode* region = new RegionNode(3);
6200   region-&gt;init_req(1, instof_false);
6201   Node* src = argument(1);
6202   Node* dest = argument(4);
6203   Node* cmp_src_dest = _gvn.transform(new CmpPNode(src, dest));
6204   Node* bool_src_dest = _gvn.transform(new BoolNode(cmp_src_dest, BoolTest::eq));
6205   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
6206   region-&gt;init_req(2, src_dest_conjoint);
6207 
6208   record_for_igvn(region);
6209   return _gvn.transform(region);
6210 }
6211 
6212 //------------------------------inline_ghash_processBlocks
6213 bool LibraryCallKit::inline_ghash_processBlocks() {
6214   address stubAddr;
6215   const char *stubName;
6216   assert(UseGHASHIntrinsics, "need GHASH intrinsics support");
6217 
6218   stubAddr = StubRoutines::ghash_processBlocks();
6219   stubName = "ghash_processBlocks";
6220 
6221   Node* data           = argument(0);
6222   Node* offset         = argument(1);
6223   Node* len            = argument(2);
6224   Node* state          = argument(3);
6225   Node* subkeyH        = argument(4);
6226 
6227   Node* state_start  = array_element_address(state, intcon(0), T_LONG);
6228   assert(state_start, "state is NULL");
6229   Node* subkeyH_start  = array_element_address(subkeyH, intcon(0), T_LONG);
6230   assert(subkeyH_start, "subkeyH is NULL");
6231   Node* data_start  = array_element_address(data, offset, T_BYTE);
6232   assert(data_start, "data is NULL");
6233 
6234   Node* ghash = make_runtime_call(RC_LEAF|RC_NO_FP,
6235                                   OptoRuntime::ghash_processBlocks_Type(),
6236                                   stubAddr, stubName, TypePtr::BOTTOM,
6237                                   state_start, subkeyH_start, data_start, len);
6238   return true;
6239 }
6240 
6241 //------------------------------inline_sha_implCompress-----------------------
6242 //
6243 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6244 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6245 //
6246 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6247 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6248 //
6249 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6250 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6251 //
6252 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6253   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6254 
6255   Node* sha_obj = argument(0);
6256   Node* src     = argument(1); // type oop
6257   Node* ofs     = argument(2); // type int
6258 
6259   const Type* src_type = src-&gt;Value(&amp;_gvn);
6260   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6261   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6262     // failed array check
6263     return false;
6264   }
6265   // Figure out the size and type of the elements we will be copying.
6266   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6267   if (src_elem != T_BYTE) {
6268     return false;
6269   }
6270   // 'src_start' points to src array + offset
6271   Node* src_start = array_element_address(src, ofs, src_elem);
6272   Node* state = NULL;
6273   address stubAddr;
6274   const char *stubName;
6275 
6276   switch(id) {
6277   case vmIntrinsics::_sha_implCompress:
6278     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6279     state = get_state_from_sha_object(sha_obj);
6280     stubAddr = StubRoutines::sha1_implCompress();
6281     stubName = "sha1_implCompress";
6282     break;
6283   case vmIntrinsics::_sha2_implCompress:
6284     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6285     state = get_state_from_sha_object(sha_obj);
6286     stubAddr = StubRoutines::sha256_implCompress();
6287     stubName = "sha256_implCompress";
6288     break;
6289   case vmIntrinsics::_sha5_implCompress:
6290     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6291     state = get_state_from_sha5_object(sha_obj);
6292     stubAddr = StubRoutines::sha512_implCompress();
6293     stubName = "sha512_implCompress";
6294     break;
6295   default:
6296     fatal_unexpected_iid(id);
6297     return false;
6298   }
6299   if (state == NULL) return false;
6300 
6301   // Call the stub.
6302   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6303                                  stubAddr, stubName, TypePtr::BOTTOM,
6304                                  src_start, state);
6305 
6306   return true;
6307 }
6308 
6309 //------------------------------inline_digestBase_implCompressMB-----------------------
6310 //
6311 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6312 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6313 //
6314 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6315   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6316          "need SHA1/SHA256/SHA512 instruction support");
6317   assert((uint)predicate &lt; 3, "sanity");
6318   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6319 
6320   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6321   Node* src            = argument(1); // byte[] array
6322   Node* ofs            = argument(2); // type int
6323   Node* limit          = argument(3); // type int
6324 
6325   const Type* src_type = src-&gt;Value(&amp;_gvn);
6326   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6327   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6328     // failed array check
6329     return false;
6330   }
6331   // Figure out the size and type of the elements we will be copying.
6332   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6333   if (src_elem != T_BYTE) {
6334     return false;
6335   }
6336   // 'src_start' points to src array + offset
6337   Node* src_start = array_element_address(src, ofs, src_elem);
6338 
6339   const char* klass_SHA_name = NULL;
6340   const char* stub_name = NULL;
6341   address     stub_addr = NULL;
6342   bool        long_state = false;
6343 
6344   switch (predicate) {
6345   case 0:
6346     if (UseSHA1Intrinsics) {
6347       klass_SHA_name = "sun/security/provider/SHA";
6348       stub_name = "sha1_implCompressMB";
6349       stub_addr = StubRoutines::sha1_implCompressMB();
6350     }
6351     break;
6352   case 1:
6353     if (UseSHA256Intrinsics) {
6354       klass_SHA_name = "sun/security/provider/SHA2";
6355       stub_name = "sha256_implCompressMB";
6356       stub_addr = StubRoutines::sha256_implCompressMB();
6357     }
6358     break;
6359   case 2:
6360     if (UseSHA512Intrinsics) {
6361       klass_SHA_name = "sun/security/provider/SHA5";
6362       stub_name = "sha512_implCompressMB";
6363       stub_addr = StubRoutines::sha512_implCompressMB();
6364       long_state = true;
6365     }
6366     break;
6367   default:
6368     fatal("unknown SHA intrinsic predicate: %d", predicate);
6369   }
6370   if (klass_SHA_name != NULL) {
6371     // get DigestBase klass to lookup for SHA klass
6372     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6373     assert(tinst != NULL, "digestBase_obj is not instance???");
6374     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6375 
6376     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6377     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6378     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6379     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6380   }
6381   return false;
6382 }
6383 //------------------------------inline_sha_implCompressMB-----------------------
6384 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6385                                                bool long_state, address stubAddr, const char *stubName,
6386                                                Node* src_start, Node* ofs, Node* limit) {
6387   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6388   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6389   Node* sha_obj = new CheckCastPPNode(control(), digestBase_obj, xtype);
6390   sha_obj = _gvn.transform(sha_obj);
6391 
6392   Node* state;
6393   if (long_state) {
6394     state = get_state_from_sha5_object(sha_obj);
6395   } else {
6396     state = get_state_from_sha_object(sha_obj);
6397   }
6398   if (state == NULL) return false;
6399 
6400   // Call the stub.
6401   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6402                                  OptoRuntime::digestBase_implCompressMB_Type(),
6403                                  stubAddr, stubName, TypePtr::BOTTOM,
6404                                  src_start, state, ofs, limit);
6405   // return ofs (int)
6406   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
6407   set_result(result);
6408 
6409   return true;
6410 }
6411 
6412 //------------------------------get_state_from_sha_object-----------------------
6413 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6414   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6415   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6416   if (sha_state == NULL) return (Node *) NULL;
6417 
6418   // now have the array, need to get the start address of the state array
6419   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6420   return state;
6421 }
6422 
6423 //------------------------------get_state_from_sha5_object-----------------------
6424 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6425   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6426   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6427   if (sha_state == NULL) return (Node *) NULL;
6428 
6429   // now have the array, need to get the start address of the state array
6430   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6431   return state;
6432 }
6433 
6434 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6435 // Return node representing slow path of predicate check.
6436 // the pseudo code we want to emulate with this predicate is:
6437 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6438 //
6439 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6440   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6441          "need SHA1/SHA256/SHA512 instruction support");
6442   assert((uint)predicate &lt; 3, "sanity");
6443 
6444   // The receiver was checked for NULL already.
6445   Node* digestBaseObj = argument(0);
6446 
6447   // get DigestBase klass for instanceOf check
6448   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6449   assert(tinst != NULL, "digestBaseObj is null");
6450   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6451 
6452   const char* klass_SHA_name = NULL;
6453   switch (predicate) {
6454   case 0:
6455     if (UseSHA1Intrinsics) {
6456       // we want to do an instanceof comparison against the SHA class
6457       klass_SHA_name = "sun/security/provider/SHA";
6458     }
6459     break;
6460   case 1:
6461     if (UseSHA256Intrinsics) {
6462       // we want to do an instanceof comparison against the SHA2 class
6463       klass_SHA_name = "sun/security/provider/SHA2";
6464     }
6465     break;
6466   case 2:
6467     if (UseSHA512Intrinsics) {
6468       // we want to do an instanceof comparison against the SHA5 class
6469       klass_SHA_name = "sun/security/provider/SHA5";
6470     }
6471     break;
6472   default:
6473     fatal("unknown SHA intrinsic predicate: %d", predicate);
6474   }
6475 
6476   ciKlass* klass_SHA = NULL;
6477   if (klass_SHA_name != NULL) {
6478     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6479   }
6480   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6481     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6482     Node* ctrl = control();
6483     set_control(top()); // no intrinsic path
6484     return ctrl;
6485   }
6486   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6487 
6488   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6489   Node* cmp_instof = _gvn.transform(new CmpINode(instofSHA, intcon(1)));
6490   Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6491   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6492 
6493   return instof_false;  // even if it is NULL
6494 }
6495 
6496 bool LibraryCallKit::inline_profileBoolean() {
6497   Node* counts = argument(1);
6498   const TypeAryPtr* ary = NULL;
6499   ciArray* aobj = NULL;
6500   if (counts-&gt;is_Con()
6501       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL
6502       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL
6503       &amp;&amp; (aobj-&gt;length() == 2)) {
6504     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
6505     jint false_cnt = aobj-&gt;element_value(0).as_int();
6506     jint  true_cnt = aobj-&gt;element_value(1).as_int();
6507 
6508     if (C-&gt;log() != NULL) {
6509       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",
6510                      false_cnt, true_cnt);
6511     }
6512 
6513     if (false_cnt + true_cnt == 0) {
6514       // According to profile, never executed.
6515       uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6516                           Deoptimization::Action_reinterpret);
6517       return true;
6518     }
6519 
6520     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)
6521     // is a number of each value occurrences.
6522     Node* result = argument(0);
6523     if (false_cnt == 0 || true_cnt == 0) {
6524       // According to profile, one value has been never seen.
6525       int expected_val = (false_cnt == 0) ? 1 : 0;
6526 
6527       Node* cmp  = _gvn.transform(new CmpINode(result, intcon(expected_val)));
6528       Node* test = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
6529 
6530       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
6531       Node* fast_path = _gvn.transform(new IfTrueNode(check));
6532       Node* slow_path = _gvn.transform(new IfFalseNode(check));
6533 
6534       { // Slow path: uncommon trap for never seen value and then reexecute
6535         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
6536         // the value has been seen at least once.
6537         PreserveJVMState pjvms(this);
6538         PreserveReexecuteState preexecs(this);
6539         jvms()-&gt;set_should_reexecute(true);
6540 
6541         set_control(slow_path);
6542         set_i_o(i_o());
6543 
6544         uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6545                             Deoptimization::Action_reinterpret);
6546       }
6547       // The guard for never seen value enables sharpening of the result and
6548       // returning a constant. It allows to eliminate branches on the same value
6549       // later on.
6550       set_control(fast_path);
6551       result = intcon(expected_val);
6552     }
6553     // Stop profiling.
6554     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
6555     // By replacing method body with profile data (represented as ProfileBooleanNode
6556     // on IR level) we effectively disable profiling.
6557     // It enables full speed execution once optimized code is generated.
6558     Node* profile = _gvn.transform(new ProfileBooleanNode(result, false_cnt, true_cnt));
6559     C-&gt;record_for_igvn(profile);
6560     set_result(profile);
6561     return true;
6562   } else {
6563     // Continue profiling.
6564     // Profile data isn't available at the moment. So, execute method's bytecode version.
6565     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
6566     // is compiled and counters aren't available since corresponding MethodHandle
6567     // isn't a compile-time constant.
6568     return false;
6569   }
6570 }
6571 
6572 bool LibraryCallKit::inline_isCompileConstant() {
6573   Node* n = argument(0);
6574   set_result(n-&gt;is_Con() ? intcon(1) : intcon(0));
6575   return true;
6576 }
<a name="5" id="anc5"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="5" type="hidden" /></form></body></html>
