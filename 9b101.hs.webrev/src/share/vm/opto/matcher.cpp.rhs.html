<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "memory/allocation.inline.hpp"
  27 #include "opto/ad.hpp"
  28 #include "opto/addnode.hpp"
  29 #include "opto/callnode.hpp"
  30 #include "opto/idealGraphPrinter.hpp"
  31 #include "opto/matcher.hpp"
  32 #include "opto/memnode.hpp"
  33 #include "opto/movenode.hpp"
  34 #include "opto/opcodes.hpp"
  35 #include "opto/regmask.hpp"
  36 #include "opto/rootnode.hpp"
  37 #include "opto/runtime.hpp"
  38 #include "opto/type.hpp"
  39 #include "opto/vectornode.hpp"
  40 #include "runtime/os.hpp"
  41 #include "runtime/sharedRuntime.hpp"
  42 
  43 OptoReg::Name OptoReg::c_frame_pointer;
  44 
  45 const RegMask *Matcher::idealreg2regmask[_last_machine_leaf];
  46 RegMask Matcher::mreg2regmask[_last_Mach_Reg];
  47 RegMask Matcher::STACK_ONLY_mask;
  48 RegMask Matcher::c_frame_ptr_mask;
  49 const uint Matcher::_begin_rematerialize = _BEGIN_REMATERIALIZE;
  50 const uint Matcher::_end_rematerialize   = _END_REMATERIALIZE;
  51 
  52 //---------------------------Matcher-------------------------------------------
  53 Matcher::Matcher()
  54 : PhaseTransform( Phase::Ins_Select ),
  55 #ifdef ASSERT
  56   _old2new_map(C-&gt;comp_arena()),
  57   _new2old_map(C-&gt;comp_arena()),
  58 #endif
  59   _shared_nodes(C-&gt;comp_arena()),
  60   _reduceOp(reduceOp), _leftOp(leftOp), _rightOp(rightOp),
  61   _swallowed(swallowed),
  62   _begin_inst_chain_rule(_BEGIN_INST_CHAIN_RULE),
  63   _end_inst_chain_rule(_END_INST_CHAIN_RULE),
  64   _must_clone(must_clone),
  65   _register_save_policy(register_save_policy),
  66   _c_reg_save_policy(c_reg_save_policy),
  67   _register_save_type(register_save_type),
  68   _ruleName(ruleName),
  69   _allocation_started(false),
  70   _states_arena(Chunk::medium_size),
  71   _visited(&amp;_states_arena),
  72   _shared(&amp;_states_arena),
  73   _dontcare(&amp;_states_arena) {
  74   C-&gt;set_matcher(this);
  75 
  76   idealreg2spillmask  [Op_RegI] = NULL;
  77   idealreg2spillmask  [Op_RegN] = NULL;
  78   idealreg2spillmask  [Op_RegL] = NULL;
  79   idealreg2spillmask  [Op_RegF] = NULL;
  80   idealreg2spillmask  [Op_RegD] = NULL;
  81   idealreg2spillmask  [Op_RegP] = NULL;
  82   idealreg2spillmask  [Op_VecS] = NULL;
  83   idealreg2spillmask  [Op_VecD] = NULL;
  84   idealreg2spillmask  [Op_VecX] = NULL;
  85   idealreg2spillmask  [Op_VecY] = NULL;
  86   idealreg2spillmask  [Op_VecZ] = NULL;
  87 
  88   idealreg2debugmask  [Op_RegI] = NULL;
  89   idealreg2debugmask  [Op_RegN] = NULL;
  90   idealreg2debugmask  [Op_RegL] = NULL;
  91   idealreg2debugmask  [Op_RegF] = NULL;
  92   idealreg2debugmask  [Op_RegD] = NULL;
  93   idealreg2debugmask  [Op_RegP] = NULL;
  94   idealreg2debugmask  [Op_VecS] = NULL;
  95   idealreg2debugmask  [Op_VecD] = NULL;
  96   idealreg2debugmask  [Op_VecX] = NULL;
  97   idealreg2debugmask  [Op_VecY] = NULL;
  98   idealreg2debugmask  [Op_VecZ] = NULL;
  99 
 100   idealreg2mhdebugmask[Op_RegI] = NULL;
 101   idealreg2mhdebugmask[Op_RegN] = NULL;
 102   idealreg2mhdebugmask[Op_RegL] = NULL;
 103   idealreg2mhdebugmask[Op_RegF] = NULL;
 104   idealreg2mhdebugmask[Op_RegD] = NULL;
 105   idealreg2mhdebugmask[Op_RegP] = NULL;
 106   idealreg2mhdebugmask[Op_VecS] = NULL;
 107   idealreg2mhdebugmask[Op_VecD] = NULL;
 108   idealreg2mhdebugmask[Op_VecX] = NULL;
 109   idealreg2mhdebugmask[Op_VecY] = NULL;
 110   idealreg2mhdebugmask[Op_VecZ] = NULL;
 111 
 112   debug_only(_mem_node = NULL;)   // Ideal memory node consumed by mach node
 113 }
 114 
 115 //------------------------------warp_incoming_stk_arg------------------------
 116 // This warps a VMReg into an OptoReg::Name
 117 OptoReg::Name Matcher::warp_incoming_stk_arg( VMReg reg ) {
 118   OptoReg::Name warped;
 119   if( reg-&gt;is_stack() ) {  // Stack slot argument?
 120     warped = OptoReg::add(_old_SP, reg-&gt;reg2stack() );
 121     warped = OptoReg::add(warped, C-&gt;out_preserve_stack_slots());
 122     if( warped &gt;= _in_arg_limit )
 123       _in_arg_limit = OptoReg::add(warped, 1); // Bump max stack slot seen
 124     if (!RegMask::can_represent_arg(warped)) {
 125       // the compiler cannot represent this method's calling sequence
 126       C-&gt;record_method_not_compilable_all_tiers("unsupported incoming calling sequence");
 127       return OptoReg::Bad;
 128     }
 129     return warped;
 130   }
 131   return OptoReg::as_OptoReg(reg);
 132 }
 133 
 134 //---------------------------compute_old_SP------------------------------------
 135 OptoReg::Name Compile::compute_old_SP() {
 136   int fixed    = fixed_slots();
 137   int preserve = in_preserve_stack_slots();
 138   return OptoReg::stack2reg(round_to(fixed + preserve, Matcher::stack_alignment_in_slots()));
 139 }
 140 
 141 
 142 
 143 #ifdef ASSERT
 144 void Matcher::verify_new_nodes_only(Node* xroot) {
 145   // Make sure that the new graph only references new nodes
 146   ResourceMark rm;
 147   Unique_Node_List worklist;
 148   VectorSet visited(Thread::current()-&gt;resource_area());
 149   worklist.push(xroot);
 150   while (worklist.size() &gt; 0) {
 151     Node* n = worklist.pop();
 152     visited &lt;&lt;= n-&gt;_idx;
 153     assert(C-&gt;node_arena()-&gt;contains(n), "dead node");
 154     for (uint j = 0; j &lt; n-&gt;req(); j++) {
 155       Node* in = n-&gt;in(j);
 156       if (in != NULL) {
 157         assert(C-&gt;node_arena()-&gt;contains(in), "dead node");
 158         if (!visited.test(in-&gt;_idx)) {
 159           worklist.push(in);
 160         }
 161       }
 162     }
 163   }
 164 }
 165 #endif
 166 
 167 
 168 //---------------------------match---------------------------------------------
 169 void Matcher::match( ) {
 170   if( MaxLabelRootDepth &lt; 100 ) { // Too small?
 171     assert(false, "invalid MaxLabelRootDepth, increase it to 100 minimum");
 172     MaxLabelRootDepth = 100;
 173   }
 174   // One-time initialization of some register masks.
 175   init_spill_mask( C-&gt;root()-&gt;in(1) );
 176   _return_addr_mask = return_addr();
 177 #ifdef _LP64
 178   // Pointers take 2 slots in 64-bit land
 179   _return_addr_mask.Insert(OptoReg::add(return_addr(),1));
 180 #endif
 181 
 182   // Map a Java-signature return type into return register-value
 183   // machine registers for 0, 1 and 2 returned values.
 184   const TypeTuple *range = C-&gt;tf()-&gt;range();
 185   if( range-&gt;cnt() &gt; TypeFunc::Parms ) { // If not a void function
 186     // Get ideal-register return type
 187     int ireg = range-&gt;field_at(TypeFunc::Parms)-&gt;ideal_reg();
 188     // Get machine return register
 189     uint sop = C-&gt;start()-&gt;Opcode();
 190     OptoRegPair regs = return_value(ireg, false);
 191 
 192     // And mask for same
 193     _return_value_mask = RegMask(regs.first());
 194     if( OptoReg::is_valid(regs.second()) )
 195       _return_value_mask.Insert(regs.second());
 196   }
 197 
 198   // ---------------
 199   // Frame Layout
 200 
 201   // Need the method signature to determine the incoming argument types,
 202   // because the types determine which registers the incoming arguments are
 203   // in, and this affects the matched code.
 204   const TypeTuple *domain = C-&gt;tf()-&gt;domain();
 205   uint             argcnt = domain-&gt;cnt() - TypeFunc::Parms;
 206   BasicType *sig_bt        = NEW_RESOURCE_ARRAY( BasicType, argcnt );
 207   VMRegPair *vm_parm_regs  = NEW_RESOURCE_ARRAY( VMRegPair, argcnt );
 208   _parm_regs               = NEW_RESOURCE_ARRAY( OptoRegPair, argcnt );
 209   _calling_convention_mask = NEW_RESOURCE_ARRAY( RegMask, argcnt );
 210   uint i;
 211   for( i = 0; i&lt;argcnt; i++ ) {
 212     sig_bt[i] = domain-&gt;field_at(i+TypeFunc::Parms)-&gt;basic_type();
 213   }
 214 
 215   // Pass array of ideal registers and length to USER code (from the AD file)
 216   // that will convert this to an array of register numbers.
 217   const StartNode *start = C-&gt;start();
 218   start-&gt;calling_convention( sig_bt, vm_parm_regs, argcnt );
 219 #ifdef ASSERT
 220   // Sanity check users' calling convention.  Real handy while trying to
 221   // get the initial port correct.
 222   { for (uint i = 0; i&lt;argcnt; i++) {
 223       if( !vm_parm_regs[i].first()-&gt;is_valid() &amp;&amp; !vm_parm_regs[i].second()-&gt;is_valid() ) {
 224         assert(domain-&gt;field_at(i+TypeFunc::Parms)==Type::HALF, "only allowed on halve" );
 225         _parm_regs[i].set_bad();
 226         continue;
 227       }
 228       VMReg parm_reg = vm_parm_regs[i].first();
 229       assert(parm_reg-&gt;is_valid(), "invalid arg?");
 230       if (parm_reg-&gt;is_reg()) {
 231         OptoReg::Name opto_parm_reg = OptoReg::as_OptoReg(parm_reg);
 232         assert(can_be_java_arg(opto_parm_reg) ||
 233                C-&gt;stub_function() == CAST_FROM_FN_PTR(address, OptoRuntime::rethrow_C) ||
 234                opto_parm_reg == inline_cache_reg(),
 235                "parameters in register must be preserved by runtime stubs");
 236       }
 237       for (uint j = 0; j &lt; i; j++) {
 238         assert(parm_reg != vm_parm_regs[j].first(),
 239                "calling conv. must produce distinct regs");
 240       }
 241     }
 242   }
 243 #endif
 244 
 245   // Do some initial frame layout.
 246 
 247   // Compute the old incoming SP (may be called FP) as
 248   //   OptoReg::stack0() + locks + in_preserve_stack_slots + pad2.
 249   _old_SP = C-&gt;compute_old_SP();
 250   assert( is_even(_old_SP), "must be even" );
 251 
 252   // Compute highest incoming stack argument as
 253   //   _old_SP + out_preserve_stack_slots + incoming argument size.
 254   _in_arg_limit = OptoReg::add(_old_SP, C-&gt;out_preserve_stack_slots());
 255   assert( is_even(_in_arg_limit), "out_preserve must be even" );
 256   for( i = 0; i &lt; argcnt; i++ ) {
 257     // Permit args to have no register
 258     _calling_convention_mask[i].Clear();
 259     if( !vm_parm_regs[i].first()-&gt;is_valid() &amp;&amp; !vm_parm_regs[i].second()-&gt;is_valid() ) {
 260       continue;
 261     }
 262     // calling_convention returns stack arguments as a count of
 263     // slots beyond OptoReg::stack0()/VMRegImpl::stack0.  We need to convert this to
 264     // the allocators point of view, taking into account all the
 265     // preserve area, locks &amp; pad2.
 266 
 267     OptoReg::Name reg1 = warp_incoming_stk_arg(vm_parm_regs[i].first());
 268     if( OptoReg::is_valid(reg1))
 269       _calling_convention_mask[i].Insert(reg1);
 270 
 271     OptoReg::Name reg2 = warp_incoming_stk_arg(vm_parm_regs[i].second());
 272     if( OptoReg::is_valid(reg2))
 273       _calling_convention_mask[i].Insert(reg2);
 274 
 275     // Saved biased stack-slot register number
 276     _parm_regs[i].set_pair(reg2, reg1);
 277   }
 278 
 279   // Finally, make sure the incoming arguments take up an even number of
 280   // words, in case the arguments or locals need to contain doubleword stack
 281   // slots.  The rest of the system assumes that stack slot pairs (in
 282   // particular, in the spill area) which look aligned will in fact be
 283   // aligned relative to the stack pointer in the target machine.  Double
 284   // stack slots will always be allocated aligned.
 285   _new_SP = OptoReg::Name(round_to(_in_arg_limit, RegMask::SlotsPerLong));
 286 
 287   // Compute highest outgoing stack argument as
 288   //   _new_SP + out_preserve_stack_slots + max(outgoing argument size).
 289   _out_arg_limit = OptoReg::add(_new_SP, C-&gt;out_preserve_stack_slots());
 290   assert( is_even(_out_arg_limit), "out_preserve must be even" );
 291 
 292   if (!RegMask::can_represent_arg(OptoReg::add(_out_arg_limit,-1))) {
 293     // the compiler cannot represent this method's calling sequence
 294     C-&gt;record_method_not_compilable("must be able to represent all call arguments in reg mask");
 295   }
 296 
 297   if (C-&gt;failing())  return;  // bailed out on incoming arg failure
 298 
 299   // ---------------
 300   // Collect roots of matcher trees.  Every node for which
 301   // _shared[_idx] is cleared is guaranteed to not be shared, and thus
 302   // can be a valid interior of some tree.
 303   find_shared( C-&gt;root() );
 304   find_shared( C-&gt;top() );
 305 
 306   C-&gt;print_method(PHASE_BEFORE_MATCHING);
 307 
 308   // Create new ideal node ConP #NULL even if it does exist in old space
 309   // to avoid false sharing if the corresponding mach node is not used.
 310   // The corresponding mach node is only used in rare cases for derived
 311   // pointers.
 312   Node* new_ideal_null = ConNode::make(TypePtr::NULL_PTR);
 313 
 314   // Swap out to old-space; emptying new-space
 315   Arena *old = C-&gt;node_arena()-&gt;move_contents(C-&gt;old_arena());
 316 
 317   // Save debug and profile information for nodes in old space:
 318   _old_node_note_array = C-&gt;node_note_array();
 319   if (_old_node_note_array != NULL) {
 320     C-&gt;set_node_note_array(new(C-&gt;comp_arena()) GrowableArray&lt;Node_Notes*&gt;
 321                            (C-&gt;comp_arena(), _old_node_note_array-&gt;length(),
 322                             0, NULL));
 323   }
 324 
 325   // Pre-size the new_node table to avoid the need for range checks.
 326   grow_new_node_array(C-&gt;unique());
 327 
 328   // Reset node counter so MachNodes start with _idx at 0
 329   int live_nodes = C-&gt;live_nodes();
 330   C-&gt;set_unique(0);
 331   C-&gt;reset_dead_node_list();
 332 
 333   // Recursively match trees from old space into new space.
 334   // Correct leaves of new-space Nodes; they point to old-space.
 335   _visited.Clear();             // Clear visit bits for xform call
 336   C-&gt;set_cached_top_node(xform( C-&gt;top(), live_nodes ));
 337   if (!C-&gt;failing()) {
 338     Node* xroot =        xform( C-&gt;root(), 1 );
 339     if (xroot == NULL) {
 340       Matcher::soft_match_failure();  // recursive matching process failed
 341       C-&gt;record_method_not_compilable("instruction match failed");
 342     } else {
 343       // During matching shared constants were attached to C-&gt;root()
 344       // because xroot wasn't available yet, so transfer the uses to
 345       // the xroot.
 346       for( DUIterator_Fast jmax, j = C-&gt;root()-&gt;fast_outs(jmax); j &lt; jmax; j++ ) {
 347         Node* n = C-&gt;root()-&gt;fast_out(j);
 348         if (C-&gt;node_arena()-&gt;contains(n)) {
 349           assert(n-&gt;in(0) == C-&gt;root(), "should be control user");
 350           n-&gt;set_req(0, xroot);
 351           --j;
 352           --jmax;
 353         }
 354       }
 355 
 356       // Generate new mach node for ConP #NULL
 357       assert(new_ideal_null != NULL, "sanity");
 358       _mach_null = match_tree(new_ideal_null);
 359       // Don't set control, it will confuse GCM since there are no uses.
 360       // The control will be set when this node is used first time
 361       // in find_base_for_derived().
 362       assert(_mach_null != NULL, "");
 363 
 364       C-&gt;set_root(xroot-&gt;is_Root() ? xroot-&gt;as_Root() : NULL);
 365 
 366 #ifdef ASSERT
 367       verify_new_nodes_only(xroot);
 368 #endif
 369     }
 370   }
 371   if (C-&gt;top() == NULL || C-&gt;root() == NULL) {
 372     C-&gt;record_method_not_compilable("graph lost"); // %%% cannot happen?
 373   }
 374   if (C-&gt;failing()) {
 375     // delete old;
 376     old-&gt;destruct_contents();
 377     return;
 378   }
 379   assert( C-&gt;top(), "" );
 380   assert( C-&gt;root(), "" );
 381   validate_null_checks();
 382 
 383   // Now smoke old-space
 384   NOT_DEBUG( old-&gt;destruct_contents() );
 385 
 386   // ------------------------
 387   // Set up save-on-entry registers
 388   Fixup_Save_On_Entry( );
 389 }
 390 
 391 
 392 //------------------------------Fixup_Save_On_Entry----------------------------
 393 // The stated purpose of this routine is to take care of save-on-entry
 394 // registers.  However, the overall goal of the Match phase is to convert into
 395 // machine-specific instructions which have RegMasks to guide allocation.
 396 // So what this procedure really does is put a valid RegMask on each input
 397 // to the machine-specific variations of all Return, TailCall and Halt
 398 // instructions.  It also adds edgs to define the save-on-entry values (and of
 399 // course gives them a mask).
 400 
 401 static RegMask *init_input_masks( uint size, RegMask &amp;ret_adr, RegMask &amp;fp ) {
 402   RegMask *rms = NEW_RESOURCE_ARRAY( RegMask, size );
 403   // Do all the pre-defined register masks
 404   rms[TypeFunc::Control  ] = RegMask::Empty;
 405   rms[TypeFunc::I_O      ] = RegMask::Empty;
 406   rms[TypeFunc::Memory   ] = RegMask::Empty;
 407   rms[TypeFunc::ReturnAdr] = ret_adr;
 408   rms[TypeFunc::FramePtr ] = fp;
 409   return rms;
 410 }
 411 
 412 //---------------------------init_first_stack_mask-----------------------------
 413 // Create the initial stack mask used by values spilling to the stack.
 414 // Disallow any debug info in outgoing argument areas by setting the
 415 // initial mask accordingly.
 416 void Matcher::init_first_stack_mask() {
 417 
 418   // Allocate storage for spill masks as masks for the appropriate load type.
 419   RegMask *rms = (RegMask*)C-&gt;comp_arena()-&gt;Amalloc_D(sizeof(RegMask) * (3*6+5));
 420 
 421   idealreg2spillmask  [Op_RegN] = &amp;rms[0];
 422   idealreg2spillmask  [Op_RegI] = &amp;rms[1];
 423   idealreg2spillmask  [Op_RegL] = &amp;rms[2];
 424   idealreg2spillmask  [Op_RegF] = &amp;rms[3];
 425   idealreg2spillmask  [Op_RegD] = &amp;rms[4];
 426   idealreg2spillmask  [Op_RegP] = &amp;rms[5];
 427 
 428   idealreg2debugmask  [Op_RegN] = &amp;rms[6];
 429   idealreg2debugmask  [Op_RegI] = &amp;rms[7];
 430   idealreg2debugmask  [Op_RegL] = &amp;rms[8];
 431   idealreg2debugmask  [Op_RegF] = &amp;rms[9];
 432   idealreg2debugmask  [Op_RegD] = &amp;rms[10];
 433   idealreg2debugmask  [Op_RegP] = &amp;rms[11];
 434 
 435   idealreg2mhdebugmask[Op_RegN] = &amp;rms[12];
 436   idealreg2mhdebugmask[Op_RegI] = &amp;rms[13];
 437   idealreg2mhdebugmask[Op_RegL] = &amp;rms[14];
 438   idealreg2mhdebugmask[Op_RegF] = &amp;rms[15];
 439   idealreg2mhdebugmask[Op_RegD] = &amp;rms[16];
 440   idealreg2mhdebugmask[Op_RegP] = &amp;rms[17];
 441 
 442   idealreg2spillmask  [Op_VecS] = &amp;rms[18];
 443   idealreg2spillmask  [Op_VecD] = &amp;rms[19];
 444   idealreg2spillmask  [Op_VecX] = &amp;rms[20];
 445   idealreg2spillmask  [Op_VecY] = &amp;rms[21];
 446   idealreg2spillmask  [Op_VecZ] = &amp;rms[22];
 447 
 448   OptoReg::Name i;
 449 
 450   // At first, start with the empty mask
 451   C-&gt;FIRST_STACK_mask().Clear();
 452 
 453   // Add in the incoming argument area
 454   OptoReg::Name init_in = OptoReg::add(_old_SP, C-&gt;out_preserve_stack_slots());
 455   for (i = init_in; i &lt; _in_arg_limit; i = OptoReg::add(i,1)) {
 456     C-&gt;FIRST_STACK_mask().Insert(i);
 457   }
 458   // Add in all bits past the outgoing argument area
 459   guarantee(RegMask::can_represent_arg(OptoReg::add(_out_arg_limit,-1)),
 460             "must be able to represent all call arguments in reg mask");
 461   OptoReg::Name init = _out_arg_limit;
 462   for (i = init; RegMask::can_represent(i); i = OptoReg::add(i,1)) {
 463     C-&gt;FIRST_STACK_mask().Insert(i);
 464   }
 465   // Finally, set the "infinite stack" bit.
 466   C-&gt;FIRST_STACK_mask().set_AllStack();
 467 
 468   // Make spill masks.  Registers for their class, plus FIRST_STACK_mask.
 469   RegMask aligned_stack_mask = C-&gt;FIRST_STACK_mask();
 470   // Keep spill masks aligned.
 471   aligned_stack_mask.clear_to_pairs();
 472   assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 473 
 474   *idealreg2spillmask[Op_RegP] = *idealreg2regmask[Op_RegP];
 475 #ifdef _LP64
 476   *idealreg2spillmask[Op_RegN] = *idealreg2regmask[Op_RegN];
 477    idealreg2spillmask[Op_RegN]-&gt;OR(C-&gt;FIRST_STACK_mask());
 478    idealreg2spillmask[Op_RegP]-&gt;OR(aligned_stack_mask);
 479 #else
 480    idealreg2spillmask[Op_RegP]-&gt;OR(C-&gt;FIRST_STACK_mask());
 481 #endif
 482   *idealreg2spillmask[Op_RegI] = *idealreg2regmask[Op_RegI];
 483    idealreg2spillmask[Op_RegI]-&gt;OR(C-&gt;FIRST_STACK_mask());
 484   *idealreg2spillmask[Op_RegL] = *idealreg2regmask[Op_RegL];
 485    idealreg2spillmask[Op_RegL]-&gt;OR(aligned_stack_mask);
 486   *idealreg2spillmask[Op_RegF] = *idealreg2regmask[Op_RegF];
 487    idealreg2spillmask[Op_RegF]-&gt;OR(C-&gt;FIRST_STACK_mask());
 488   *idealreg2spillmask[Op_RegD] = *idealreg2regmask[Op_RegD];
 489    idealreg2spillmask[Op_RegD]-&gt;OR(aligned_stack_mask);
 490 
 491   if (Matcher::vector_size_supported(T_BYTE,4)) {
 492     *idealreg2spillmask[Op_VecS] = *idealreg2regmask[Op_VecS];
 493      idealreg2spillmask[Op_VecS]-&gt;OR(C-&gt;FIRST_STACK_mask());
 494   }
 495   if (Matcher::vector_size_supported(T_FLOAT,2)) {
 496     // For VecD we need dual alignment and 8 bytes (2 slots) for spills.
 497     // RA guarantees such alignment since it is needed for Double and Long values.
 498     *idealreg2spillmask[Op_VecD] = *idealreg2regmask[Op_VecD];
 499      idealreg2spillmask[Op_VecD]-&gt;OR(aligned_stack_mask);
 500   }
 501   if (Matcher::vector_size_supported(T_FLOAT,4)) {
 502     // For VecX we need quadro alignment and 16 bytes (4 slots) for spills.
 503     //
 504     // RA can use input arguments stack slots for spills but until RA
 505     // we don't know frame size and offset of input arg stack slots.
 506     //
 507     // Exclude last input arg stack slots to avoid spilling vectors there
 508     // otherwise vector spills could stomp over stack slots in caller frame.
 509     OptoReg::Name in = OptoReg::add(_in_arg_limit, -1);
 510     for (int k = 1; (in &gt;= init_in) &amp;&amp; (k &lt; RegMask::SlotsPerVecX); k++) {
 511       aligned_stack_mask.Remove(in);
 512       in = OptoReg::add(in, -1);
 513     }
 514      aligned_stack_mask.clear_to_sets(RegMask::SlotsPerVecX);
 515      assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 516     *idealreg2spillmask[Op_VecX] = *idealreg2regmask[Op_VecX];
 517      idealreg2spillmask[Op_VecX]-&gt;OR(aligned_stack_mask);
 518   }
 519   if (Matcher::vector_size_supported(T_FLOAT,8)) {
 520     // For VecY we need octo alignment and 32 bytes (8 slots) for spills.
 521     OptoReg::Name in = OptoReg::add(_in_arg_limit, -1);
 522     for (int k = 1; (in &gt;= init_in) &amp;&amp; (k &lt; RegMask::SlotsPerVecY); k++) {
 523       aligned_stack_mask.Remove(in);
 524       in = OptoReg::add(in, -1);
 525     }
 526      aligned_stack_mask.clear_to_sets(RegMask::SlotsPerVecY);
 527      assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 528     *idealreg2spillmask[Op_VecY] = *idealreg2regmask[Op_VecY];
 529      idealreg2spillmask[Op_VecY]-&gt;OR(aligned_stack_mask);
 530   }
 531   if (Matcher::vector_size_supported(T_FLOAT,16)) {
 532     // For VecZ we need enough alignment and 64 bytes (16 slots) for spills.
 533     OptoReg::Name in = OptoReg::add(_in_arg_limit, -1);
 534     for (int k = 1; (in &gt;= init_in) &amp;&amp; (k &lt; RegMask::SlotsPerVecZ); k++) {
 535       aligned_stack_mask.Remove(in);
 536       in = OptoReg::add(in, -1);
 537     }
 538      aligned_stack_mask.clear_to_sets(RegMask::SlotsPerVecZ);
 539      assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 540     *idealreg2spillmask[Op_VecZ] = *idealreg2regmask[Op_VecZ];
 541      idealreg2spillmask[Op_VecZ]-&gt;OR(aligned_stack_mask);
 542   }
 543    if (UseFPUForSpilling) {
 544      // This mask logic assumes that the spill operations are
 545      // symmetric and that the registers involved are the same size.
 546      // On sparc for instance we may have to use 64 bit moves will
 547      // kill 2 registers when used with F0-F31.
 548      idealreg2spillmask[Op_RegI]-&gt;OR(*idealreg2regmask[Op_RegF]);
 549      idealreg2spillmask[Op_RegF]-&gt;OR(*idealreg2regmask[Op_RegI]);
 550 #ifdef _LP64
 551      idealreg2spillmask[Op_RegN]-&gt;OR(*idealreg2regmask[Op_RegF]);
 552      idealreg2spillmask[Op_RegL]-&gt;OR(*idealreg2regmask[Op_RegD]);
 553      idealreg2spillmask[Op_RegD]-&gt;OR(*idealreg2regmask[Op_RegL]);
 554      idealreg2spillmask[Op_RegP]-&gt;OR(*idealreg2regmask[Op_RegD]);
 555 #else
 556      idealreg2spillmask[Op_RegP]-&gt;OR(*idealreg2regmask[Op_RegF]);
 557 #ifdef ARM
 558      // ARM has support for moving 64bit values between a pair of
 559      // integer registers and a double register
 560      idealreg2spillmask[Op_RegL]-&gt;OR(*idealreg2regmask[Op_RegD]);
 561      idealreg2spillmask[Op_RegD]-&gt;OR(*idealreg2regmask[Op_RegL]);
 562 #endif
 563 #endif
 564    }
 565 
 566   // Make up debug masks.  Any spill slot plus callee-save registers.
 567   // Caller-save registers are assumed to be trashable by the various
 568   // inline-cache fixup routines.
 569   *idealreg2debugmask  [Op_RegN]= *idealreg2spillmask[Op_RegN];
 570   *idealreg2debugmask  [Op_RegI]= *idealreg2spillmask[Op_RegI];
 571   *idealreg2debugmask  [Op_RegL]= *idealreg2spillmask[Op_RegL];
 572   *idealreg2debugmask  [Op_RegF]= *idealreg2spillmask[Op_RegF];
 573   *idealreg2debugmask  [Op_RegD]= *idealreg2spillmask[Op_RegD];
 574   *idealreg2debugmask  [Op_RegP]= *idealreg2spillmask[Op_RegP];
 575 
 576   *idealreg2mhdebugmask[Op_RegN]= *idealreg2spillmask[Op_RegN];
 577   *idealreg2mhdebugmask[Op_RegI]= *idealreg2spillmask[Op_RegI];
 578   *idealreg2mhdebugmask[Op_RegL]= *idealreg2spillmask[Op_RegL];
 579   *idealreg2mhdebugmask[Op_RegF]= *idealreg2spillmask[Op_RegF];
 580   *idealreg2mhdebugmask[Op_RegD]= *idealreg2spillmask[Op_RegD];
 581   *idealreg2mhdebugmask[Op_RegP]= *idealreg2spillmask[Op_RegP];
 582 
 583   // Prevent stub compilations from attempting to reference
 584   // callee-saved registers from debug info
 585   bool exclude_soe = !Compile::current()-&gt;is_method_compilation();
 586 
 587   for( i=OptoReg::Name(0); i&lt;OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {
 588     // registers the caller has to save do not work
 589     if( _register_save_policy[i] == 'C' ||
 590         _register_save_policy[i] == 'A' ||
 591         (_register_save_policy[i] == 'E' &amp;&amp; exclude_soe) ) {
 592       idealreg2debugmask  [Op_RegN]-&gt;Remove(i);
 593       idealreg2debugmask  [Op_RegI]-&gt;Remove(i); // Exclude save-on-call
 594       idealreg2debugmask  [Op_RegL]-&gt;Remove(i); // registers from debug
 595       idealreg2debugmask  [Op_RegF]-&gt;Remove(i); // masks
 596       idealreg2debugmask  [Op_RegD]-&gt;Remove(i);
 597       idealreg2debugmask  [Op_RegP]-&gt;Remove(i);
 598 
 599       idealreg2mhdebugmask[Op_RegN]-&gt;Remove(i);
 600       idealreg2mhdebugmask[Op_RegI]-&gt;Remove(i);
 601       idealreg2mhdebugmask[Op_RegL]-&gt;Remove(i);
 602       idealreg2mhdebugmask[Op_RegF]-&gt;Remove(i);
 603       idealreg2mhdebugmask[Op_RegD]-&gt;Remove(i);
 604       idealreg2mhdebugmask[Op_RegP]-&gt;Remove(i);
 605     }
 606   }
 607 
 608   // Subtract the register we use to save the SP for MethodHandle
 609   // invokes to from the debug mask.
 610   const RegMask save_mask = method_handle_invoke_SP_save_mask();
 611   idealreg2mhdebugmask[Op_RegN]-&gt;SUBTRACT(save_mask);
 612   idealreg2mhdebugmask[Op_RegI]-&gt;SUBTRACT(save_mask);
 613   idealreg2mhdebugmask[Op_RegL]-&gt;SUBTRACT(save_mask);
 614   idealreg2mhdebugmask[Op_RegF]-&gt;SUBTRACT(save_mask);
 615   idealreg2mhdebugmask[Op_RegD]-&gt;SUBTRACT(save_mask);
 616   idealreg2mhdebugmask[Op_RegP]-&gt;SUBTRACT(save_mask);
 617 }
 618 
 619 //---------------------------is_save_on_entry----------------------------------
 620 bool Matcher::is_save_on_entry( int reg ) {
 621   return
 622     _register_save_policy[reg] == 'E' ||
 623     _register_save_policy[reg] == 'A' || // Save-on-entry register?
 624     // Also save argument registers in the trampolining stubs
 625     (C-&gt;save_argument_registers() &amp;&amp; is_spillable_arg(reg));
 626 }
 627 
 628 //---------------------------Fixup_Save_On_Entry-------------------------------
 629 void Matcher::Fixup_Save_On_Entry( ) {
 630   init_first_stack_mask();
 631 
 632   Node *root = C-&gt;root();       // Short name for root
 633   // Count number of save-on-entry registers.
 634   uint soe_cnt = number_of_saved_registers();
 635   uint i;
 636 
 637   // Find the procedure Start Node
 638   StartNode *start = C-&gt;start();
 639   assert( start, "Expect a start node" );
 640 
 641   // Save argument registers in the trampolining stubs
 642   if( C-&gt;save_argument_registers() )
 643     for( i = 0; i &lt; _last_Mach_Reg; i++ )
 644       if( is_spillable_arg(i) )
 645         soe_cnt++;
 646 
 647   // Input RegMask array shared by all Returns.
 648   // The type for doubles and longs has a count of 2, but
 649   // there is only 1 returned value
 650   uint ret_edge_cnt = TypeFunc::Parms + ((C-&gt;tf()-&gt;range()-&gt;cnt() == TypeFunc::Parms) ? 0 : 1);
 651   RegMask *ret_rms  = init_input_masks( ret_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 652   // Returns have 0 or 1 returned values depending on call signature.
 653   // Return register is specified by return_value in the AD file.
 654   if (ret_edge_cnt &gt; TypeFunc::Parms)
 655     ret_rms[TypeFunc::Parms+0] = _return_value_mask;
 656 
 657   // Input RegMask array shared by all Rethrows.
 658   uint reth_edge_cnt = TypeFunc::Parms+1;
 659   RegMask *reth_rms  = init_input_masks( reth_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 660   // Rethrow takes exception oop only, but in the argument 0 slot.
 661   reth_rms[TypeFunc::Parms] = mreg2regmask[find_receiver(false)];
 662 #ifdef _LP64
 663   // Need two slots for ptrs in 64-bit land
 664   reth_rms[TypeFunc::Parms].Insert(OptoReg::add(OptoReg::Name(find_receiver(false)),1));
 665 #endif
 666 
 667   // Input RegMask array shared by all TailCalls
 668   uint tail_call_edge_cnt = TypeFunc::Parms+2;
 669   RegMask *tail_call_rms = init_input_masks( tail_call_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 670 
 671   // Input RegMask array shared by all TailJumps
 672   uint tail_jump_edge_cnt = TypeFunc::Parms+2;
 673   RegMask *tail_jump_rms = init_input_masks( tail_jump_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 674 
 675   // TailCalls have 2 returned values (target &amp; moop), whose masks come
 676   // from the usual MachNode/MachOper mechanism.  Find a sample
 677   // TailCall to extract these masks and put the correct masks into
 678   // the tail_call_rms array.
 679   for( i=1; i &lt; root-&gt;req(); i++ ) {
 680     MachReturnNode *m = root-&gt;in(i)-&gt;as_MachReturn();
 681     if( m-&gt;ideal_Opcode() == Op_TailCall ) {
 682       tail_call_rms[TypeFunc::Parms+0] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+0);
 683       tail_call_rms[TypeFunc::Parms+1] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+1);
 684       break;
 685     }
 686   }
 687 
 688   // TailJumps have 2 returned values (target &amp; ex_oop), whose masks come
 689   // from the usual MachNode/MachOper mechanism.  Find a sample
 690   // TailJump to extract these masks and put the correct masks into
 691   // the tail_jump_rms array.
 692   for( i=1; i &lt; root-&gt;req(); i++ ) {
 693     MachReturnNode *m = root-&gt;in(i)-&gt;as_MachReturn();
 694     if( m-&gt;ideal_Opcode() == Op_TailJump ) {
 695       tail_jump_rms[TypeFunc::Parms+0] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+0);
 696       tail_jump_rms[TypeFunc::Parms+1] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+1);
 697       break;
 698     }
 699   }
 700 
 701   // Input RegMask array shared by all Halts
 702   uint halt_edge_cnt = TypeFunc::Parms;
 703   RegMask *halt_rms = init_input_masks( halt_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 704 
 705   // Capture the return input masks into each exit flavor
 706   for( i=1; i &lt; root-&gt;req(); i++ ) {
 707     MachReturnNode *exit = root-&gt;in(i)-&gt;as_MachReturn();
 708     switch( exit-&gt;ideal_Opcode() ) {
 709       case Op_Return   : exit-&gt;_in_rms = ret_rms;  break;
 710       case Op_Rethrow  : exit-&gt;_in_rms = reth_rms; break;
 711       case Op_TailCall : exit-&gt;_in_rms = tail_call_rms; break;
 712       case Op_TailJump : exit-&gt;_in_rms = tail_jump_rms; break;
 713       case Op_Halt     : exit-&gt;_in_rms = halt_rms; break;
 714       default          : ShouldNotReachHere();
 715     }
 716   }
 717 
 718   // Next unused projection number from Start.
 719   int proj_cnt = C-&gt;tf()-&gt;domain()-&gt;cnt();
 720 
 721   // Do all the save-on-entry registers.  Make projections from Start for
 722   // them, and give them a use at the exit points.  To the allocator, they
 723   // look like incoming register arguments.
 724   for( i = 0; i &lt; _last_Mach_Reg; i++ ) {
 725     if( is_save_on_entry(i) ) {
 726 
 727       // Add the save-on-entry to the mask array
 728       ret_rms      [      ret_edge_cnt] = mreg2regmask[i];
 729       reth_rms     [     reth_edge_cnt] = mreg2regmask[i];
 730       tail_call_rms[tail_call_edge_cnt] = mreg2regmask[i];
 731       tail_jump_rms[tail_jump_edge_cnt] = mreg2regmask[i];
 732       // Halts need the SOE registers, but only in the stack as debug info.
 733       // A just-prior uncommon-trap or deoptimization will use the SOE regs.
 734       halt_rms     [     halt_edge_cnt] = *idealreg2spillmask[_register_save_type[i]];
 735 
 736       Node *mproj;
 737 
 738       // Is this a RegF low half of a RegD?  Double up 2 adjacent RegF's
 739       // into a single RegD.
 740       if( (i&amp;1) == 0 &amp;&amp;
 741           _register_save_type[i  ] == Op_RegF &amp;&amp;
 742           _register_save_type[i+1] == Op_RegF &amp;&amp;
 743           is_save_on_entry(i+1) ) {
 744         // Add other bit for double
 745         ret_rms      [      ret_edge_cnt].Insert(OptoReg::Name(i+1));
 746         reth_rms     [     reth_edge_cnt].Insert(OptoReg::Name(i+1));
 747         tail_call_rms[tail_call_edge_cnt].Insert(OptoReg::Name(i+1));
 748         tail_jump_rms[tail_jump_edge_cnt].Insert(OptoReg::Name(i+1));
 749         halt_rms     [     halt_edge_cnt].Insert(OptoReg::Name(i+1));
 750         mproj = new MachProjNode( start, proj_cnt, ret_rms[ret_edge_cnt], Op_RegD );
 751         proj_cnt += 2;          // Skip 2 for doubles
 752       }
 753       else if( (i&amp;1) == 1 &amp;&amp;    // Else check for high half of double
 754                _register_save_type[i-1] == Op_RegF &amp;&amp;
 755                _register_save_type[i  ] == Op_RegF &amp;&amp;
 756                is_save_on_entry(i-1) ) {
 757         ret_rms      [      ret_edge_cnt] = RegMask::Empty;
 758         reth_rms     [     reth_edge_cnt] = RegMask::Empty;
 759         tail_call_rms[tail_call_edge_cnt] = RegMask::Empty;
 760         tail_jump_rms[tail_jump_edge_cnt] = RegMask::Empty;
 761         halt_rms     [     halt_edge_cnt] = RegMask::Empty;
 762         mproj = C-&gt;top();
 763       }
 764       // Is this a RegI low half of a RegL?  Double up 2 adjacent RegI's
 765       // into a single RegL.
 766       else if( (i&amp;1) == 0 &amp;&amp;
 767           _register_save_type[i  ] == Op_RegI &amp;&amp;
 768           _register_save_type[i+1] == Op_RegI &amp;&amp;
 769         is_save_on_entry(i+1) ) {
 770         // Add other bit for long
 771         ret_rms      [      ret_edge_cnt].Insert(OptoReg::Name(i+1));
 772         reth_rms     [     reth_edge_cnt].Insert(OptoReg::Name(i+1));
 773         tail_call_rms[tail_call_edge_cnt].Insert(OptoReg::Name(i+1));
 774         tail_jump_rms[tail_jump_edge_cnt].Insert(OptoReg::Name(i+1));
 775         halt_rms     [     halt_edge_cnt].Insert(OptoReg::Name(i+1));
 776         mproj = new MachProjNode( start, proj_cnt, ret_rms[ret_edge_cnt], Op_RegL );
 777         proj_cnt += 2;          // Skip 2 for longs
 778       }
 779       else if( (i&amp;1) == 1 &amp;&amp;    // Else check for high half of long
 780                _register_save_type[i-1] == Op_RegI &amp;&amp;
 781                _register_save_type[i  ] == Op_RegI &amp;&amp;
 782                is_save_on_entry(i-1) ) {
 783         ret_rms      [      ret_edge_cnt] = RegMask::Empty;
 784         reth_rms     [     reth_edge_cnt] = RegMask::Empty;
 785         tail_call_rms[tail_call_edge_cnt] = RegMask::Empty;
 786         tail_jump_rms[tail_jump_edge_cnt] = RegMask::Empty;
 787         halt_rms     [     halt_edge_cnt] = RegMask::Empty;
 788         mproj = C-&gt;top();
 789       } else {
 790         // Make a projection for it off the Start
 791         mproj = new MachProjNode( start, proj_cnt++, ret_rms[ret_edge_cnt], _register_save_type[i] );
 792       }
 793 
 794       ret_edge_cnt ++;
 795       reth_edge_cnt ++;
 796       tail_call_edge_cnt ++;
 797       tail_jump_edge_cnt ++;
 798       halt_edge_cnt ++;
 799 
 800       // Add a use of the SOE register to all exit paths
 801       for( uint j=1; j &lt; root-&gt;req(); j++ )
 802         root-&gt;in(j)-&gt;add_req(mproj);
 803     } // End of if a save-on-entry register
 804   } // End of for all machine registers
 805 }
 806 
 807 //------------------------------init_spill_mask--------------------------------
 808 void Matcher::init_spill_mask( Node *ret ) {
 809   if( idealreg2regmask[Op_RegI] ) return; // One time only init
 810 
 811   OptoReg::c_frame_pointer = c_frame_pointer();
 812   c_frame_ptr_mask = c_frame_pointer();
 813 #ifdef _LP64
 814   // pointers are twice as big
 815   c_frame_ptr_mask.Insert(OptoReg::add(c_frame_pointer(),1));
 816 #endif
 817 
 818   // Start at OptoReg::stack0()
 819   STACK_ONLY_mask.Clear();
 820   OptoReg::Name init = OptoReg::stack2reg(0);
 821   // STACK_ONLY_mask is all stack bits
 822   OptoReg::Name i;
 823   for (i = init; RegMask::can_represent(i); i = OptoReg::add(i,1))
 824     STACK_ONLY_mask.Insert(i);
 825   // Also set the "infinite stack" bit.
 826   STACK_ONLY_mask.set_AllStack();
 827 
 828   // Copy the register names over into the shared world
 829   for( i=OptoReg::Name(0); i&lt;OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {
 830     // SharedInfo::regName[i] = regName[i];
 831     // Handy RegMasks per machine register
 832     mreg2regmask[i].Insert(i);
 833   }
 834 
 835   // Grab the Frame Pointer
 836   Node *fp  = ret-&gt;in(TypeFunc::FramePtr);
 837   Node *mem = ret-&gt;in(TypeFunc::Memory);
 838   const TypePtr* atp = TypePtr::BOTTOM;
 839   // Share frame pointer while making spill ops
 840   set_shared(fp);
 841 
 842   // Compute generic short-offset Loads
 843 #ifdef _LP64
 844   MachNode *spillCP = match_tree(new LoadNNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));
 845 #endif
 846   MachNode *spillI  = match_tree(new LoadINode(NULL,mem,fp,atp,TypeInt::INT,MemNode::unordered));
 847   MachNode *spillL  = match_tree(new LoadLNode(NULL,mem,fp,atp,TypeLong::LONG,MemNode::unordered, LoadNode::DependsOnlyOnTest, false));
 848   MachNode *spillF  = match_tree(new LoadFNode(NULL,mem,fp,atp,Type::FLOAT,MemNode::unordered));
 849   MachNode *spillD  = match_tree(new LoadDNode(NULL,mem,fp,atp,Type::DOUBLE,MemNode::unordered));
 850   MachNode *spillP  = match_tree(new LoadPNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));
 851   assert(spillI != NULL &amp;&amp; spillL != NULL &amp;&amp; spillF != NULL &amp;&amp;
 852          spillD != NULL &amp;&amp; spillP != NULL, "");
 853   // Get the ADLC notion of the right regmask, for each basic type.
 854 #ifdef _LP64
 855   idealreg2regmask[Op_RegN] = &amp;spillCP-&gt;out_RegMask();
 856 #endif
 857   idealreg2regmask[Op_RegI] = &amp;spillI-&gt;out_RegMask();
 858   idealreg2regmask[Op_RegL] = &amp;spillL-&gt;out_RegMask();
 859   idealreg2regmask[Op_RegF] = &amp;spillF-&gt;out_RegMask();
 860   idealreg2regmask[Op_RegD] = &amp;spillD-&gt;out_RegMask();
 861   idealreg2regmask[Op_RegP] = &amp;spillP-&gt;out_RegMask();
 862 
 863   // Vector regmasks.
 864   if (Matcher::vector_size_supported(T_BYTE,4)) {
 865     TypeVect::VECTS = TypeVect::make(T_BYTE, 4);
 866     MachNode *spillVectS = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTS));
 867     idealreg2regmask[Op_VecS] = &amp;spillVectS-&gt;out_RegMask();
 868   }
 869   if (Matcher::vector_size_supported(T_FLOAT,2)) {
 870     MachNode *spillVectD = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTD));
 871     idealreg2regmask[Op_VecD] = &amp;spillVectD-&gt;out_RegMask();
 872   }
 873   if (Matcher::vector_size_supported(T_FLOAT,4)) {
 874     MachNode *spillVectX = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTX));
 875     idealreg2regmask[Op_VecX] = &amp;spillVectX-&gt;out_RegMask();
 876   }
 877   if (Matcher::vector_size_supported(T_FLOAT,8)) {
 878     MachNode *spillVectY = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTY));
 879     idealreg2regmask[Op_VecY] = &amp;spillVectY-&gt;out_RegMask();
 880   }
 881   if (Matcher::vector_size_supported(T_FLOAT,16)) {
 882     MachNode *spillVectZ = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTZ));
 883     idealreg2regmask[Op_VecZ] = &amp;spillVectZ-&gt;out_RegMask();
 884   }
 885 }
 886 
 887 #ifdef ASSERT
 888 static void match_alias_type(Compile* C, Node* n, Node* m) {
 889   if (!VerifyAliases)  return;  // do not go looking for trouble by default
 890   const TypePtr* nat = n-&gt;adr_type();
 891   const TypePtr* mat = m-&gt;adr_type();
 892   int nidx = C-&gt;get_alias_index(nat);
 893   int midx = C-&gt;get_alias_index(mat);
 894   // Detune the assert for cases like (AndI 0xFF (LoadB p)).
 895   if (nidx == Compile::AliasIdxTop &amp;&amp; midx &gt;= Compile::AliasIdxRaw) {
 896     for (uint i = 1; i &lt; n-&gt;req(); i++) {
 897       Node* n1 = n-&gt;in(i);
 898       const TypePtr* n1at = n1-&gt;adr_type();
 899       if (n1at != NULL) {
 900         nat = n1at;
 901         nidx = C-&gt;get_alias_index(n1at);
 902       }
 903     }
 904   }
 905   // %%% Kludgery.  Instead, fix ideal adr_type methods for all these cases:
 906   if (nidx == Compile::AliasIdxTop &amp;&amp; midx == Compile::AliasIdxRaw) {
 907     switch (n-&gt;Opcode()) {
 908     case Op_PrefetchAllocation:
 909       nidx = Compile::AliasIdxRaw;
 910       nat = TypeRawPtr::BOTTOM;
 911       break;
 912     }
 913   }
 914   if (nidx == Compile::AliasIdxRaw &amp;&amp; midx == Compile::AliasIdxTop) {
 915     switch (n-&gt;Opcode()) {
 916     case Op_ClearArray:
 917       midx = Compile::AliasIdxRaw;
 918       mat = TypeRawPtr::BOTTOM;
 919       break;
 920     }
 921   }
 922   if (nidx == Compile::AliasIdxTop &amp;&amp; midx == Compile::AliasIdxBot) {
 923     switch (n-&gt;Opcode()) {
 924     case Op_Return:
 925     case Op_Rethrow:
 926     case Op_Halt:
 927     case Op_TailCall:
 928     case Op_TailJump:
 929       nidx = Compile::AliasIdxBot;
 930       nat = TypePtr::BOTTOM;
 931       break;
 932     }
 933   }
 934   if (nidx == Compile::AliasIdxBot &amp;&amp; midx == Compile::AliasIdxTop) {
 935     switch (n-&gt;Opcode()) {
 936     case Op_StrComp:
 937     case Op_StrEquals:
 938     case Op_StrIndexOf:
 939     case Op_StrIndexOfChar:
 940     case Op_AryEq:
 941     case Op_HasNegatives:
 942     case Op_MemBarVolatile:
 943     case Op_MemBarCPUOrder: // %%% these ideals should have narrower adr_type?
 944     case Op_StrInflatedCopy:
 945     case Op_StrCompressedCopy:
<a name="1" id="anc1"></a><span class="new"> 946     case Op_OnSpinWait:</span>
 947     case Op_EncodeISOArray:
 948       nidx = Compile::AliasIdxTop;
 949       nat = NULL;
 950       break;
 951     }
 952   }
 953   if (nidx != midx) {
 954     if (PrintOpto || (PrintMiscellaneous &amp;&amp; (WizardMode || Verbose))) {
 955       tty-&gt;print_cr("==== Matcher alias shift %d =&gt; %d", nidx, midx);
 956       n-&gt;dump();
 957       m-&gt;dump();
 958     }
 959     assert(C-&gt;subsume_loads() &amp;&amp; C-&gt;must_alias(nat, midx),
 960            "must not lose alias info when matching");
 961   }
 962 }
 963 #endif
 964 
 965 
 966 //------------------------------MStack-----------------------------------------
 967 // State and MStack class used in xform() and find_shared() iterative methods.
 968 enum Node_State { Pre_Visit,  // node has to be pre-visited
 969                       Visit,  // visit node
 970                  Post_Visit,  // post-visit node
 971              Alt_Post_Visit   // alternative post-visit path
 972                 };
 973 
 974 class MStack: public Node_Stack {
 975   public:
 976     MStack(int size) : Node_Stack(size) { }
 977 
 978     void push(Node *n, Node_State ns) {
 979       Node_Stack::push(n, (uint)ns);
 980     }
 981     void push(Node *n, Node_State ns, Node *parent, int indx) {
 982       ++_inode_top;
 983       if ((_inode_top + 1) &gt;= _inode_max) grow();
 984       _inode_top-&gt;node = parent;
 985       _inode_top-&gt;indx = (uint)indx;
 986       ++_inode_top;
 987       _inode_top-&gt;node = n;
 988       _inode_top-&gt;indx = (uint)ns;
 989     }
 990     Node *parent() {
 991       pop();
 992       return node();
 993     }
 994     Node_State state() const {
 995       return (Node_State)index();
 996     }
 997     void set_state(Node_State ns) {
 998       set_index((uint)ns);
 999     }
1000 };
1001 
1002 
1003 //------------------------------xform------------------------------------------
1004 // Given a Node in old-space, Match him (Label/Reduce) to produce a machine
1005 // Node in new-space.  Given a new-space Node, recursively walk his children.
1006 Node *Matcher::transform( Node *n ) { ShouldNotCallThis(); return n; }
1007 Node *Matcher::xform( Node *n, int max_stack ) {
1008   // Use one stack to keep both: child's node/state and parent's node/index
1009   MStack mstack(max_stack * 2 * 2); // usually: C-&gt;live_nodes() * 2 * 2
1010   mstack.push(n, Visit, NULL, -1);  // set NULL as parent to indicate root
1011 
1012   while (mstack.is_nonempty()) {
1013     C-&gt;check_node_count(NodeLimitFudgeFactor, "too many nodes matching instructions");
1014     if (C-&gt;failing()) return NULL;
1015     n = mstack.node();          // Leave node on stack
1016     Node_State nstate = mstack.state();
1017     if (nstate == Visit) {
1018       mstack.set_state(Post_Visit);
1019       Node *oldn = n;
1020       // Old-space or new-space check
1021       if (!C-&gt;node_arena()-&gt;contains(n)) {
1022         // Old space!
1023         Node* m;
1024         if (has_new_node(n)) {  // Not yet Label/Reduced
1025           m = new_node(n);
1026         } else {
1027           if (!is_dontcare(n)) { // Matcher can match this guy
1028             // Calls match special.  They match alone with no children.
1029             // Their children, the incoming arguments, match normally.
1030             m = n-&gt;is_SafePoint() ? match_sfpt(n-&gt;as_SafePoint()):match_tree(n);
1031             if (C-&gt;failing())  return NULL;
1032             if (m == NULL) { Matcher::soft_match_failure(); return NULL; }
1033           } else {                  // Nothing the matcher cares about
1034             if( n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;is_Multi()) {       // Projections?
1035               // Convert to machine-dependent projection
1036               m = n-&gt;in(0)-&gt;as_Multi()-&gt;match( n-&gt;as_Proj(), this );
1037 #ifdef ASSERT
1038               _new2old_map.map(m-&gt;_idx, n);
1039 #endif
1040               if (m-&gt;in(0) != NULL) // m might be top
1041                 collect_null_checks(m, n);
1042             } else {                // Else just a regular 'ol guy
1043               m = n-&gt;clone();       // So just clone into new-space
1044 #ifdef ASSERT
1045               _new2old_map.map(m-&gt;_idx, n);
1046 #endif
1047               // Def-Use edges will be added incrementally as Uses
1048               // of this node are matched.
1049               assert(m-&gt;outcnt() == 0, "no Uses of this clone yet");
1050             }
1051           }
1052 
1053           set_new_node(n, m);       // Map old to new
1054           if (_old_node_note_array != NULL) {
1055             Node_Notes* nn = C-&gt;locate_node_notes(_old_node_note_array,
1056                                                   n-&gt;_idx);
1057             C-&gt;set_node_notes_at(m-&gt;_idx, nn);
1058           }
1059           debug_only(match_alias_type(C, n, m));
1060         }
1061         n = m;    // n is now a new-space node
1062         mstack.set_node(n);
1063       }
1064 
1065       // New space!
1066       if (_visited.test_set(n-&gt;_idx)) continue; // while(mstack.is_nonempty())
1067 
1068       int i;
1069       // Put precedence edges on stack first (match them last).
1070       for (i = oldn-&gt;req(); (uint)i &lt; oldn-&gt;len(); i++) {
1071         Node *m = oldn-&gt;in(i);
1072         if (m == NULL) break;
1073         // set -1 to call add_prec() instead of set_req() during Step1
1074         mstack.push(m, Visit, n, -1);
1075       }
1076 
1077       // Handle precedence edges for interior nodes
1078       for (i = n-&gt;len()-1; (uint)i &gt;= n-&gt;req(); i--) {
1079         Node *m = n-&gt;in(i);
1080         if (m == NULL || C-&gt;node_arena()-&gt;contains(m)) continue;
1081         n-&gt;rm_prec(i);
1082         // set -1 to call add_prec() instead of set_req() during Step1
1083         mstack.push(m, Visit, n, -1);
1084       }
1085 
1086       // For constant debug info, I'd rather have unmatched constants.
1087       int cnt = n-&gt;req();
1088       JVMState* jvms = n-&gt;jvms();
1089       int debug_cnt = jvms ? jvms-&gt;debug_start() : cnt;
1090 
1091       // Now do only debug info.  Clone constants rather than matching.
1092       // Constants are represented directly in the debug info without
1093       // the need for executable machine instructions.
1094       // Monitor boxes are also represented directly.
1095       for (i = cnt - 1; i &gt;= debug_cnt; --i) { // For all debug inputs do
1096         Node *m = n-&gt;in(i);          // Get input
1097         int op = m-&gt;Opcode();
1098         assert((op == Op_BoxLock) == jvms-&gt;is_monitor_use(i), "boxes only at monitor sites");
1099         if( op == Op_ConI || op == Op_ConP || op == Op_ConN || op == Op_ConNKlass ||
1100             op == Op_ConF || op == Op_ConD || op == Op_ConL
1101             // || op == Op_BoxLock  // %%%% enable this and remove (+++) in chaitin.cpp
1102             ) {
1103           m = m-&gt;clone();
1104 #ifdef ASSERT
1105           _new2old_map.map(m-&gt;_idx, n);
1106 #endif
1107           mstack.push(m, Post_Visit, n, i); // Don't need to visit
1108           mstack.push(m-&gt;in(0), Visit, m, 0);
1109         } else {
1110           mstack.push(m, Visit, n, i);
1111         }
1112       }
1113 
1114       // And now walk his children, and convert his inputs to new-space.
1115       for( ; i &gt;= 0; --i ) { // For all normal inputs do
1116         Node *m = n-&gt;in(i);  // Get input
1117         if(m != NULL)
1118           mstack.push(m, Visit, n, i);
1119       }
1120 
1121     }
1122     else if (nstate == Post_Visit) {
1123       // Set xformed input
1124       Node *p = mstack.parent();
1125       if (p != NULL) { // root doesn't have parent
1126         int i = (int)mstack.index();
1127         if (i &gt;= 0)
1128           p-&gt;set_req(i, n); // required input
1129         else if (i == -1)
1130           p-&gt;add_prec(n);   // precedence input
1131         else
1132           ShouldNotReachHere();
1133       }
1134       mstack.pop(); // remove processed node from stack
1135     }
1136     else {
1137       ShouldNotReachHere();
1138     }
1139   } // while (mstack.is_nonempty())
1140   return n; // Return new-space Node
1141 }
1142 
1143 //------------------------------warp_outgoing_stk_arg------------------------
1144 OptoReg::Name Matcher::warp_outgoing_stk_arg( VMReg reg, OptoReg::Name begin_out_arg_area, OptoReg::Name &amp;out_arg_limit_per_call ) {
1145   // Convert outgoing argument location to a pre-biased stack offset
1146   if (reg-&gt;is_stack()) {
1147     OptoReg::Name warped = reg-&gt;reg2stack();
1148     // Adjust the stack slot offset to be the register number used
1149     // by the allocator.
1150     warped = OptoReg::add(begin_out_arg_area, warped);
1151     // Keep track of the largest numbered stack slot used for an arg.
1152     // Largest used slot per call-site indicates the amount of stack
1153     // that is killed by the call.
1154     if( warped &gt;= out_arg_limit_per_call )
1155       out_arg_limit_per_call = OptoReg::add(warped,1);
1156     if (!RegMask::can_represent_arg(warped)) {
1157       C-&gt;record_method_not_compilable_all_tiers("unsupported calling sequence");
1158       return OptoReg::Bad;
1159     }
1160     return warped;
1161   }
1162   return OptoReg::as_OptoReg(reg);
1163 }
1164 
1165 
1166 //------------------------------match_sfpt-------------------------------------
1167 // Helper function to match call instructions.  Calls match special.
1168 // They match alone with no children.  Their children, the incoming
1169 // arguments, match normally.
1170 MachNode *Matcher::match_sfpt( SafePointNode *sfpt ) {
1171   MachSafePointNode *msfpt = NULL;
1172   MachCallNode      *mcall = NULL;
1173   uint               cnt;
1174   // Split out case for SafePoint vs Call
1175   CallNode *call;
1176   const TypeTuple *domain;
1177   ciMethod*        method = NULL;
1178   bool             is_method_handle_invoke = false;  // for special kill effects
1179   if( sfpt-&gt;is_Call() ) {
1180     call = sfpt-&gt;as_Call();
1181     domain = call-&gt;tf()-&gt;domain();
1182     cnt = domain-&gt;cnt();
1183 
1184     // Match just the call, nothing else
1185     MachNode *m = match_tree(call);
1186     if (C-&gt;failing())  return NULL;
1187     if( m == NULL ) { Matcher::soft_match_failure(); return NULL; }
1188 
1189     // Copy data from the Ideal SafePoint to the machine version
1190     mcall = m-&gt;as_MachCall();
1191 
1192     mcall-&gt;set_tf(         call-&gt;tf());
1193     mcall-&gt;set_entry_point(call-&gt;entry_point());
1194     mcall-&gt;set_cnt(        call-&gt;cnt());
1195 
1196     if( mcall-&gt;is_MachCallJava() ) {
1197       MachCallJavaNode *mcall_java  = mcall-&gt;as_MachCallJava();
1198       const CallJavaNode *call_java =  call-&gt;as_CallJava();
1199       method = call_java-&gt;method();
1200       mcall_java-&gt;_method = method;
1201       mcall_java-&gt;_bci = call_java-&gt;_bci;
1202       mcall_java-&gt;_optimized_virtual = call_java-&gt;is_optimized_virtual();
1203       is_method_handle_invoke = call_java-&gt;is_method_handle_invoke();
1204       mcall_java-&gt;_method_handle_invoke = is_method_handle_invoke;
1205       if (is_method_handle_invoke) {
1206         C-&gt;set_has_method_handle_invokes(true);
1207       }
1208       if( mcall_java-&gt;is_MachCallStaticJava() )
1209         mcall_java-&gt;as_MachCallStaticJava()-&gt;_name =
1210          call_java-&gt;as_CallStaticJava()-&gt;_name;
1211       if( mcall_java-&gt;is_MachCallDynamicJava() )
1212         mcall_java-&gt;as_MachCallDynamicJava()-&gt;_vtable_index =
1213          call_java-&gt;as_CallDynamicJava()-&gt;_vtable_index;
1214     }
1215     else if( mcall-&gt;is_MachCallRuntime() ) {
1216       mcall-&gt;as_MachCallRuntime()-&gt;_name = call-&gt;as_CallRuntime()-&gt;_name;
1217     }
1218     msfpt = mcall;
1219   }
1220   // This is a non-call safepoint
1221   else {
1222     call = NULL;
1223     domain = NULL;
1224     MachNode *mn = match_tree(sfpt);
1225     if (C-&gt;failing())  return NULL;
1226     msfpt = mn-&gt;as_MachSafePoint();
1227     cnt = TypeFunc::Parms;
1228   }
1229 
1230   // Advertise the correct memory effects (for anti-dependence computation).
1231   msfpt-&gt;set_adr_type(sfpt-&gt;adr_type());
1232 
1233   // Allocate a private array of RegMasks.  These RegMasks are not shared.
1234   msfpt-&gt;_in_rms = NEW_RESOURCE_ARRAY( RegMask, cnt );
1235   // Empty them all.
1236   memset( msfpt-&gt;_in_rms, 0, sizeof(RegMask)*cnt );
1237 
1238   // Do all the pre-defined non-Empty register masks
1239   msfpt-&gt;_in_rms[TypeFunc::ReturnAdr] = _return_addr_mask;
1240   msfpt-&gt;_in_rms[TypeFunc::FramePtr ] = c_frame_ptr_mask;
1241 
1242   // Place first outgoing argument can possibly be put.
1243   OptoReg::Name begin_out_arg_area = OptoReg::add(_new_SP, C-&gt;out_preserve_stack_slots());
1244   assert( is_even(begin_out_arg_area), "" );
1245   // Compute max outgoing register number per call site.
1246   OptoReg::Name out_arg_limit_per_call = begin_out_arg_area;
1247   // Calls to C may hammer extra stack slots above and beyond any arguments.
1248   // These are usually backing store for register arguments for varargs.
1249   if( call != NULL &amp;&amp; call-&gt;is_CallRuntime() )
1250     out_arg_limit_per_call = OptoReg::add(out_arg_limit_per_call,C-&gt;varargs_C_out_slots_killed());
1251 
1252 
1253   // Do the normal argument list (parameters) register masks
1254   int argcnt = cnt - TypeFunc::Parms;
1255   if( argcnt &gt; 0 ) {          // Skip it all if we have no args
1256     BasicType *sig_bt  = NEW_RESOURCE_ARRAY( BasicType, argcnt );
1257     VMRegPair *parm_regs = NEW_RESOURCE_ARRAY( VMRegPair, argcnt );
1258     int i;
1259     for( i = 0; i &lt; argcnt; i++ ) {
1260       sig_bt[i] = domain-&gt;field_at(i+TypeFunc::Parms)-&gt;basic_type();
1261     }
1262     // V-call to pick proper calling convention
1263     call-&gt;calling_convention( sig_bt, parm_regs, argcnt );
1264 
1265 #ifdef ASSERT
1266     // Sanity check users' calling convention.  Really handy during
1267     // the initial porting effort.  Fairly expensive otherwise.
1268     { for (int i = 0; i&lt;argcnt; i++) {
1269       if( !parm_regs[i].first()-&gt;is_valid() &amp;&amp;
1270           !parm_regs[i].second()-&gt;is_valid() ) continue;
1271       VMReg reg1 = parm_regs[i].first();
1272       VMReg reg2 = parm_regs[i].second();
1273       for (int j = 0; j &lt; i; j++) {
1274         if( !parm_regs[j].first()-&gt;is_valid() &amp;&amp;
1275             !parm_regs[j].second()-&gt;is_valid() ) continue;
1276         VMReg reg3 = parm_regs[j].first();
1277         VMReg reg4 = parm_regs[j].second();
1278         if( !reg1-&gt;is_valid() ) {
1279           assert( !reg2-&gt;is_valid(), "valid halvsies" );
1280         } else if( !reg3-&gt;is_valid() ) {
1281           assert( !reg4-&gt;is_valid(), "valid halvsies" );
1282         } else {
1283           assert( reg1 != reg2, "calling conv. must produce distinct regs");
1284           assert( reg1 != reg3, "calling conv. must produce distinct regs");
1285           assert( reg1 != reg4, "calling conv. must produce distinct regs");
1286           assert( reg2 != reg3, "calling conv. must produce distinct regs");
1287           assert( reg2 != reg4 || !reg2-&gt;is_valid(), "calling conv. must produce distinct regs");
1288           assert( reg3 != reg4, "calling conv. must produce distinct regs");
1289         }
1290       }
1291     }
1292     }
1293 #endif
1294 
1295     // Visit each argument.  Compute its outgoing register mask.
1296     // Return results now can have 2 bits returned.
1297     // Compute max over all outgoing arguments both per call-site
1298     // and over the entire method.
1299     for( i = 0; i &lt; argcnt; i++ ) {
1300       // Address of incoming argument mask to fill in
1301       RegMask *rm = &amp;mcall-&gt;_in_rms[i+TypeFunc::Parms];
1302       if( !parm_regs[i].first()-&gt;is_valid() &amp;&amp;
1303           !parm_regs[i].second()-&gt;is_valid() ) {
1304         continue;               // Avoid Halves
1305       }
1306       // Grab first register, adjust stack slots and insert in mask.
1307       OptoReg::Name reg1 = warp_outgoing_stk_arg(parm_regs[i].first(), begin_out_arg_area, out_arg_limit_per_call );
1308       if (OptoReg::is_valid(reg1))
1309         rm-&gt;Insert( reg1 );
1310       // Grab second register (if any), adjust stack slots and insert in mask.
1311       OptoReg::Name reg2 = warp_outgoing_stk_arg(parm_regs[i].second(), begin_out_arg_area, out_arg_limit_per_call );
1312       if (OptoReg::is_valid(reg2))
1313         rm-&gt;Insert( reg2 );
1314     } // End of for all arguments
1315 
1316     // Compute number of stack slots needed to restore stack in case of
1317     // Pascal-style argument popping.
1318     mcall-&gt;_argsize = out_arg_limit_per_call - begin_out_arg_area;
1319   }
1320 
1321   // Compute the max stack slot killed by any call.  These will not be
1322   // available for debug info, and will be used to adjust FIRST_STACK_mask
1323   // after all call sites have been visited.
1324   if( _out_arg_limit &lt; out_arg_limit_per_call)
1325     _out_arg_limit = out_arg_limit_per_call;
1326 
1327   if (mcall) {
1328     // Kill the outgoing argument area, including any non-argument holes and
1329     // any legacy C-killed slots.  Use Fat-Projections to do the killing.
1330     // Since the max-per-method covers the max-per-call-site and debug info
1331     // is excluded on the max-per-method basis, debug info cannot land in
1332     // this killed area.
1333     uint r_cnt = mcall-&gt;tf()-&gt;range()-&gt;cnt();
1334     MachProjNode *proj = new MachProjNode( mcall, r_cnt+10000, RegMask::Empty, MachProjNode::fat_proj );
1335     if (!RegMask::can_represent_arg(OptoReg::Name(out_arg_limit_per_call-1))) {
1336       C-&gt;record_method_not_compilable_all_tiers("unsupported outgoing calling sequence");
1337     } else {
1338       for (int i = begin_out_arg_area; i &lt; out_arg_limit_per_call; i++)
1339         proj-&gt;_rout.Insert(OptoReg::Name(i));
1340     }
1341     if (proj-&gt;_rout.is_NotEmpty()) {
1342       push_projection(proj);
1343     }
1344   }
1345   // Transfer the safepoint information from the call to the mcall
1346   // Move the JVMState list
1347   msfpt-&gt;set_jvms(sfpt-&gt;jvms());
1348   for (JVMState* jvms = msfpt-&gt;jvms(); jvms; jvms = jvms-&gt;caller()) {
1349     jvms-&gt;set_map(sfpt);
1350   }
1351 
1352   // Debug inputs begin just after the last incoming parameter
1353   assert((mcall == NULL) || (mcall-&gt;jvms() == NULL) ||
1354          (mcall-&gt;jvms()-&gt;debug_start() + mcall-&gt;_jvmadj == mcall-&gt;tf()-&gt;domain()-&gt;cnt()), "");
1355 
1356   // Move the OopMap
1357   msfpt-&gt;_oop_map = sfpt-&gt;_oop_map;
1358 
1359   // Add additional edges.
1360   if (msfpt-&gt;mach_constant_base_node_input() != (uint)-1 &amp;&amp; !msfpt-&gt;is_MachCallLeaf()) {
1361     // For these calls we can not add MachConstantBase in expand(), as the
1362     // ins are not complete then.
1363     msfpt-&gt;ins_req(msfpt-&gt;mach_constant_base_node_input(), C-&gt;mach_constant_base_node());
1364     if (msfpt-&gt;jvms() &amp;&amp;
1365         msfpt-&gt;mach_constant_base_node_input() &lt;= msfpt-&gt;jvms()-&gt;debug_start() + msfpt-&gt;_jvmadj) {
1366       // We added an edge before jvms, so we must adapt the position of the ins.
1367       msfpt-&gt;jvms()-&gt;adapt_position(+1);
1368     }
1369   }
1370 
1371   // Registers killed by the call are set in the local scheduling pass
1372   // of Global Code Motion.
1373   return msfpt;
1374 }
1375 
1376 //---------------------------match_tree----------------------------------------
1377 // Match a Ideal Node DAG - turn it into a tree; Label &amp; Reduce.  Used as part
1378 // of the whole-sale conversion from Ideal to Mach Nodes.  Also used for
1379 // making GotoNodes while building the CFG and in init_spill_mask() to identify
1380 // a Load's result RegMask for memoization in idealreg2regmask[]
1381 MachNode *Matcher::match_tree( const Node *n ) {
1382   assert( n-&gt;Opcode() != Op_Phi, "cannot match" );
1383   assert( !n-&gt;is_block_start(), "cannot match" );
1384   // Set the mark for all locally allocated State objects.
1385   // When this call returns, the _states_arena arena will be reset
1386   // freeing all State objects.
1387   ResourceMark rm( &amp;_states_arena );
1388 
1389   LabelRootDepth = 0;
1390 
1391   // StoreNodes require their Memory input to match any LoadNodes
1392   Node *mem = n-&gt;is_Store() ? n-&gt;in(MemNode::Memory) : (Node*)1 ;
1393 #ifdef ASSERT
1394   Node* save_mem_node = _mem_node;
1395   _mem_node = n-&gt;is_Store() ? (Node*)n : NULL;
1396 #endif
1397   // State object for root node of match tree
1398   // Allocate it on _states_arena - stack allocation can cause stack overflow.
1399   State *s = new (&amp;_states_arena) State;
1400   s-&gt;_kids[0] = NULL;
1401   s-&gt;_kids[1] = NULL;
1402   s-&gt;_leaf = (Node*)n;
1403   // Label the input tree, allocating labels from top-level arena
1404   Label_Root( n, s, n-&gt;in(0), mem );
1405   if (C-&gt;failing())  return NULL;
1406 
1407   // The minimum cost match for the whole tree is found at the root State
1408   uint mincost = max_juint;
1409   uint cost = max_juint;
1410   uint i;
1411   for( i = 0; i &lt; NUM_OPERANDS; i++ ) {
1412     if( s-&gt;valid(i) &amp;&amp;                // valid entry and
1413         s-&gt;_cost[i] &lt; cost &amp;&amp;         // low cost and
1414         s-&gt;_rule[i] &gt;= NUM_OPERANDS ) // not an operand
1415       cost = s-&gt;_cost[mincost=i];
1416   }
1417   if (mincost == max_juint) {
1418 #ifndef PRODUCT
1419     tty-&gt;print("No matching rule for:");
1420     s-&gt;dump();
1421 #endif
1422     Matcher::soft_match_failure();
1423     return NULL;
1424   }
1425   // Reduce input tree based upon the state labels to machine Nodes
1426   MachNode *m = ReduceInst( s, s-&gt;_rule[mincost], mem );
1427 #ifdef ASSERT
1428   _old2new_map.map(n-&gt;_idx, m);
1429   _new2old_map.map(m-&gt;_idx, (Node*)n);
1430 #endif
1431 
1432   // Add any Matcher-ignored edges
1433   uint cnt = n-&gt;req();
1434   uint start = 1;
1435   if( mem != (Node*)1 ) start = MemNode::Memory+1;
1436   if( n-&gt;is_AddP() ) {
1437     assert( mem == (Node*)1, "" );
1438     start = AddPNode::Base+1;
1439   }
1440   for( i = start; i &lt; cnt; i++ ) {
1441     if( !n-&gt;match_edge(i) ) {
1442       if( i &lt; m-&gt;req() )
1443         m-&gt;ins_req( i, n-&gt;in(i) );
1444       else
1445         m-&gt;add_req( n-&gt;in(i) );
1446     }
1447   }
1448 
1449   debug_only( _mem_node = save_mem_node; )
1450   return m;
1451 }
1452 
1453 
1454 //------------------------------match_into_reg---------------------------------
1455 // Choose to either match this Node in a register or part of the current
1456 // match tree.  Return true for requiring a register and false for matching
1457 // as part of the current match tree.
1458 static bool match_into_reg( const Node *n, Node *m, Node *control, int i, bool shared ) {
1459 
1460   const Type *t = m-&gt;bottom_type();
1461 
1462   if (t-&gt;singleton()) {
1463     // Never force constants into registers.  Allow them to match as
1464     // constants or registers.  Copies of the same value will share
1465     // the same register.  See find_shared_node.
1466     return false;
1467   } else {                      // Not a constant
1468     // Stop recursion if they have different Controls.
1469     Node* m_control = m-&gt;in(0);
1470     // Control of load's memory can post-dominates load's control.
1471     // So use it since load can't float above its memory.
1472     Node* mem_control = (m-&gt;is_Load()) ? m-&gt;in(MemNode::Memory)-&gt;in(0) : NULL;
1473     if (control &amp;&amp; m_control &amp;&amp; control != m_control &amp;&amp; control != mem_control) {
1474 
1475       // Actually, we can live with the most conservative control we
1476       // find, if it post-dominates the others.  This allows us to
1477       // pick up load/op/store trees where the load can float a little
1478       // above the store.
1479       Node *x = control;
1480       const uint max_scan = 6;  // Arbitrary scan cutoff
1481       uint j;
1482       for (j=0; j&lt;max_scan; j++) {
1483         if (x-&gt;is_Region())     // Bail out at merge points
1484           return true;
1485         x = x-&gt;in(0);
1486         if (x == m_control)     // Does 'control' post-dominate
1487           break;                // m-&gt;in(0)?  If so, we can use it
1488         if (x == mem_control)   // Does 'control' post-dominate
1489           break;                // mem_control?  If so, we can use it
1490       }
1491       if (j == max_scan)        // No post-domination before scan end?
1492         return true;            // Then break the match tree up
1493     }
1494     if ((m-&gt;is_DecodeN() &amp;&amp; Matcher::narrow_oop_use_complex_address()) ||
1495         (m-&gt;is_DecodeNKlass() &amp;&amp; Matcher::narrow_klass_use_complex_address())) {
1496       // These are commonly used in address expressions and can
1497       // efficiently fold into them on X64 in some cases.
1498       return false;
1499     }
1500   }
1501 
1502   // Not forceable cloning.  If shared, put it into a register.
1503   return shared;
1504 }
1505 
1506 
1507 //------------------------------Instruction Selection--------------------------
1508 // Label method walks a "tree" of nodes, using the ADLC generated DFA to match
1509 // ideal nodes to machine instructions.  Trees are delimited by shared Nodes,
1510 // things the Matcher does not match (e.g., Memory), and things with different
1511 // Controls (hence forced into different blocks).  We pass in the Control
1512 // selected for this entire State tree.
1513 
1514 // The Matcher works on Trees, but an Intel add-to-memory requires a DAG: the
1515 // Store and the Load must have identical Memories (as well as identical
1516 // pointers).  Since the Matcher does not have anything for Memory (and
1517 // does not handle DAGs), I have to match the Memory input myself.  If the
1518 // Tree root is a Store, I require all Loads to have the identical memory.
1519 Node *Matcher::Label_Root( const Node *n, State *svec, Node *control, const Node *mem){
1520   // Since Label_Root is a recursive function, its possible that we might run
1521   // out of stack space.  See bugs 6272980 &amp; 6227033 for more info.
1522   LabelRootDepth++;
1523   if (LabelRootDepth &gt; MaxLabelRootDepth) {
1524     C-&gt;record_method_not_compilable_all_tiers("Out of stack space, increase MaxLabelRootDepth");
1525     return NULL;
1526   }
1527   uint care = 0;                // Edges matcher cares about
1528   uint cnt = n-&gt;req();
1529   uint i = 0;
1530 
1531   // Examine children for memory state
1532   // Can only subsume a child into your match-tree if that child's memory state
1533   // is not modified along the path to another input.
1534   // It is unsafe even if the other inputs are separate roots.
1535   Node *input_mem = NULL;
1536   for( i = 1; i &lt; cnt; i++ ) {
1537     if( !n-&gt;match_edge(i) ) continue;
1538     Node *m = n-&gt;in(i);         // Get ith input
1539     assert( m, "expect non-null children" );
1540     if( m-&gt;is_Load() ) {
1541       if( input_mem == NULL ) {
1542         input_mem = m-&gt;in(MemNode::Memory);
1543       } else if( input_mem != m-&gt;in(MemNode::Memory) ) {
1544         input_mem = NodeSentinel;
1545       }
1546     }
1547   }
1548 
1549   for( i = 1; i &lt; cnt; i++ ){// For my children
1550     if( !n-&gt;match_edge(i) ) continue;
1551     Node *m = n-&gt;in(i);         // Get ith input
1552     // Allocate states out of a private arena
1553     State *s = new (&amp;_states_arena) State;
1554     svec-&gt;_kids[care++] = s;
1555     assert( care &lt;= 2, "binary only for now" );
1556 
1557     // Recursively label the State tree.
1558     s-&gt;_kids[0] = NULL;
1559     s-&gt;_kids[1] = NULL;
1560     s-&gt;_leaf = m;
1561 
1562     // Check for leaves of the State Tree; things that cannot be a part of
1563     // the current tree.  If it finds any, that value is matched as a
1564     // register operand.  If not, then the normal matching is used.
1565     if( match_into_reg(n, m, control, i, is_shared(m)) ||
1566         //
1567         // Stop recursion if this is LoadNode and the root of this tree is a
1568         // StoreNode and the load &amp; store have different memories.
1569         ((mem!=(Node*)1) &amp;&amp; m-&gt;is_Load() &amp;&amp; m-&gt;in(MemNode::Memory) != mem) ||
1570         // Can NOT include the match of a subtree when its memory state
1571         // is used by any of the other subtrees
1572         (input_mem == NodeSentinel) ) {
1573       // Print when we exclude matching due to different memory states at input-loads
1574       if (PrintOpto &amp;&amp; (Verbose &amp;&amp; WizardMode) &amp;&amp; (input_mem == NodeSentinel)
1575         &amp;&amp; !((mem!=(Node*)1) &amp;&amp; m-&gt;is_Load() &amp;&amp; m-&gt;in(MemNode::Memory) != mem)) {
1576         tty-&gt;print_cr("invalid input_mem");
1577       }
1578       // Switch to a register-only opcode; this value must be in a register
1579       // and cannot be subsumed as part of a larger instruction.
1580       s-&gt;DFA( m-&gt;ideal_reg(), m );
1581 
1582     } else {
1583       // If match tree has no control and we do, adopt it for entire tree
1584       if( control == NULL &amp;&amp; m-&gt;in(0) != NULL &amp;&amp; m-&gt;req() &gt; 1 )
1585         control = m-&gt;in(0);         // Pick up control
1586       // Else match as a normal part of the match tree.
1587       control = Label_Root(m,s,control,mem);
1588       if (C-&gt;failing()) return NULL;
1589     }
1590   }
1591 
1592 
1593   // Call DFA to match this node, and return
1594   svec-&gt;DFA( n-&gt;Opcode(), n );
1595 
1596 #ifdef ASSERT
1597   uint x;
1598   for( x = 0; x &lt; _LAST_MACH_OPER; x++ )
1599     if( svec-&gt;valid(x) )
1600       break;
1601 
1602   if (x &gt;= _LAST_MACH_OPER) {
1603     n-&gt;dump();
1604     svec-&gt;dump();
1605     assert( false, "bad AD file" );
1606   }
1607 #endif
1608   return control;
1609 }
1610 
1611 
1612 // Con nodes reduced using the same rule can share their MachNode
1613 // which reduces the number of copies of a constant in the final
1614 // program.  The register allocator is free to split uses later to
1615 // split live ranges.
1616 MachNode* Matcher::find_shared_node(Node* leaf, uint rule) {
1617   if (!leaf-&gt;is_Con() &amp;&amp; !leaf-&gt;is_DecodeNarrowPtr()) return NULL;
1618 
1619   // See if this Con has already been reduced using this rule.
1620   if (_shared_nodes.Size() &lt;= leaf-&gt;_idx) return NULL;
1621   MachNode* last = (MachNode*)_shared_nodes.at(leaf-&gt;_idx);
1622   if (last != NULL &amp;&amp; rule == last-&gt;rule()) {
1623     // Don't expect control change for DecodeN
1624     if (leaf-&gt;is_DecodeNarrowPtr())
1625       return last;
1626     // Get the new space root.
1627     Node* xroot = new_node(C-&gt;root());
1628     if (xroot == NULL) {
1629       // This shouldn't happen give the order of matching.
1630       return NULL;
1631     }
1632 
1633     // Shared constants need to have their control be root so they
1634     // can be scheduled properly.
1635     Node* control = last-&gt;in(0);
1636     if (control != xroot) {
1637       if (control == NULL || control == C-&gt;root()) {
1638         last-&gt;set_req(0, xroot);
1639       } else {
1640         assert(false, "unexpected control");
1641         return NULL;
1642       }
1643     }
1644     return last;
1645   }
1646   return NULL;
1647 }
1648 
1649 
1650 //------------------------------ReduceInst-------------------------------------
1651 // Reduce a State tree (with given Control) into a tree of MachNodes.
1652 // This routine (and it's cohort ReduceOper) convert Ideal Nodes into
1653 // complicated machine Nodes.  Each MachNode covers some tree of Ideal Nodes.
1654 // Each MachNode has a number of complicated MachOper operands; each
1655 // MachOper also covers a further tree of Ideal Nodes.
1656 
1657 // The root of the Ideal match tree is always an instruction, so we enter
1658 // the recursion here.  After building the MachNode, we need to recurse
1659 // the tree checking for these cases:
1660 // (1) Child is an instruction -
1661 //     Build the instruction (recursively), add it as an edge.
1662 //     Build a simple operand (register) to hold the result of the instruction.
1663 // (2) Child is an interior part of an instruction -
1664 //     Skip over it (do nothing)
1665 // (3) Child is the start of a operand -
1666 //     Build the operand, place it inside the instruction
1667 //     Call ReduceOper.
1668 MachNode *Matcher::ReduceInst( State *s, int rule, Node *&amp;mem ) {
1669   assert( rule &gt;= NUM_OPERANDS, "called with operand rule" );
1670 
1671   MachNode* shared_node = find_shared_node(s-&gt;_leaf, rule);
1672   if (shared_node != NULL) {
1673     return shared_node;
1674   }
1675 
1676   // Build the object to represent this state &amp; prepare for recursive calls
1677   MachNode *mach = s-&gt;MachNodeGenerator(rule);
1678   mach-&gt;_opnds[0] = s-&gt;MachOperGenerator(_reduceOp[rule]);
1679   assert( mach-&gt;_opnds[0] != NULL, "Missing result operand" );
1680   Node *leaf = s-&gt;_leaf;
1681   // Check for instruction or instruction chain rule
1682   if( rule &gt;= _END_INST_CHAIN_RULE || rule &lt; _BEGIN_INST_CHAIN_RULE ) {
1683     assert(C-&gt;node_arena()-&gt;contains(s-&gt;_leaf) || !has_new_node(s-&gt;_leaf),
1684            "duplicating node that's already been matched");
1685     // Instruction
1686     mach-&gt;add_req( leaf-&gt;in(0) ); // Set initial control
1687     // Reduce interior of complex instruction
1688     ReduceInst_Interior( s, rule, mem, mach, 1 );
1689   } else {
1690     // Instruction chain rules are data-dependent on their inputs
1691     mach-&gt;add_req(0);             // Set initial control to none
1692     ReduceInst_Chain_Rule( s, rule, mem, mach );
1693   }
1694 
1695   // If a Memory was used, insert a Memory edge
1696   if( mem != (Node*)1 ) {
1697     mach-&gt;ins_req(MemNode::Memory,mem);
1698 #ifdef ASSERT
1699     // Verify adr type after matching memory operation
1700     const MachOper* oper = mach-&gt;memory_operand();
1701     if (oper != NULL &amp;&amp; oper != (MachOper*)-1) {
1702       // It has a unique memory operand.  Find corresponding ideal mem node.
1703       Node* m = NULL;
1704       if (leaf-&gt;is_Mem()) {
1705         m = leaf;
1706       } else {
1707         m = _mem_node;
1708         assert(m != NULL &amp;&amp; m-&gt;is_Mem(), "expecting memory node");
1709       }
1710       const Type* mach_at = mach-&gt;adr_type();
1711       // DecodeN node consumed by an address may have different type
1712       // then its input. Don't compare types for such case.
1713       if (m-&gt;adr_type() != mach_at &amp;&amp;
1714           (m-&gt;in(MemNode::Address)-&gt;is_DecodeNarrowPtr() ||
1715            m-&gt;in(MemNode::Address)-&gt;is_AddP() &amp;&amp;
1716            m-&gt;in(MemNode::Address)-&gt;in(AddPNode::Address)-&gt;is_DecodeNarrowPtr() ||
1717            m-&gt;in(MemNode::Address)-&gt;is_AddP() &amp;&amp;
1718            m-&gt;in(MemNode::Address)-&gt;in(AddPNode::Address)-&gt;is_AddP() &amp;&amp;
1719            m-&gt;in(MemNode::Address)-&gt;in(AddPNode::Address)-&gt;in(AddPNode::Address)-&gt;is_DecodeNarrowPtr())) {
1720         mach_at = m-&gt;adr_type();
1721       }
1722       if (m-&gt;adr_type() != mach_at) {
1723         m-&gt;dump();
1724         tty-&gt;print_cr("mach:");
1725         mach-&gt;dump(1);
1726       }
1727       assert(m-&gt;adr_type() == mach_at, "matcher should not change adr type");
1728     }
1729 #endif
1730   }
1731 
1732   // If the _leaf is an AddP, insert the base edge
1733   if (leaf-&gt;is_AddP()) {
1734     mach-&gt;ins_req(AddPNode::Base,leaf-&gt;in(AddPNode::Base));
1735   }
1736 
1737   uint number_of_projections_prior = number_of_projections();
1738 
1739   // Perform any 1-to-many expansions required
1740   MachNode *ex = mach-&gt;Expand(s, _projection_list, mem);
1741   if (ex != mach) {
1742     assert(ex-&gt;ideal_reg() == mach-&gt;ideal_reg(), "ideal types should match");
1743     if( ex-&gt;in(1)-&gt;is_Con() )
1744       ex-&gt;in(1)-&gt;set_req(0, C-&gt;root());
1745     // Remove old node from the graph
1746     for( uint i=0; i&lt;mach-&gt;req(); i++ ) {
1747       mach-&gt;set_req(i,NULL);
1748     }
1749 #ifdef ASSERT
1750     _new2old_map.map(ex-&gt;_idx, s-&gt;_leaf);
1751 #endif
1752   }
1753 
1754   // PhaseChaitin::fixup_spills will sometimes generate spill code
1755   // via the matcher.  By the time, nodes have been wired into the CFG,
1756   // and any further nodes generated by expand rules will be left hanging
1757   // in space, and will not get emitted as output code.  Catch this.
1758   // Also, catch any new register allocation constraints ("projections")
1759   // generated belatedly during spill code generation.
1760   if (_allocation_started) {
1761     guarantee(ex == mach, "no expand rules during spill generation");
1762     guarantee(number_of_projections_prior == number_of_projections(), "no allocation during spill generation");
1763   }
1764 
1765   if (leaf-&gt;is_Con() || leaf-&gt;is_DecodeNarrowPtr()) {
1766     // Record the con for sharing
1767     _shared_nodes.map(leaf-&gt;_idx, ex);
1768   }
1769 
1770   return ex;
1771 }
1772 
1773 void Matcher::handle_precedence_edges(Node* n, MachNode *mach) {
1774   for (uint i = n-&gt;req(); i &lt; n-&gt;len(); i++) {
1775     if (n-&gt;in(i) != NULL) {
1776       mach-&gt;add_prec(n-&gt;in(i));
1777     }
1778   }
1779 }
1780 
1781 void Matcher::ReduceInst_Chain_Rule( State *s, int rule, Node *&amp;mem, MachNode *mach ) {
1782   // 'op' is what I am expecting to receive
1783   int op = _leftOp[rule];
1784   // Operand type to catch childs result
1785   // This is what my child will give me.
1786   int opnd_class_instance = s-&gt;_rule[op];
1787   // Choose between operand class or not.
1788   // This is what I will receive.
1789   int catch_op = (FIRST_OPERAND_CLASS &lt;= op &amp;&amp; op &lt; NUM_OPERANDS) ? opnd_class_instance : op;
1790   // New rule for child.  Chase operand classes to get the actual rule.
1791   int newrule = s-&gt;_rule[catch_op];
1792 
1793   if( newrule &lt; NUM_OPERANDS ) {
1794     // Chain from operand or operand class, may be output of shared node
1795     assert( 0 &lt;= opnd_class_instance &amp;&amp; opnd_class_instance &lt; NUM_OPERANDS,
1796             "Bad AD file: Instruction chain rule must chain from operand");
1797     // Insert operand into array of operands for this instruction
1798     mach-&gt;_opnds[1] = s-&gt;MachOperGenerator(opnd_class_instance);
1799 
1800     ReduceOper( s, newrule, mem, mach );
1801   } else {
1802     // Chain from the result of an instruction
1803     assert( newrule &gt;= _LAST_MACH_OPER, "Do NOT chain from internal operand");
1804     mach-&gt;_opnds[1] = s-&gt;MachOperGenerator(_reduceOp[catch_op]);
1805     Node *mem1 = (Node*)1;
1806     debug_only(Node *save_mem_node = _mem_node;)
1807     mach-&gt;add_req( ReduceInst(s, newrule, mem1) );
1808     debug_only(_mem_node = save_mem_node;)
1809   }
1810   return;
1811 }
1812 
1813 
1814 uint Matcher::ReduceInst_Interior( State *s, int rule, Node *&amp;mem, MachNode *mach, uint num_opnds ) {
1815   handle_precedence_edges(s-&gt;_leaf, mach);
1816 
1817   if( s-&gt;_leaf-&gt;is_Load() ) {
1818     Node *mem2 = s-&gt;_leaf-&gt;in(MemNode::Memory);
1819     assert( mem == (Node*)1 || mem == mem2, "multiple Memories being matched at once?" );
1820     debug_only( if( mem == (Node*)1 ) _mem_node = s-&gt;_leaf;)
1821     mem = mem2;
1822   }
1823   if( s-&gt;_leaf-&gt;in(0) != NULL &amp;&amp; s-&gt;_leaf-&gt;req() &gt; 1) {
1824     if( mach-&gt;in(0) == NULL )
1825       mach-&gt;set_req(0, s-&gt;_leaf-&gt;in(0));
1826   }
1827 
1828   // Now recursively walk the state tree &amp; add operand list.
1829   for( uint i=0; i&lt;2; i++ ) {   // binary tree
1830     State *newstate = s-&gt;_kids[i];
1831     if( newstate == NULL ) break;      // Might only have 1 child
1832     // 'op' is what I am expecting to receive
1833     int op;
1834     if( i == 0 ) {
1835       op = _leftOp[rule];
1836     } else {
1837       op = _rightOp[rule];
1838     }
1839     // Operand type to catch childs result
1840     // This is what my child will give me.
1841     int opnd_class_instance = newstate-&gt;_rule[op];
1842     // Choose between operand class or not.
1843     // This is what I will receive.
1844     int catch_op = (op &gt;= FIRST_OPERAND_CLASS &amp;&amp; op &lt; NUM_OPERANDS) ? opnd_class_instance : op;
1845     // New rule for child.  Chase operand classes to get the actual rule.
1846     int newrule = newstate-&gt;_rule[catch_op];
1847 
1848     if( newrule &lt; NUM_OPERANDS ) { // Operand/operandClass or internalOp/instruction?
1849       // Operand/operandClass
1850       // Insert operand into array of operands for this instruction
1851       mach-&gt;_opnds[num_opnds++] = newstate-&gt;MachOperGenerator(opnd_class_instance);
1852       ReduceOper( newstate, newrule, mem, mach );
1853 
1854     } else {                    // Child is internal operand or new instruction
1855       if( newrule &lt; _LAST_MACH_OPER ) { // internal operand or instruction?
1856         // internal operand --&gt; call ReduceInst_Interior
1857         // Interior of complex instruction.  Do nothing but recurse.
1858         num_opnds = ReduceInst_Interior( newstate, newrule, mem, mach, num_opnds );
1859       } else {
1860         // instruction --&gt; call build operand(  ) to catch result
1861         //             --&gt; ReduceInst( newrule )
1862         mach-&gt;_opnds[num_opnds++] = s-&gt;MachOperGenerator(_reduceOp[catch_op]);
1863         Node *mem1 = (Node*)1;
1864         debug_only(Node *save_mem_node = _mem_node;)
1865         mach-&gt;add_req( ReduceInst( newstate, newrule, mem1 ) );
1866         debug_only(_mem_node = save_mem_node;)
1867       }
1868     }
1869     assert( mach-&gt;_opnds[num_opnds-1], "" );
1870   }
1871   return num_opnds;
1872 }
1873 
1874 // This routine walks the interior of possible complex operands.
1875 // At each point we check our children in the match tree:
1876 // (1) No children -
1877 //     We are a leaf; add _leaf field as an input to the MachNode
1878 // (2) Child is an internal operand -
1879 //     Skip over it ( do nothing )
1880 // (3) Child is an instruction -
1881 //     Call ReduceInst recursively and
1882 //     and instruction as an input to the MachNode
1883 void Matcher::ReduceOper( State *s, int rule, Node *&amp;mem, MachNode *mach ) {
1884   assert( rule &lt; _LAST_MACH_OPER, "called with operand rule" );
1885   State *kid = s-&gt;_kids[0];
1886   assert( kid == NULL || s-&gt;_leaf-&gt;in(0) == NULL, "internal operands have no control" );
1887 
1888   // Leaf?  And not subsumed?
1889   if( kid == NULL &amp;&amp; !_swallowed[rule] ) {
1890     mach-&gt;add_req( s-&gt;_leaf );  // Add leaf pointer
1891     return;                     // Bail out
1892   }
1893 
1894   if( s-&gt;_leaf-&gt;is_Load() ) {
1895     assert( mem == (Node*)1, "multiple Memories being matched at once?" );
1896     mem = s-&gt;_leaf-&gt;in(MemNode::Memory);
1897     debug_only(_mem_node = s-&gt;_leaf;)
1898   }
1899 
1900   handle_precedence_edges(s-&gt;_leaf, mach);
1901 
1902   if( s-&gt;_leaf-&gt;in(0) &amp;&amp; s-&gt;_leaf-&gt;req() &gt; 1) {
1903     if( !mach-&gt;in(0) )
1904       mach-&gt;set_req(0,s-&gt;_leaf-&gt;in(0));
1905     else {
1906       assert( s-&gt;_leaf-&gt;in(0) == mach-&gt;in(0), "same instruction, differing controls?" );
1907     }
1908   }
1909 
1910   for( uint i=0; kid != NULL &amp;&amp; i&lt;2; kid = s-&gt;_kids[1], i++ ) {   // binary tree
1911     int newrule;
1912     if( i == 0)
1913       newrule = kid-&gt;_rule[_leftOp[rule]];
1914     else
1915       newrule = kid-&gt;_rule[_rightOp[rule]];
1916 
1917     if( newrule &lt; _LAST_MACH_OPER ) { // Operand or instruction?
1918       // Internal operand; recurse but do nothing else
1919       ReduceOper( kid, newrule, mem, mach );
1920 
1921     } else {                    // Child is a new instruction
1922       // Reduce the instruction, and add a direct pointer from this
1923       // machine instruction to the newly reduced one.
1924       Node *mem1 = (Node*)1;
1925       debug_only(Node *save_mem_node = _mem_node;)
1926       mach-&gt;add_req( ReduceInst( kid, newrule, mem1 ) );
1927       debug_only(_mem_node = save_mem_node;)
1928     }
1929   }
1930 }
1931 
1932 
1933 // -------------------------------------------------------------------------
1934 // Java-Java calling convention
1935 // (what you use when Java calls Java)
1936 
1937 //------------------------------find_receiver----------------------------------
1938 // For a given signature, return the OptoReg for parameter 0.
1939 OptoReg::Name Matcher::find_receiver( bool is_outgoing ) {
1940   VMRegPair regs;
1941   BasicType sig_bt = T_OBJECT;
1942   calling_convention(&amp;sig_bt, &amp;regs, 1, is_outgoing);
1943   // Return argument 0 register.  In the LP64 build pointers
1944   // take 2 registers, but the VM wants only the 'main' name.
1945   return OptoReg::as_OptoReg(regs.first());
1946 }
1947 
1948 // This function identifies sub-graphs in which a 'load' node is
1949 // input to two different nodes, and such that it can be matched
1950 // with BMI instructions like blsi, blsr, etc.
1951 // Example : for b = -a[i] &amp; a[i] can be matched to blsi r32, m32.
1952 // The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*
1953 // refers to the same node.
1954 #ifdef X86
1955 // Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)
1956 // This is a temporary solution until we make DAGs expressible in ADL.
1957 template&lt;typename ConType&gt;
1958 class FusedPatternMatcher {
1959   Node* _op1_node;
1960   Node* _mop_node;
1961   int _con_op;
1962 
1963   static int match_next(Node* n, int next_op, int next_op_idx) {
1964     if (n-&gt;in(1) == NULL || n-&gt;in(2) == NULL) {
1965       return -1;
1966     }
1967 
1968     if (next_op_idx == -1) { // n is commutative, try rotations
1969       if (n-&gt;in(1)-&gt;Opcode() == next_op) {
1970         return 1;
1971       } else if (n-&gt;in(2)-&gt;Opcode() == next_op) {
1972         return 2;
1973       }
1974     } else {
1975       assert(next_op_idx &gt; 0 &amp;&amp; next_op_idx &lt;= 2, "Bad argument index");
1976       if (n-&gt;in(next_op_idx)-&gt;Opcode() == next_op) {
1977         return next_op_idx;
1978       }
1979     }
1980     return -1;
1981   }
1982 public:
1983   FusedPatternMatcher(Node* op1_node, Node *mop_node, int con_op) :
1984     _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }
1985 
1986   bool match(int op1, int op1_op2_idx,  // op1 and the index of the op1-&gt;op2 edge, -1 if op1 is commutative
1987              int op2, int op2_con_idx,  // op2 and the index of the op2-&gt;con edge, -1 if op2 is commutative
1988              typename ConType::NativeType con_value) {
1989     if (_op1_node-&gt;Opcode() != op1) {
1990       return false;
1991     }
1992     if (_mop_node-&gt;outcnt() &gt; 2) {
1993       return false;
1994     }
1995     op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);
1996     if (op1_op2_idx == -1) {
1997       return false;
1998     }
1999     // Memory operation must be the other edge
2000     int op1_mop_idx = (op1_op2_idx &amp; 1) + 1;
2001 
2002     // Check that the mop node is really what we want
2003     if (_op1_node-&gt;in(op1_mop_idx) == _mop_node) {
2004       Node *op2_node = _op1_node-&gt;in(op1_op2_idx);
2005       if (op2_node-&gt;outcnt() &gt; 1) {
2006         return false;
2007       }
2008       assert(op2_node-&gt;Opcode() == op2, "Should be");
2009       op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);
2010       if (op2_con_idx == -1) {
2011         return false;
2012       }
2013       // Memory operation must be the other edge
2014       int op2_mop_idx = (op2_con_idx &amp; 1) + 1;
2015       // Check that the memory operation is the same node
2016       if (op2_node-&gt;in(op2_mop_idx) == _mop_node) {
2017         // Now check the constant
2018         const Type* con_type = op2_node-&gt;in(op2_con_idx)-&gt;bottom_type();
2019         if (con_type != Type::TOP &amp;&amp; ConType::as_self(con_type)-&gt;get_con() == con_value) {
2020           return true;
2021         }
2022       }
2023     }
2024     return false;
2025   }
2026 };
2027 
2028 
2029 bool Matcher::is_bmi_pattern(Node *n, Node *m) {
2030   if (n != NULL &amp;&amp; m != NULL) {
2031     if (m-&gt;Opcode() == Op_LoadI) {
2032       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
2033       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
2034              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
2035              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
2036     } else if (m-&gt;Opcode() == Op_LoadL) {
2037       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
2038       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
2039              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
2040              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
2041     }
2042   }
2043   return false;
2044 }
2045 #endif // X86
2046 
2047 // A method-klass-holder may be passed in the inline_cache_reg
2048 // and then expanded into the inline_cache_reg and a method_oop register
2049 //   defined in ad_&lt;arch&gt;.cpp
2050 
2051 // Check for shift by small constant as well
2052 static bool clone_shift(Node* shift, Matcher* matcher, MStack&amp; mstack, VectorSet&amp; address_visited) {
2053   if (shift-&gt;Opcode() == Op_LShiftX &amp;&amp; shift-&gt;in(2)-&gt;is_Con() &amp;&amp;
2054       shift-&gt;in(2)-&gt;get_int() &lt;= 3 &amp;&amp;
2055       // Are there other uses besides address expressions?
2056       !matcher-&gt;is_visited(shift)) {
2057     address_visited.set(shift-&gt;_idx); // Flag as address_visited
2058     mstack.push(shift-&gt;in(2), Visit);
2059     Node *conv = shift-&gt;in(1);
2060 #ifdef _LP64
2061     // Allow Matcher to match the rule which bypass
2062     // ConvI2L operation for an array index on LP64
2063     // if the index value is positive.
2064     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
2065         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
2066         // Are there other uses besides address expressions?
2067         !matcher-&gt;is_visited(conv)) {
2068       address_visited.set(conv-&gt;_idx); // Flag as address_visited
2069       mstack.push(conv-&gt;in(1), Pre_Visit);
2070     } else
2071 #endif
2072       mstack.push(conv, Pre_Visit);
2073     return true;
2074   }
2075   return false;
2076 }
2077 
2078 
2079 //------------------------------find_shared------------------------------------
2080 // Set bits if Node is shared or otherwise a root
2081 void Matcher::find_shared( Node *n ) {
2082   // Allocate stack of size C-&gt;live_nodes() * 2 to avoid frequent realloc
2083   MStack mstack(C-&gt;live_nodes() * 2);
2084   // Mark nodes as address_visited if they are inputs to an address expression
2085   VectorSet address_visited(Thread::current()-&gt;resource_area());
2086   mstack.push(n, Visit);     // Don't need to pre-visit root node
2087   while (mstack.is_nonempty()) {
2088     n = mstack.node();       // Leave node on stack
2089     Node_State nstate = mstack.state();
2090     uint nop = n-&gt;Opcode();
2091     if (nstate == Pre_Visit) {
2092       if (address_visited.test(n-&gt;_idx)) { // Visited in address already?
2093         // Flag as visited and shared now.
2094         set_visited(n);
2095       }
2096       if (is_visited(n)) {   // Visited already?
2097         // Node is shared and has no reason to clone.  Flag it as shared.
2098         // This causes it to match into a register for the sharing.
2099         set_shared(n);       // Flag as shared and
2100         mstack.pop();        // remove node from stack
2101         continue;
2102       }
2103       nstate = Visit; // Not already visited; so visit now
2104     }
2105     if (nstate == Visit) {
2106       mstack.set_state(Post_Visit);
2107       set_visited(n);   // Flag as visited now
2108       bool mem_op = false;
2109 
2110       switch( nop ) {  // Handle some opcodes special
2111       case Op_Phi:             // Treat Phis as shared roots
2112       case Op_Parm:
2113       case Op_Proj:            // All handled specially during matching
2114       case Op_SafePointScalarObject:
2115         set_shared(n);
2116         set_dontcare(n);
2117         break;
2118       case Op_If:
2119       case Op_CountedLoopEnd:
2120         mstack.set_state(Alt_Post_Visit); // Alternative way
2121         // Convert (If (Bool (CmpX A B))) into (If (Bool) (CmpX A B)).  Helps
2122         // with matching cmp/branch in 1 instruction.  The Matcher needs the
2123         // Bool and CmpX side-by-side, because it can only get at constants
2124         // that are at the leaves of Match trees, and the Bool's condition acts
2125         // as a constant here.
2126         mstack.push(n-&gt;in(1), Visit);         // Clone the Bool
2127         mstack.push(n-&gt;in(0), Pre_Visit);     // Visit control input
2128         continue; // while (mstack.is_nonempty())
2129       case Op_ConvI2D:         // These forms efficiently match with a prior
2130       case Op_ConvI2F:         //   Load but not a following Store
2131         if( n-&gt;in(1)-&gt;is_Load() &amp;&amp;        // Prior load
2132             n-&gt;outcnt() == 1 &amp;&amp;           // Not already shared
2133             n-&gt;unique_out()-&gt;is_Store() ) // Following store
2134           set_shared(n);       // Force it to be a root
2135         break;
2136       case Op_ReverseBytesI:
2137       case Op_ReverseBytesL:
2138         if( n-&gt;in(1)-&gt;is_Load() &amp;&amp;        // Prior load
2139             n-&gt;outcnt() == 1 )            // Not already shared
2140           set_shared(n);                  // Force it to be a root
2141         break;
2142       case Op_BoxLock:         // Cant match until we get stack-regs in ADLC
2143       case Op_IfFalse:
2144       case Op_IfTrue:
2145       case Op_MachProj:
2146       case Op_MergeMem:
2147       case Op_Catch:
2148       case Op_CatchProj:
2149       case Op_CProj:
2150       case Op_JumpProj:
2151       case Op_JProj:
2152       case Op_NeverBranch:
2153         set_dontcare(n);
2154         break;
2155       case Op_Jump:
2156         mstack.push(n-&gt;in(1), Pre_Visit);     // Switch Value (could be shared)
2157         mstack.push(n-&gt;in(0), Pre_Visit);     // Visit Control input
2158         continue;                             // while (mstack.is_nonempty())
2159       case Op_StrComp:
2160       case Op_StrEquals:
2161       case Op_StrIndexOf:
2162       case Op_StrIndexOfChar:
2163       case Op_AryEq:
2164       case Op_HasNegatives:
2165       case Op_StrInflatedCopy:
2166       case Op_StrCompressedCopy:
2167       case Op_EncodeISOArray:
2168         set_shared(n); // Force result into register (it will be anyways)
2169         break;
2170       case Op_ConP: {  // Convert pointers above the centerline to NUL
2171         TypeNode *tn = n-&gt;as_Type(); // Constants derive from type nodes
2172         const TypePtr* tp = tn-&gt;type()-&gt;is_ptr();
2173         if (tp-&gt;_ptr == TypePtr::AnyNull) {
2174           tn-&gt;set_type(TypePtr::NULL_PTR);
2175         }
2176         break;
2177       }
2178       case Op_ConN: {  // Convert narrow pointers above the centerline to NUL
2179         TypeNode *tn = n-&gt;as_Type(); // Constants derive from type nodes
2180         const TypePtr* tp = tn-&gt;type()-&gt;make_ptr();
2181         if (tp &amp;&amp; tp-&gt;_ptr == TypePtr::AnyNull) {
2182           tn-&gt;set_type(TypeNarrowOop::NULL_PTR);
2183         }
2184         break;
2185       }
2186       case Op_Binary:         // These are introduced in the Post_Visit state.
2187         ShouldNotReachHere();
2188         break;
2189       case Op_ClearArray:
2190       case Op_SafePoint:
2191         mem_op = true;
2192         break;
2193       default:
2194         if( n-&gt;is_Store() ) {
2195           // Do match stores, despite no ideal reg
2196           mem_op = true;
2197           break;
2198         }
2199         if( n-&gt;is_Mem() ) { // Loads and LoadStores
2200           mem_op = true;
2201           // Loads must be root of match tree due to prior load conflict
2202           if( C-&gt;subsume_loads() == false )
2203             set_shared(n);
2204         }
2205         // Fall into default case
2206         if( !n-&gt;ideal_reg() )
2207           set_dontcare(n);  // Unmatchable Nodes
2208       } // end_switch
2209 
2210       for(int i = n-&gt;req() - 1; i &gt;= 0; --i) { // For my children
2211         Node *m = n-&gt;in(i); // Get ith input
2212         if (m == NULL) continue;  // Ignore NULLs
2213         uint mop = m-&gt;Opcode();
2214 
2215         // Must clone all producers of flags, or we will not match correctly.
2216         // Suppose a compare setting int-flags is shared (e.g., a switch-tree)
2217         // then it will match into an ideal Op_RegFlags.  Alas, the fp-flags
2218         // are also there, so we may match a float-branch to int-flags and
2219         // expect the allocator to haul the flags from the int-side to the
2220         // fp-side.  No can do.
2221         if( _must_clone[mop] ) {
2222           mstack.push(m, Visit);
2223           continue; // for(int i = ...)
2224         }
2225 
2226         if( mop == Op_AddP &amp;&amp; m-&gt;in(AddPNode::Base)-&gt;is_DecodeNarrowPtr()) {
2227           // Bases used in addresses must be shared but since
2228           // they are shared through a DecodeN they may appear
2229           // to have a single use so force sharing here.
2230           set_shared(m-&gt;in(AddPNode::Base)-&gt;in(1));
2231         }
2232 
2233         // if 'n' and 'm' are part of a graph for BMI instruction, clone this node.
2234 #ifdef X86
2235         if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
2236           mstack.push(m, Visit);
2237           continue;
2238         }
2239 #endif
2240 
2241         // Clone addressing expressions as they are "free" in memory access instructions
2242         if (mem_op &amp;&amp; i == MemNode::Address &amp;&amp; mop == Op_AddP &amp;&amp;
2243             // When there are other uses besides address expressions
2244             // put it on stack and mark as shared.
2245             !is_visited(m)) {
2246           // Some inputs for address expression are not put on stack
2247           // to avoid marking them as shared and forcing them into register
2248           // if they are used only in address expressions.
2249           // But they should be marked as shared if there are other uses
2250           // besides address expressions.
2251 
2252           Node *off = m-&gt;in(AddPNode::Offset);
2253           if (off-&gt;is_Con()) {
2254             address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2255             Node *adr = m-&gt;in(AddPNode::Address);
2256 
2257             // Intel, ARM and friends can handle 2 adds in addressing mode
2258             if( clone_shift_expressions &amp;&amp; adr-&gt;is_AddP() &amp;&amp;
2259                 // AtomicAdd is not an addressing expression.
2260                 // Cheap to find it by looking for screwy base.
2261                 !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
2262                 // Are there other uses besides address expressions?
2263                 !is_visited(adr) ) {
2264               address_visited.set(adr-&gt;_idx); // Flag as address_visited
2265               Node *shift = adr-&gt;in(AddPNode::Offset);
2266               if (!clone_shift(shift, this, mstack, address_visited)) {
2267                 mstack.push(shift, Pre_Visit);
2268               }
2269               mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
2270               mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
2271             } else {  // Sparc, Alpha, PPC and friends
2272               mstack.push(adr, Pre_Visit);
2273             }
2274 
2275             // Clone X+offset as it also folds into most addressing expressions
2276             mstack.push(off, Visit);
2277             mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2278             continue; // for(int i = ...)
2279           } else if (clone_shift_expressions &amp;&amp;
2280                      clone_shift(off, this, mstack, address_visited)) {
2281               address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2282               mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
2283               mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2284               continue;
2285           } // if( off-&gt;is_Con() )
2286         }   // if( mem_op &amp;&amp;
2287         mstack.push(m, Pre_Visit);
2288       }     // for(int i = ...)
2289     }
2290     else if (nstate == Alt_Post_Visit) {
2291       mstack.pop(); // Remove node from stack
2292       // We cannot remove the Cmp input from the Bool here, as the Bool may be
2293       // shared and all users of the Bool need to move the Cmp in parallel.
2294       // This leaves both the Bool and the If pointing at the Cmp.  To
2295       // prevent the Matcher from trying to Match the Cmp along both paths
2296       // BoolNode::match_edge always returns a zero.
2297 
2298       // We reorder the Op_If in a pre-order manner, so we can visit without
2299       // accidentally sharing the Cmp (the Bool and the If make 2 users).
2300       n-&gt;add_req( n-&gt;in(1)-&gt;in(1) ); // Add the Cmp next to the Bool
2301     }
2302     else if (nstate == Post_Visit) {
2303       mstack.pop(); // Remove node from stack
2304 
2305       // Now hack a few special opcodes
2306       switch( n-&gt;Opcode() ) {       // Handle some opcodes special
2307       case Op_StorePConditional:
2308       case Op_StoreIConditional:
2309       case Op_StoreLConditional:
2310       case Op_CompareAndSwapI:
2311       case Op_CompareAndSwapL:
2312       case Op_CompareAndSwapP:
2313       case Op_CompareAndSwapN: {   // Convert trinary to binary-tree
2314         Node *newval = n-&gt;in(MemNode::ValueIn );
2315         Node *oldval  = n-&gt;in(LoadStoreConditionalNode::ExpectedIn);
2316         Node *pair = new BinaryNode( oldval, newval );
2317         n-&gt;set_req(MemNode::ValueIn,pair);
2318         n-&gt;del_req(LoadStoreConditionalNode::ExpectedIn);
2319         break;
2320       }
2321       case Op_CMoveD:              // Convert trinary to binary-tree
2322       case Op_CMoveF:
2323       case Op_CMoveI:
2324       case Op_CMoveL:
2325       case Op_CMoveN:
2326       case Op_CMoveP:
2327       case Op_CMoveVD:  {
2328         // Restructure into a binary tree for Matching.  It's possible that
2329         // we could move this code up next to the graph reshaping for IfNodes
2330         // or vice-versa, but I do not want to debug this for Ladybird.
2331         // 10/2/2000 CNC.
2332         Node *pair1 = new BinaryNode(n-&gt;in(1),n-&gt;in(1)-&gt;in(1));
2333         n-&gt;set_req(1,pair1);
2334         Node *pair2 = new BinaryNode(n-&gt;in(2),n-&gt;in(3));
2335         n-&gt;set_req(2,pair2);
2336         n-&gt;del_req(3);
2337         break;
2338       }
2339       case Op_LoopLimit: {
2340         Node *pair1 = new BinaryNode(n-&gt;in(1),n-&gt;in(2));
2341         n-&gt;set_req(1,pair1);
2342         n-&gt;set_req(2,n-&gt;in(3));
2343         n-&gt;del_req(3);
2344         break;
2345       }
2346       case Op_StrEquals:
2347       case Op_StrIndexOfChar: {
2348         Node *pair1 = new BinaryNode(n-&gt;in(2),n-&gt;in(3));
2349         n-&gt;set_req(2,pair1);
2350         n-&gt;set_req(3,n-&gt;in(4));
2351         n-&gt;del_req(4);
2352         break;
2353       }
2354       case Op_StrComp:
2355       case Op_StrIndexOf: {
2356         Node *pair1 = new BinaryNode(n-&gt;in(2),n-&gt;in(3));
2357         n-&gt;set_req(2,pair1);
2358         Node *pair2 = new BinaryNode(n-&gt;in(4),n-&gt;in(5));
2359         n-&gt;set_req(3,pair2);
2360         n-&gt;del_req(5);
2361         n-&gt;del_req(4);
2362         break;
2363       }
2364       case Op_StrCompressedCopy:
2365       case Op_StrInflatedCopy:
2366       case Op_EncodeISOArray: {
2367         // Restructure into a binary tree for Matching.
2368         Node* pair = new BinaryNode(n-&gt;in(3), n-&gt;in(4));
2369         n-&gt;set_req(3, pair);
2370         n-&gt;del_req(4);
2371         break;
2372       }
2373       default:
2374         break;
2375       }
2376     }
2377     else {
2378       ShouldNotReachHere();
2379     }
2380   } // end of while (mstack.is_nonempty())
2381 }
2382 
2383 #ifdef ASSERT
2384 // machine-independent root to machine-dependent root
2385 void Matcher::dump_old2new_map() {
2386   _old2new_map.dump();
2387 }
2388 #endif
2389 
2390 //---------------------------collect_null_checks-------------------------------
2391 // Find null checks in the ideal graph; write a machine-specific node for
2392 // it.  Used by later implicit-null-check handling.  Actually collects
2393 // either an IfTrue or IfFalse for the common NOT-null path, AND the ideal
2394 // value being tested.
2395 void Matcher::collect_null_checks( Node *proj, Node *orig_proj ) {
2396   Node *iff = proj-&gt;in(0);
2397   if( iff-&gt;Opcode() == Op_If ) {
2398     // During matching If's have Bool &amp; Cmp side-by-side
2399     BoolNode *b = iff-&gt;in(1)-&gt;as_Bool();
2400     Node *cmp = iff-&gt;in(2);
2401     int opc = cmp-&gt;Opcode();
2402     if (opc != Op_CmpP &amp;&amp; opc != Op_CmpN) return;
2403 
2404     const Type* ct = cmp-&gt;in(2)-&gt;bottom_type();
2405     if (ct == TypePtr::NULL_PTR ||
2406         (opc == Op_CmpN &amp;&amp; ct == TypeNarrowOop::NULL_PTR)) {
2407 
2408       bool push_it = false;
2409       if( proj-&gt;Opcode() == Op_IfTrue ) {
2410         extern int all_null_checks_found;
2411         all_null_checks_found++;
2412         if( b-&gt;_test._test == BoolTest::ne ) {
2413           push_it = true;
2414         }
2415       } else {
2416         assert( proj-&gt;Opcode() == Op_IfFalse, "" );
2417         if( b-&gt;_test._test == BoolTest::eq ) {
2418           push_it = true;
2419         }
2420       }
2421       if( push_it ) {
2422         _null_check_tests.push(proj);
2423         Node* val = cmp-&gt;in(1);
2424 #ifdef _LP64
2425         if (val-&gt;bottom_type()-&gt;isa_narrowoop() &amp;&amp;
2426             !Matcher::narrow_oop_use_complex_address()) {
2427           //
2428           // Look for DecodeN node which should be pinned to orig_proj.
2429           // On platforms (Sparc) which can not handle 2 adds
2430           // in addressing mode we have to keep a DecodeN node and
2431           // use it to do implicit NULL check in address.
2432           //
2433           // DecodeN node was pinned to non-null path (orig_proj) during
2434           // CastPP transformation in final_graph_reshaping_impl().
2435           //
2436           uint cnt = orig_proj-&gt;outcnt();
2437           for (uint i = 0; i &lt; orig_proj-&gt;outcnt(); i++) {
2438             Node* d = orig_proj-&gt;raw_out(i);
2439             if (d-&gt;is_DecodeN() &amp;&amp; d-&gt;in(1) == val) {
2440               val = d;
2441               val-&gt;set_req(0, NULL); // Unpin now.
2442               // Mark this as special case to distinguish from
2443               // a regular case: CmpP(DecodeN, NULL).
2444               val = (Node*)(((intptr_t)val) | 1);
2445               break;
2446             }
2447           }
2448         }
2449 #endif
2450         _null_check_tests.push(val);
2451       }
2452     }
2453   }
2454 }
2455 
2456 //---------------------------validate_null_checks------------------------------
2457 // Its possible that the value being NULL checked is not the root of a match
2458 // tree.  If so, I cannot use the value in an implicit null check.
2459 void Matcher::validate_null_checks( ) {
2460   uint cnt = _null_check_tests.size();
2461   for( uint i=0; i &lt; cnt; i+=2 ) {
2462     Node *test = _null_check_tests[i];
2463     Node *val = _null_check_tests[i+1];
2464     bool is_decoden = ((intptr_t)val) &amp; 1;
2465     val = (Node*)(((intptr_t)val) &amp; ~1);
2466     if (has_new_node(val)) {
2467       Node* new_val = new_node(val);
2468       if (is_decoden) {
2469         assert(val-&gt;is_DecodeNarrowPtr() &amp;&amp; val-&gt;in(0) == NULL, "sanity");
2470         // Note: new_val may have a control edge if
2471         // the original ideal node DecodeN was matched before
2472         // it was unpinned in Matcher::collect_null_checks().
2473         // Unpin the mach node and mark it.
2474         new_val-&gt;set_req(0, NULL);
2475         new_val = (Node*)(((intptr_t)new_val) | 1);
2476       }
2477       // Is a match-tree root, so replace with the matched value
2478       _null_check_tests.map(i+1, new_val);
2479     } else {
2480       // Yank from candidate list
2481       _null_check_tests.map(i+1,_null_check_tests[--cnt]);
2482       _null_check_tests.map(i,_null_check_tests[--cnt]);
2483       _null_check_tests.pop();
2484       _null_check_tests.pop();
2485       i-=2;
2486     }
2487   }
2488 }
2489 
2490 // Used by the DFA in dfa_xxx.cpp.  Check for a following barrier or
2491 // atomic instruction acting as a store_load barrier without any
2492 // intervening volatile load, and thus we don't need a barrier here.
2493 // We retain the Node to act as a compiler ordering barrier.
2494 bool Matcher::post_store_load_barrier(const Node* vmb) {
2495   Compile* C = Compile::current();
2496   assert(vmb-&gt;is_MemBar(), "");
2497   assert(vmb-&gt;Opcode() != Op_MemBarAcquire &amp;&amp; vmb-&gt;Opcode() != Op_LoadFence, "");
2498   const MemBarNode* membar = vmb-&gt;as_MemBar();
2499 
2500   // Get the Ideal Proj node, ctrl, that can be used to iterate forward
2501   Node* ctrl = NULL;
2502   for (DUIterator_Fast imax, i = membar-&gt;fast_outs(imax); i &lt; imax; i++) {
2503     Node* p = membar-&gt;fast_out(i);
2504     assert(p-&gt;is_Proj(), "only projections here");
2505     if ((p-&gt;as_Proj()-&gt;_con == TypeFunc::Control) &amp;&amp;
2506         !C-&gt;node_arena()-&gt;contains(p)) { // Unmatched old-space only
2507       ctrl = p;
2508       break;
2509     }
2510   }
2511   assert((ctrl != NULL), "missing control projection");
2512 
2513   for (DUIterator_Fast jmax, j = ctrl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2514     Node *x = ctrl-&gt;fast_out(j);
2515     int xop = x-&gt;Opcode();
2516 
2517     // We don't need current barrier if we see another or a lock
2518     // before seeing volatile load.
2519     //
2520     // Op_Fastunlock previously appeared in the Op_* list below.
2521     // With the advent of 1-0 lock operations we're no longer guaranteed
2522     // that a monitor exit operation contains a serializing instruction.
2523 
2524     if (xop == Op_MemBarVolatile ||
2525         xop == Op_CompareAndSwapL ||
2526         xop == Op_CompareAndSwapP ||
2527         xop == Op_CompareAndSwapN ||
2528         xop == Op_CompareAndSwapI) {
2529       return true;
2530     }
2531 
2532     // Op_FastLock previously appeared in the Op_* list above.
2533     // With biased locking we're no longer guaranteed that a monitor
2534     // enter operation contains a serializing instruction.
2535     if ((xop == Op_FastLock) &amp;&amp; !UseBiasedLocking) {
2536       return true;
2537     }
2538 
2539     if (x-&gt;is_MemBar()) {
2540       // We must retain this membar if there is an upcoming volatile
2541       // load, which will be followed by acquire membar.
2542       if (xop == Op_MemBarAcquire || xop == Op_LoadFence) {
2543         return false;
2544       } else {
2545         // For other kinds of barriers, check by pretending we
2546         // are them, and seeing if we can be removed.
2547         return post_store_load_barrier(x-&gt;as_MemBar());
2548       }
2549     }
2550 
2551     // probably not necessary to check for these
2552     if (x-&gt;is_Call() || x-&gt;is_SafePoint() || x-&gt;is_block_proj()) {
2553       return false;
2554     }
2555   }
2556   return false;
2557 }
2558 
2559 // Check whether node n is a branch to an uncommon trap that we could
2560 // optimize as test with very high branch costs in case of going to
2561 // the uncommon trap. The code must be able to be recompiled to use
2562 // a cheaper test.
2563 bool Matcher::branches_to_uncommon_trap(const Node *n) {
2564   // Don't do it for natives, adapters, or runtime stubs
2565   Compile *C = Compile::current();
2566   if (!C-&gt;is_method_compilation()) return false;
2567 
2568   assert(n-&gt;is_If(), "You should only call this on if nodes.");
2569   IfNode *ifn = n-&gt;as_If();
2570 
2571   Node *ifFalse = NULL;
2572   for (DUIterator_Fast imax, i = ifn-&gt;fast_outs(imax); i &lt; imax; i++) {
2573     if (ifn-&gt;fast_out(i)-&gt;is_IfFalse()) {
2574       ifFalse = ifn-&gt;fast_out(i);
2575       break;
2576     }
2577   }
2578   assert(ifFalse, "An If should have an ifFalse. Graph is broken.");
2579 
2580   Node *reg = ifFalse;
2581   int cnt = 4; // We must protect against cycles.  Limit to 4 iterations.
2582                // Alternatively use visited set?  Seems too expensive.
2583   while (reg != NULL &amp;&amp; cnt &gt; 0) {
2584     CallNode *call = NULL;
2585     RegionNode *nxt_reg = NULL;
2586     for (DUIterator_Fast imax, i = reg-&gt;fast_outs(imax); i &lt; imax; i++) {
2587       Node *o = reg-&gt;fast_out(i);
2588       if (o-&gt;is_Call()) {
2589         call = o-&gt;as_Call();
2590       }
2591       if (o-&gt;is_Region()) {
2592         nxt_reg = o-&gt;as_Region();
2593       }
2594     }
2595 
2596     if (call &amp;&amp;
2597         call-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point()) {
2598       const Type* trtype = call-&gt;in(TypeFunc::Parms)-&gt;bottom_type();
2599       if (trtype-&gt;isa_int() &amp;&amp; trtype-&gt;is_int()-&gt;is_con()) {
2600         jint tr_con = trtype-&gt;is_int()-&gt;get_con();
2601         Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(tr_con);
2602         Deoptimization::DeoptAction action = Deoptimization::trap_request_action(tr_con);
2603         assert((int)reason &lt; (int)BitsPerInt, "recode bit map");
2604 
2605         if (is_set_nth_bit(C-&gt;allowed_deopt_reasons(), (int)reason)
2606             &amp;&amp; action != Deoptimization::Action_none) {
2607           // This uncommon trap is sure to recompile, eventually.
2608           // When that happens, C-&gt;too_many_traps will prevent
2609           // this transformation from happening again.
2610           return true;
2611         }
2612       }
2613     }
2614 
2615     reg = nxt_reg;
2616     cnt--;
2617   }
2618 
2619   return false;
2620 }
2621 
2622 //=============================================================================
2623 //---------------------------State---------------------------------------------
2624 State::State(void) {
2625 #ifdef ASSERT
2626   _id = 0;
2627   _kids[0] = _kids[1] = (State*)(intptr_t) CONST64(0xcafebabecafebabe);
2628   _leaf = (Node*)(intptr_t) CONST64(0xbaadf00dbaadf00d);
2629   //memset(_cost, -1, sizeof(_cost));
2630   //memset(_rule, -1, sizeof(_rule));
2631 #endif
2632   memset(_valid, 0, sizeof(_valid));
2633 }
2634 
2635 #ifdef ASSERT
2636 State::~State() {
2637   _id = 99;
2638   _kids[0] = _kids[1] = (State*)(intptr_t) CONST64(0xcafebabecafebabe);
2639   _leaf = (Node*)(intptr_t) CONST64(0xbaadf00dbaadf00d);
2640   memset(_cost, -3, sizeof(_cost));
2641   memset(_rule, -3, sizeof(_rule));
2642 }
2643 #endif
2644 
2645 #ifndef PRODUCT
2646 //---------------------------dump----------------------------------------------
2647 void State::dump() {
2648   tty-&gt;print("\n");
2649   dump(0);
2650 }
2651 
2652 void State::dump(int depth) {
2653   for( int j = 0; j &lt; depth; j++ )
2654     tty-&gt;print("   ");
2655   tty-&gt;print("--N: ");
2656   _leaf-&gt;dump();
2657   uint i;
2658   for( i = 0; i &lt; _LAST_MACH_OPER; i++ )
2659     // Check for valid entry
2660     if( valid(i) ) {
2661       for( int j = 0; j &lt; depth; j++ )
2662         tty-&gt;print("   ");
2663         assert(_cost[i] != max_juint, "cost must be a valid value");
2664         assert(_rule[i] &lt; _last_Mach_Node, "rule[i] must be valid rule");
2665         tty-&gt;print_cr("%s  %d  %s",
2666                       ruleName[i], _cost[i], ruleName[_rule[i]] );
2667       }
2668   tty-&gt;cr();
2669 
2670   for( i=0; i&lt;2; i++ )
2671     if( _kids[i] )
2672       _kids[i]-&gt;dump(depth+1);
2673 }
2674 #endif
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
