<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/opto/library_call.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "classfile/vmSymbols.hpp"
  29 #include "compiler/compileBroker.hpp"
  30 #include "compiler/compileLog.hpp"
  31 #include "oops/objArrayKlass.hpp"
  32 #include "opto/addnode.hpp"
  33 #include "opto/arraycopynode.hpp"
  34 #include "opto/callGenerator.hpp"
  35 #include "opto/castnode.hpp"
  36 #include "opto/cfgnode.hpp"
  37 #include "opto/convertnode.hpp"
  38 #include "opto/countbitsnode.hpp"
  39 #include "opto/intrinsicnode.hpp"
  40 #include "opto/idealKit.hpp"
  41 #include "opto/mathexactnode.hpp"
  42 #include "opto/movenode.hpp"
  43 #include "opto/mulnode.hpp"
  44 #include "opto/narrowptrnode.hpp"
  45 #include "opto/opaquenode.hpp"
  46 #include "opto/parse.hpp"
  47 #include "opto/runtime.hpp"
  48 #include "opto/subnode.hpp"
  49 #include "prims/nativeLookup.hpp"
  50 #include "runtime/sharedRuntime.hpp"
  51 #include "trace/traceMacros.hpp"
  52 
  53 class LibraryIntrinsic : public InlineCallGenerator {
  54   // Extend the set of intrinsics known to the runtime:
  55  public:
  56  private:
  57   bool             _is_virtual;
  58   bool             _does_virtual_dispatch;
  59   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  60   int8_t           _last_predicate; // Last generated predicate
  61   vmIntrinsics::ID _intrinsic_id;
  62 
  63  public:
  64   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  65     : InlineCallGenerator(m),
  66       _is_virtual(is_virtual),
  67       _does_virtual_dispatch(does_virtual_dispatch),
  68       _predicates_count((int8_t)predicates_count),
  69       _last_predicate((int8_t)-1),
  70       _intrinsic_id(id)
  71   {
  72   }
  73   virtual bool is_intrinsic() const { return true; }
  74   virtual bool is_virtual()   const { return _is_virtual; }
  75   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  76   virtual int  predicates_count() const { return _predicates_count; }
  77   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  78   virtual JVMState* generate(JVMState* jvms);
  79   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  80   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  81 };
  82 
  83 
  84 // Local helper class for LibraryIntrinsic:
  85 class LibraryCallKit : public GraphKit {
  86  private:
  87   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  88   Node*             _result;        // the result node, if any
  89   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  90 
  91   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  92 
  93  public:
  94   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  95     : GraphKit(jvms),
  96       _intrinsic(intrinsic),
  97       _result(NULL)
  98   {
  99     // Check if this is a root compile.  In that case we don't have a caller.
 100     if (!jvms-&gt;has_method()) {
 101       _reexecute_sp = sp();
 102     } else {
 103       // Find out how many arguments the interpreter needs when deoptimizing
 104       // and save the stack pointer value so it can used by uncommon_trap.
 105       // We find the argument count by looking at the declared signature.
 106       bool ignored_will_link;
 107       ciSignature* declared_signature = NULL;
 108       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 109       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 110       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 111     }
 112   }
 113 
 114   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 115 
 116   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 117   int               bci()       const    { return jvms()-&gt;bci(); }
 118   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 119   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 120   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 121 
 122   bool  try_to_inline(int predicate);
 123   Node* try_to_predicate(int predicate);
 124 
 125   void push_result() {
 126     // Push the result onto the stack.
 127     if (!stopped() &amp;&amp; result() != NULL) {
 128       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 129       push_node(bt, result());
 130     }
 131   }
 132 
 133  private:
 134   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 135     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 136   }
 137 
 138   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 139   void  set_result(RegionNode* region, PhiNode* value);
 140   Node*     result() { return _result; }
 141 
 142   virtual int reexecute_sp() { return _reexecute_sp; }
 143 
 144   // Helper functions to inline natives
 145   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 146   Node* generate_slow_guard(Node* test, RegionNode* region);
 147   Node* generate_fair_guard(Node* test, RegionNode* region);
 148   Node* generate_negative_guard(Node* index, RegionNode* region,
 149                                 // resulting CastII of index:
 150                                 Node* *pos_index = NULL);
 151   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 152                              Node* array_length,
 153                              RegionNode* region);
 154   Node* generate_current_thread(Node* &amp;tls_output);
 155   Node* load_mirror_from_klass(Node* klass);
 156   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 157                                       RegionNode* region, int null_path,
 158                                       int offset);
 159   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 160                                RegionNode* region, int null_path) {
 161     int offset = java_lang_Class::klass_offset_in_bytes();
 162     return load_klass_from_mirror_common(mirror, never_see_null,
 163                                          region, null_path,
 164                                          offset);
 165   }
 166   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 167                                      RegionNode* region, int null_path) {
 168     int offset = java_lang_Class::array_klass_offset_in_bytes();
 169     return load_klass_from_mirror_common(mirror, never_see_null,
 170                                          region, null_path,
 171                                          offset);
 172   }
 173   Node* generate_access_flags_guard(Node* kls,
 174                                     int modifier_mask, int modifier_bits,
 175                                     RegionNode* region);
 176   Node* generate_interface_guard(Node* kls, RegionNode* region);
 177   Node* generate_array_guard(Node* kls, RegionNode* region) {
 178     return generate_array_guard_common(kls, region, false, false);
 179   }
 180   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 181     return generate_array_guard_common(kls, region, false, true);
 182   }
 183   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 184     return generate_array_guard_common(kls, region, true, false);
 185   }
 186   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 187     return generate_array_guard_common(kls, region, true, true);
 188   }
 189   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 190                                     bool obj_array, bool not_array);
 191   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 192   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 193                                      bool is_virtual = false, bool is_static = false);
 194   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 195     return generate_method_call(method_id, false, true);
 196   }
 197   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 198     return generate_method_call(method_id, true, false);
 199   }
 200   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 201 
 202   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 203   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 204   bool inline_string_compareTo();
 205   bool inline_string_indexOf();
 206   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 207   bool inline_string_equals();
 208   Node* round_double_node(Node* n);
 209   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 210   bool inline_math_native(vmIntrinsics::ID id);
 211   bool inline_trig(vmIntrinsics::ID id);
 212   bool inline_math(vmIntrinsics::ID id);
 213   template &lt;typename OverflowOp&gt;
 214   bool inline_math_overflow(Node* arg1, Node* arg2);
 215   void inline_math_mathExact(Node* math, Node* test);
 216   bool inline_math_addExactI(bool is_increment);
 217   bool inline_math_addExactL(bool is_increment);
 218   bool inline_math_multiplyExactI();
 219   bool inline_math_multiplyExactL();
 220   bool inline_math_negateExactI();
 221   bool inline_math_negateExactL();
 222   bool inline_math_subtractExactI(bool is_decrement);
 223   bool inline_math_subtractExactL(bool is_decrement);
 224   bool inline_exp();
 225   bool inline_pow();
 226   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 227   bool inline_min_max(vmIntrinsics::ID id);
 228   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 229   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 230   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 231   Node* make_unsafe_address(Node* base, Node* offset);
 232   // Helper for inline_unsafe_access.
 233   // Generates the guards that check whether the result of
 234   // Unsafe.getObject should be recorded in an SATB log buffer.
 235   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 236   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 237   static bool klass_needs_init_guard(Node* kls);
 238   bool inline_unsafe_allocate();
 239   bool inline_unsafe_copyMemory();
 240   bool inline_native_currentThread();
 241 #ifdef TRACE_HAVE_INTRINSICS
 242   bool inline_native_classID();
 243   bool inline_native_threadID();
 244 #endif
 245   bool inline_native_time_funcs(address method, const char* funcName);
 246   bool inline_native_isInterrupted();
 247   bool inline_native_Class_query(vmIntrinsics::ID id);
 248   bool inline_native_subtype_check();
 249 
 250   bool inline_native_newArray();
 251   bool inline_native_getLength();
 252   bool inline_array_copyOf(bool is_copyOfRange);
 253   bool inline_array_equals();
 254   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 255   bool inline_native_clone(bool is_virtual);
 256   bool inline_native_Reflection_getCallerClass();
 257   // Helper function for inlining native object hash method
 258   bool inline_native_hashcode(bool is_virtual, bool is_static);
 259   bool inline_native_getClass();
 260 
 261   // Helper functions for inlining arraycopy
 262   bool inline_arraycopy();
 263   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 264                                                 RegionNode* slow_region);
 265   JVMState* arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp);
 266   void arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp);
 267 
 268   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 269   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 270   bool inline_unsafe_ordered_store(BasicType type);
 271   bool inline_unsafe_fence(vmIntrinsics::ID id);
 272   bool inline_fp_conversions(vmIntrinsics::ID id);
 273   bool inline_number_methods(vmIntrinsics::ID id);
 274   bool inline_reference_get();
 275   bool inline_Class_cast();
 276   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 277   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 278   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 279   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 280   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 281   bool inline_ghash_processBlocks();
 282   bool inline_sha_implCompress(vmIntrinsics::ID id);
 283   bool inline_digestBase_implCompressMB(int predicate);
 284   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 285                                  bool long_state, address stubAddr, const char *stubName,
 286                                  Node* src_start, Node* ofs, Node* limit);
 287   Node* get_state_from_sha_object(Node *sha_object);
 288   Node* get_state_from_sha5_object(Node *sha_object);
 289   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 290   bool inline_encodeISOArray();
 291   bool inline_updateCRC32();
 292   bool inline_updateBytesCRC32();
 293   bool inline_updateByteBufferCRC32();
 294   Node* get_table_from_crc32c_class(ciInstanceKlass *crc32c_class);
 295   bool inline_updateBytesCRC32C();
 296   bool inline_updateDirectByteBufferCRC32C();
 297   bool inline_multiplyToLen();
 298   bool inline_squareToLen();
 299   bool inline_mulAdd();
 300 
 301   bool inline_profileBoolean();
 302   bool inline_isCompileConstant();
 303 };
 304 
 305 
 306 //---------------------------make_vm_intrinsic----------------------------
 307 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 308   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 309   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 310 
 311   ccstr disable_intr = NULL;
 312 
 313   if ((DisableIntrinsic[0] != '\0'
 314        &amp;&amp; strstr(DisableIntrinsic, vmIntrinsics::name_at(id)) != NULL) ||
 315       (method_has_option_value("DisableIntrinsic", disable_intr)
 316        &amp;&amp; strstr(disable_intr, vmIntrinsics::name_at(id)) != NULL)) {
 317     // disabled by a user request on the command line:
 318     // example: -XX:DisableIntrinsic=_hashCode,_getClass
 319     return NULL;
 320   }
 321 
 322   if (!m-&gt;is_loaded()) {
 323     // do not attempt to inline unloaded methods
 324     return NULL;
 325   }
 326 
 327   // Only a few intrinsics implement a virtual dispatch.
 328   // They are expensive calls which are also frequently overridden.
 329   if (is_virtual) {
 330     switch (id) {
 331     case vmIntrinsics::_hashCode:
 332     case vmIntrinsics::_clone:
 333       // OK, Object.hashCode and Object.clone intrinsics come in both flavors
 334       break;
 335     default:
 336       return NULL;
 337     }
 338   }
 339 
 340   // -XX:-InlineNatives disables nearly all intrinsics:
 341   if (!InlineNatives) {
 342     switch (id) {
 343     case vmIntrinsics::_indexOf:
 344     case vmIntrinsics::_compareTo:
 345     case vmIntrinsics::_equals:
 346     case vmIntrinsics::_equalsC:
 347     case vmIntrinsics::_getAndAddInt:
 348     case vmIntrinsics::_getAndAddLong:
 349     case vmIntrinsics::_getAndSetInt:
 350     case vmIntrinsics::_getAndSetLong:
 351     case vmIntrinsics::_getAndSetObject:
 352     case vmIntrinsics::_loadFence:
 353     case vmIntrinsics::_storeFence:
 354     case vmIntrinsics::_fullFence:
 355       break;  // InlineNatives does not control String.compareTo
 356     case vmIntrinsics::_Reference_get:
 357       break;  // InlineNatives does not control Reference.get
 358     default:
 359       return NULL;
 360     }
 361   }
 362 
 363   int predicates = 0;
 364   bool does_virtual_dispatch = false;
 365 
 366   switch (id) {
 367   case vmIntrinsics::_compareTo:
 368     if (!SpecialStringCompareTo)  return NULL;
 369     if (!Matcher::match_rule_supported(Op_StrComp))  return NULL;
 370     break;
 371   case vmIntrinsics::_indexOf:
 372     if (!SpecialStringIndexOf)  return NULL;
 373     break;
 374   case vmIntrinsics::_equals:
 375     if (!SpecialStringEquals)  return NULL;
 376     if (!Matcher::match_rule_supported(Op_StrEquals))  return NULL;
 377     break;
 378   case vmIntrinsics::_equalsC:
 379     if (!SpecialArraysEquals)  return NULL;
 380     if (!Matcher::match_rule_supported(Op_AryEq))  return NULL;
 381     break;
 382   case vmIntrinsics::_arraycopy:
 383     if (!InlineArrayCopy)  return NULL;
 384     break;
 385   case vmIntrinsics::_copyMemory:
 386     if (StubRoutines::unsafe_arraycopy() == NULL)  return NULL;
 387     if (!InlineArrayCopy)  return NULL;
 388     break;
 389   case vmIntrinsics::_hashCode:
 390     if (!InlineObjectHash)  return NULL;
 391     does_virtual_dispatch = true;
 392     break;
 393   case vmIntrinsics::_clone:
 394     does_virtual_dispatch = true;
 395   case vmIntrinsics::_copyOf:
 396   case vmIntrinsics::_copyOfRange:
 397     if (!InlineObjectCopy)  return NULL;
 398     // These also use the arraycopy intrinsic mechanism:
 399     if (!InlineArrayCopy)  return NULL;
 400     break;
 401   case vmIntrinsics::_encodeISOArray:
 402     if (!SpecialEncodeISOArray)  return NULL;
 403     if (!Matcher::match_rule_supported(Op_EncodeISOArray))  return NULL;
 404     break;
 405   case vmIntrinsics::_checkIndex:
 406     // We do not intrinsify this.  The optimizer does fine with it.
 407     return NULL;
 408 
 409   case vmIntrinsics::_getCallerClass:
 410     if (!InlineReflectionGetCallerClass)  return NULL;
 411     if (SystemDictionary::reflect_CallerSensitive_klass() == NULL)  return NULL;
 412     break;
 413 
 414   case vmIntrinsics::_bitCount_i:
 415     if (!Matcher::match_rule_supported(Op_PopCountI)) return NULL;
 416     break;
 417 
 418   case vmIntrinsics::_bitCount_l:
 419     if (!Matcher::match_rule_supported(Op_PopCountL)) return NULL;
 420     break;
 421 
 422   case vmIntrinsics::_numberOfLeadingZeros_i:
 423     if (!Matcher::match_rule_supported(Op_CountLeadingZerosI)) return NULL;
 424     break;
 425 
 426   case vmIntrinsics::_numberOfLeadingZeros_l:
 427     if (!Matcher::match_rule_supported(Op_CountLeadingZerosL)) return NULL;
 428     break;
 429 
 430   case vmIntrinsics::_numberOfTrailingZeros_i:
 431     if (!Matcher::match_rule_supported(Op_CountTrailingZerosI)) return NULL;
 432     break;
 433 
 434   case vmIntrinsics::_numberOfTrailingZeros_l:
 435     if (!Matcher::match_rule_supported(Op_CountTrailingZerosL)) return NULL;
 436     break;
 437 
 438   case vmIntrinsics::_reverseBytes_c:
 439     if (!Matcher::match_rule_supported(Op_ReverseBytesUS)) return NULL;
 440     break;
 441   case vmIntrinsics::_reverseBytes_s:
 442     if (!Matcher::match_rule_supported(Op_ReverseBytesS))  return NULL;
 443     break;
 444   case vmIntrinsics::_reverseBytes_i:
 445     if (!Matcher::match_rule_supported(Op_ReverseBytesI))  return NULL;
 446     break;
 447   case vmIntrinsics::_reverseBytes_l:
 448     if (!Matcher::match_rule_supported(Op_ReverseBytesL))  return NULL;
 449     break;
 450 
 451   case vmIntrinsics::_Reference_get:
 452     // Use the intrinsic version of Reference.get() so that the value in
 453     // the referent field can be registered by the G1 pre-barrier code.
 454     // Also add memory barrier to prevent commoning reads from this field
 455     // across safepoint since GC can change it value.
 456     break;
 457 
 458   case vmIntrinsics::_compareAndSwapObject:
 459 #ifdef _LP64
 460     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_CompareAndSwapP)) return NULL;
 461 #endif
 462     break;
 463 
 464   case vmIntrinsics::_compareAndSwapLong:
 465     if (!Matcher::match_rule_supported(Op_CompareAndSwapL)) return NULL;
 466     break;
 467 
 468   case vmIntrinsics::_getAndAddInt:
 469     if (!Matcher::match_rule_supported(Op_GetAndAddI)) return NULL;
 470     break;
 471 
 472   case vmIntrinsics::_getAndAddLong:
 473     if (!Matcher::match_rule_supported(Op_GetAndAddL)) return NULL;
 474     break;
 475 
 476   case vmIntrinsics::_getAndSetInt:
 477     if (!Matcher::match_rule_supported(Op_GetAndSetI)) return NULL;
 478     break;
 479 
 480   case vmIntrinsics::_getAndSetLong:
 481     if (!Matcher::match_rule_supported(Op_GetAndSetL)) return NULL;
 482     break;
 483 
 484   case vmIntrinsics::_getAndSetObject:
 485 #ifdef _LP64
 486     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 487     if (UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetN)) return NULL;
 488     break;
 489 #else
 490     if (!Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 491     break;
 492 #endif
 493 
 494   case vmIntrinsics::_aescrypt_encryptBlock:
 495   case vmIntrinsics::_aescrypt_decryptBlock:
 496     if (!UseAESIntrinsics) return NULL;
 497     break;
 498 
 499   case vmIntrinsics::_multiplyToLen:
 500     if (!UseMultiplyToLenIntrinsic) return NULL;
 501     break;
 502 
 503   case vmIntrinsics::_squareToLen:
 504     if (!UseSquareToLenIntrinsic) return NULL;
 505     break;
 506 
 507   case vmIntrinsics::_mulAdd:
 508     if (!UseMulAddIntrinsic) return NULL;
 509     break;
 510 
 511   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 512   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 513     if (!UseAESIntrinsics) return NULL;
 514     // these two require the predicated logic
 515     predicates = 1;
 516     break;
 517 
 518   case vmIntrinsics::_sha_implCompress:
 519     if (!UseSHA1Intrinsics) return NULL;
 520     break;
 521 
 522   case vmIntrinsics::_sha2_implCompress:
 523     if (!UseSHA256Intrinsics) return NULL;
 524     break;
 525 
 526   case vmIntrinsics::_sha5_implCompress:
 527     if (!UseSHA512Intrinsics) return NULL;
 528     break;
 529 
 530   case vmIntrinsics::_digestBase_implCompressMB:
 531     if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) return NULL;
 532     predicates = 3;
 533     break;
 534 
 535   case vmIntrinsics::_ghash_processBlocks:
 536     if (!UseGHASHIntrinsics) return NULL;
 537     break;
 538 
 539   case vmIntrinsics::_updateCRC32:
 540   case vmIntrinsics::_updateBytesCRC32:
 541   case vmIntrinsics::_updateByteBufferCRC32:
 542     if (!UseCRC32Intrinsics) return NULL;
 543     break;
 544 
 545   case vmIntrinsics::_updateBytesCRC32C:
 546   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 547     if (!UseCRC32CIntrinsics) return NULL;
 548     break;
 549 
 550   case vmIntrinsics::_incrementExactI:
 551   case vmIntrinsics::_addExactI:
 552     if (!Matcher::match_rule_supported(Op_OverflowAddI) || !UseMathExactIntrinsics) return NULL;
 553     break;
 554   case vmIntrinsics::_incrementExactL:
 555   case vmIntrinsics::_addExactL:
 556     if (!Matcher::match_rule_supported(Op_OverflowAddL) || !UseMathExactIntrinsics) return NULL;
 557     break;
 558   case vmIntrinsics::_decrementExactI:
 559   case vmIntrinsics::_subtractExactI:
 560     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 561     break;
 562   case vmIntrinsics::_decrementExactL:
 563   case vmIntrinsics::_subtractExactL:
 564     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 565     break;
 566   case vmIntrinsics::_negateExactI:
 567     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 568     break;
 569   case vmIntrinsics::_negateExactL:
 570     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 571     break;
 572   case vmIntrinsics::_multiplyExactI:
 573     if (!Matcher::match_rule_supported(Op_OverflowMulI) || !UseMathExactIntrinsics) return NULL;
 574     break;
 575   case vmIntrinsics::_multiplyExactL:
 576     if (!Matcher::match_rule_supported(Op_OverflowMulL) || !UseMathExactIntrinsics) return NULL;
 577     break;
 578 
 579   case vmIntrinsics::_getShortUnaligned:
 580   case vmIntrinsics::_getCharUnaligned:
 581   case vmIntrinsics::_getIntUnaligned:
 582   case vmIntrinsics::_getLongUnaligned:
 583   case vmIntrinsics::_putShortUnaligned:
 584   case vmIntrinsics::_putCharUnaligned:
 585   case vmIntrinsics::_putIntUnaligned:
 586   case vmIntrinsics::_putLongUnaligned:
 587     if (!UseUnalignedAccesses) return NULL;
 588     break;
 589 
 590  default:
 591     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 592     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 593     break;
 594   }
 595 
 596   // -XX:-InlineClassNatives disables natives from the Class class.
 597   // The flag applies to all reflective calls, notably Array.newArray
 598   // (visible to Java programmers as Array.newInstance).
 599   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Class() ||
 600       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_reflect_Array()) {
 601     if (!InlineClassNatives)  return NULL;
 602   }
 603 
 604   // -XX:-InlineThreadNatives disables natives from the Thread class.
 605   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Thread()) {
 606     if (!InlineThreadNatives)  return NULL;
 607   }
 608 
 609   // -XX:-InlineMathNatives disables natives from the Math,Float and Double classes.
 610   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Math() ||
 611       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Float() ||
 612       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Double()) {
 613     if (!InlineMathNatives)  return NULL;
 614   }
 615 
 616   // -XX:-InlineUnsafeOps disables natives from the Unsafe class.
 617   if (m-&gt;holder()-&gt;name() == ciSymbol::sun_misc_Unsafe()) {
 618     if (!InlineUnsafeOps)  return NULL;
 619   }
 620 
 621   return new LibraryIntrinsic(m, is_virtual, predicates, does_virtual_dispatch, (vmIntrinsics::ID) id);
 622 }
 623 
 624 //----------------------register_library_intrinsics-----------------------
 625 // Initialize this file's data structures, for each Compile instance.
 626 void Compile::register_library_intrinsics() {
 627   // Nothing to do here.
 628 }
 629 
 630 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 631   LibraryCallKit kit(jvms, this);
 632   Compile* C = kit.C;
 633   int nodes = C-&gt;unique();
 634 #ifndef PRODUCT
 635   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 636     char buf[1000];
 637     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 638     tty-&gt;print_cr("Intrinsic %s", str);
 639   }
 640 #endif
 641   ciMethod* callee = kit.callee();
 642   const int bci    = kit.bci();
 643 
 644   // Try to inline the intrinsic.
 645   if (kit.try_to_inline(_last_predicate)) {
 646     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 647       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 648     }
 649     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 650     if (C-&gt;log()) {
 651       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 652                      vmIntrinsics::name_at(intrinsic_id()),
 653                      (is_virtual() ? " virtual='1'" : ""),
 654                      C-&gt;unique() - nodes);
 655     }
 656     // Push the result from the inlined method onto the stack.
 657     kit.push_result();
 658     C-&gt;print_inlining_update(this);
 659     return kit.transfer_exceptions_into_jvms();
 660   }
 661 
 662   // The intrinsic bailed out
 663   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 664     if (jvms-&gt;has_method()) {
 665       // Not a root compile.
 666       const char* msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 667       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 668     } else {
 669       // Root compile
 670       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 671                vmIntrinsics::name_at(intrinsic_id()),
 672                (is_virtual() ? " (virtual)" : ""), bci);
 673     }
 674   }
 675   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 676   C-&gt;print_inlining_update(this);
 677   return NULL;
 678 }
 679 
 680 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 681   LibraryCallKit kit(jvms, this);
 682   Compile* C = kit.C;
 683   int nodes = C-&gt;unique();
 684   _last_predicate = predicate;
 685 #ifndef PRODUCT
 686   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 687   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 688     char buf[1000];
 689     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 690     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 691   }
 692 #endif
 693   ciMethod* callee = kit.callee();
 694   const int bci    = kit.bci();
 695 
 696   Node* slow_ctl = kit.try_to_predicate(predicate);
 697   if (!kit.failing()) {
 698     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 699       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 700     }
 701     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 702     if (C-&gt;log()) {
 703       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 704                      vmIntrinsics::name_at(intrinsic_id()),
 705                      (is_virtual() ? " virtual='1'" : ""),
 706                      C-&gt;unique() - nodes);
 707     }
 708     return slow_ctl; // Could be NULL if the check folds.
 709   }
 710 
 711   // The intrinsic bailed out
 712   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 713     if (jvms-&gt;has_method()) {
 714       // Not a root compile.
 715       const char* msg = "failed to generate predicate for intrinsic";
 716       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 717     } else {
 718       // Root compile
 719       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 720                                         vmIntrinsics::name_at(intrinsic_id()),
 721                                         (is_virtual() ? " (virtual)" : ""), bci);
 722     }
 723   }
 724   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 725   return NULL;
 726 }
 727 
 728 bool LibraryCallKit::try_to_inline(int predicate) {
 729   // Handle symbolic names for otherwise undistinguished boolean switches:
 730   const bool is_store       = true;
 731   const bool is_native_ptr  = true;
 732   const bool is_static      = true;
 733   const bool is_volatile    = true;
 734 
 735   if (!jvms()-&gt;has_method()) {
 736     // Root JVMState has a null method.
 737     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 738     // Insert the memory aliasing node
 739     set_all_memory(reset_memory());
 740   }
 741   assert(merged_memory(), "");
 742 
 743 
 744   switch (intrinsic_id()) {
 745   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 746   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 747   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 748 
 749   case vmIntrinsics::_dsin:
 750   case vmIntrinsics::_dcos:
 751   case vmIntrinsics::_dtan:
 752   case vmIntrinsics::_dabs:
 753   case vmIntrinsics::_datan2:
 754   case vmIntrinsics::_dsqrt:
 755   case vmIntrinsics::_dexp:
 756   case vmIntrinsics::_dlog:
 757   case vmIntrinsics::_dlog10:
 758   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 759 
 760   case vmIntrinsics::_min:
 761   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 762 
 763   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 764   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 765   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 766   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 767   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 768   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 769   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 770   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 771   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 772   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 773   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 774   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 775 
 776   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 777 
 778   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 779   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 780   case vmIntrinsics::_equals:                   return inline_string_equals();
 781 
 782   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 783   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 784   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 785   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 786   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 787   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 788   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 789   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 790   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 791 
 792   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 793   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 794   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 795   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 796   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 797   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 798   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 799   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 800   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 801 
 802   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 803   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 804   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 805   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 806   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 807   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 808   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 809   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 810 
 811   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 812   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 813   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 814   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 815   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 816   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 817   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 818   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 819 
 820   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 821   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 822   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 823   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 824   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 825   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 826   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 827   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 828   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 829 
 830   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 831   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 832   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 833   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 834   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 835   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 836   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 837   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 838   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 839 
 840   case vmIntrinsics::_getShortUnaligned:        return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 841   case vmIntrinsics::_getCharUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 842   case vmIntrinsics::_getIntUnaligned:          return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 843   case vmIntrinsics::_getLongUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 844 
 845   case vmIntrinsics::_putShortUnaligned:        return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 846   case vmIntrinsics::_putCharUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 847   case vmIntrinsics::_putIntUnaligned:          return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 848   case vmIntrinsics::_putLongUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 849 
 850   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 851   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 852   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 853 
 854   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 855   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 856   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 857 
 858   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 859   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 860   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 861   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 862   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 863 
 864   case vmIntrinsics::_loadFence:
 865   case vmIntrinsics::_storeFence:
 866   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 867 
 868   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 869   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 870 
 871 #ifdef TRACE_HAVE_INTRINSICS
 872   case vmIntrinsics::_classID:                  return inline_native_classID();
 873   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 874   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 875 #endif
 876   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 877   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 878   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 879   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 880   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 881   case vmIntrinsics::_getLength:                return inline_native_getLength();
 882   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 883   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 884   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 885   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 886 
 887   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 888 
 889   case vmIntrinsics::_isInstance:
 890   case vmIntrinsics::_getModifiers:
 891   case vmIntrinsics::_isInterface:
 892   case vmIntrinsics::_isArray:
 893   case vmIntrinsics::_isPrimitive:
 894   case vmIntrinsics::_getSuperclass:
 895   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 896 
 897   case vmIntrinsics::_floatToRawIntBits:
 898   case vmIntrinsics::_floatToIntBits:
 899   case vmIntrinsics::_intBitsToFloat:
 900   case vmIntrinsics::_doubleToRawLongBits:
 901   case vmIntrinsics::_doubleToLongBits:
 902   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 903 
 904   case vmIntrinsics::_numberOfLeadingZeros_i:
 905   case vmIntrinsics::_numberOfLeadingZeros_l:
 906   case vmIntrinsics::_numberOfTrailingZeros_i:
 907   case vmIntrinsics::_numberOfTrailingZeros_l:
 908   case vmIntrinsics::_bitCount_i:
 909   case vmIntrinsics::_bitCount_l:
 910   case vmIntrinsics::_reverseBytes_i:
 911   case vmIntrinsics::_reverseBytes_l:
 912   case vmIntrinsics::_reverseBytes_s:
 913   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 914 
 915   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 916 
 917   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 918 
 919   case vmIntrinsics::_Class_cast:               return inline_Class_cast();
 920 
 921   case vmIntrinsics::_aescrypt_encryptBlock:
 922   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 923 
 924   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 925   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 926     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 927 
 928   case vmIntrinsics::_sha_implCompress:
 929   case vmIntrinsics::_sha2_implCompress:
 930   case vmIntrinsics::_sha5_implCompress:
 931     return inline_sha_implCompress(intrinsic_id());
 932 
 933   case vmIntrinsics::_digestBase_implCompressMB:
 934     return inline_digestBase_implCompressMB(predicate);
 935 
 936   case vmIntrinsics::_multiplyToLen:
 937     return inline_multiplyToLen();
 938 
 939   case vmIntrinsics::_squareToLen:
 940     return inline_squareToLen();
 941 
 942   case vmIntrinsics::_mulAdd:
 943     return inline_mulAdd();
 944 
 945   case vmIntrinsics::_ghash_processBlocks:
 946     return inline_ghash_processBlocks();
 947 
 948   case vmIntrinsics::_encodeISOArray:
 949     return inline_encodeISOArray();
 950 
 951   case vmIntrinsics::_updateCRC32:
 952     return inline_updateCRC32();
 953   case vmIntrinsics::_updateBytesCRC32:
 954     return inline_updateBytesCRC32();
 955   case vmIntrinsics::_updateByteBufferCRC32:
 956     return inline_updateByteBufferCRC32();
 957 
 958   case vmIntrinsics::_updateBytesCRC32C:
 959     return inline_updateBytesCRC32C();
 960   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 961     return inline_updateDirectByteBufferCRC32C();
 962 
 963   case vmIntrinsics::_profileBoolean:
 964     return inline_profileBoolean();
 965   case vmIntrinsics::_isCompileConstant:
 966     return inline_isCompileConstant();
 967 
 968   default:
 969     // If you get here, it may be that someone has added a new intrinsic
 970     // to the list in vmSymbols.hpp without implementing it here.
 971 #ifndef PRODUCT
 972     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 973       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 974                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 975     }
 976 #endif
 977     return false;
 978   }
 979 }
 980 
 981 Node* LibraryCallKit::try_to_predicate(int predicate) {
 982   if (!jvms()-&gt;has_method()) {
 983     // Root JVMState has a null method.
 984     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 985     // Insert the memory aliasing node
 986     set_all_memory(reset_memory());
 987   }
 988   assert(merged_memory(), "");
 989 
 990   switch (intrinsic_id()) {
 991   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 992     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 993   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 994     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 995   case vmIntrinsics::_digestBase_implCompressMB:
 996     return inline_digestBase_implCompressMB_predicate(predicate);
 997 
 998   default:
 999     // If you get here, it may be that someone has added a new intrinsic
1000     // to the list in vmSymbols.hpp without implementing it here.
1001 #ifndef PRODUCT
1002     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
1003       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
1004                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
1005     }
1006 #endif
1007     Node* slow_ctl = control();
1008     set_control(top()); // No fast path instrinsic
1009     return slow_ctl;
1010   }
1011 }
1012 
1013 //------------------------------set_result-------------------------------
1014 // Helper function for finishing intrinsics.
1015 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
1016   record_for_igvn(region);
1017   set_control(_gvn.transform(region));
1018   set_result( _gvn.transform(value));
1019   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
1020 }
1021 
1022 //------------------------------generate_guard---------------------------
1023 // Helper function for generating guarded fast-slow graph structures.
1024 // The given 'test', if true, guards a slow path.  If the test fails
1025 // then a fast path can be taken.  (We generally hope it fails.)
1026 // In all cases, GraphKit::control() is updated to the fast path.
1027 // The returned value represents the control for the slow path.
1028 // The return value is never 'top'; it is either a valid control
1029 // or NULL if it is obvious that the slow path can never be taken.
1030 // Also, if region and the slow control are not NULL, the slow edge
1031 // is appended to the region.
1032 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
1033   if (stopped()) {
1034     // Already short circuited.
1035     return NULL;
1036   }
1037 
1038   // Build an if node and its projections.
1039   // If test is true we take the slow path, which we assume is uncommon.
1040   if (_gvn.type(test) == TypeInt::ZERO) {
1041     // The slow branch is never taken.  No need to build this guard.
1042     return NULL;
1043   }
1044 
1045   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
1046 
1047   Node* if_slow = _gvn.transform(new IfTrueNode(iff));
1048   if (if_slow == top()) {
1049     // The slow branch is never taken.  No need to build this guard.
1050     return NULL;
1051   }
1052 
1053   if (region != NULL)
1054     region-&gt;add_req(if_slow);
1055 
1056   Node* if_fast = _gvn.transform(new IfFalseNode(iff));
1057   set_control(if_fast);
1058 
1059   return if_slow;
1060 }
1061 
1062 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
1063   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
1064 }
1065 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
1066   return generate_guard(test, region, PROB_FAIR);
1067 }
1068 
1069 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
1070                                                      Node* *pos_index) {
1071   if (stopped())
1072     return NULL;                // already stopped
1073   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
1074     return NULL;                // index is already adequately typed
1075   Node* cmp_lt = _gvn.transform(new CmpINode(index, intcon(0)));
1076   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
1077   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
1078   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
1079     // Emulate effect of Parse::adjust_map_after_if.
1080     Node* ccast = new CastIINode(index, TypeInt::POS);
1081     ccast-&gt;set_req(0, control());
1082     (*pos_index) = _gvn.transform(ccast);
1083   }
1084   return is_neg;
1085 }
1086 
1087 // Make sure that 'position' is a valid limit index, in [0..length].
1088 // There are two equivalent plans for checking this:
1089 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
1090 //   B. offset  &lt;=  (arrayLength - copyLength)
1091 // We require that all of the values above, except for the sum and
1092 // difference, are already known to be non-negative.
1093 // Plan A is robust in the face of overflow, if offset and copyLength
1094 // are both hugely positive.
1095 //
1096 // Plan B is less direct and intuitive, but it does not overflow at
1097 // all, since the difference of two non-negatives is always
1098 // representable.  Whenever Java methods must perform the equivalent
1099 // check they generally use Plan B instead of Plan A.
1100 // For the moment we use Plan A.
1101 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
1102                                                   Node* subseq_length,
1103                                                   Node* array_length,
1104                                                   RegionNode* region) {
1105   if (stopped())
1106     return NULL;                // already stopped
1107   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
1108   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
1109     return NULL;                // common case of whole-array copy
1110   Node* last = subseq_length;
1111   if (!zero_offset)             // last += offset
1112     last = _gvn.transform(new AddINode(last, offset));
1113   Node* cmp_lt = _gvn.transform(new CmpUNode(array_length, last));
1114   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
1115   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
1116   return is_over;
1117 }
1118 
1119 
1120 //--------------------------generate_current_thread--------------------
1121 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1122   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1123   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1124   Node* thread = _gvn.transform(new ThreadLocalNode());
1125   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1126   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1127   tls_output = thread;
1128   return threadObj;
1129 }
1130 
1131 
1132 //------------------------------make_string_method_node------------------------
1133 // Helper method for String intrinsic functions. This version is called
1134 // with str1 and str2 pointing to String object nodes.
1135 //
1136 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
1137   Node* no_ctrl = NULL;
1138 
1139   // Get start addr of string
1140   Node* str1_value   = load_String_value(no_ctrl, str1);
1141   Node* str1_offset  = load_String_offset(no_ctrl, str1);
1142   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
1143 
1144   // Get length of string 1
1145   Node* str1_len  = load_String_length(no_ctrl, str1);
1146 
1147   Node* str2_value   = load_String_value(no_ctrl, str2);
1148   Node* str2_offset  = load_String_offset(no_ctrl, str2);
1149   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
1150 
1151   Node* str2_len = NULL;
1152   Node* result = NULL;
1153 
1154   switch (opcode) {
1155   case Op_StrIndexOf:
1156     // Get length of string 2
1157     str2_len = load_String_length(no_ctrl, str2);
1158 
1159     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1160                                 str1_start, str1_len, str2_start, str2_len);
1161     break;
1162   case Op_StrComp:
1163     // Get length of string 2
1164     str2_len = load_String_length(no_ctrl, str2);
1165 
1166     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
1167                              str1_start, str1_len, str2_start, str2_len);
1168     break;
1169   case Op_StrEquals:
1170     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1171                                str1_start, str2_start, str1_len);
1172     break;
1173   default:
1174     ShouldNotReachHere();
1175     return NULL;
1176   }
1177 
1178   // All these intrinsics have checks.
1179   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1180 
1181   return _gvn.transform(result);
1182 }
1183 
1184 // Helper method for String intrinsic functions. This version is called
1185 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
1186 // to Int nodes containing the lenghts of str1 and str2.
1187 //
1188 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
1189   Node* result = NULL;
1190   switch (opcode) {
1191   case Op_StrIndexOf:
1192     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1193                                 str1_start, cnt1, str2_start, cnt2);
1194     break;
1195   case Op_StrComp:
1196     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
1197                              str1_start, cnt1, str2_start, cnt2);
1198     break;
1199   case Op_StrEquals:
1200     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1201                                str1_start, str2_start, cnt1);
1202     break;
1203   default:
1204     ShouldNotReachHere();
1205     return NULL;
1206   }
1207 
1208   // All these intrinsics have checks.
1209   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1210 
1211   return _gvn.transform(result);
1212 }
1213 
1214 //------------------------------inline_string_compareTo------------------------
1215 // public int java.lang.String.compareTo(String anotherString);
1216 bool LibraryCallKit::inline_string_compareTo() {
1217   Node* receiver = null_check(argument(0));
1218   Node* arg      = null_check(argument(1));
1219   if (stopped()) {
1220     return true;
1221   }
1222   set_result(make_string_method_node(Op_StrComp, receiver, arg));
1223   return true;
1224 }
1225 
1226 //------------------------------inline_string_equals------------------------
1227 bool LibraryCallKit::inline_string_equals() {
1228   Node* receiver = null_check_receiver();
1229   // NOTE: Do not null check argument for String.equals() because spec
1230   // allows to specify NULL as argument.
1231   Node* argument = this-&gt;argument(1);
1232   if (stopped()) {
1233     return true;
1234   }
1235 
1236   // paths (plus control) merge
1237   RegionNode* region = new RegionNode(5);
1238   Node* phi = new PhiNode(region, TypeInt::BOOL);
1239 
1240   // does source == target string?
1241   Node* cmp = _gvn.transform(new CmpPNode(receiver, argument));
1242   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1243 
1244   Node* if_eq = generate_slow_guard(bol, NULL);
1245   if (if_eq != NULL) {
1246     // receiver == argument
1247     phi-&gt;init_req(2, intcon(1));
1248     region-&gt;init_req(2, if_eq);
1249   }
1250 
1251   // get String klass for instanceOf
1252   ciInstanceKlass* klass = env()-&gt;String_klass();
1253 
1254   if (!stopped()) {
1255     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1256     Node* cmp  = _gvn.transform(new CmpINode(inst, intcon(1)));
1257     Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1258 
1259     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1260     //instanceOf == true, fallthrough
1261 
1262     if (inst_false != NULL) {
1263       phi-&gt;init_req(3, intcon(0));
1264       region-&gt;init_req(3, inst_false);
1265     }
1266   }
1267 
1268   if (!stopped()) {
1269     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1270 
1271     // Properly cast the argument to String
1272     argument = _gvn.transform(new CheckCastPPNode(control(), argument, string_type));
1273     // This path is taken only when argument's type is String:NotNull.
1274     argument = cast_not_null(argument, false);
1275 
1276     Node* no_ctrl = NULL;
1277 
1278     // Get start addr of receiver
1279     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1280     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1281     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1282 
1283     // Get length of receiver
1284     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1285 
1286     // Get start addr of argument
1287     Node* argument_val    = load_String_value(no_ctrl, argument);
1288     Node* argument_offset = load_String_offset(no_ctrl, argument);
1289     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1290 
1291     // Get length of argument
1292     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1293 
1294     // Check for receiver count != argument count
1295     Node* cmp = _gvn.transform(new CmpINode(receiver_cnt, argument_cnt));
1296     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1297     Node* if_ne = generate_slow_guard(bol, NULL);
1298     if (if_ne != NULL) {
1299       phi-&gt;init_req(4, intcon(0));
1300       region-&gt;init_req(4, if_ne);
1301     }
1302 
1303     // Check for count == 0 is done by assembler code for StrEquals.
1304 
1305     if (!stopped()) {
1306       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1307       phi-&gt;init_req(1, equals);
1308       region-&gt;init_req(1, control());
1309     }
1310   }
1311 
1312   // post merge
1313   set_control(_gvn.transform(region));
1314   record_for_igvn(region);
1315 
1316   set_result(_gvn.transform(phi));
1317   return true;
1318 }
1319 
1320 //------------------------------inline_array_equals----------------------------
1321 bool LibraryCallKit::inline_array_equals() {
1322   Node* arg1 = argument(0);
1323   Node* arg2 = argument(1);
1324   set_result(_gvn.transform(new AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1325   return true;
1326 }
1327 
1328 // Java version of String.indexOf(constant string)
1329 // class StringDecl {
1330 //   StringDecl(char[] ca) {
1331 //     offset = 0;
1332 //     count = ca.length;
1333 //     value = ca;
1334 //   }
1335 //   int offset;
1336 //   int count;
1337 //   char[] value;
1338 // }
1339 //
1340 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1341 //                             int targetOffset, int cache_i, int md2) {
1342 //   int cache = cache_i;
1343 //   int sourceOffset = string_object.offset;
1344 //   int sourceCount = string_object.count;
1345 //   int targetCount = target_object.length;
1346 //
1347 //   int targetCountLess1 = targetCount - 1;
1348 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1349 //
1350 //   char[] source = string_object.value;
1351 //   char[] target = target_object;
1352 //   int lastChar = target[targetCountLess1];
1353 //
1354 //  outer_loop:
1355 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1356 //     int src = source[i + targetCountLess1];
1357 //     if (src == lastChar) {
1358 //       // With random strings and a 4-character alphabet,
1359 //       // reverse matching at this point sets up 0.8% fewer
1360 //       // frames, but (paradoxically) makes 0.3% more probes.
1361 //       // Since those probes are nearer the lastChar probe,
1362 //       // there is may be a net D$ win with reverse matching.
1363 //       // But, reversing loop inhibits unroll of inner loop
1364 //       // for unknown reason.  So, does running outer loop from
1365 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1366 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1367 //         if (target[targetOffset + j] != source[i+j]) {
1368 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1369 //             if (md2 &lt; j+1) {
1370 //               i += j+1;
1371 //               continue outer_loop;
1372 //             }
1373 //           }
1374 //           i += md2;
1375 //           continue outer_loop;
1376 //         }
1377 //       }
1378 //       return i - sourceOffset;
1379 //     }
1380 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1381 //       i += targetCountLess1;
1382 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1383 //     i++;
1384 //   }
1385 //   return -1;
1386 // }
1387 
1388 //------------------------------string_indexOf------------------------
1389 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1390                                      jint cache_i, jint md2_i) {
1391 
1392   Node* no_ctrl  = NULL;
1393   float likely   = PROB_LIKELY(0.9);
1394   float unlikely = PROB_UNLIKELY(0.9);
1395 
1396   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1397 
1398   Node* source        = load_String_value(no_ctrl, string_object);
1399   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1400   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1401 
1402   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1403   jint target_length = target_array-&gt;length();
1404   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1405   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1406 
1407   // String.value field is known to be @Stable.
1408   if (UseImplicitStableValues) {
1409     target = cast_array_to_stable(target, target_type);
1410   }
1411 
1412   IdealKit kit(this, false, true);
1413 #define __ kit.
1414   Node* zero             = __ ConI(0);
1415   Node* one              = __ ConI(1);
1416   Node* cache            = __ ConI(cache_i);
1417   Node* md2              = __ ConI(md2_i);
1418   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1419   Node* targetCountLess1 = __ ConI(target_length - 1);
1420   Node* targetOffset     = __ ConI(targetOffset_i);
1421   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1422 
1423   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1424   Node* outer_loop = __ make_label(2 /* goto */);
1425   Node* return_    = __ make_label(1);
1426 
1427   __ set(rtn,__ ConI(-1));
1428   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1429        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1430        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1431        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1432        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1433          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1434               Node* tpj = __ AddI(targetOffset, __ value(j));
1435               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1436               Node* ipj  = __ AddI(__ value(i), __ value(j));
1437               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1438               __ if_then(targ, BoolTest::ne, src2); {
1439                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1440                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1441                     __ increment(i, __ AddI(__ value(j), one));
1442                     __ goto_(outer_loop);
1443                   } __ end_if(); __ dead(j);
1444                 }__ end_if(); __ dead(j);
1445                 __ increment(i, md2);
1446                 __ goto_(outer_loop);
1447               }__ end_if();
1448               __ increment(j, one);
1449          }__ end_loop(); __ dead(j);
1450          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1451          __ goto_(return_);
1452        }__ end_if();
1453        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1454          __ increment(i, targetCountLess1);
1455        }__ end_if();
1456        __ increment(i, one);
1457        __ bind(outer_loop);
1458   }__ end_loop(); __ dead(i);
1459   __ bind(return_);
1460 
1461   // Final sync IdealKit and GraphKit.
1462   final_sync(kit);
1463   Node* result = __ value(rtn);
1464 #undef __
1465   C-&gt;set_has_loops(true);
1466   return result;
1467 }
1468 
1469 //------------------------------inline_string_indexOf------------------------
1470 bool LibraryCallKit::inline_string_indexOf() {
1471   Node* receiver = argument(0);
1472   Node* arg      = argument(1);
1473 
1474   Node* result;
1475   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1476       UseSSE42Intrinsics) {
1477     // Generate SSE4.2 version of indexOf
1478     // We currently only have match rules that use SSE4.2
1479 
1480     receiver = null_check(receiver);
1481     arg      = null_check(arg);
1482     if (stopped()) {
1483       return true;
1484     }
1485 
1486     // Make the merge point
1487     RegionNode* result_rgn = new RegionNode(4);
1488     Node*       result_phi = new PhiNode(result_rgn, TypeInt::INT);
1489     Node* no_ctrl  = NULL;
1490 
1491     // Get start addr of source string
1492     Node* source = load_String_value(no_ctrl, receiver);
1493     Node* source_offset = load_String_offset(no_ctrl, receiver);
1494     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1495 
1496     // Get length of source string
1497     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1498 
1499     // Get start addr of substring
1500     Node* substr = load_String_value(no_ctrl, arg);
1501     Node* substr_offset = load_String_offset(no_ctrl, arg);
1502     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1503 
1504     // Get length of source string
1505     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1506 
1507     // Check for substr count &gt; string count
1508     Node* cmp = _gvn.transform(new CmpINode(substr_cnt, source_cnt));
1509     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::gt));
1510     Node* if_gt = generate_slow_guard(bol, NULL);
1511     if (if_gt != NULL) {
1512       result_phi-&gt;init_req(2, intcon(-1));
1513       result_rgn-&gt;init_req(2, if_gt);
1514     }
1515 
1516     if (!stopped()) {
1517       // Check for substr count == 0
1518       cmp = _gvn.transform(new CmpINode(substr_cnt, intcon(0)));
1519       bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1520       Node* if_zero = generate_slow_guard(bol, NULL);
1521       if (if_zero != NULL) {
1522         result_phi-&gt;init_req(3, intcon(0));
1523         result_rgn-&gt;init_req(3, if_zero);
1524       }
1525     }
1526 
1527     if (!stopped()) {
1528       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1529       result_phi-&gt;init_req(1, result);
1530       result_rgn-&gt;init_req(1, control());
1531     }
1532     set_control(_gvn.transform(result_rgn));
1533     record_for_igvn(result_rgn);
1534     result = _gvn.transform(result_phi);
1535 
1536   } else { // Use LibraryCallKit::string_indexOf
1537     // don't intrinsify if argument isn't a constant string.
1538     if (!arg-&gt;is_Con()) {
1539      return false;
1540     }
1541     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1542     if (str_type == NULL) {
1543       return false;
1544     }
1545     ciInstanceKlass* klass = env()-&gt;String_klass();
1546     ciObject* str_const = str_type-&gt;const_oop();
1547     if (str_const == NULL || str_const-&gt;klass() != klass) {
1548       return false;
1549     }
1550     ciInstance* str = str_const-&gt;as_instance();
1551     assert(str != NULL, "must be instance");
1552 
1553     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1554     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1555 
1556     int o;
1557     int c;
1558     if (java_lang_String::has_offset_field()) {
1559       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1560       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1561     } else {
1562       o = 0;
1563       c = pat-&gt;length();
1564     }
1565 
1566     // constant strings have no offset and count == length which
1567     // simplifies the resulting code somewhat so lets optimize for that.
1568     if (o != 0 || c != pat-&gt;length()) {
1569      return false;
1570     }
1571 
1572     receiver = null_check(receiver, T_OBJECT);
1573     // NOTE: No null check on the argument is needed since it's a constant String oop.
1574     if (stopped()) {
1575       return true;
1576     }
1577 
1578     // The null string as a pattern always returns 0 (match at beginning of string)
1579     if (c == 0) {
1580       set_result(intcon(0));
1581       return true;
1582     }
1583 
1584     // Generate default indexOf
1585     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1586     int cache = 0;
1587     int i;
1588     for (i = 0; i &lt; c - 1; i++) {
1589       assert(i &lt; pat-&gt;length(), "out of range");
1590       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1591     }
1592 
1593     int md2 = c;
1594     for (i = 0; i &lt; c - 1; i++) {
1595       assert(i &lt; pat-&gt;length(), "out of range");
1596       if (pat-&gt;char_at(o + i) == lastChar) {
1597         md2 = (c - 1) - i;
1598       }
1599     }
1600 
1601     result = string_indexOf(receiver, pat, o, cache, md2);
1602   }
1603   set_result(result);
1604   return true;
1605 }
1606 
1607 //--------------------------round_double_node--------------------------------
1608 // Round a double node if necessary.
1609 Node* LibraryCallKit::round_double_node(Node* n) {
1610   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1611     n = _gvn.transform(new RoundDoubleNode(0, n));
1612   return n;
1613 }
1614 
1615 //------------------------------inline_math-----------------------------------
1616 // public static double Math.abs(double)
1617 // public static double Math.sqrt(double)
1618 // public static double Math.log(double)
1619 // public static double Math.log10(double)
1620 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1621   Node* arg = round_double_node(argument(0));
1622   Node* n;
1623   switch (id) {
1624   case vmIntrinsics::_dabs:   n = new AbsDNode(                arg);  break;
1625   case vmIntrinsics::_dsqrt:  n = new SqrtDNode(C, control(),  arg);  break;
1626   case vmIntrinsics::_dlog:   n = new LogDNode(C, control(),   arg);  break;
1627   case vmIntrinsics::_dlog10: n = new Log10DNode(C, control(), arg);  break;
1628   default:  fatal_unexpected_iid(id);  break;
1629   }
1630   set_result(_gvn.transform(n));
1631   return true;
1632 }
1633 
1634 //------------------------------inline_trig----------------------------------
1635 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1636 // argument reduction which will turn into a fast/slow diamond.
1637 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1638   Node* arg = round_double_node(argument(0));
1639   Node* n = NULL;
1640 
1641   switch (id) {
1642   case vmIntrinsics::_dsin:  n = new SinDNode(C, control(), arg);  break;
1643   case vmIntrinsics::_dcos:  n = new CosDNode(C, control(), arg);  break;
1644   case vmIntrinsics::_dtan:  n = new TanDNode(C, control(), arg);  break;
1645   default:  fatal_unexpected_iid(id);  break;
1646   }
1647   n = _gvn.transform(n);
1648 
1649   // Rounding required?  Check for argument reduction!
1650   if (Matcher::strict_fp_requires_explicit_rounding) {
1651     static const double     pi_4 =  0.7853981633974483;
1652     static const double neg_pi_4 = -0.7853981633974483;
1653     // pi/2 in 80-bit extended precision
1654     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1655     // -pi/2 in 80-bit extended precision
1656     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1657     // Cutoff value for using this argument reduction technique
1658     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1659     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1660 
1661     // Pseudocode for sin:
1662     // if (x &lt;= Math.PI / 4.0) {
1663     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1664     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1665     // } else {
1666     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1667     // }
1668     // return StrictMath.sin(x);
1669 
1670     // Pseudocode for cos:
1671     // if (x &lt;= Math.PI / 4.0) {
1672     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1673     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1674     // } else {
1675     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1676     // }
1677     // return StrictMath.cos(x);
1678 
1679     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1680     // requires a special machine instruction to load it.  Instead we'll try
1681     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1682     // probably do the math inside the SIN encoding.
1683 
1684     // Make the merge point
1685     RegionNode* r = new RegionNode(3);
1686     Node* phi = new PhiNode(r, Type::DOUBLE);
1687 
1688     // Flatten arg so we need only 1 test
1689     Node *abs = _gvn.transform(new AbsDNode(arg));
1690     // Node for PI/4 constant
1691     Node *pi4 = makecon(TypeD::make(pi_4));
1692     // Check PI/4 : abs(arg)
1693     Node *cmp = _gvn.transform(new CmpDNode(pi4,abs));
1694     // Check: If PI/4 &lt; abs(arg) then go slow
1695     Node *bol = _gvn.transform(new BoolNode( cmp, BoolTest::lt ));
1696     // Branch either way
1697     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1698     set_control(opt_iff(r,iff));
1699 
1700     // Set fast path result
1701     phi-&gt;init_req(2, n);
1702 
1703     // Slow path - non-blocking leaf call
1704     Node* call = NULL;
1705     switch (id) {
1706     case vmIntrinsics::_dsin:
1707       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1708                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1709                                "Sin", NULL, arg, top());
1710       break;
1711     case vmIntrinsics::_dcos:
1712       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1713                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1714                                "Cos", NULL, arg, top());
1715       break;
1716     case vmIntrinsics::_dtan:
1717       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1718                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1719                                "Tan", NULL, arg, top());
1720       break;
1721     }
1722     assert(control()-&gt;in(0) == call, "");
1723     Node* slow_result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1724     r-&gt;init_req(1, control());
1725     phi-&gt;init_req(1, slow_result);
1726 
1727     // Post-merge
1728     set_control(_gvn.transform(r));
1729     record_for_igvn(r);
1730     n = _gvn.transform(phi);
1731 
1732     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1733   }
1734   set_result(n);
1735   return true;
1736 }
1737 
1738 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1739   //-------------------
1740   //result=(result.isNaN())? funcAddr():result;
1741   // Check: If isNaN() by checking result!=result? then either trap
1742   // or go to runtime
1743   Node* cmpisnan = _gvn.transform(new CmpDNode(result, result));
1744   // Build the boolean node
1745   Node* bolisnum = _gvn.transform(new BoolNode(cmpisnan, BoolTest::eq));
1746 
1747   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1748     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1749       // The pow or exp intrinsic returned a NaN, which requires a call
1750       // to the runtime.  Recompile with the runtime call.
1751       uncommon_trap(Deoptimization::Reason_intrinsic,
1752                     Deoptimization::Action_make_not_entrant);
1753     }
1754     return result;
1755   } else {
1756     // If this inlining ever returned NaN in the past, we compile a call
1757     // to the runtime to properly handle corner cases
1758 
1759     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1760     Node* if_slow = _gvn.transform(new IfFalseNode(iff));
1761     Node* if_fast = _gvn.transform(new IfTrueNode(iff));
1762 
1763     if (!if_slow-&gt;is_top()) {
1764       RegionNode* result_region = new RegionNode(3);
1765       PhiNode*    result_val = new PhiNode(result_region, Type::DOUBLE);
1766 
1767       result_region-&gt;init_req(1, if_fast);
1768       result_val-&gt;init_req(1, result);
1769 
1770       set_control(if_slow);
1771 
1772       const TypePtr* no_memory_effects = NULL;
1773       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1774                                    no_memory_effects,
1775                                    x, top(), y, y ? top() : NULL);
1776       Node* value = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+0));
1777 #ifdef ASSERT
1778       Node* value_top = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+1));
1779       assert(value_top == top(), "second value must be top");
1780 #endif
1781 
1782       result_region-&gt;init_req(2, control());
1783       result_val-&gt;init_req(2, value);
1784       set_control(_gvn.transform(result_region));
1785       return _gvn.transform(result_val);
1786     } else {
1787       return result;
1788     }
1789   }
1790 }
1791 
1792 //------------------------------inline_exp-------------------------------------
1793 // Inline exp instructions, if possible.  The Intel hardware only misses
1794 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1795 bool LibraryCallKit::inline_exp() {
1796   Node* arg = round_double_node(argument(0));
1797   Node* n   = _gvn.transform(new ExpDNode(C, control(), arg));
1798 
1799   n = finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1800   set_result(n);
1801 
1802   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1803   return true;
1804 }
1805 
1806 //------------------------------inline_pow-------------------------------------
1807 // Inline power instructions, if possible.
1808 bool LibraryCallKit::inline_pow() {
1809   // Pseudocode for pow
1810   // if (y == 2) {
1811   //   return x * x;
1812   // } else {
1813   //   if (x &lt;= 0.0) {
1814   //     long longy = (long)y;
1815   //     if ((double)longy == y) { // if y is long
1816   //       if (y + 1 == y) longy = 0; // huge number: even
1817   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1818   //     } else {
1819   //       result = NaN;
1820   //     }
1821   //   } else {
1822   //     result = DPow(x,y);
1823   //   }
1824   //   if (result != result)?  {
1825   //     result = uncommon_trap() or runtime_call();
1826   //   }
1827   //   return result;
1828   // }
1829 
1830   Node* x = round_double_node(argument(0));
1831   Node* y = round_double_node(argument(2));
1832 
1833   Node* result = NULL;
1834 
1835   Node*   const_two_node = makecon(TypeD::make(2.0));
1836   Node*   cmp_node       = _gvn.transform(new CmpDNode(y, const_two_node));
1837   Node*   bool_node      = _gvn.transform(new BoolNode(cmp_node, BoolTest::eq));
1838   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1839   Node*   if_true        = _gvn.transform(new IfTrueNode(if_node));
1840   Node*   if_false       = _gvn.transform(new IfFalseNode(if_node));
1841 
1842   RegionNode* region_node = new RegionNode(3);
1843   region_node-&gt;init_req(1, if_true);
1844 
1845   Node* phi_node = new PhiNode(region_node, Type::DOUBLE);
1846   // special case for x^y where y == 2, we can convert it to x * x
1847   phi_node-&gt;init_req(1, _gvn.transform(new MulDNode(x, x)));
1848 
1849   // set control to if_false since we will now process the false branch
1850   set_control(if_false);
1851 
1852   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1853     // Short form: skip the fancy tests and just check for NaN result.
1854     result = _gvn.transform(new PowDNode(C, control(), x, y));
1855   } else {
1856     // If this inlining ever returned NaN in the past, include all
1857     // checks + call to the runtime.
1858 
1859     // Set the merge point for If node with condition of (x &lt;= 0.0)
1860     // There are four possible paths to region node and phi node
1861     RegionNode *r = new RegionNode(4);
1862     Node *phi = new PhiNode(r, Type::DOUBLE);
1863 
1864     // Build the first if node: if (x &lt;= 0.0)
1865     // Node for 0 constant
1866     Node *zeronode = makecon(TypeD::ZERO);
1867     // Check x:0
1868     Node *cmp = _gvn.transform(new CmpDNode(x, zeronode));
1869     // Check: If (x&lt;=0) then go complex path
1870     Node *bol1 = _gvn.transform(new BoolNode( cmp, BoolTest::le ));
1871     // Branch either way
1872     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1873     // Fast path taken; set region slot 3
1874     Node *fast_taken = _gvn.transform(new IfFalseNode(if1));
1875     r-&gt;init_req(3,fast_taken); // Capture fast-control
1876 
1877     // Fast path not-taken, i.e. slow path
1878     Node *complex_path = _gvn.transform(new IfTrueNode(if1));
1879 
1880     // Set fast path result
1881     Node *fast_result = _gvn.transform(new PowDNode(C, control(), x, y));
1882     phi-&gt;init_req(3, fast_result);
1883 
1884     // Complex path
1885     // Build the second if node (if y is long)
1886     // Node for (long)y
1887     Node *longy = _gvn.transform(new ConvD2LNode(y));
1888     // Node for (double)((long) y)
1889     Node *doublelongy= _gvn.transform(new ConvL2DNode(longy));
1890     // Check (double)((long) y) : y
1891     Node *cmplongy= _gvn.transform(new CmpDNode(doublelongy, y));
1892     // Check if (y isn't long) then go to slow path
1893 
1894     Node *bol2 = _gvn.transform(new BoolNode( cmplongy, BoolTest::ne ));
1895     // Branch either way
1896     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1897     Node* ylong_path = _gvn.transform(new IfFalseNode(if2));
1898 
1899     Node *slow_path = _gvn.transform(new IfTrueNode(if2));
1900 
1901     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1902     // Node for constant 1
1903     Node *conone = longcon(1);
1904     // 1&amp; (long)y
1905     Node *signnode= _gvn.transform(new AndLNode(conone, longy));
1906 
1907     // A huge number is always even. Detect a huge number by checking
1908     // if y + 1 == y and set integer to be tested for parity to 0.
1909     // Required for corner case:
1910     // (long)9.223372036854776E18 = max_jlong
1911     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1912     // max_jlong is odd but 9.223372036854776E18 is even
1913     Node* yplus1 = _gvn.transform(new AddDNode(y, makecon(TypeD::make(1))));
1914     Node *cmpyplus1= _gvn.transform(new CmpDNode(yplus1, y));
1915     Node *bolyplus1 = _gvn.transform(new BoolNode( cmpyplus1, BoolTest::eq ));
1916     Node* correctedsign = NULL;
1917     if (ConditionalMoveLimit != 0) {
1918       correctedsign = _gvn.transform(CMoveNode::make(NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1919     } else {
1920       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1921       RegionNode *r = new RegionNode(3);
1922       Node *phi = new PhiNode(r, TypeLong::LONG);
1923       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyplus1)));
1924       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyplus1)));
1925       phi-&gt;init_req(1, signnode);
1926       phi-&gt;init_req(2, longcon(0));
1927       correctedsign = _gvn.transform(phi);
1928       ylong_path = _gvn.transform(r);
1929       record_for_igvn(r);
1930     }
1931 
1932     // zero node
1933     Node *conzero = longcon(0);
1934     // Check (1&amp;(long)y)==0?
1935     Node *cmpeq1 = _gvn.transform(new CmpLNode(correctedsign, conzero));
1936     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1937     Node *bol3 = _gvn.transform(new BoolNode( cmpeq1, BoolTest::ne ));
1938     // abs(x)
1939     Node *absx=_gvn.transform(new AbsDNode(x));
1940     // abs(x)^y
1941     Node *absxpowy = _gvn.transform(new PowDNode(C, control(), absx, y));
1942     // -abs(x)^y
1943     Node *negabsxpowy = _gvn.transform(new NegDNode (absxpowy));
1944     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1945     Node *signresult = NULL;
1946     if (ConditionalMoveLimit != 0) {
1947       signresult = _gvn.transform(CMoveNode::make(NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1948     } else {
1949       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1950       RegionNode *r = new RegionNode(3);
1951       Node *phi = new PhiNode(r, Type::DOUBLE);
1952       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyeven)));
1953       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyeven)));
1954       phi-&gt;init_req(1, absxpowy);
1955       phi-&gt;init_req(2, negabsxpowy);
1956       signresult = _gvn.transform(phi);
1957       ylong_path = _gvn.transform(r);
1958       record_for_igvn(r);
1959     }
1960     // Set complex path fast result
1961     r-&gt;init_req(2, ylong_path);
1962     phi-&gt;init_req(2, signresult);
1963 
1964     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1965     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1966     r-&gt;init_req(1,slow_path);
1967     phi-&gt;init_req(1,slow_result);
1968 
1969     // Post merge
1970     set_control(_gvn.transform(r));
1971     record_for_igvn(r);
1972     result = _gvn.transform(phi);
1973   }
1974 
1975   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1976 
1977   // control from finish_pow_exp is now input to the region node
1978   region_node-&gt;set_req(2, control());
1979   // the result from finish_pow_exp is now input to the phi node
1980   phi_node-&gt;init_req(2, result);
1981   set_control(_gvn.transform(region_node));
1982   record_for_igvn(region_node);
1983   set_result(_gvn.transform(phi_node));
1984 
1985   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1986   return true;
1987 }
1988 
1989 //------------------------------runtime_math-----------------------------
1990 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1991   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1992          "must be (DD)D or (D)D type");
1993 
1994   // Inputs
1995   Node* a = round_double_node(argument(0));
1996   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1997 
1998   const TypePtr* no_memory_effects = NULL;
1999   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
2000                                  no_memory_effects,
2001                                  a, top(), b, b ? top() : NULL);
2002   Node* value = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+0));
2003 #ifdef ASSERT
2004   Node* value_top = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+1));
2005   assert(value_top == top(), "second value must be top");
2006 #endif
2007 
2008   set_result(value);
2009   return true;
2010 }
2011 
2012 //------------------------------inline_math_native-----------------------------
2013 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
2014 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
2015   switch (id) {
2016     // These intrinsics are not properly supported on all hardware
2017   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
2018     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
2019   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
2020     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
2021   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
2022     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
2023 
2024   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
2025     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
2026   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
2027     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
2028 
2029     // These intrinsics are supported on all hardware
2030   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
2031   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
2032 
2033   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
2034     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
2035   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
2036     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
2037 #undef FN_PTR
2038 
2039    // These intrinsics are not yet correctly implemented
2040   case vmIntrinsics::_datan2:
2041     return false;
2042 
2043   default:
2044     fatal_unexpected_iid(id);
2045     return false;
2046   }
2047 }
2048 
2049 static bool is_simple_name(Node* n) {
2050   return (n-&gt;req() == 1         // constant
2051           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
2052           || n-&gt;is_Proj()       // parameter or return value
2053           || n-&gt;is_Phi()        // local of some sort
2054           );
2055 }
2056 
2057 //----------------------------inline_min_max-----------------------------------
2058 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
2059   set_result(generate_min_max(id, argument(0), argument(1)));
2060   return true;
2061 }
2062 
2063 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
2064   Node* bol = _gvn.transform( new BoolNode(test, BoolTest::overflow) );
2065   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
2066   Node* fast_path = _gvn.transform( new IfFalseNode(check));
2067   Node* slow_path = _gvn.transform( new IfTrueNode(check) );
2068 
2069   {
2070     PreserveJVMState pjvms(this);
2071     PreserveReexecuteState preexecs(this);
2072     jvms()-&gt;set_should_reexecute(true);
2073 
2074     set_control(slow_path);
2075     set_i_o(i_o());
2076 
2077     uncommon_trap(Deoptimization::Reason_intrinsic,
2078                   Deoptimization::Action_none);
2079   }
2080 
2081   set_control(fast_path);
2082   set_result(math);
2083 }
2084 
2085 template &lt;typename OverflowOp&gt;
2086 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2087   typedef typename OverflowOp::MathOp MathOp;
2088 
2089   MathOp* mathOp = new MathOp(arg1, arg2);
2090   Node* operation = _gvn.transform( mathOp );
2091   Node* ofcheck = _gvn.transform( new OverflowOp(arg1, arg2) );
2092   inline_math_mathExact(operation, ofcheck);
2093   return true;
2094 }
2095 
2096 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2097   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2098 }
2099 
2100 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2101   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2102 }
2103 
2104 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2105   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2106 }
2107 
2108 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2109   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2110 }
2111 
2112 bool LibraryCallKit::inline_math_negateExactI() {
2113   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2114 }
2115 
2116 bool LibraryCallKit::inline_math_negateExactL() {
2117   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2118 }
2119 
2120 bool LibraryCallKit::inline_math_multiplyExactI() {
2121   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2122 }
2123 
2124 bool LibraryCallKit::inline_math_multiplyExactL() {
2125   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2126 }
2127 
2128 Node*
2129 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2130   // These are the candidate return value:
2131   Node* xvalue = x0;
2132   Node* yvalue = y0;
2133 
2134   if (xvalue == yvalue) {
2135     return xvalue;
2136   }
2137 
2138   bool want_max = (id == vmIntrinsics::_max);
2139 
2140   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2141   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2142   if (txvalue == NULL || tyvalue == NULL)  return top();
2143   // This is not really necessary, but it is consistent with a
2144   // hypothetical MaxINode::Value method:
2145   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2146 
2147   // %%% This folding logic should (ideally) be in a different place.
2148   // Some should be inside IfNode, and there to be a more reliable
2149   // transformation of ?: style patterns into cmoves.  We also want
2150   // more powerful optimizations around cmove and min/max.
2151 
2152   // Try to find a dominating comparison of these guys.
2153   // It can simplify the index computation for Arrays.copyOf
2154   // and similar uses of System.arraycopy.
2155   // First, compute the normalized version of CmpI(x, y).
2156   int   cmp_op = Op_CmpI;
2157   Node* xkey = xvalue;
2158   Node* ykey = yvalue;
2159   Node* ideal_cmpxy = _gvn.transform(new CmpINode(xkey, ykey));
2160   if (ideal_cmpxy-&gt;is_Cmp()) {
2161     // E.g., if we have CmpI(length - offset, count),
2162     // it might idealize to CmpI(length, count + offset)
2163     cmp_op = ideal_cmpxy-&gt;Opcode();
2164     xkey = ideal_cmpxy-&gt;in(1);
2165     ykey = ideal_cmpxy-&gt;in(2);
2166   }
2167 
2168   // Start by locating any relevant comparisons.
2169   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2170   Node* cmpxy = NULL;
2171   Node* cmpyx = NULL;
2172   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2173     Node* cmp = start_from-&gt;fast_out(k);
2174     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2175         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2176         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2177       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2178       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2179     }
2180   }
2181 
2182   const int NCMPS = 2;
2183   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2184   int cmpn;
2185   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2186     if (cmps[cmpn] != NULL)  break;     // find a result
2187   }
2188   if (cmpn &lt; NCMPS) {
2189     // Look for a dominating test that tells us the min and max.
2190     int depth = 0;                // Limit search depth for speed
2191     Node* dom = control();
2192     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2193       if (++depth &gt;= 100)  break;
2194       Node* ifproj = dom;
2195       if (!ifproj-&gt;is_Proj())  continue;
2196       Node* iff = ifproj-&gt;in(0);
2197       if (!iff-&gt;is_If())  continue;
2198       Node* bol = iff-&gt;in(1);
2199       if (!bol-&gt;is_Bool())  continue;
2200       Node* cmp = bol-&gt;in(1);
2201       if (cmp == NULL)  continue;
2202       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2203         if (cmps[cmpn] == cmp)  break;
2204       if (cmpn == NCMPS)  continue;
2205       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2206       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2207       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2208       // At this point, we know that 'x btest y' is true.
2209       switch (btest) {
2210       case BoolTest::eq:
2211         // They are proven equal, so we can collapse the min/max.
2212         // Either value is the answer.  Choose the simpler.
2213         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2214           return yvalue;
2215         return xvalue;
2216       case BoolTest::lt:          // x &lt; y
2217       case BoolTest::le:          // x &lt;= y
2218         return (want_max ? yvalue : xvalue);
2219       case BoolTest::gt:          // x &gt; y
2220       case BoolTest::ge:          // x &gt;= y
2221         return (want_max ? xvalue : yvalue);
2222       }
2223     }
2224   }
2225 
2226   // We failed to find a dominating test.
2227   // Let's pick a test that might GVN with prior tests.
2228   Node*          best_bol   = NULL;
2229   BoolTest::mask best_btest = BoolTest::illegal;
2230   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2231     Node* cmp = cmps[cmpn];
2232     if (cmp == NULL)  continue;
2233     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2234       Node* bol = cmp-&gt;fast_out(j);
2235       if (!bol-&gt;is_Bool())  continue;
2236       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2237       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2238       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2239       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2240         best_bol   = bol-&gt;as_Bool();
2241         best_btest = btest;
2242       }
2243     }
2244   }
2245 
2246   Node* answer_if_true  = NULL;
2247   Node* answer_if_false = NULL;
2248   switch (best_btest) {
2249   default:
2250     if (cmpxy == NULL)
2251       cmpxy = ideal_cmpxy;
2252     best_bol = _gvn.transform(new BoolNode(cmpxy, BoolTest::lt));
2253     // and fall through:
2254   case BoolTest::lt:          // x &lt; y
2255   case BoolTest::le:          // x &lt;= y
2256     answer_if_true  = (want_max ? yvalue : xvalue);
2257     answer_if_false = (want_max ? xvalue : yvalue);
2258     break;
2259   case BoolTest::gt:          // x &gt; y
2260   case BoolTest::ge:          // x &gt;= y
2261     answer_if_true  = (want_max ? xvalue : yvalue);
2262     answer_if_false = (want_max ? yvalue : xvalue);
2263     break;
2264   }
2265 
2266   jint hi, lo;
2267   if (want_max) {
2268     // We can sharpen the minimum.
2269     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2270     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2271   } else {
2272     // We can sharpen the maximum.
2273     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2274     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2275   }
2276 
2277   // Use a flow-free graph structure, to avoid creating excess control edges
2278   // which could hinder other optimizations.
2279   // Since Math.min/max is often used with arraycopy, we want
2280   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2281   Node* cmov = CMoveNode::make(NULL, best_bol,
2282                                answer_if_false, answer_if_true,
2283                                TypeInt::make(lo, hi, widen));
2284 
2285   return _gvn.transform(cmov);
2286 
2287   /*
2288   // This is not as desirable as it may seem, since Min and Max
2289   // nodes do not have a full set of optimizations.
2290   // And they would interfere, anyway, with 'if' optimizations
2291   // and with CMoveI canonical forms.
2292   switch (id) {
2293   case vmIntrinsics::_min:
2294     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2295   case vmIntrinsics::_max:
2296     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2297   default:
2298     ShouldNotReachHere();
2299   }
2300   */
2301 }
2302 
2303 inline int
2304 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2305   const TypePtr* base_type = TypePtr::NULL_PTR;
2306   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2307   if (base_type == NULL) {
2308     // Unknown type.
2309     return Type::AnyPtr;
2310   } else if (base_type == TypePtr::NULL_PTR) {
2311     // Since this is a NULL+long form, we have to switch to a rawptr.
2312     base   = _gvn.transform(new CastX2PNode(offset));
2313     offset = MakeConX(0);
2314     return Type::RawPtr;
2315   } else if (base_type-&gt;base() == Type::RawPtr) {
2316     return Type::RawPtr;
2317   } else if (base_type-&gt;isa_oopptr()) {
2318     // Base is never null =&gt; always a heap address.
2319     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2320       return Type::OopPtr;
2321     }
2322     // Offset is small =&gt; always a heap address.
2323     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2324     if (offset_type != NULL &amp;&amp;
2325         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2326         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2327         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2328       return Type::OopPtr;
2329     }
2330     // Otherwise, it might either be oop+off or NULL+addr.
2331     return Type::AnyPtr;
2332   } else {
2333     // No information:
2334     return Type::AnyPtr;
2335   }
2336 }
2337 
2338 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2339   int kind = classify_unsafe_addr(base, offset);
2340   if (kind == Type::RawPtr) {
2341     return basic_plus_adr(top(), base, offset);
2342   } else {
2343     return basic_plus_adr(base, offset);
2344   }
2345 }
2346 
2347 //--------------------------inline_number_methods-----------------------------
2348 // inline int     Integer.numberOfLeadingZeros(int)
2349 // inline int        Long.numberOfLeadingZeros(long)
2350 //
2351 // inline int     Integer.numberOfTrailingZeros(int)
2352 // inline int        Long.numberOfTrailingZeros(long)
2353 //
2354 // inline int     Integer.bitCount(int)
2355 // inline int        Long.bitCount(long)
2356 //
2357 // inline char  Character.reverseBytes(char)
2358 // inline short     Short.reverseBytes(short)
2359 // inline int     Integer.reverseBytes(int)
2360 // inline long       Long.reverseBytes(long)
2361 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2362   Node* arg = argument(0);
2363   Node* n;
2364   switch (id) {
2365   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new CountLeadingZerosINode( arg);  break;
2366   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new CountLeadingZerosLNode( arg);  break;
2367   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new CountTrailingZerosINode(arg);  break;
2368   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new CountTrailingZerosLNode(arg);  break;
2369   case vmIntrinsics::_bitCount_i:               n = new PopCountINode(          arg);  break;
2370   case vmIntrinsics::_bitCount_l:               n = new PopCountLNode(          arg);  break;
2371   case vmIntrinsics::_reverseBytes_c:           n = new ReverseBytesUSNode(0,   arg);  break;
2372   case vmIntrinsics::_reverseBytes_s:           n = new ReverseBytesSNode( 0,   arg);  break;
2373   case vmIntrinsics::_reverseBytes_i:           n = new ReverseBytesINode( 0,   arg);  break;
2374   case vmIntrinsics::_reverseBytes_l:           n = new ReverseBytesLNode( 0,   arg);  break;
2375   default:  fatal_unexpected_iid(id);  break;
2376   }
2377   set_result(_gvn.transform(n));
2378   return true;
2379 }
2380 
2381 //----------------------------inline_unsafe_access----------------------------
2382 
2383 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2384 
2385 // Helper that guards and inserts a pre-barrier.
2386 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2387                                         Node* pre_val, bool need_mem_bar) {
2388   // We could be accessing the referent field of a reference object. If so, when G1
2389   // is enabled, we need to log the value in the referent field in an SATB buffer.
2390   // This routine performs some compile time filters and generates suitable
2391   // runtime filters that guard the pre-barrier code.
2392   // Also add memory barrier for non volatile load from the referent field
2393   // to prevent commoning of loads across safepoint.
2394   if (!UseG1GC &amp;&amp; !need_mem_bar)
2395     return;
2396 
2397   // Some compile time checks.
2398 
2399   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2400   const TypeX* otype = offset-&gt;find_intptr_t_type();
2401   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2402       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2403     // Constant offset but not the reference_offset so just return
2404     return;
2405   }
2406 
2407   // We only need to generate the runtime guards for instances.
2408   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2409   if (btype != NULL) {
2410     if (btype-&gt;isa_aryptr()) {
2411       // Array type so nothing to do
2412       return;
2413     }
2414 
2415     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2416     if (itype != NULL) {
2417       // Can the klass of base_oop be statically determined to be
2418       // _not_ a sub-class of Reference and _not_ Object?
2419       ciKlass* klass = itype-&gt;klass();
2420       if ( klass-&gt;is_loaded() &amp;&amp;
2421           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2422           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2423         return;
2424       }
2425     }
2426   }
2427 
2428   // The compile time filters did not reject base_oop/offset so
2429   // we need to generate the following runtime filters
2430   //
2431   // if (offset == java_lang_ref_Reference::_reference_offset) {
2432   //   if (instance_of(base, java.lang.ref.Reference)) {
2433   //     pre_barrier(_, pre_val, ...);
2434   //   }
2435   // }
2436 
2437   float likely   = PROB_LIKELY(  0.999);
2438   float unlikely = PROB_UNLIKELY(0.999);
2439 
2440   IdealKit ideal(this);
2441 #define __ ideal.
2442 
2443   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2444 
2445   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2446       // Update graphKit memory and control from IdealKit.
2447       sync_kit(ideal);
2448 
2449       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2450       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2451 
2452       // Update IdealKit memory and control from graphKit.
2453       __ sync_kit(this);
2454 
2455       Node* one = __ ConI(1);
2456       // is_instof == 0 if base_oop == NULL
2457       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2458 
2459         // Update graphKit from IdeakKit.
2460         sync_kit(ideal);
2461 
2462         // Use the pre-barrier to record the value in the referent field
2463         pre_barrier(false /* do_load */,
2464                     __ ctrl(),
2465                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2466                     pre_val /* pre_val */,
2467                     T_OBJECT);
2468         if (need_mem_bar) {
2469           // Add memory barrier to prevent commoning reads from this field
2470           // across safepoint since GC can change its value.
2471           insert_mem_bar(Op_MemBarCPUOrder);
2472         }
2473         // Update IdealKit from graphKit.
2474         __ sync_kit(this);
2475 
2476       } __ end_if(); // _ref_type != ref_none
2477   } __ end_if(); // offset == referent_offset
2478 
2479   // Final sync IdealKit and GraphKit.
2480   final_sync(ideal);
2481 #undef __
2482 }
2483 
2484 
2485 // Interpret Unsafe.fieldOffset cookies correctly:
2486 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2487 
2488 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2489   // Attempt to infer a sharper value type from the offset and base type.
2490   ciKlass* sharpened_klass = NULL;
2491 
2492   // See if it is an instance field, with an object type.
2493   if (alias_type-&gt;field() != NULL) {
2494     assert(!is_native_ptr, "native pointer op cannot use a java address");
2495     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2496       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2497     }
2498   }
2499 
2500   // See if it is a narrow oop array.
2501   if (adr_type-&gt;isa_aryptr()) {
2502     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2503       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2504       if (elem_type != NULL) {
2505         sharpened_klass = elem_type-&gt;klass();
2506       }
2507     }
2508   }
2509 
2510   // The sharpened class might be unloaded if there is no class loader
2511   // contraint in place.
2512   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2513     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2514 
2515 #ifndef PRODUCT
2516     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2517       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2518       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2519     }
2520 #endif
2521     // Sharpen the value type.
2522     return tjp;
2523   }
2524   return NULL;
2525 }
2526 
2527 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2528   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2529 
2530 #ifndef PRODUCT
2531   {
2532     ResourceMark rm;
2533     // Check the signatures.
2534     ciSignature* sig = callee()-&gt;signature();
2535 #ifdef ASSERT
2536     if (!is_store) {
2537       // Object getObject(Object base, int/long offset), etc.
2538       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2539       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2540           rtype = T_ADDRESS;  // it is really a C void*
2541       assert(rtype == type, "getter must return the expected value");
2542       if (!is_native_ptr) {
2543         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2544         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2545         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2546       } else {
2547         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2548         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2549       }
2550     } else {
2551       // void putObject(Object base, int/long offset, Object x), etc.
2552       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2553       if (!is_native_ptr) {
2554         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2555         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2556         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2557       } else {
2558         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2559         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2560       }
2561       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2562       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2563         vtype = T_ADDRESS;  // it is really a C void*
2564       assert(vtype == type, "putter must accept the expected value");
2565     }
2566 #endif // ASSERT
2567  }
2568 #endif //PRODUCT
2569 
2570   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2571 
2572   Node* receiver = argument(0);  // type: oop
2573 
2574   // Build address expression.
2575   Node* adr;
2576   Node* heap_base_oop = top();
2577   Node* offset = top();
2578   Node* val;
2579 
2580   if (!is_native_ptr) {
2581     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2582     Node* base = argument(1);  // type: oop
2583     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2584     offset = argument(2);  // type: long
2585     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2586     // to be plain byte offsets, which are also the same as those accepted
2587     // by oopDesc::field_base.
2588     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2589            "fieldOffset must be byte-scaled");
2590     // 32-bit machines ignore the high half!
2591     offset = ConvL2X(offset);
2592     adr = make_unsafe_address(base, offset);
2593     heap_base_oop = base;
2594     val = is_store ? argument(4) : NULL;
2595   } else {
2596     Node* ptr = argument(1);  // type: long
2597     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2598     adr = make_unsafe_address(NULL, ptr);
2599     val = is_store ? argument(3) : NULL;
2600   }
2601 
2602   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2603 
2604   // First guess at the value type.
2605   const Type *value_type = Type::get_const_basic_type(type);
2606 
2607   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2608   // there was not enough information to nail it down.
2609   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2610   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2611 
2612   // We will need memory barriers unless we can determine a unique
2613   // alias category for this reference.  (Note:  If for some reason
2614   // the barriers get omitted and the unsafe reference begins to "pollute"
2615   // the alias analysis of the rest of the graph, either Compile::can_alias
2616   // or Compile::must_alias will throw a diagnostic assert.)
2617   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2618 
2619   // If we are reading the value of the referent field of a Reference
2620   // object (either by using Unsafe directly or through reflection)
2621   // then, if G1 is enabled, we need to record the referent in an
2622   // SATB log buffer using the pre-barrier mechanism.
2623   // Also we need to add memory barrier to prevent commoning reads
2624   // from this field across safepoint since GC can change its value.
2625   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2626                            offset != top() &amp;&amp; heap_base_oop != top();
2627 
2628   if (!is_store &amp;&amp; type == T_OBJECT) {
2629     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2630     if (tjp != NULL) {
2631       value_type = tjp;
2632     }
2633   }
2634 
2635   receiver = null_check(receiver);
2636   if (stopped()) {
2637     return true;
2638   }
2639   // Heap pointers get a null-check from the interpreter,
2640   // as a courtesy.  However, this is not guaranteed by Unsafe,
2641   // and it is not possible to fully distinguish unintended nulls
2642   // from intended ones in this API.
2643 
2644   if (is_volatile) {
2645     // We need to emit leading and trailing CPU membars (see below) in
2646     // addition to memory membars when is_volatile. This is a little
2647     // too strong, but avoids the need to insert per-alias-type
2648     // volatile membars (for stores; compare Parse::do_put_xxx), which
2649     // we cannot do effectively here because we probably only have a
2650     // rough approximation of type.
2651     need_mem_bar = true;
2652     // For Stores, place a memory ordering barrier now.
2653     if (is_store) {
2654       insert_mem_bar(Op_MemBarRelease);
2655     } else {
2656       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2657         insert_mem_bar(Op_MemBarVolatile);
2658       }
2659     }
2660   }
2661 
2662   // Memory barrier to prevent normal and 'unsafe' accesses from
2663   // bypassing each other.  Happens after null checks, so the
2664   // exception paths do not take memory state from the memory barrier,
2665   // so there's no problems making a strong assert about mixing users
2666   // of safe &amp; unsafe memory.
2667   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2668 
2669   if (!is_store) {
2670     MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2671     // To be valid, unsafe loads may depend on other conditions than
2672     // the one that guards them: pin the Load node
2673     Node* p = make_load(control(), adr, value_type, type, adr_type, mo, LoadNode::Pinned, is_volatile);
2674     // load value
2675     switch (type) {
2676     case T_BOOLEAN:
2677     case T_CHAR:
2678     case T_BYTE:
2679     case T_SHORT:
2680     case T_INT:
2681     case T_LONG:
2682     case T_FLOAT:
2683     case T_DOUBLE:
2684       break;
2685     case T_OBJECT:
2686       if (need_read_barrier) {
2687         insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2688       }
2689       break;
2690     case T_ADDRESS:
2691       // Cast to an int type.
2692       p = _gvn.transform(new CastP2XNode(NULL, p));
2693       p = ConvX2UL(p);
2694       break;
2695     default:
2696       fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2697       break;
2698     }
2699     // The load node has the control of the preceding MemBarCPUOrder.  All
2700     // following nodes will have the control of the MemBarCPUOrder inserted at
2701     // the end of this method.  So, pushing the load onto the stack at a later
2702     // point is fine.
2703     set_result(p);
2704   } else {
2705     // place effect of store into memory
2706     switch (type) {
2707     case T_DOUBLE:
2708       val = dstore_rounding(val);
2709       break;
2710     case T_ADDRESS:
2711       // Repackage the long as a pointer.
2712       val = ConvL2X(val);
2713       val = _gvn.transform(new CastX2PNode(val));
2714       break;
2715     }
2716 
2717     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2718     if (type != T_OBJECT ) {
2719       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2720     } else {
2721       // Possibly an oop being stored to Java heap or native memory
2722       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2723         // oop to Java heap.
2724         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2725       } else {
2726         // We can't tell at compile time if we are storing in the Java heap or outside
2727         // of it. So we need to emit code to conditionally do the proper type of
2728         // store.
2729 
2730         IdealKit ideal(this);
2731 #define __ ideal.
2732         // QQQ who knows what probability is here??
2733         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2734           // Sync IdealKit and graphKit.
2735           sync_kit(ideal);
2736           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2737           // Update IdealKit memory.
2738           __ sync_kit(this);
2739         } __ else_(); {
2740           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2741         } __ end_if();
2742         // Final sync IdealKit and GraphKit.
2743         final_sync(ideal);
2744 #undef __
2745       }
2746     }
2747   }
2748 
2749   if (is_volatile) {
2750     if (!is_store) {
2751       insert_mem_bar(Op_MemBarAcquire);
2752     } else {
2753       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2754         insert_mem_bar(Op_MemBarVolatile);
2755       }
2756     }
2757   }
2758 
2759   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2760 
2761   return true;
2762 }
2763 
2764 //----------------------------inline_unsafe_load_store----------------------------
2765 // This method serves a couple of different customers (depending on LoadStoreKind):
2766 //
2767 // LS_cmpxchg:
2768 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2769 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2770 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2771 //
2772 // LS_xadd:
2773 //   public int  getAndAddInt( Object o, long offset, int  delta)
2774 //   public long getAndAddLong(Object o, long offset, long delta)
2775 //
2776 // LS_xchg:
2777 //   int    getAndSet(Object o, long offset, int    newValue)
2778 //   long   getAndSet(Object o, long offset, long   newValue)
2779 //   Object getAndSet(Object o, long offset, Object newValue)
2780 //
2781 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2782   // This basic scheme here is the same as inline_unsafe_access, but
2783   // differs in enough details that combining them would make the code
2784   // overly confusing.  (This is a true fact! I originally combined
2785   // them, but even I was confused by it!) As much code/comments as
2786   // possible are retained from inline_unsafe_access though to make
2787   // the correspondences clearer. - dl
2788 
2789   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2790 
2791 #ifndef PRODUCT
2792   BasicType rtype;
2793   {
2794     ResourceMark rm;
2795     // Check the signatures.
2796     ciSignature* sig = callee()-&gt;signature();
2797     rtype = sig-&gt;return_type()-&gt;basic_type();
2798     if (kind == LS_xadd || kind == LS_xchg) {
2799       // Check the signatures.
2800 #ifdef ASSERT
2801       assert(rtype == type, "get and set must return the expected type");
2802       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2803       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2804       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2805       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2806 #endif // ASSERT
2807     } else if (kind == LS_cmpxchg) {
2808       // Check the signatures.
2809 #ifdef ASSERT
2810       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2811       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2812       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2813       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2814 #endif // ASSERT
2815     } else {
2816       ShouldNotReachHere();
2817     }
2818   }
2819 #endif //PRODUCT
2820 
2821   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2822 
2823   // Get arguments:
2824   Node* receiver = NULL;
2825   Node* base     = NULL;
2826   Node* offset   = NULL;
2827   Node* oldval   = NULL;
2828   Node* newval   = NULL;
2829   if (kind == LS_cmpxchg) {
2830     const bool two_slot_type = type2size[type] == 2;
2831     receiver = argument(0);  // type: oop
2832     base     = argument(1);  // type: oop
2833     offset   = argument(2);  // type: long
2834     oldval   = argument(4);  // type: oop, int, or long
2835     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2836   } else if (kind == LS_xadd || kind == LS_xchg){
2837     receiver = argument(0);  // type: oop
2838     base     = argument(1);  // type: oop
2839     offset   = argument(2);  // type: long
2840     oldval   = NULL;
2841     newval   = argument(4);  // type: oop, int, or long
2842   }
2843 
2844   // Null check receiver.
2845   receiver = null_check(receiver);
2846   if (stopped()) {
2847     return true;
2848   }
2849 
2850   // Build field offset expression.
2851   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2852   // to be plain byte offsets, which are also the same as those accepted
2853   // by oopDesc::field_base.
2854   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2855   // 32-bit machines ignore the high half of long offsets
2856   offset = ConvL2X(offset);
2857   Node* adr = make_unsafe_address(base, offset);
2858   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2859 
2860   // For CAS, unlike inline_unsafe_access, there seems no point in
2861   // trying to refine types. Just use the coarse types here.
2862   const Type *value_type = Type::get_const_basic_type(type);
2863   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2864   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2865 
2866   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2867     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2868     if (tjp != NULL) {
2869       value_type = tjp;
2870     }
2871   }
2872 
2873   int alias_idx = C-&gt;get_alias_index(adr_type);
2874 
2875   // Memory-model-wise, a LoadStore acts like a little synchronized
2876   // block, so needs barriers on each side.  These don't translate
2877   // into actual barriers on most machines, but we still need rest of
2878   // compiler to respect ordering.
2879 
2880   insert_mem_bar(Op_MemBarRelease);
2881   insert_mem_bar(Op_MemBarCPUOrder);
2882 
2883   // 4984716: MemBars must be inserted before this
2884   //          memory node in order to avoid a false
2885   //          dependency which will confuse the scheduler.
2886   Node *mem = memory(alias_idx);
2887 
2888   // For now, we handle only those cases that actually exist: ints,
2889   // longs, and Object. Adding others should be straightforward.
2890   Node* load_store;
2891   switch(type) {
2892   case T_INT:
2893     if (kind == LS_xadd) {
2894       load_store = _gvn.transform(new GetAndAddINode(control(), mem, adr, newval, adr_type));
2895     } else if (kind == LS_xchg) {
2896       load_store = _gvn.transform(new GetAndSetINode(control(), mem, adr, newval, adr_type));
2897     } else if (kind == LS_cmpxchg) {
2898       load_store = _gvn.transform(new CompareAndSwapINode(control(), mem, adr, newval, oldval));
2899     } else {
2900       ShouldNotReachHere();
2901     }
2902     break;
2903   case T_LONG:
2904     if (kind == LS_xadd) {
2905       load_store = _gvn.transform(new GetAndAddLNode(control(), mem, adr, newval, adr_type));
2906     } else if (kind == LS_xchg) {
2907       load_store = _gvn.transform(new GetAndSetLNode(control(), mem, adr, newval, adr_type));
2908     } else if (kind == LS_cmpxchg) {
2909       load_store = _gvn.transform(new CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2910     } else {
2911       ShouldNotReachHere();
2912     }
2913     break;
2914   case T_OBJECT:
2915     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2916     // could be delayed during Parse (for example, in adjust_map_after_if()).
2917     // Execute transformation here to avoid barrier generation in such case.
2918     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2919       newval = _gvn.makecon(TypePtr::NULL_PTR);
2920 
2921     // Reference stores need a store barrier.
2922     if (kind == LS_xchg) {
2923       // If pre-barrier must execute before the oop store, old value will require do_load here.
2924       if (!can_move_pre_barrier()) {
2925         pre_barrier(true /* do_load*/,
2926                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2927                     NULL /* pre_val*/,
2928                     T_OBJECT);
2929       } // Else move pre_barrier to use load_store value, see below.
2930     } else if (kind == LS_cmpxchg) {
2931       // Same as for newval above:
2932       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2933         oldval = _gvn.makecon(TypePtr::NULL_PTR);
2934       }
2935       // The only known value which might get overwritten is oldval.
2936       pre_barrier(false /* do_load */,
2937                   control(), NULL, NULL, max_juint, NULL, NULL,
2938                   oldval /* pre_val */,
2939                   T_OBJECT);
2940     } else {
2941       ShouldNotReachHere();
2942     }
2943 
2944 #ifdef _LP64
2945     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2946       Node *newval_enc = _gvn.transform(new EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2947       if (kind == LS_xchg) {
2948         load_store = _gvn.transform(new GetAndSetNNode(control(), mem, adr,
2949                                                        newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2950       } else {
2951         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2952         Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2953         load_store = _gvn.transform(new CompareAndSwapNNode(control(), mem, adr,
2954                                                                 newval_enc, oldval_enc));
2955       }
2956     } else
2957 #endif
2958     {
2959       if (kind == LS_xchg) {
2960         load_store = _gvn.transform(new GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2961       } else {
2962         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2963         load_store = _gvn.transform(new CompareAndSwapPNode(control(), mem, adr, newval, oldval));
2964       }
2965     }
2966     post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2967     break;
2968   default:
2969     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2970     break;
2971   }
2972 
2973   // SCMemProjNodes represent the memory state of a LoadStore. Their
2974   // main role is to prevent LoadStore nodes from being optimized away
2975   // when their results aren't used.
2976   Node* proj = _gvn.transform(new SCMemProjNode(load_store));
2977   set_memory(proj, alias_idx);
2978 
2979   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
2980 #ifdef _LP64
2981     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2982       load_store = _gvn.transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
2983     }
2984 #endif
2985     if (can_move_pre_barrier()) {
2986       // Don't need to load pre_val. The old value is returned by load_store.
2987       // The pre_barrier can execute after the xchg as long as no safepoint
2988       // gets inserted between them.
2989       pre_barrier(false /* do_load */,
2990                   control(), NULL, NULL, max_juint, NULL, NULL,
2991                   load_store /* pre_val */,
2992                   T_OBJECT);
2993     }
2994   }
2995 
2996   // Add the trailing membar surrounding the access
2997   insert_mem_bar(Op_MemBarCPUOrder);
2998   insert_mem_bar(Op_MemBarAcquire);
2999 
3000   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
3001   set_result(load_store);
3002   return true;
3003 }
3004 
3005 //----------------------------inline_unsafe_ordered_store----------------------
3006 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
3007 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
3008 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
3009 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
3010   // This is another variant of inline_unsafe_access, differing in
3011   // that it always issues store-store ("release") barrier and ensures
3012   // store-atomicity (which only matters for "long").
3013 
3014   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3015 
3016 #ifndef PRODUCT
3017   {
3018     ResourceMark rm;
3019     // Check the signatures.
3020     ciSignature* sig = callee()-&gt;signature();
3021 #ifdef ASSERT
3022     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3023     assert(rtype == T_VOID, "must return void");
3024     assert(sig-&gt;count() == 3, "has 3 arguments");
3025     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3026     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3027 #endif // ASSERT
3028   }
3029 #endif //PRODUCT
3030 
3031   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3032 
3033   // Get arguments:
3034   Node* receiver = argument(0);  // type: oop
3035   Node* base     = argument(1);  // type: oop
3036   Node* offset   = argument(2);  // type: long
3037   Node* val      = argument(4);  // type: oop, int, or long
3038 
3039   // Null check receiver.
3040   receiver = null_check(receiver);
3041   if (stopped()) {
3042     return true;
3043   }
3044 
3045   // Build field offset expression.
3046   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3047   // 32-bit machines ignore the high half of long offsets
3048   offset = ConvL2X(offset);
3049   Node* adr = make_unsafe_address(base, offset);
3050   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3051   const Type *value_type = Type::get_const_basic_type(type);
3052   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3053 
3054   insert_mem_bar(Op_MemBarRelease);
3055   insert_mem_bar(Op_MemBarCPUOrder);
3056   // Ensure that the store is atomic for longs:
3057   const bool require_atomic_access = true;
3058   Node* store;
3059   if (type == T_OBJECT) // reference stores need a store barrier.
3060     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3061   else {
3062     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3063   }
3064   insert_mem_bar(Op_MemBarCPUOrder);
3065   return true;
3066 }
3067 
3068 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3069   // Regardless of form, don't allow previous ld/st to move down,
3070   // then issue acquire, release, or volatile mem_bar.
3071   insert_mem_bar(Op_MemBarCPUOrder);
3072   switch(id) {
3073     case vmIntrinsics::_loadFence:
3074       insert_mem_bar(Op_LoadFence);
3075       return true;
3076     case vmIntrinsics::_storeFence:
3077       insert_mem_bar(Op_StoreFence);
3078       return true;
3079     case vmIntrinsics::_fullFence:
3080       insert_mem_bar(Op_MemBarVolatile);
3081       return true;
3082     default:
3083       fatal_unexpected_iid(id);
3084       return false;
3085   }
3086 }
3087 
3088 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3089   if (!kls-&gt;is_Con()) {
3090     return true;
3091   }
3092   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3093   if (klsptr == NULL) {
3094     return true;
3095   }
3096   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3097   // don't need a guard for a klass that is already initialized
3098   return !ik-&gt;is_initialized();
3099 }
3100 
3101 //----------------------------inline_unsafe_allocate---------------------------
3102 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
3103 bool LibraryCallKit::inline_unsafe_allocate() {
3104   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3105 
3106   null_check_receiver();  // null-check, then ignore
3107   Node* cls = null_check(argument(1));
3108   if (stopped())  return true;
3109 
3110   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3111   kls = null_check(kls);
3112   if (stopped())  return true;  // argument was like int.class
3113 
3114   Node* test = NULL;
3115   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3116     // Note:  The argument might still be an illegal value like
3117     // Serializable.class or Object[].class.   The runtime will handle it.
3118     // But we must make an explicit check for initialization.
3119     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3120     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3121     // can generate code to load it as unsigned byte.
3122     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3123     Node* bits = intcon(InstanceKlass::fully_initialized);
3124     test = _gvn.transform(new SubINode(inst, bits));
3125     // The 'test' is non-zero if we need to take a slow path.
3126   }
3127 
3128   Node* obj = new_instance(kls, test);
3129   set_result(obj);
3130   return true;
3131 }
3132 
3133 #ifdef TRACE_HAVE_INTRINSICS
3134 /*
3135  * oop -&gt; myklass
3136  * myklass-&gt;trace_id |= USED
3137  * return myklass-&gt;trace_id &amp; ~0x3
3138  */
3139 bool LibraryCallKit::inline_native_classID() {
3140   null_check_receiver();  // null-check, then ignore
3141   Node* cls = null_check(argument(1), T_OBJECT);
3142   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3143   kls = null_check(kls, T_OBJECT);
3144   ByteSize offset = TRACE_ID_OFFSET;
3145   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3146   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3147   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3148   Node* andl = _gvn.transform(new AndLNode(tvalue, bits));
3149   Node* clsused = longcon(0x01l); // set the class bit
3150   Node* orl = _gvn.transform(new OrLNode(tvalue, clsused));
3151 
3152   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3153   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3154   set_result(andl);
3155   return true;
3156 }
3157 
3158 bool LibraryCallKit::inline_native_threadID() {
3159   Node* tls_ptr = NULL;
3160   Node* cur_thr = generate_current_thread(tls_ptr);
3161   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3162   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3163   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3164 
3165   Node* threadid = NULL;
3166   size_t thread_id_size = OSThread::thread_id_size();
3167   if (thread_id_size == (size_t) BytesPerLong) {
3168     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3169   } else if (thread_id_size == (size_t) BytesPerInt) {
3170     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3171   } else {
3172     ShouldNotReachHere();
3173   }
3174   set_result(threadid);
3175   return true;
3176 }
3177 #endif
3178 
3179 //------------------------inline_native_time_funcs--------------
3180 // inline code for System.currentTimeMillis() and System.nanoTime()
3181 // these have the same type and signature
3182 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3183   const TypeFunc* tf = OptoRuntime::void_long_Type();
3184   const TypePtr* no_memory_effects = NULL;
3185   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3186   Node* value = _gvn.transform(new ProjNode(time, TypeFunc::Parms+0));
3187 #ifdef ASSERT
3188   Node* value_top = _gvn.transform(new ProjNode(time, TypeFunc::Parms+1));
3189   assert(value_top == top(), "second value must be top");
3190 #endif
3191   set_result(value);
3192   return true;
3193 }
3194 
3195 //------------------------inline_native_currentThread------------------
3196 bool LibraryCallKit::inline_native_currentThread() {
3197   Node* junk = NULL;
3198   set_result(generate_current_thread(junk));
3199   return true;
3200 }
3201 
3202 //------------------------inline_native_isInterrupted------------------
3203 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3204 bool LibraryCallKit::inline_native_isInterrupted() {
3205   // Add a fast path to t.isInterrupted(clear_int):
3206   //   (t == Thread.current() &amp;&amp;
3207   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3208   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3209   // So, in the common case that the interrupt bit is false,
3210   // we avoid making a call into the VM.  Even if the interrupt bit
3211   // is true, if the clear_int argument is false, we avoid the VM call.
3212   // However, if the receiver is not currentThread, we must call the VM,
3213   // because there must be some locking done around the operation.
3214 
3215   // We only go to the fast case code if we pass two guards.
3216   // Paths which do not pass are accumulated in the slow_region.
3217 
3218   enum {
3219     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3220     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3221     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3222     PATH_LIMIT
3223   };
3224 
3225   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3226   // out of the function.
3227   insert_mem_bar(Op_MemBarCPUOrder);
3228 
3229   RegionNode* result_rgn = new RegionNode(PATH_LIMIT);
3230   PhiNode*    result_val = new PhiNode(result_rgn, TypeInt::BOOL);
3231 
3232   RegionNode* slow_region = new RegionNode(1);
3233   record_for_igvn(slow_region);
3234 
3235   // (a) Receiving thread must be the current thread.
3236   Node* rec_thr = argument(0);
3237   Node* tls_ptr = NULL;
3238   Node* cur_thr = generate_current_thread(tls_ptr);
3239   Node* cmp_thr = _gvn.transform(new CmpPNode(cur_thr, rec_thr));
3240   Node* bol_thr = _gvn.transform(new BoolNode(cmp_thr, BoolTest::ne));
3241 
3242   generate_slow_guard(bol_thr, slow_region);
3243 
3244   // (b) Interrupt bit on TLS must be false.
3245   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3246   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3247   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3248 
3249   // Set the control input on the field _interrupted read to prevent it floating up.
3250   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3251   Node* cmp_bit = _gvn.transform(new CmpINode(int_bit, intcon(0)));
3252   Node* bol_bit = _gvn.transform(new BoolNode(cmp_bit, BoolTest::ne));
3253 
3254   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3255 
3256   // First fast path:  if (!TLS._interrupted) return false;
3257   Node* false_bit = _gvn.transform(new IfFalseNode(iff_bit));
3258   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3259   result_val-&gt;init_req(no_int_result_path, intcon(0));
3260 
3261   // drop through to next case
3262   set_control( _gvn.transform(new IfTrueNode(iff_bit)));
3263 
3264 #ifndef TARGET_OS_FAMILY_windows
3265   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3266   Node* clr_arg = argument(1);
3267   Node* cmp_arg = _gvn.transform(new CmpINode(clr_arg, intcon(0)));
3268   Node* bol_arg = _gvn.transform(new BoolNode(cmp_arg, BoolTest::ne));
3269   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3270 
3271   // Second fast path:  ... else if (!clear_int) return true;
3272   Node* false_arg = _gvn.transform(new IfFalseNode(iff_arg));
3273   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3274   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3275 
3276   // drop through to next case
3277   set_control( _gvn.transform(new IfTrueNode(iff_arg)));
3278 #else
3279   // To return true on Windows you must read the _interrupted field
3280   // and check the the event state i.e. take the slow path.
3281 #endif // TARGET_OS_FAMILY_windows
3282 
3283   // (d) Otherwise, go to the slow path.
3284   slow_region-&gt;add_req(control());
3285   set_control( _gvn.transform(slow_region));
3286 
3287   if (stopped()) {
3288     // There is no slow path.
3289     result_rgn-&gt;init_req(slow_result_path, top());
3290     result_val-&gt;init_req(slow_result_path, top());
3291   } else {
3292     // non-virtual because it is a private non-static
3293     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3294 
3295     Node* slow_val = set_results_for_java_call(slow_call);
3296     // this-&gt;control() comes from set_results_for_java_call
3297 
3298     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3299     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3300 
3301     // These two phis are pre-filled with copies of of the fast IO and Memory
3302     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3303     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3304 
3305     result_rgn-&gt;init_req(slow_result_path, control());
3306     result_io -&gt;init_req(slow_result_path, i_o());
3307     result_mem-&gt;init_req(slow_result_path, reset_memory());
3308     result_val-&gt;init_req(slow_result_path, slow_val);
3309 
3310     set_all_memory(_gvn.transform(result_mem));
3311     set_i_o(       _gvn.transform(result_io));
3312   }
3313 
3314   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3315   set_result(result_rgn, result_val);
3316   return true;
3317 }
3318 
3319 //---------------------------load_mirror_from_klass----------------------------
3320 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3321 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3322   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3323   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3324 }
3325 
3326 //-----------------------load_klass_from_mirror_common-------------------------
3327 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3328 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3329 // and branch to the given path on the region.
3330 // If never_see_null, take an uncommon trap on null, so we can optimistically
3331 // compile for the non-null case.
3332 // If the region is NULL, force never_see_null = true.
3333 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3334                                                     bool never_see_null,
3335                                                     RegionNode* region,
3336                                                     int null_path,
3337                                                     int offset) {
3338   if (region == NULL)  never_see_null = true;
3339   Node* p = basic_plus_adr(mirror, offset);
3340   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3341   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3342   Node* null_ctl = top();
3343   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3344   if (region != NULL) {
3345     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3346     region-&gt;init_req(null_path, null_ctl);
3347   } else {
3348     assert(null_ctl == top(), "no loose ends");
3349   }
3350   return kls;
3351 }
3352 
3353 //--------------------(inline_native_Class_query helpers)---------------------
3354 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3355 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3356 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3357   // Branch around if the given klass has the given modifier bit set.
3358   // Like generate_guard, adds a new path onto the region.
3359   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3360   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3361   Node* mask = intcon(modifier_mask);
3362   Node* bits = intcon(modifier_bits);
3363   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3364   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3365   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3366   return generate_fair_guard(bol, region);
3367 }
3368 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3369   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3370 }
3371 
3372 //-------------------------inline_native_Class_query-------------------
3373 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3374   const Type* return_type = TypeInt::BOOL;
3375   Node* prim_return_value = top();  // what happens if it's a primitive class?
3376   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3377   bool expect_prim = false;     // most of these guys expect to work on refs
3378 
3379   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3380 
3381   Node* mirror = argument(0);
3382   Node* obj    = top();
3383 
3384   switch (id) {
3385   case vmIntrinsics::_isInstance:
3386     // nothing is an instance of a primitive type
3387     prim_return_value = intcon(0);
3388     obj = argument(1);
3389     break;
3390   case vmIntrinsics::_getModifiers:
3391     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3392     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3393     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3394     break;
3395   case vmIntrinsics::_isInterface:
3396     prim_return_value = intcon(0);
3397     break;
3398   case vmIntrinsics::_isArray:
3399     prim_return_value = intcon(0);
3400     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3401     break;
3402   case vmIntrinsics::_isPrimitive:
3403     prim_return_value = intcon(1);
3404     expect_prim = true;  // obviously
3405     break;
3406   case vmIntrinsics::_getSuperclass:
3407     prim_return_value = null();
3408     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3409     break;
3410   case vmIntrinsics::_getClassAccessFlags:
3411     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3412     return_type = TypeInt::INT;  // not bool!  6297094
3413     break;
3414   default:
3415     fatal_unexpected_iid(id);
3416     break;
3417   }
3418 
3419   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3420   if (mirror_con == NULL)  return false;  // cannot happen?
3421 
3422 #ifndef PRODUCT
3423   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3424     ciType* k = mirror_con-&gt;java_mirror_type();
3425     if (k) {
3426       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3427       k-&gt;print_name();
3428       tty-&gt;cr();
3429     }
3430   }
3431 #endif
3432 
3433   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3434   RegionNode* region = new RegionNode(PATH_LIMIT);
3435   record_for_igvn(region);
3436   PhiNode* phi = new PhiNode(region, return_type);
3437 
3438   // The mirror will never be null of Reflection.getClassAccessFlags, however
3439   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3440   // if it is. See bug 4774291.
3441 
3442   // For Reflection.getClassAccessFlags(), the null check occurs in
3443   // the wrong place; see inline_unsafe_access(), above, for a similar
3444   // situation.
3445   mirror = null_check(mirror);
3446   // If mirror or obj is dead, only null-path is taken.
3447   if (stopped())  return true;
3448 
3449   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3450 
3451   // Now load the mirror's klass metaobject, and null-check it.
3452   // Side-effects region with the control path if the klass is null.
3453   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3454   // If kls is null, we have a primitive mirror.
3455   phi-&gt;init_req(_prim_path, prim_return_value);
3456   if (stopped()) { set_result(region, phi); return true; }
3457   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3458 
3459   Node* p;  // handy temp
3460   Node* null_ctl;
3461 
3462   // Now that we have the non-null klass, we can perform the real query.
3463   // For constant classes, the query will constant-fold in LoadNode::Value.
3464   Node* query_value = top();
3465   switch (id) {
3466   case vmIntrinsics::_isInstance:
3467     // nothing is an instance of a primitive type
3468     query_value = gen_instanceof(obj, kls, safe_for_replace);
3469     break;
3470 
3471   case vmIntrinsics::_getModifiers:
3472     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3473     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3474     break;
3475 
3476   case vmIntrinsics::_isInterface:
3477     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3478     if (generate_interface_guard(kls, region) != NULL)
3479       // A guard was added.  If the guard is taken, it was an interface.
3480       phi-&gt;add_req(intcon(1));
3481     // If we fall through, it's a plain class.
3482     query_value = intcon(0);
3483     break;
3484 
3485   case vmIntrinsics::_isArray:
3486     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3487     if (generate_array_guard(kls, region) != NULL)
3488       // A guard was added.  If the guard is taken, it was an array.
3489       phi-&gt;add_req(intcon(1));
3490     // If we fall through, it's a plain class.
3491     query_value = intcon(0);
3492     break;
3493 
3494   case vmIntrinsics::_isPrimitive:
3495     query_value = intcon(0); // "normal" path produces false
3496     break;
3497 
3498   case vmIntrinsics::_getSuperclass:
3499     // The rules here are somewhat unfortunate, but we can still do better
3500     // with random logic than with a JNI call.
3501     // Interfaces store null or Object as _super, but must report null.
3502     // Arrays store an intermediate super as _super, but must report Object.
3503     // Other types can report the actual _super.
3504     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3505     if (generate_interface_guard(kls, region) != NULL)
3506       // A guard was added.  If the guard is taken, it was an interface.
3507       phi-&gt;add_req(null());
3508     if (generate_array_guard(kls, region) != NULL)
3509       // A guard was added.  If the guard is taken, it was an array.
3510       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3511     // If we fall through, it's a plain class.  Get its _super.
3512     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3513     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3514     null_ctl = top();
3515     kls = null_check_oop(kls, &amp;null_ctl);
3516     if (null_ctl != top()) {
3517       // If the guard is taken, Object.superClass is null (both klass and mirror).
3518       region-&gt;add_req(null_ctl);
3519       phi   -&gt;add_req(null());
3520     }
3521     if (!stopped()) {
3522       query_value = load_mirror_from_klass(kls);
3523     }
3524     break;
3525 
3526   case vmIntrinsics::_getClassAccessFlags:
3527     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3528     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3529     break;
3530 
3531   default:
3532     fatal_unexpected_iid(id);
3533     break;
3534   }
3535 
3536   // Fall-through is the normal case of a query to a real class.
3537   phi-&gt;init_req(1, query_value);
3538   region-&gt;init_req(1, control());
3539 
3540   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3541   set_result(region, phi);
3542   return true;
3543 }
3544 
3545 //-------------------------inline_Class_cast-------------------
3546 bool LibraryCallKit::inline_Class_cast() {
3547   Node* mirror = argument(0); // Class
3548   Node* obj    = argument(1);
3549   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3550   if (mirror_con == NULL) {
3551     return false;  // dead path (mirror-&gt;is_top()).
3552   }
3553   if (obj == NULL || obj-&gt;is_top()) {
3554     return false;  // dead path
3555   }
3556   const TypeOopPtr* tp = _gvn.type(obj)-&gt;isa_oopptr();
3557 
3558   // First, see if Class.cast() can be folded statically.
3559   // java_mirror_type() returns non-null for compile-time Class constants.
3560   ciType* tm = mirror_con-&gt;java_mirror_type();
3561   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp;
3562       tp != NULL &amp;&amp; tp-&gt;klass() != NULL) {
3563     if (!tp-&gt;klass()-&gt;is_loaded()) {
3564       // Don't use intrinsic when class is not loaded.
3565       return false;
3566     } else {
3567       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), tp-&gt;klass());
3568       if (static_res == Compile::SSC_always_true) {
3569         // isInstance() is true - fold the code.
3570         set_result(obj);
3571         return true;
3572       } else if (static_res == Compile::SSC_always_false) {
3573         // Don't use intrinsic, have to throw ClassCastException.
3574         // If the reference is null, the non-intrinsic bytecode will
3575         // be optimized appropriately.
3576         return false;
3577       }
3578     }
3579   }
3580 
3581   // Bailout intrinsic and do normal inlining if exception path is frequent.
3582   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3583     return false;
3584   }
3585 
3586   // Generate dynamic checks.
3587   // Class.cast() is java implementation of _checkcast bytecode.
3588   // Do checkcast (Parse::do_checkcast()) optimizations here.
3589 
3590   mirror = null_check(mirror);
3591   // If mirror is dead, only null-path is taken.
3592   if (stopped()) {
3593     return true;
3594   }
3595 
3596   // Not-subtype or the mirror's klass ptr is NULL (in case it is a primitive).
3597   enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };
3598   RegionNode* region = new RegionNode(PATH_LIMIT);
3599   record_for_igvn(region);
3600 
3601   // Now load the mirror's klass metaobject, and null-check it.
3602   // If kls is null, we have a primitive mirror and
3603   // nothing is an instance of a primitive type.
3604   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3605 
3606   Node* res = top();
3607   if (!stopped()) {
3608     Node* bad_type_ctrl = top();
3609     // Do checkcast optimizations.
3610     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3611     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3612   }
3613   if (region-&gt;in(_prim_path) != top() ||
3614       region-&gt;in(_bad_type_path) != top()) {
3615     // Let Interpreter throw ClassCastException.
3616     PreserveJVMState pjvms(this);
3617     set_control(_gvn.transform(region));
3618     uncommon_trap(Deoptimization::Reason_intrinsic,
3619                   Deoptimization::Action_maybe_recompile);
3620   }
3621   if (!stopped()) {
3622     set_result(res);
3623   }
3624   return true;
3625 }
3626 
3627 
3628 //--------------------------inline_native_subtype_check------------------------
3629 // This intrinsic takes the JNI calls out of the heart of
3630 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3631 bool LibraryCallKit::inline_native_subtype_check() {
3632   // Pull both arguments off the stack.
3633   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3634   args[0] = argument(0);
3635   args[1] = argument(1);
3636   Node* klasses[2];             // corresponding Klasses: superk, subk
3637   klasses[0] = klasses[1] = top();
3638 
3639   enum {
3640     // A full decision tree on {superc is prim, subc is prim}:
3641     _prim_0_path = 1,           // {P,N} =&gt; false
3642                                 // {P,P} &amp; superc!=subc =&gt; false
3643     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3644     _prim_1_path,               // {N,P} =&gt; false
3645     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3646     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3647     PATH_LIMIT
3648   };
3649 
3650   RegionNode* region = new RegionNode(PATH_LIMIT);
3651   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3652   record_for_igvn(region);
3653 
3654   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3655   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3656   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3657 
3658   // First null-check both mirrors and load each mirror's klass metaobject.
3659   int which_arg;
3660   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3661     Node* arg = args[which_arg];
3662     arg = null_check(arg);
3663     if (stopped())  break;
3664     args[which_arg] = arg;
3665 
3666     Node* p = basic_plus_adr(arg, class_klass_offset);
3667     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3668     klasses[which_arg] = _gvn.transform(kls);
3669   }
3670 
3671   // Having loaded both klasses, test each for null.
3672   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3673   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3674     Node* kls = klasses[which_arg];
3675     Node* null_ctl = top();
3676     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3677     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3678     region-&gt;init_req(prim_path, null_ctl);
3679     if (stopped())  break;
3680     klasses[which_arg] = kls;
3681   }
3682 
3683   if (!stopped()) {
3684     // now we have two reference types, in klasses[0..1]
3685     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3686     Node* superk = klasses[0];  // the receiver
3687     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3688     // now we have a successful reference subtype check
3689     region-&gt;set_req(_ref_subtype_path, control());
3690   }
3691 
3692   // If both operands are primitive (both klasses null), then
3693   // we must return true when they are identical primitives.
3694   // It is convenient to test this after the first null klass check.
3695   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3696   if (!stopped()) {
3697     // Since superc is primitive, make a guard for the superc==subc case.
3698     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3699     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
3700     generate_guard(bol_eq, region, PROB_FAIR);
3701     if (region-&gt;req() == PATH_LIMIT+1) {
3702       // A guard was added.  If the added guard is taken, superc==subc.
3703       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3704       region-&gt;del_req(PATH_LIMIT);
3705     }
3706     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3707   }
3708 
3709   // these are the only paths that produce 'true':
3710   phi-&gt;set_req(_prim_same_path,   intcon(1));
3711   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3712 
3713   // pull together the cases:
3714   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3715   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3716     Node* ctl = region-&gt;in(i);
3717     if (ctl == NULL || ctl == top()) {
3718       region-&gt;set_req(i, top());
3719       phi   -&gt;set_req(i, top());
3720     } else if (phi-&gt;in(i) == NULL) {
3721       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3722     }
3723   }
3724 
3725   set_control(_gvn.transform(region));
3726   set_result(_gvn.transform(phi));
3727   return true;
3728 }
3729 
3730 //---------------------generate_array_guard_common------------------------
3731 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3732                                                   bool obj_array, bool not_array) {
3733 
3734   if (stopped()) {
3735     return NULL;
3736   }
3737 
3738   // If obj_array/non_array==false/false:
3739   // Branch around if the given klass is in fact an array (either obj or prim).
3740   // If obj_array/non_array==false/true:
3741   // Branch around if the given klass is not an array klass of any kind.
3742   // If obj_array/non_array==true/true:
3743   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3744   // If obj_array/non_array==true/false:
3745   // Branch around if the kls is an oop array (Object[] or subtype)
3746   //
3747   // Like generate_guard, adds a new path onto the region.
3748   jint  layout_con = 0;
3749   Node* layout_val = get_layout_helper(kls, layout_con);
3750   if (layout_val == NULL) {
3751     bool query = (obj_array
3752                   ? Klass::layout_helper_is_objArray(layout_con)
3753                   : Klass::layout_helper_is_array(layout_con));
3754     if (query == not_array) {
3755       return NULL;                       // never a branch
3756     } else {                             // always a branch
3757       Node* always_branch = control();
3758       if (region != NULL)
3759         region-&gt;add_req(always_branch);
3760       set_control(top());
3761       return always_branch;
3762     }
3763   }
3764   // Now test the correct condition.
3765   jint  nval = (obj_array
3766                 ? ((jint)Klass::_lh_array_tag_type_value
3767                    &lt;&lt;    Klass::_lh_array_tag_shift)
3768                 : Klass::_lh_neutral_value);
3769   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3770   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3771   // invert the test if we are looking for a non-array
3772   if (not_array)  btest = BoolTest(btest).negate();
3773   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3774   return generate_fair_guard(bol, region);
3775 }
3776 
3777 
3778 //-----------------------inline_native_newArray--------------------------
3779 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3780 bool LibraryCallKit::inline_native_newArray() {
3781   Node* mirror    = argument(0);
3782   Node* count_val = argument(1);
3783 
3784   mirror = null_check(mirror);
3785   // If mirror or obj is dead, only null-path is taken.
3786   if (stopped())  return true;
3787 
3788   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3789   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3790   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3791   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3792   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3793 
3794   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3795   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3796                                                   result_reg, _slow_path);
3797   Node* normal_ctl   = control();
3798   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3799 
3800   // Generate code for the slow case.  We make a call to newArray().
3801   set_control(no_array_ctl);
3802   if (!stopped()) {
3803     // Either the input type is void.class, or else the
3804     // array klass has not yet been cached.  Either the
3805     // ensuing call will throw an exception, or else it
3806     // will cache the array klass for next time.
3807     PreserveJVMState pjvms(this);
3808     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3809     Node* slow_result = set_results_for_java_call(slow_call);
3810     // this-&gt;control() comes from set_results_for_java_call
3811     result_reg-&gt;set_req(_slow_path, control());
3812     result_val-&gt;set_req(_slow_path, slow_result);
3813     result_io -&gt;set_req(_slow_path, i_o());
3814     result_mem-&gt;set_req(_slow_path, reset_memory());
3815   }
3816 
3817   set_control(normal_ctl);
3818   if (!stopped()) {
3819     // Normal case:  The array type has been cached in the java.lang.Class.
3820     // The following call works fine even if the array type is polymorphic.
3821     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3822     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3823     result_reg-&gt;init_req(_normal_path, control());
3824     result_val-&gt;init_req(_normal_path, obj);
3825     result_io -&gt;init_req(_normal_path, i_o());
3826     result_mem-&gt;init_req(_normal_path, reset_memory());
3827   }
3828 
3829   // Return the combined state.
3830   set_i_o(        _gvn.transform(result_io)  );
3831   set_all_memory( _gvn.transform(result_mem));
3832 
3833   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3834   set_result(result_reg, result_val);
3835   return true;
3836 }
3837 
3838 //----------------------inline_native_getLength--------------------------
3839 // public static native int java.lang.reflect.Array.getLength(Object array);
3840 bool LibraryCallKit::inline_native_getLength() {
3841   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3842 
3843   Node* array = null_check(argument(0));
3844   // If array is dead, only null-path is taken.
3845   if (stopped())  return true;
3846 
3847   // Deoptimize if it is a non-array.
3848   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3849 
3850   if (non_array != NULL) {
3851     PreserveJVMState pjvms(this);
3852     set_control(non_array);
3853     uncommon_trap(Deoptimization::Reason_intrinsic,
3854                   Deoptimization::Action_maybe_recompile);
3855   }
3856 
3857   // If control is dead, only non-array-path is taken.
3858   if (stopped())  return true;
3859 
3860   // The works fine even if the array type is polymorphic.
3861   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3862   Node* result = load_array_length(array);
3863 
3864   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3865   set_result(result);
3866   return true;
3867 }
3868 
3869 //------------------------inline_array_copyOf----------------------------
3870 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3871 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3872 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3873   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3874 
3875   // Get the arguments.
3876   Node* original          = argument(0);
3877   Node* start             = is_copyOfRange? argument(1): intcon(0);
3878   Node* end               = is_copyOfRange? argument(2): argument(1);
3879   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3880 
3881   Node* newcopy;
3882 
3883   // Set the original stack and the reexecute bit for the interpreter to reexecute
3884   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3885   { PreserveReexecuteState preexecs(this);
3886     jvms()-&gt;set_should_reexecute(true);
3887 
3888     array_type_mirror = null_check(array_type_mirror);
3889     original          = null_check(original);
3890 
3891     // Check if a null path was taken unconditionally.
3892     if (stopped())  return true;
3893 
3894     Node* orig_length = load_array_length(original);
3895 
3896     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3897     klass_node = null_check(klass_node);
3898 
3899     RegionNode* bailout = new RegionNode(1);
3900     record_for_igvn(bailout);
3901 
3902     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3903     // Bail out if that is so.
3904     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3905     if (not_objArray != NULL) {
3906       // Improve the klass node's type from the new optimistic assumption:
3907       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3908       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3909       Node* cast = new CastPPNode(klass_node, akls);
3910       cast-&gt;init_req(0, control());
3911       klass_node = _gvn.transform(cast);
3912     }
3913 
3914     // Bail out if either start or end is negative.
3915     generate_negative_guard(start, bailout, &amp;start);
3916     generate_negative_guard(end,   bailout, &amp;end);
3917 
3918     Node* length = end;
3919     if (_gvn.type(start) != TypeInt::ZERO) {
3920       length = _gvn.transform(new SubINode(end, start));
3921     }
3922 
3923     // Bail out if length is negative.
3924     // Without this the new_array would throw
3925     // NegativeArraySizeException but IllegalArgumentException is what
3926     // should be thrown
3927     generate_negative_guard(length, bailout, &amp;length);
3928 
3929     if (bailout-&gt;req() &gt; 1) {
3930       PreserveJVMState pjvms(this);
3931       set_control(_gvn.transform(bailout));
3932       uncommon_trap(Deoptimization::Reason_intrinsic,
3933                     Deoptimization::Action_maybe_recompile);
3934     }
3935 
3936     if (!stopped()) {
3937       // How many elements will we copy from the original?
3938       // The answer is MinI(orig_length - start, length).
3939       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
3940       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3941 
3942       // Generate a direct call to the right arraycopy function(s).
3943       // We know the copy is disjoint but we might not know if the
3944       // oop stores need checking.
3945       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3946       // This will fail a store-check if x contains any non-nulls.
3947 
3948       // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3949       // loads/stores but it is legal only if we're sure the
3950       // Arrays.copyOf would succeed. So we need all input arguments
3951       // to the copyOf to be validated, including that the copy to the
3952       // new array won't trigger an ArrayStoreException. That subtype
3953       // check can be optimized if we know something on the type of
3954       // the input array from type speculation.
3955       if (_gvn.type(klass_node)-&gt;singleton()) {
3956         ciKlass* subk   = _gvn.type(load_object_klass(original))-&gt;is_klassptr()-&gt;klass();
3957         ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3958 
3959         int test = C-&gt;static_subtype_check(superk, subk);
3960         if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3961           const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3962           if (t_original-&gt;speculative_type() != NULL) {
3963             original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3964           }
3965         }
3966       }
3967 
3968       bool validated = false;
3969       // Reason_class_check rather than Reason_intrinsic because we
3970       // want to intrinsify even if this traps.
3971       if (!too_many_traps(Deoptimization::Reason_class_check)) {
3972         Node* not_subtype_ctrl = gen_subtype_check(load_object_klass(original),
3973                                                    klass_node);
3974 
3975         if (not_subtype_ctrl != top()) {
3976           PreserveJVMState pjvms(this);
3977           set_control(not_subtype_ctrl);
3978           uncommon_trap(Deoptimization::Reason_class_check,
3979                         Deoptimization::Action_make_not_entrant);
3980           assert(stopped(), "Should be stopped");
3981         }
3982         validated = true;
3983       }
3984 
3985       if (!stopped()) {
3986         newcopy = new_array(klass_node, length, 0);  // no arguments to push
3987 
3988         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true,
3989                                                 load_object_klass(original), klass_node);
3990         if (!is_copyOfRange) {
3991           ac-&gt;set_copyof(validated);
3992         } else {
3993           ac-&gt;set_copyofrange(validated);
3994         }
3995         Node* n = _gvn.transform(ac);
3996         if (n == ac) {
3997           ac-&gt;connect_outputs(this);
3998         } else {
3999           assert(validated, "shouldn't transform if all arguments not validated");
4000           set_all_memory(n);
4001         }
4002       }
4003     }
4004   } // original reexecute is set back here
4005 
4006   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4007   if (!stopped()) {
4008     set_result(newcopy);
4009   }
4010   return true;
4011 }
4012 
4013 
4014 //----------------------generate_virtual_guard---------------------------
4015 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
4016 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
4017                                              RegionNode* slow_region) {
4018   ciMethod* method = callee();
4019   int vtable_index = method-&gt;vtable_index();
4020   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4021          err_msg_res("bad index %d", vtable_index));
4022   // Get the Method* out of the appropriate vtable entry.
4023   int entry_offset  = (InstanceKlass::vtable_start_offset() +
4024                      vtable_index*vtableEntry::size()) * wordSize +
4025                      vtableEntry::method_offset_in_bytes();
4026   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
4027   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4028 
4029   // Compare the target method with the expected method (e.g., Object.hashCode).
4030   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
4031 
4032   Node* native_call = makecon(native_call_addr);
4033   Node* chk_native  = _gvn.transform(new CmpPNode(target_call, native_call));
4034   Node* test_native = _gvn.transform(new BoolNode(chk_native, BoolTest::ne));
4035 
4036   return generate_slow_guard(test_native, slow_region);
4037 }
4038 
4039 //-----------------------generate_method_call----------------------------
4040 // Use generate_method_call to make a slow-call to the real
4041 // method if the fast path fails.  An alternative would be to
4042 // use a stub like OptoRuntime::slow_arraycopy_Java.
4043 // This only works for expanding the current library call,
4044 // not another intrinsic.  (E.g., don't use this for making an
4045 // arraycopy call inside of the copyOf intrinsic.)
4046 CallJavaNode*
4047 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
4048   // When compiling the intrinsic method itself, do not use this technique.
4049   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
4050 
4051   ciMethod* method = callee();
4052   // ensure the JVMS we have will be correct for this call
4053   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
4054 
4055   const TypeFunc* tf = TypeFunc::make(method);
4056   CallJavaNode* slow_call;
4057   if (is_static) {
4058     assert(!is_virtual, "");
4059     slow_call = new CallStaticJavaNode(C, tf,
4060                            SharedRuntime::get_resolve_static_call_stub(),
4061                            method, bci());
4062   } else if (is_virtual) {
4063     null_check_receiver();
4064     int vtable_index = Method::invalid_vtable_index;
4065     if (UseInlineCaches) {
4066       // Suppress the vtable call
4067     } else {
4068       // hashCode and clone are not a miranda methods,
4069       // so the vtable index is fixed.
4070       // No need to use the linkResolver to get it.
4071        vtable_index = method-&gt;vtable_index();
4072        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4073               err_msg_res("bad index %d", vtable_index));
4074     }
4075     slow_call = new CallDynamicJavaNode(tf,
4076                           SharedRuntime::get_resolve_virtual_call_stub(),
4077                           method, vtable_index, bci());
4078   } else {  // neither virtual nor static:  opt_virtual
4079     null_check_receiver();
4080     slow_call = new CallStaticJavaNode(C, tf,
4081                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
4082                                 method, bci());
4083     slow_call-&gt;set_optimized_virtual(true);
4084   }
4085   set_arguments_for_java_call(slow_call);
4086   set_edges_for_java_call(slow_call);
4087   return slow_call;
4088 }
4089 
4090 
4091 /**
4092  * Build special case code for calls to hashCode on an object. This call may
4093  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4094  * slightly different code.
4095  */
4096 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4097   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
4098   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
4099 
4100   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4101 
4102   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4103   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
4104   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
4105   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4106   Node* obj = NULL;
4107   if (!is_static) {
4108     // Check for hashing null object
4109     obj = null_check_receiver();
4110     if (stopped())  return true;        // unconditionally null
4111     result_reg-&gt;init_req(_null_path, top());
4112     result_val-&gt;init_req(_null_path, top());
4113   } else {
4114     // Do a null check, and return zero if null.
4115     // System.identityHashCode(null) == 0
4116     obj = argument(0);
4117     Node* null_ctl = top();
4118     obj = null_check_oop(obj, &amp;null_ctl);
4119     result_reg-&gt;init_req(_null_path, null_ctl);
4120     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4121   }
4122 
4123   // Unconditionally null?  Then return right away.
4124   if (stopped()) {
4125     set_control( result_reg-&gt;in(_null_path));
4126     if (!stopped())
4127       set_result(result_val-&gt;in(_null_path));
4128     return true;
4129   }
4130 
4131   // We only go to the fast case code if we pass a number of guards.  The
4132   // paths which do not pass are accumulated in the slow_region.
4133   RegionNode* slow_region = new RegionNode(1);
4134   record_for_igvn(slow_region);
4135 
4136   // If this is a virtual call, we generate a funny guard.  We pull out
4137   // the vtable entry corresponding to hashCode() from the target object.
4138   // If the target method which we are calling happens to be the native
4139   // Object hashCode() method, we pass the guard.  We do not need this
4140   // guard for non-virtual calls -- the caller is known to be the native
4141   // Object hashCode().
4142   if (is_virtual) {
4143     // After null check, get the object's klass.
4144     Node* obj_klass = load_object_klass(obj);
4145     generate_virtual_guard(obj_klass, slow_region);
4146   }
4147 
4148   // Get the header out of the object, use LoadMarkNode when available
4149   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4150   // The control of the load must be NULL. Otherwise, the load can move before
4151   // the null check after castPP removal.
4152   Node* no_ctrl = NULL;
4153   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4154 
4155   // Test the header to see if it is unlocked.
4156   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4157   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
4158   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4159   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
4160   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
4161 
4162   generate_slow_guard(test_unlocked, slow_region);
4163 
4164   // Get the hash value and check to see that it has been properly assigned.
4165   // We depend on hash_mask being at most 32 bits and avoid the use of
4166   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4167   // vm: see markOop.hpp.
4168   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4169   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4170   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
4171   // This hack lets the hash bits live anywhere in the mark object now, as long
4172   // as the shift drops the relevant bits into the low 32 bits.  Note that
4173   // Java spec says that HashCode is an int so there's no point in capturing
4174   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4175   hshifted_header      = ConvX2I(hshifted_header);
4176   Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));
4177 
4178   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4179   Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));
4180   Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));
4181 
4182   generate_slow_guard(test_assigned, slow_region);
4183 
4184   Node* init_mem = reset_memory();
4185   // fill in the rest of the null path:
4186   result_io -&gt;init_req(_null_path, i_o());
4187   result_mem-&gt;init_req(_null_path, init_mem);
4188 
4189   result_val-&gt;init_req(_fast_path, hash_val);
4190   result_reg-&gt;init_req(_fast_path, control());
4191   result_io -&gt;init_req(_fast_path, i_o());
4192   result_mem-&gt;init_req(_fast_path, init_mem);
4193 
4194   // Generate code for the slow case.  We make a call to hashCode().
4195   set_control(_gvn.transform(slow_region));
4196   if (!stopped()) {
4197     // No need for PreserveJVMState, because we're using up the present state.
4198     set_all_memory(init_mem);
4199     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4200     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4201     Node* slow_result = set_results_for_java_call(slow_call);
4202     // this-&gt;control() comes from set_results_for_java_call
4203     result_reg-&gt;init_req(_slow_path, control());
4204     result_val-&gt;init_req(_slow_path, slow_result);
4205     result_io  -&gt;set_req(_slow_path, i_o());
4206     result_mem -&gt;set_req(_slow_path, reset_memory());
4207   }
4208 
4209   // Return the combined state.
4210   set_i_o(        _gvn.transform(result_io)  );
4211   set_all_memory( _gvn.transform(result_mem));
4212 
4213   set_result(result_reg, result_val);
4214   return true;
4215 }
4216 
4217 //---------------------------inline_native_getClass----------------------------
4218 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4219 //
4220 // Build special case code for calls to getClass on an object.
4221 bool LibraryCallKit::inline_native_getClass() {
4222   Node* obj = null_check_receiver();
4223   if (stopped())  return true;
4224   set_result(load_mirror_from_klass(load_object_klass(obj)));
4225   return true;
4226 }
4227 
4228 //-----------------inline_native_Reflection_getCallerClass---------------------
4229 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4230 //
4231 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4232 //
4233 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4234 // in that it must skip particular security frames and checks for
4235 // caller sensitive methods.
4236 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4237 #ifndef PRODUCT
4238   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4239     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4240   }
4241 #endif
4242 
4243   if (!jvms()-&gt;has_method()) {
4244 #ifndef PRODUCT
4245     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4246       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4247     }
4248 #endif
4249     return false;
4250   }
4251 
4252   // Walk back up the JVM state to find the caller at the required
4253   // depth.
4254   JVMState* caller_jvms = jvms();
4255 
4256   // Cf. JVM_GetCallerClass
4257   // NOTE: Start the loop at depth 1 because the current JVM state does
4258   // not include the Reflection.getCallerClass() frame.
4259   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4260     ciMethod* m = caller_jvms-&gt;method();
4261     switch (n) {
4262     case 0:
4263       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4264       break;
4265     case 1:
4266       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4267       if (!m-&gt;caller_sensitive()) {
4268 #ifndef PRODUCT
4269         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4270           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4271         }
4272 #endif
4273         return false;  // bail-out; let JVM_GetCallerClass do the work
4274       }
4275       break;
4276     default:
4277       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4278         // We have reached the desired frame; return the holder class.
4279         // Acquire method holder as java.lang.Class and push as constant.
4280         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4281         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4282         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4283 
4284 #ifndef PRODUCT
4285         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4286           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4287           tty-&gt;print_cr("  JVM state at this point:");
4288           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4289             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4290             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4291           }
4292         }
4293 #endif
4294         return true;
4295       }
4296       break;
4297     }
4298   }
4299 
4300 #ifndef PRODUCT
4301   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4302     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4303     tty-&gt;print_cr("  JVM state at this point:");
4304     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4305       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4306       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4307     }
4308   }
4309 #endif
4310 
4311   return false;  // bail-out; let JVM_GetCallerClass do the work
4312 }
4313 
4314 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4315   Node* arg = argument(0);
4316   Node* result;
4317 
4318   switch (id) {
4319   case vmIntrinsics::_floatToRawIntBits:    result = new MoveF2INode(arg);  break;
4320   case vmIntrinsics::_intBitsToFloat:       result = new MoveI2FNode(arg);  break;
4321   case vmIntrinsics::_doubleToRawLongBits:  result = new MoveD2LNode(arg);  break;
4322   case vmIntrinsics::_longBitsToDouble:     result = new MoveL2DNode(arg);  break;
4323 
4324   case vmIntrinsics::_doubleToLongBits: {
4325     // two paths (plus control) merge in a wood
4326     RegionNode *r = new RegionNode(3);
4327     Node *phi = new PhiNode(r, TypeLong::LONG);
4328 
4329     Node *cmpisnan = _gvn.transform(new CmpDNode(arg, arg));
4330     // Build the boolean node
4331     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4332 
4333     // Branch either way.
4334     // NaN case is less traveled, which makes all the difference.
4335     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4336     Node *opt_isnan = _gvn.transform(ifisnan);
4337     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4338     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4339     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4340 
4341     set_control(iftrue);
4342 
4343     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4344     Node *slow_result = longcon(nan_bits); // return NaN
4345     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4346     r-&gt;init_req(1, iftrue);
4347 
4348     // Else fall through
4349     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4350     set_control(iffalse);
4351 
4352     phi-&gt;init_req(2, _gvn.transform(new MoveD2LNode(arg)));
4353     r-&gt;init_req(2, iffalse);
4354 
4355     // Post merge
4356     set_control(_gvn.transform(r));
4357     record_for_igvn(r);
4358 
4359     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4360     result = phi;
4361     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4362     break;
4363   }
4364 
4365   case vmIntrinsics::_floatToIntBits: {
4366     // two paths (plus control) merge in a wood
4367     RegionNode *r = new RegionNode(3);
4368     Node *phi = new PhiNode(r, TypeInt::INT);
4369 
4370     Node *cmpisnan = _gvn.transform(new CmpFNode(arg, arg));
4371     // Build the boolean node
4372     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4373 
4374     // Branch either way.
4375     // NaN case is less traveled, which makes all the difference.
4376     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4377     Node *opt_isnan = _gvn.transform(ifisnan);
4378     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4379     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4380     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4381 
4382     set_control(iftrue);
4383 
4384     static const jint nan_bits = 0x7fc00000;
4385     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4386     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4387     r-&gt;init_req(1, iftrue);
4388 
4389     // Else fall through
4390     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4391     set_control(iffalse);
4392 
4393     phi-&gt;init_req(2, _gvn.transform(new MoveF2INode(arg)));
4394     r-&gt;init_req(2, iffalse);
4395 
4396     // Post merge
4397     set_control(_gvn.transform(r));
4398     record_for_igvn(r);
4399 
4400     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4401     result = phi;
4402     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4403     break;
4404   }
4405 
4406   default:
4407     fatal_unexpected_iid(id);
4408     break;
4409   }
4410   set_result(_gvn.transform(result));
4411   return true;
4412 }
4413 
4414 #ifdef _LP64
4415 #define XTOP ,top() /*additional argument*/
4416 #else  //_LP64
4417 #define XTOP        /*no additional argument*/
4418 #endif //_LP64
4419 
4420 //----------------------inline_unsafe_copyMemory-------------------------
4421 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4422 bool LibraryCallKit::inline_unsafe_copyMemory() {
4423   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4424   null_check_receiver();  // null-check receiver
4425   if (stopped())  return true;
4426 
4427   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4428 
4429   Node* src_ptr =         argument(1);   // type: oop
4430   Node* src_off = ConvL2X(argument(2));  // type: long
4431   Node* dst_ptr =         argument(4);   // type: oop
4432   Node* dst_off = ConvL2X(argument(5));  // type: long
4433   Node* size    = ConvL2X(argument(7));  // type: long
4434 
4435   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4436          "fieldOffset must be byte-scaled");
4437 
4438   Node* src = make_unsafe_address(src_ptr, src_off);
4439   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4440 
4441   // Conservatively insert a memory barrier on all memory slices.
4442   // Do not let writes of the copy source or destination float below the copy.
4443   insert_mem_bar(Op_MemBarCPUOrder);
4444 
4445   // Call it.  Note that the length argument is not scaled.
4446   make_runtime_call(RC_LEAF|RC_NO_FP,
4447                     OptoRuntime::fast_arraycopy_Type(),
4448                     StubRoutines::unsafe_arraycopy(),
4449                     "unsafe_arraycopy",
4450                     TypeRawPtr::BOTTOM,
4451                     src, dst, size XTOP);
4452 
4453   // Do not let reads of the copy destination float above the copy.
4454   insert_mem_bar(Op_MemBarCPUOrder);
4455 
4456   return true;
4457 }
4458 
4459 //------------------------clone_coping-----------------------------------
4460 // Helper function for inline_native_clone.
4461 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4462   assert(obj_size != NULL, "");
4463   Node* raw_obj = alloc_obj-&gt;in(1);
4464   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4465 
4466   AllocateNode* alloc = NULL;
4467   if (ReduceBulkZeroing) {
4468     // We will be completely responsible for initializing this object -
4469     // mark Initialize node as complete.
4470     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4471     // The object was just allocated - there should be no any stores!
4472     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4473     // Mark as complete_with_arraycopy so that on AllocateNode
4474     // expansion, we know this AllocateNode is initialized by an array
4475     // copy and a StoreStore barrier exists after the array copy.
4476     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4477   }
4478 
4479   // Copy the fastest available way.
4480   // TODO: generate fields copies for small objects instead.
4481   Node* src  = obj;
4482   Node* dest = alloc_obj;
4483   Node* size = _gvn.transform(obj_size);
4484 
4485   // Exclude the header but include array length to copy by 8 bytes words.
4486   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4487   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4488                             instanceOopDesc::base_offset_in_bytes();
4489   // base_off:
4490   // 8  - 32-bit VM
4491   // 12 - 64-bit VM, compressed klass
4492   // 16 - 64-bit VM, normal klass
4493   if (base_off % BytesPerLong != 0) {
4494     assert(UseCompressedClassPointers, "");
4495     if (is_array) {
4496       // Exclude length to copy by 8 bytes words.
4497       base_off += sizeof(int);
4498     } else {
4499       // Include klass to copy by 8 bytes words.
4500       base_off = instanceOopDesc::klass_offset_in_bytes();
4501     }
4502     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4503   }
4504   src  = basic_plus_adr(src,  base_off);
4505   dest = basic_plus_adr(dest, base_off);
4506 
4507   // Compute the length also, if needed:
4508   Node* countx = size;
4509   countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
4510   countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong) ));
4511 
4512   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4513 
4514   ArrayCopyNode* ac = ArrayCopyNode::make(this, false, src, NULL, dest, NULL, countx, false);
4515   ac-&gt;set_clonebasic();
4516   Node* n = _gvn.transform(ac);
4517   if (n == ac) {
4518     set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
4519   } else {
4520     set_all_memory(n);
4521   }
4522 
4523   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4524   if (card_mark) {
4525     assert(!is_array, "");
4526     // Put in store barrier for any and all oops we are sticking
4527     // into this object.  (We could avoid this if we could prove
4528     // that the object type contains no oop fields at all.)
4529     Node* no_particular_value = NULL;
4530     Node* no_particular_field = NULL;
4531     int raw_adr_idx = Compile::AliasIdxRaw;
4532     post_barrier(control(),
4533                  memory(raw_adr_type),
4534                  alloc_obj,
4535                  no_particular_field,
4536                  raw_adr_idx,
4537                  no_particular_value,
4538                  T_OBJECT,
4539                  false);
4540   }
4541 
4542   // Do not let reads from the cloned object float above the arraycopy.
4543   if (alloc != NULL) {
4544     // Do not let stores that initialize this object be reordered with
4545     // a subsequent store that would make this object accessible by
4546     // other threads.
4547     // Record what AllocateNode this StoreStore protects so that
4548     // escape analysis can go from the MemBarStoreStoreNode to the
4549     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4550     // based on the escape status of the AllocateNode.
4551     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4552   } else {
4553     insert_mem_bar(Op_MemBarCPUOrder);
4554   }
4555 }
4556 
4557 //------------------------inline_native_clone----------------------------
4558 // protected native Object java.lang.Object.clone();
4559 //
4560 // Here are the simple edge cases:
4561 //  null receiver =&gt; normal trap
4562 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4563 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4564 //
4565 // The general case has two steps, allocation and copying.
4566 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4567 //
4568 // Copying also has two cases, oop arrays and everything else.
4569 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4570 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4571 //
4572 // These steps fold up nicely if and when the cloned object's klass
4573 // can be sharply typed as an object array, a type array, or an instance.
4574 //
4575 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4576   PhiNode* result_val;
4577 
4578   // Set the reexecute bit for the interpreter to reexecute
4579   // the bytecode that invokes Object.clone if deoptimization happens.
4580   { PreserveReexecuteState preexecs(this);
4581     jvms()-&gt;set_should_reexecute(true);
4582 
4583     Node* obj = null_check_receiver();
4584     if (stopped())  return true;
4585 
4586     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4587 
4588     // If we are going to clone an instance, we need its exact type to
4589     // know the number and types of fields to convert the clone to
4590     // loads/stores. Maybe a speculative type can help us.
4591     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4592         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4593         obj_type-&gt;speculative_type()-&gt;is_instance_klass()) {
4594       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4595       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4596           !spec_ik-&gt;has_injected_fields()) {
4597         ciKlass* k = obj_type-&gt;klass();
4598         if (!k-&gt;is_instance_klass() ||
4599             k-&gt;as_instance_klass()-&gt;is_interface() ||
4600             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4601           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4602         }
4603       }
4604     }
4605 
4606     Node* obj_klass = load_object_klass(obj);
4607     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4608     const TypeOopPtr*   toop   = ((tklass != NULL)
4609                                 ? tklass-&gt;as_instance_type()
4610                                 : TypeInstPtr::NOTNULL);
4611 
4612     // Conservatively insert a memory barrier on all memory slices.
4613     // Do not let writes into the original float below the clone.
4614     insert_mem_bar(Op_MemBarCPUOrder);
4615 
4616     // paths into result_reg:
4617     enum {
4618       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4619       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4620       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4621       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4622       PATH_LIMIT
4623     };
4624     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4625     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4626     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4627     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4628     record_for_igvn(result_reg);
4629 
4630     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4631     int raw_adr_idx = Compile::AliasIdxRaw;
4632 
4633     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4634     if (array_ctl != NULL) {
4635       // It's an array.
4636       PreserveJVMState pjvms(this);
4637       set_control(array_ctl);
4638       Node* obj_length = load_array_length(obj);
4639       Node* obj_size  = NULL;
4640       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4641 
4642       if (!use_ReduceInitialCardMarks()) {
4643         // If it is an oop array, it requires very special treatment,
4644         // because card marking is required on each card of the array.
4645         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4646         if (is_obja != NULL) {
4647           PreserveJVMState pjvms2(this);
4648           set_control(is_obja);
4649           // Generate a direct call to the right arraycopy function(s).
4650           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4651           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL);
4652           ac-&gt;set_cloneoop();
4653           Node* n = _gvn.transform(ac);
4654           assert(n == ac, "cannot disappear");
4655           ac-&gt;connect_outputs(this);
4656 
4657           result_reg-&gt;init_req(_objArray_path, control());
4658           result_val-&gt;init_req(_objArray_path, alloc_obj);
4659           result_i_o -&gt;set_req(_objArray_path, i_o());
4660           result_mem -&gt;set_req(_objArray_path, reset_memory());
4661         }
4662       }
4663       // Otherwise, there are no card marks to worry about.
4664       // (We can dispense with card marks if we know the allocation
4665       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4666       //  causes the non-eden paths to take compensating steps to
4667       //  simulate a fresh allocation, so that no further
4668       //  card marks are required in compiled code to initialize
4669       //  the object.)
4670 
4671       if (!stopped()) {
4672         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4673 
4674         // Present the results of the copy.
4675         result_reg-&gt;init_req(_array_path, control());
4676         result_val-&gt;init_req(_array_path, alloc_obj);
4677         result_i_o -&gt;set_req(_array_path, i_o());
4678         result_mem -&gt;set_req(_array_path, reset_memory());
4679       }
4680     }
4681 
4682     // We only go to the instance fast case code if we pass a number of guards.
4683     // The paths which do not pass are accumulated in the slow_region.
4684     RegionNode* slow_region = new RegionNode(1);
4685     record_for_igvn(slow_region);
4686     if (!stopped()) {
4687       // It's an instance (we did array above).  Make the slow-path tests.
4688       // If this is a virtual call, we generate a funny guard.  We grab
4689       // the vtable entry corresponding to clone() from the target object.
4690       // If the target method which we are calling happens to be the
4691       // Object clone() method, we pass the guard.  We do not need this
4692       // guard for non-virtual calls; the caller is known to be the native
4693       // Object clone().
4694       if (is_virtual) {
4695         generate_virtual_guard(obj_klass, slow_region);
4696       }
4697 
4698       // The object must be cloneable and must not have a finalizer.
4699       // Both of these conditions may be checked in a single test.
4700       // We could optimize the cloneable test further, but we don't care.
4701       generate_access_flags_guard(obj_klass,
4702                                   // Test both conditions:
4703                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4704                                   // Must be cloneable but not finalizer:
4705                                   JVM_ACC_IS_CLONEABLE,
4706                                   slow_region);
4707     }
4708 
4709     if (!stopped()) {
4710       // It's an instance, and it passed the slow-path tests.
4711       PreserveJVMState pjvms(this);
4712       Node* obj_size  = NULL;
4713       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4714       // is reexecuted if deoptimization occurs and there could be problems when merging
4715       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4716       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4717 
4718       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4719 
4720       // Present the results of the slow call.
4721       result_reg-&gt;init_req(_instance_path, control());
4722       result_val-&gt;init_req(_instance_path, alloc_obj);
4723       result_i_o -&gt;set_req(_instance_path, i_o());
4724       result_mem -&gt;set_req(_instance_path, reset_memory());
4725     }
4726 
4727     // Generate code for the slow case.  We make a call to clone().
4728     set_control(_gvn.transform(slow_region));
4729     if (!stopped()) {
4730       PreserveJVMState pjvms(this);
4731       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4732       Node* slow_result = set_results_for_java_call(slow_call);
4733       // this-&gt;control() comes from set_results_for_java_call
4734       result_reg-&gt;init_req(_slow_path, control());
4735       result_val-&gt;init_req(_slow_path, slow_result);
4736       result_i_o -&gt;set_req(_slow_path, i_o());
4737       result_mem -&gt;set_req(_slow_path, reset_memory());
4738     }
4739 
4740     // Return the combined state.
4741     set_control(    _gvn.transform(result_reg));
4742     set_i_o(        _gvn.transform(result_i_o));
4743     set_all_memory( _gvn.transform(result_mem));
4744   } // original reexecute is set back here
4745 
4746   set_result(_gvn.transform(result_val));
4747   return true;
4748 }
4749 
4750 // If we have a tighly coupled allocation, the arraycopy may take care
4751 // of the array initialization. If one of the guards we insert between
4752 // the allocation and the arraycopy causes a deoptimization, an
4753 // unitialized array will escape the compiled method. To prevent that
4754 // we set the JVM state for uncommon traps between the allocation and
4755 // the arraycopy to the state before the allocation so, in case of
4756 // deoptimization, we'll reexecute the allocation and the
4757 // initialization.
4758 JVMState* LibraryCallKit::arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp) {
4759   if (alloc != NULL) {
4760     ciMethod* trap_method = alloc-&gt;jvms()-&gt;method();
4761     int trap_bci = alloc-&gt;jvms()-&gt;bci();
4762 
4763     if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;
4764           !C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
4765       // Make sure there's no store between the allocation and the
4766       // arraycopy otherwise visible side effects could be rexecuted
4767       // in case of deoptimization and cause incorrect execution.
4768       bool no_interfering_store = true;
4769       Node* mem = alloc-&gt;in(TypeFunc::Memory);
4770       if (mem-&gt;is_MergeMem()) {
4771         for (MergeMemStream mms(merged_memory(), mem-&gt;as_MergeMem()); mms.next_non_empty2(); ) {
4772           Node* n = mms.memory();
4773           if (n != mms.memory2() &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4774             assert(n-&gt;is_Store(), "what else?");
4775             no_interfering_store = false;
4776             break;
4777           }
4778         }
4779       } else {
4780         for (MergeMemStream mms(merged_memory()); mms.next_non_empty(); ) {
4781           Node* n = mms.memory();
4782           if (n != mem &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4783             assert(n-&gt;is_Store(), "what else?");
4784             no_interfering_store = false;
4785             break;
4786           }
4787         }
4788       }
4789 
4790       if (no_interfering_store) {
4791         JVMState* old_jvms = alloc-&gt;jvms()-&gt;clone_shallow(C);
4792         uint size = alloc-&gt;req();
4793         SafePointNode* sfpt = new SafePointNode(size, old_jvms);
4794         old_jvms-&gt;set_map(sfpt);
4795         for (uint i = 0; i &lt; size; i++) {
4796           sfpt-&gt;init_req(i, alloc-&gt;in(i));
4797         }
4798         // re-push array length for deoptimization
4799         sfpt-&gt;ins_req(old_jvms-&gt;stkoff() + old_jvms-&gt;sp(), alloc-&gt;in(AllocateNode::ALength));
4800         old_jvms-&gt;set_sp(old_jvms-&gt;sp()+1);
4801         old_jvms-&gt;set_monoff(old_jvms-&gt;monoff()+1);
4802         old_jvms-&gt;set_scloff(old_jvms-&gt;scloff()+1);
4803         old_jvms-&gt;set_endoff(old_jvms-&gt;endoff()+1);
4804         old_jvms-&gt;set_should_reexecute(true);
4805 
4806         sfpt-&gt;set_i_o(map()-&gt;i_o());
4807         sfpt-&gt;set_memory(map()-&gt;memory());
4808         sfpt-&gt;set_control(map()-&gt;control());
4809 
4810         JVMState* saved_jvms = jvms();
4811         saved_reexecute_sp = _reexecute_sp;
4812 
4813         set_jvms(sfpt-&gt;jvms());
4814         _reexecute_sp = jvms()-&gt;sp();
4815 
4816         return saved_jvms;
4817       }
4818     }
4819   }
4820   return NULL;
4821 }
4822 
4823 // In case of a deoptimization, we restart execution at the
4824 // allocation, allocating a new array. We would leave an uninitialized
4825 // array in the heap that GCs wouldn't expect. Move the allocation
4826 // after the traps so we don't allocate the array if we
4827 // deoptimize. This is possible because tightly_coupled_allocation()
4828 // guarantees there's no observer of the allocated array at this point
4829 // and the control flow is simple enough.
4830 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp) {
4831   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4832     assert(alloc != NULL, "only with a tightly coupled allocation");
4833     // restore JVM state to the state at the arraycopy
4834     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4835     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), "memory state changed?");
4836     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), "IO state changed?");
4837     // If we've improved the types of some nodes (null check) while
4838     // emitting the guards, propagate them to the current state
4839     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map());
4840     set_jvms(saved_jvms);
4841     _reexecute_sp = saved_reexecute_sp;
4842 
4843     // Remove the allocation from above the guards
4844     CallProjections callprojs;
4845     alloc-&gt;extract_projections(&amp;callprojs, true);
4846     InitializeNode* init = alloc-&gt;initialization();
4847     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
4848     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));
4849     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4850     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4851 
4852     // move the allocation here (after the guards)
4853     _gvn.hash_delete(alloc);
4854     alloc-&gt;set_req(TypeFunc::Control, control());
4855     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4856     Node *mem = reset_memory();
4857     set_all_memory(mem);
4858     alloc-&gt;set_req(TypeFunc::Memory, mem);
4859     set_control(init-&gt;proj_out(TypeFunc::Control));
4860     set_i_o(callprojs.fallthrough_ioproj);
4861 
4862     // Update memory as done in GraphKit::set_output_for_allocation()
4863     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4864     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4865     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4866       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4867     }
4868     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4869     int            elemidx  = C-&gt;get_alias_index(telemref);
4870     set_memory(init-&gt;proj_out(TypeFunc::Memory), Compile::AliasIdxRaw);
4871     set_memory(init-&gt;proj_out(TypeFunc::Memory), elemidx);
4872 
4873     Node* allocx = _gvn.transform(alloc);
4874     assert(allocx == alloc, "where has the allocation gone?");
4875     assert(dest-&gt;is_CheckCastPP(), "not an allocation result?");
4876 
4877     _gvn.hash_delete(dest);
4878     dest-&gt;set_req(0, control());
4879     Node* destx = _gvn.transform(dest);
4880     assert(destx == dest, "where has the allocation result gone?");
4881   }
4882 }
4883 
4884 
4885 //------------------------------inline_arraycopy-----------------------
4886 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4887 //                                                      Object dest, int destPos,
4888 //                                                      int length);
4889 bool LibraryCallKit::inline_arraycopy() {
4890   // Get the arguments.
4891   Node* src         = argument(0);  // type: oop
4892   Node* src_offset  = argument(1);  // type: int
4893   Node* dest        = argument(2);  // type: oop
4894   Node* dest_offset = argument(3);  // type: int
4895   Node* length      = argument(4);  // type: int
4896 
4897 
4898   // Check for allocation before we add nodes that would confuse
4899   // tightly_coupled_allocation()
4900   AllocateArrayNode* alloc = tightly_coupled_allocation(dest, NULL);
4901 
4902   int saved_reexecute_sp = -1;
4903   JVMState* saved_jvms = arraycopy_restore_alloc_state(alloc, saved_reexecute_sp);
4904   // See arraycopy_restore_alloc_state() comment
4905   // if alloc == NULL we don't have to worry about a tightly coupled allocation so we can emit all needed guards
4906   // if saved_jvms != NULL (then alloc != NULL) then we can handle guards and a tightly coupled allocation
4907   // if saved_jvms == NULL and alloc != NULL, we cant emit any guards
4908   bool can_emit_guards = (alloc == NULL || saved_jvms != NULL);
4909 
4910   // The following tests must be performed
4911   // (1) src and dest are arrays.
4912   // (2) src and dest arrays must have elements of the same BasicType
4913   // (3) src and dest must not be null.
4914   // (4) src_offset must not be negative.
4915   // (5) dest_offset must not be negative.
4916   // (6) length must not be negative.
4917   // (7) src_offset + length must not exceed length of src.
4918   // (8) dest_offset + length must not exceed length of dest.
4919   // (9) each element of an oop array must be assignable
4920 
4921   // (3) src and dest must not be null.
4922   // always do this here because we need the JVM state for uncommon traps
4923   Node* null_ctl = top();
4924   src  = saved_jvms != NULL ? null_check_oop(src, &amp;null_ctl, true, true) : null_check(src,  T_ARRAY);
4925   assert(null_ctl-&gt;is_top(), "no null control here");
4926   dest = null_check(dest, T_ARRAY);
4927 
4928   if (!can_emit_guards) {
4929     // if saved_jvms == NULL and alloc != NULL, we don't emit any
4930     // guards but the arraycopy node could still take advantage of a
4931     // tightly allocated allocation. tightly_coupled_allocation() is
4932     // called again to make sure it takes the null check above into
4933     // account: the null check is mandatory and if it caused an
4934     // uncommon trap to be emitted then the allocation can't be
4935     // considered tightly coupled in this context.
4936     alloc = tightly_coupled_allocation(dest, NULL);
4937   }
4938 
4939   bool validated = false;
4940 
4941   const Type* src_type  = _gvn.type(src);
4942   const Type* dest_type = _gvn.type(dest);
4943   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4944   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4945 
4946   // Do we have the type of src?
4947   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4948   // Do we have the type of dest?
4949   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4950   // Is the type for src from speculation?
4951   bool src_spec = false;
4952   // Is the type for dest from speculation?
4953   bool dest_spec = false;
4954 
4955   if ((!has_src || !has_dest) &amp;&amp; can_emit_guards) {
4956     // We don't have sufficient type information, let's see if
4957     // speculative types can help. We need to have types for both src
4958     // and dest so that it pays off.
4959 
4960     // Do we already have or could we have type information for src
4961     bool could_have_src = has_src;
4962     // Do we already have or could we have type information for dest
4963     bool could_have_dest = has_dest;
4964 
4965     ciKlass* src_k = NULL;
4966     if (!has_src) {
4967       src_k = src_type-&gt;speculative_type_not_null();
4968       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4969         could_have_src = true;
4970       }
4971     }
4972 
4973     ciKlass* dest_k = NULL;
4974     if (!has_dest) {
4975       dest_k = dest_type-&gt;speculative_type_not_null();
4976       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4977         could_have_dest = true;
4978       }
4979     }
4980 
4981     if (could_have_src &amp;&amp; could_have_dest) {
4982       // This is going to pay off so emit the required guards
4983       if (!has_src) {
4984         src = maybe_cast_profiled_obj(src, src_k, true);
4985         src_type  = _gvn.type(src);
4986         top_src  = src_type-&gt;isa_aryptr();
4987         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4988         src_spec = true;
4989       }
4990       if (!has_dest) {
4991         dest = maybe_cast_profiled_obj(dest, dest_k, true);
4992         dest_type  = _gvn.type(dest);
4993         top_dest  = dest_type-&gt;isa_aryptr();
4994         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4995         dest_spec = true;
4996       }
4997     }
4998   }
4999 
5000   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
5001     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5002     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5003     if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
5004     if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
5005 
5006     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
5007       // If both arrays are object arrays then having the exact types
5008       // for both will remove the need for a subtype check at runtime
5009       // before the call and may make it possible to pick a faster copy
5010       // routine (without a subtype check on every element)
5011       // Do we have the exact type of src?
5012       bool could_have_src = src_spec;
5013       // Do we have the exact type of dest?
5014       bool could_have_dest = dest_spec;
5015       ciKlass* src_k = top_src-&gt;klass();
5016       ciKlass* dest_k = top_dest-&gt;klass();
5017       if (!src_spec) {
5018         src_k = src_type-&gt;speculative_type_not_null();
5019         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
5020           could_have_src = true;
5021         }
5022       }
5023       if (!dest_spec) {
5024         dest_k = dest_type-&gt;speculative_type_not_null();
5025         if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
5026           could_have_dest = true;
5027         }
5028       }
5029       if (could_have_src &amp;&amp; could_have_dest) {
5030         // If we can have both exact types, emit the missing guards
5031         if (could_have_src &amp;&amp; !src_spec) {
5032           src = maybe_cast_profiled_obj(src, src_k, true);
5033         }
5034         if (could_have_dest &amp;&amp; !dest_spec) {
5035           dest = maybe_cast_profiled_obj(dest, dest_k, true);
5036         }
5037       }
5038     }
5039   }
5040 
5041   ciMethod* trap_method = method();
5042   int trap_bci = bci();
5043   if (saved_jvms != NULL) {
5044     trap_method = alloc-&gt;jvms()-&gt;method();
5045     trap_bci = alloc-&gt;jvms()-&gt;bci();
5046   }
5047 
5048   if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;&amp;
5049       can_emit_guards &amp;&amp;
5050       !src-&gt;is_top() &amp;&amp; !dest-&gt;is_top()) {
5051     // validate arguments: enables transformation the ArrayCopyNode
5052     validated = true;
5053 
5054     RegionNode* slow_region = new RegionNode(1);
5055     record_for_igvn(slow_region);
5056 
5057     // (1) src and dest are arrays.
5058     generate_non_array_guard(load_object_klass(src), slow_region);
5059     generate_non_array_guard(load_object_klass(dest), slow_region);
5060 
5061     // (2) src and dest arrays must have elements of the same BasicType
5062     // done at macro expansion or at Ideal transformation time
5063 
5064     // (4) src_offset must not be negative.
5065     generate_negative_guard(src_offset, slow_region);
5066 
5067     // (5) dest_offset must not be negative.
5068     generate_negative_guard(dest_offset, slow_region);
5069 
5070     // (7) src_offset + length must not exceed length of src.
5071     generate_limit_guard(src_offset, length,
5072                          load_array_length(src),
5073                          slow_region);
5074 
5075     // (8) dest_offset + length must not exceed length of dest.
5076     generate_limit_guard(dest_offset, length,
5077                          load_array_length(dest),
5078                          slow_region);
5079 
5080     // (9) each element of an oop array must be assignable
5081     Node* src_klass  = load_object_klass(src);
5082     Node* dest_klass = load_object_klass(dest);
5083     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5084 
5085     if (not_subtype_ctrl != top()) {
5086       PreserveJVMState pjvms(this);
5087       set_control(not_subtype_ctrl);
5088       uncommon_trap(Deoptimization::Reason_intrinsic,
5089                     Deoptimization::Action_make_not_entrant);
5090       assert(stopped(), "Should be stopped");
5091     }
5092     {
5093       PreserveJVMState pjvms(this);
5094       set_control(_gvn.transform(slow_region));
5095       uncommon_trap(Deoptimization::Reason_intrinsic,
5096                     Deoptimization::Action_make_not_entrant);
5097       assert(stopped(), "Should be stopped");
5098     }
5099   }
5100 
5101   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp);
5102 
5103   if (stopped()) {
5104     return true;
5105   }
5106 
5107   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL,
5108                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
5109                                           // so the compiler has a chance to eliminate them: during macro expansion,
5110                                           // we have to set their control (CastPP nodes are eliminated).
5111                                           load_object_klass(src), load_object_klass(dest),
5112                                           load_array_length(src), load_array_length(dest));
5113 
5114   ac-&gt;set_arraycopy(validated);
5115 
5116   Node* n = _gvn.transform(ac);
5117   if (n == ac) {
5118     ac-&gt;connect_outputs(this);
5119   } else {
5120     assert(validated, "shouldn't transform if all arguments not validated");
5121     set_all_memory(n);
5122   }
5123 
5124   return true;
5125 }
5126 
5127 
5128 // Helper function which determines if an arraycopy immediately follows
5129 // an allocation, with no intervening tests or other escapes for the object.
5130 AllocateArrayNode*
5131 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5132                                            RegionNode* slow_region) {
5133   if (stopped())             return NULL;  // no fast path
5134   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5135 
5136   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5137   if (alloc == NULL)  return NULL;
5138 
5139   Node* rawmem = memory(Compile::AliasIdxRaw);
5140   // Is the allocation's memory state untouched?
5141   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5142     // Bail out if there have been raw-memory effects since the allocation.
5143     // (Example:  There might have been a call or safepoint.)
5144     return NULL;
5145   }
5146   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5147   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5148     return NULL;
5149   }
5150 
5151   // There must be no unexpected observers of this allocation.
5152   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5153     Node* obs = ptr-&gt;fast_out(i);
5154     if (obs != this-&gt;map()) {
5155       return NULL;
5156     }
5157   }
5158 
5159   // This arraycopy must unconditionally follow the allocation of the ptr.
5160   Node* alloc_ctl = ptr-&gt;in(0);
5161   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5162 
5163   Node* ctl = control();
5164   while (ctl != alloc_ctl) {
5165     // There may be guards which feed into the slow_region.
5166     // Any other control flow means that we might not get a chance
5167     // to finish initializing the allocated object.
5168     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5169       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5170       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5171       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5172       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5173         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5174         continue;
5175       }
5176       // One more try:  Various low-level checks bottom out in
5177       // uncommon traps.  If the debug-info of the trap omits
5178       // any reference to the allocation, as we've already
5179       // observed, then there can be no objection to the trap.
5180       bool found_trap = false;
5181       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5182         Node* obs = not_ctl-&gt;fast_out(j);
5183         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5184             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5185           found_trap = true; break;
5186         }
5187       }
5188       if (found_trap) {
5189         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5190         continue;
5191       }
5192     }
5193     return NULL;
5194   }
5195 
5196   // If we get this far, we have an allocation which immediately
5197   // precedes the arraycopy, and we can take over zeroing the new object.
5198   // The arraycopy will finish the initialization, and provide
5199   // a new control state to which we will anchor the destination pointer.
5200 
5201   return alloc;
5202 }
5203 
5204 //-------------inline_encodeISOArray-----------------------------------
5205 // encode char[] to byte[] in ISO_8859_1
5206 bool LibraryCallKit::inline_encodeISOArray() {
5207   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5208   // no receiver since it is static method
5209   Node *src         = argument(0);
5210   Node *src_offset  = argument(1);
5211   Node *dst         = argument(2);
5212   Node *dst_offset  = argument(3);
5213   Node *length      = argument(4);
5214 
5215   const Type* src_type = src-&gt;Value(&amp;_gvn);
5216   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5217   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5218   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5219   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5220       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5221     // failed array check
5222     return false;
5223   }
5224 
5225   // Figure out the size and type of the elements we will be copying.
5226   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5227   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5228   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5229     return false;
5230   }
5231   Node* src_start = array_element_address(src, src_offset, src_elem);
5232   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5233   // 'src_start' points to src array + scaled offset
5234   // 'dst_start' points to dst array + scaled offset
5235 
5236   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5237   Node* enc = new EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5238   enc = _gvn.transform(enc);
5239   Node* res_mem = _gvn.transform(new SCMemProjNode(enc));
5240   set_memory(res_mem, mtype);
5241   set_result(enc);
5242   return true;
5243 }
5244 
5245 //-------------inline_multiplyToLen-----------------------------------
5246 bool LibraryCallKit::inline_multiplyToLen() {
5247   assert(UseMultiplyToLenIntrinsic, "not implementated on this platform");
5248 
5249   address stubAddr = StubRoutines::multiplyToLen();
5250   if (stubAddr == NULL) {
5251     return false; // Intrinsic's stub is not implemented on this platform
5252   }
5253   const char* stubName = "multiplyToLen";
5254 
5255   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5256 
5257   Node* x    = argument(1);
5258   Node* xlen = argument(2);
5259   Node* y    = argument(3);
5260   Node* ylen = argument(4);
5261   Node* z    = argument(5);
5262 
5263   const Type* x_type = x-&gt;Value(&amp;_gvn);
5264   const Type* y_type = y-&gt;Value(&amp;_gvn);
5265   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5266   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5267   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5268       top_y == NULL || top_y-&gt;klass() == NULL) {
5269     // failed array check
5270     return false;
5271   }
5272 
5273   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5274   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5275   if (x_elem != T_INT || y_elem != T_INT) {
5276     return false;
5277   }
5278 
5279   // Set the original stack and the reexecute bit for the interpreter to reexecute
5280   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5281   // on the return from z array allocation in runtime.
5282   { PreserveReexecuteState preexecs(this);
5283     jvms()-&gt;set_should_reexecute(true);
5284 
5285     Node* x_start = array_element_address(x, intcon(0), x_elem);
5286     Node* y_start = array_element_address(y, intcon(0), y_elem);
5287     // 'x_start' points to x array + scaled xlen
5288     // 'y_start' points to y array + scaled ylen
5289 
5290     // Allocate the result array
5291     Node* zlen = _gvn.transform(new AddINode(xlen, ylen));
5292     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5293     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5294 
5295     IdealKit ideal(this);
5296 
5297 #define __ ideal.
5298      Node* one = __ ConI(1);
5299      Node* zero = __ ConI(0);
5300      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5301      __ set(need_alloc, zero);
5302      __ set(z_alloc, z);
5303      __ if_then(z, BoolTest::eq, null()); {
5304        __ increment (need_alloc, one);
5305      } __ else_(); {
5306        // Update graphKit memory and control from IdealKit.
5307        sync_kit(ideal);
5308        Node* zlen_arg = load_array_length(z);
5309        // Update IdealKit memory and control from graphKit.
5310        __ sync_kit(this);
5311        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5312          __ increment (need_alloc, one);
5313        } __ end_if();
5314      } __ end_if();
5315 
5316      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5317        // Update graphKit memory and control from IdealKit.
5318        sync_kit(ideal);
5319        Node * narr = new_array(klass_node, zlen, 1);
5320        // Update IdealKit memory and control from graphKit.
5321        __ sync_kit(this);
5322        __ set(z_alloc, narr);
5323      } __ end_if();
5324 
5325      sync_kit(ideal);
5326      z = __ value(z_alloc);
5327      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5328      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5329      // Final sync IdealKit and GraphKit.
5330      final_sync(ideal);
5331 #undef __
5332 
5333     Node* z_start = array_element_address(z, intcon(0), T_INT);
5334 
5335     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5336                                    OptoRuntime::multiplyToLen_Type(),
5337                                    stubAddr, stubName, TypePtr::BOTTOM,
5338                                    x_start, xlen, y_start, ylen, z_start, zlen);
5339   } // original reexecute is set back here
5340 
5341   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5342   set_result(z);
5343   return true;
5344 }
5345 
5346 //-------------inline_squareToLen------------------------------------
5347 bool LibraryCallKit::inline_squareToLen() {
5348   assert(UseSquareToLenIntrinsic, "not implementated on this platform");
5349 
5350   address stubAddr = StubRoutines::squareToLen();
5351   if (stubAddr == NULL) {
5352     return false; // Intrinsic's stub is not implemented on this platform
5353   }
5354   const char* stubName = "squareToLen";
5355 
5356   assert(callee()-&gt;signature()-&gt;size() == 4, "implSquareToLen has 4 parameters");
5357 
5358   Node* x    = argument(0);
5359   Node* len  = argument(1);
5360   Node* z    = argument(2);
5361   Node* zlen = argument(3);
5362 
5363   const Type* x_type = x-&gt;Value(&amp;_gvn);
5364   const Type* z_type = z-&gt;Value(&amp;_gvn);
5365   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5366   const TypeAryPtr* top_z = z_type-&gt;isa_aryptr();
5367   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5368       top_z  == NULL || top_z-&gt;klass()  == NULL) {
5369     // failed array check
5370     return false;
5371   }
5372 
5373   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5374   BasicType z_elem = z_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5375   if (x_elem != T_INT || z_elem != T_INT) {
5376     return false;
5377   }
5378 
5379 
5380   Node* x_start = array_element_address(x, intcon(0), x_elem);
5381   Node* z_start = array_element_address(z, intcon(0), z_elem);
5382 
5383   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5384                                   OptoRuntime::squareToLen_Type(),
5385                                   stubAddr, stubName, TypePtr::BOTTOM,
5386                                   x_start, len, z_start, zlen);
5387 
5388   set_result(z);
5389   return true;
5390 }
5391 
5392 //-------------inline_mulAdd------------------------------------------
5393 bool LibraryCallKit::inline_mulAdd() {
5394   assert(UseMulAddIntrinsic, "not implementated on this platform");
5395 
5396   address stubAddr = StubRoutines::mulAdd();
5397   if (stubAddr == NULL) {
5398     return false; // Intrinsic's stub is not implemented on this platform
5399   }
5400   const char* stubName = "mulAdd";
5401 
5402   assert(callee()-&gt;signature()-&gt;size() == 5, "mulAdd has 5 parameters");
5403 
5404   Node* out      = argument(0);
5405   Node* in       = argument(1);
5406   Node* offset   = argument(2);
5407   Node* len      = argument(3);
5408   Node* k        = argument(4);
5409 
5410   const Type* out_type = out-&gt;Value(&amp;_gvn);
5411   const Type* in_type = in-&gt;Value(&amp;_gvn);
5412   const TypeAryPtr* top_out = out_type-&gt;isa_aryptr();
5413   const TypeAryPtr* top_in = in_type-&gt;isa_aryptr();
5414   if (top_out  == NULL || top_out-&gt;klass()  == NULL ||
5415       top_in == NULL || top_in-&gt;klass() == NULL) {
5416     // failed array check
5417     return false;
5418   }
5419 
5420   BasicType out_elem = out_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5421   BasicType in_elem = in_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5422   if (out_elem != T_INT || in_elem != T_INT) {
5423     return false;
5424   }
5425 
5426   Node* outlen = load_array_length(out);
5427   Node* new_offset = _gvn.transform(new SubINode(outlen, offset));
5428   Node* out_start = array_element_address(out, intcon(0), out_elem);
5429   Node* in_start = array_element_address(in, intcon(0), in_elem);
5430 
5431   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5432                                   OptoRuntime::mulAdd_Type(),
5433                                   stubAddr, stubName, TypePtr::BOTTOM,
5434                                   out_start,in_start, new_offset, len, k);
5435   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5436   set_result(result);
5437   return true;
5438 }
5439 
5440 
5441 /**
5442  * Calculate CRC32 for byte.
5443  * int java.util.zip.CRC32.update(int crc, int b)
5444  */
5445 bool LibraryCallKit::inline_updateCRC32() {
5446   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5447   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5448   // no receiver since it is static method
5449   Node* crc  = argument(0); // type: int
5450   Node* b    = argument(1); // type: int
5451 
5452   /*
5453    *    int c = ~ crc;
5454    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5455    *    b = b ^ (c &gt;&gt;&gt; 8);
5456    *    crc = ~b;
5457    */
5458 
5459   Node* M1 = intcon(-1);
5460   crc = _gvn.transform(new XorINode(crc, M1));
5461   Node* result = _gvn.transform(new XorINode(crc, b));
5462   result = _gvn.transform(new AndINode(result, intcon(0xFF)));
5463 
5464   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5465   Node* offset = _gvn.transform(new LShiftINode(result, intcon(0x2)));
5466   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5467   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5468 
5469   crc = _gvn.transform(new URShiftINode(crc, intcon(8)));
5470   result = _gvn.transform(new XorINode(crc, result));
5471   result = _gvn.transform(new XorINode(result, M1));
5472   set_result(result);
5473   return true;
5474 }
5475 
5476 /**
5477  * Calculate CRC32 for byte[] array.
5478  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5479  */
5480 bool LibraryCallKit::inline_updateBytesCRC32() {
5481   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5482   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5483   // no receiver since it is static method
5484   Node* crc     = argument(0); // type: int
5485   Node* src     = argument(1); // type: oop
5486   Node* offset  = argument(2); // type: int
5487   Node* length  = argument(3); // type: int
5488 
5489   const Type* src_type = src-&gt;Value(&amp;_gvn);
5490   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5491   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5492     // failed array check
5493     return false;
5494   }
5495 
5496   // Figure out the size and type of the elements we will be copying.
5497   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5498   if (src_elem != T_BYTE) {
5499     return false;
5500   }
5501 
5502   // 'src_start' points to src array + scaled offset
5503   Node* src_start = array_element_address(src, offset, src_elem);
5504 
5505   // We assume that range check is done by caller.
5506   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5507 
5508   // Call the stub.
5509   address stubAddr = StubRoutines::updateBytesCRC32();
5510   const char *stubName = "updateBytesCRC32";
5511 
5512   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5513                                  stubAddr, stubName, TypePtr::BOTTOM,
5514                                  crc, src_start, length);
5515   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5516   set_result(result);
5517   return true;
5518 }
5519 
5520 /**
5521  * Calculate CRC32 for ByteBuffer.
5522  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5523  */
5524 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5525   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5526   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5527   // no receiver since it is static method
5528   Node* crc     = argument(0); // type: int
5529   Node* src     = argument(1); // type: long
5530   Node* offset  = argument(3); // type: int
5531   Node* length  = argument(4); // type: int
5532 
5533   src = ConvL2X(src);  // adjust Java long to machine word
5534   Node* base = _gvn.transform(new CastX2PNode(src));
5535   offset = ConvI2X(offset);
5536 
5537   // 'src_start' points to src array + scaled offset
5538   Node* src_start = basic_plus_adr(top(), base, offset);
5539 
5540   // Call the stub.
5541   address stubAddr = StubRoutines::updateBytesCRC32();
5542   const char *stubName = "updateBytesCRC32";
5543 
5544   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5545                                  stubAddr, stubName, TypePtr::BOTTOM,
5546                                  crc, src_start, length);
5547   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5548   set_result(result);
5549   return true;
5550 }
5551 
5552 //------------------------------get_table_from_crc32c_class-----------------------
5553 Node * LibraryCallKit::get_table_from_crc32c_class(ciInstanceKlass *crc32c_class) {
5554   Node* table = load_field_from_object(NULL, "byteTable", "[I", /*is_exact*/ false, /*is_static*/ true, crc32c_class);
5555   assert (table != NULL, "wrong version of java.util.zip.CRC32C");
5556 
5557   return table;
5558 }
5559 
5560 //------------------------------inline_updateBytesCRC32C-----------------------
5561 //
5562 // Calculate CRC32C for byte[] array.
5563 // int java.util.zip.CRC32C.updateBytes(int crc, byte[] buf, int off, int end)
5564 //
5565 bool LibraryCallKit::inline_updateBytesCRC32C() {
5566   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5567   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5568   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5569   // no receiver since it is a static method
5570   Node* crc     = argument(0); // type: int
5571   Node* src     = argument(1); // type: oop
5572   Node* offset  = argument(2); // type: int
5573   Node* end     = argument(3); // type: int
5574 
5575   Node* length = _gvn.transform(new SubINode(end, offset));
5576 
5577   const Type* src_type = src-&gt;Value(&amp;_gvn);
5578   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5579   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5580     // failed array check
5581     return false;
5582   }
5583 
5584   // Figure out the size and type of the elements we will be copying.
5585   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5586   if (src_elem != T_BYTE) {
5587     return false;
5588   }
5589 
5590   // 'src_start' points to src array + scaled offset
5591   Node* src_start = array_element_address(src, offset, src_elem);
5592 
5593   // static final int[] byteTable in class CRC32C
5594   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5595   Node* table_start = array_element_address(table, intcon(0), T_INT);
5596 
5597   // We assume that range check is done by caller.
5598   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5599 
5600   // Call the stub.
5601   address stubAddr = StubRoutines::updateBytesCRC32C();
5602   const char *stubName = "updateBytesCRC32C";
5603 
5604   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5605                                  stubAddr, stubName, TypePtr::BOTTOM,
5606                                  crc, src_start, length, table_start);
5607   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5608   set_result(result);
5609   return true;
5610 }
5611 
5612 //------------------------------inline_updateDirectByteBufferCRC32C-----------------------
5613 //
5614 // Calculate CRC32C for DirectByteBuffer.
5615 // int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)
5616 //
5617 bool LibraryCallKit::inline_updateDirectByteBufferCRC32C() {
5618   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5619   assert(callee()-&gt;signature()-&gt;size() == 5, "updateDirectByteBuffer has 4 parameters and one is long");
5620   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5621   // no receiver since it is a static method
5622   Node* crc     = argument(0); // type: int
5623   Node* src     = argument(1); // type: long
5624   Node* offset  = argument(3); // type: int
5625   Node* end     = argument(4); // type: int
5626 
5627   Node* length = _gvn.transform(new SubINode(end, offset));
5628 
5629   src = ConvL2X(src);  // adjust Java long to machine word
5630   Node* base = _gvn.transform(new CastX2PNode(src));
5631   offset = ConvI2X(offset);
5632 
5633   // 'src_start' points to src array + scaled offset
5634   Node* src_start = basic_plus_adr(top(), base, offset);
5635 
5636   // static final int[] byteTable in class CRC32C
5637   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5638   Node* table_start = array_element_address(table, intcon(0), T_INT);
5639 
5640   // Call the stub.
5641   address stubAddr = StubRoutines::updateBytesCRC32C();
5642   const char *stubName = "updateBytesCRC32C";
5643 
5644   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5645                                  stubAddr, stubName, TypePtr::BOTTOM,
5646                                  crc, src_start, length, table_start);
5647   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5648   set_result(result);
5649   return true;
5650 }
5651 
5652 //----------------------------inline_reference_get----------------------------
5653 // public T java.lang.ref.Reference.get();
5654 bool LibraryCallKit::inline_reference_get() {
5655   const int referent_offset = java_lang_ref_Reference::referent_offset;
5656   guarantee(referent_offset &gt; 0, "should have already been set");
5657 
5658   // Get the argument:
5659   Node* reference_obj = null_check_receiver();
5660   if (stopped()) return true;
5661 
5662   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5663 
5664   ciInstanceKlass* klass = env()-&gt;Object_klass();
5665   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5666 
5667   Node* no_ctrl = NULL;
5668   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5669 
5670   // Use the pre-barrier to record the value in the referent field
5671   pre_barrier(false /* do_load */,
5672               control(),
5673               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5674               result /* pre_val */,
5675               T_OBJECT);
5676 
5677   // Add memory barrier to prevent commoning reads from this field
5678   // across safepoint since GC can change its value.
5679   insert_mem_bar(Op_MemBarCPUOrder);
5680 
5681   set_result(result);
5682   return true;
5683 }
5684 
5685 
5686 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5687                                               bool is_exact=true, bool is_static=false,
5688                                               ciInstanceKlass * fromKls=NULL) {
5689   if (fromKls == NULL) {
5690     const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5691     assert(tinst != NULL, "obj is null");
5692     assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5693     assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5694     fromKls = tinst-&gt;klass()-&gt;as_instance_klass();
5695   } else {
5696     assert(is_static, "only for static field access");
5697   }
5698   ciField* field = fromKls-&gt;get_field_by_name(ciSymbol::make(fieldName),
5699                                               ciSymbol::make(fieldTypeString),
5700                                               is_static);
5701 
5702   assert (field != NULL, "undefined field");
5703   if (field == NULL) return (Node *) NULL;
5704 
5705   if (is_static) {
5706     const TypeInstPtr* tip = TypeInstPtr::make(fromKls-&gt;java_mirror());
5707     fromObj = makecon(tip);
5708   }
5709 
5710   // Next code  copied from Parse::do_get_xxx():
5711 
5712   // Compute address and memory type.
5713   int offset  = field-&gt;offset_in_bytes();
5714   bool is_vol = field-&gt;is_volatile();
5715   ciType* field_klass = field-&gt;type();
5716   assert(field_klass-&gt;is_loaded(), "should be loaded");
5717   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5718   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5719   BasicType bt = field-&gt;layout_type();
5720 
5721   // Build the resultant type of the load
5722   const Type *type;
5723   if (bt == T_OBJECT) {
5724     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5725   } else {
5726     type = Type::get_const_basic_type(bt);
5727   }
5728 
5729   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
5730     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
5731   }
5732   // Build the load.
5733   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
5734   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, LoadNode::DependsOnlyOnTest, is_vol);
5735   // If reference is volatile, prevent following memory ops from
5736   // floating up past the volatile read.  Also prevents commoning
5737   // another volatile read.
5738   if (is_vol) {
5739     // Memory barrier includes bogus read of value to force load BEFORE membar
5740     insert_mem_bar(Op_MemBarAcquire, loadedField);
5741   }
5742   return loadedField;
5743 }
5744 
5745 
5746 //------------------------------inline_aescrypt_Block-----------------------
5747 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
5748   address stubAddr;
5749   const char *stubName;
5750   assert(UseAES, "need AES instruction support");
5751 
5752   switch(id) {
5753   case vmIntrinsics::_aescrypt_encryptBlock:
5754     stubAddr = StubRoutines::aescrypt_encryptBlock();
5755     stubName = "aescrypt_encryptBlock";
5756     break;
5757   case vmIntrinsics::_aescrypt_decryptBlock:
5758     stubAddr = StubRoutines::aescrypt_decryptBlock();
5759     stubName = "aescrypt_decryptBlock";
5760     break;
5761   }
5762   if (stubAddr == NULL) return false;
5763 
5764   Node* aescrypt_object = argument(0);
5765   Node* src             = argument(1);
5766   Node* src_offset      = argument(2);
5767   Node* dest            = argument(3);
5768   Node* dest_offset     = argument(4);
5769 
5770   // (1) src and dest are arrays.
5771   const Type* src_type = src-&gt;Value(&amp;_gvn);
5772   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5773   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5774   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5775   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5776 
5777   // for the quick and dirty code we will skip all the checks.
5778   // we are just trying to get the call to be generated.
5779   Node* src_start  = src;
5780   Node* dest_start = dest;
5781   if (src_offset != NULL || dest_offset != NULL) {
5782     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5783     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5784     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5785   }
5786 
5787   // now need to get the start of its expanded key array
5788   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5789   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5790   if (k_start == NULL) return false;
5791 
5792   if (Matcher::pass_original_key_for_aes()) {
5793     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5794     // compatibility issues between Java key expansion and SPARC crypto instructions
5795     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5796     if (original_k_start == NULL) return false;
5797 
5798     // Call the stub.
5799     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5800                       stubAddr, stubName, TypePtr::BOTTOM,
5801                       src_start, dest_start, k_start, original_k_start);
5802   } else {
5803     // Call the stub.
5804     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5805                       stubAddr, stubName, TypePtr::BOTTOM,
5806                       src_start, dest_start, k_start);
5807   }
5808 
5809   return true;
5810 }
5811 
5812 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
5813 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
5814   address stubAddr;
5815   const char *stubName;
5816 
5817   assert(UseAES, "need AES instruction support");
5818 
5819   switch(id) {
5820   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
5821     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
5822     stubName = "cipherBlockChaining_encryptAESCrypt";
5823     break;
5824   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
5825     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
5826     stubName = "cipherBlockChaining_decryptAESCrypt";
5827     break;
5828   }
5829   if (stubAddr == NULL) return false;
5830 
5831   Node* cipherBlockChaining_object = argument(0);
5832   Node* src                        = argument(1);
5833   Node* src_offset                 = argument(2);
5834   Node* len                        = argument(3);
5835   Node* dest                       = argument(4);
5836   Node* dest_offset                = argument(5);
5837 
5838   // (1) src and dest are arrays.
5839   const Type* src_type = src-&gt;Value(&amp;_gvn);
5840   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5841   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5842   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5843   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
5844           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5845 
5846   // checks are the responsibility of the caller
5847   Node* src_start  = src;
5848   Node* dest_start = dest;
5849   if (src_offset != NULL || dest_offset != NULL) {
5850     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5851     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5852     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5853   }
5854 
5855   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
5856   // (because of the predicated logic executed earlier).
5857   // so we cast it here safely.
5858   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5859 
5860   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5861   if (embeddedCipherObj == NULL) return false;
5862 
5863   // cast it to what we know it will be at runtime
5864   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
5865   assert(tinst != NULL, "CBC obj is null");
5866   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
5867   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5868   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
5869 
5870   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5871   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
5872   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
5873   Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
5874   aescrypt_object = _gvn.transform(aescrypt_object);
5875 
5876   // we need to get the start of the aescrypt_object's expanded key array
5877   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5878   if (k_start == NULL) return false;
5879 
5880   // similarly, get the start address of the r vector
5881   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
5882   if (objRvec == NULL) return false;
5883   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
5884 
5885   Node* cbcCrypt;
5886   if (Matcher::pass_original_key_for_aes()) {
5887     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5888     // compatibility issues between Java key expansion and SPARC crypto instructions
5889     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5890     if (original_k_start == NULL) return false;
5891 
5892     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
5893     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5894                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5895                                  stubAddr, stubName, TypePtr::BOTTOM,
5896                                  src_start, dest_start, k_start, r_start, len, original_k_start);
5897   } else {
5898     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
5899     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5900                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5901                                  stubAddr, stubName, TypePtr::BOTTOM,
5902                                  src_start, dest_start, k_start, r_start, len);
5903   }
5904 
5905   // return cipher length (int)
5906   Node* retvalue = _gvn.transform(new ProjNode(cbcCrypt, TypeFunc::Parms));
5907   set_result(retvalue);
5908   return true;
5909 }
5910 
5911 //------------------------------get_key_start_from_aescrypt_object-----------------------
5912 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
5913   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
5914   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5915   if (objAESCryptKey == NULL) return (Node *) NULL;
5916 
5917   // now have the array, need to get the start address of the K array
5918   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
5919   return k_start;
5920 }
5921 
5922 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
5923 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
5924   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
5925   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5926   if (objAESCryptKey == NULL) return (Node *) NULL;
5927 
5928   // now have the array, need to get the start address of the lastKey array
5929   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
5930   return original_k_start;
5931 }
5932 
5933 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
5934 // Return node representing slow path of predicate check.
5935 // the pseudo code we want to emulate with this predicate is:
5936 // for encryption:
5937 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
5938 // for decryption:
5939 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
5940 //    note cipher==plain is more conservative than the original java code but that's OK
5941 //
5942 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
5943   // The receiver was checked for NULL already.
5944   Node* objCBC = argument(0);
5945 
5946   // Load embeddedCipher field of CipherBlockChaining object.
5947   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5948 
5949   // get AESCrypt klass for instanceOf check
5950   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
5951   // will have same classloader as CipherBlockChaining object
5952   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
5953   assert(tinst != NULL, "CBCobj is null");
5954   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
5955 
5956   // we want to do an instanceof comparison against the AESCrypt class
5957   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5958   if (!klass_AESCrypt-&gt;is_loaded()) {
5959     // if AESCrypt is not even loaded, we never take the intrinsic fast path
5960     Node* ctrl = control();
5961     set_control(top()); // no regular fast path
5962     return ctrl;
5963   }
5964   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5965 
5966   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
5967   Node* cmp_instof  = _gvn.transform(new CmpINode(instof, intcon(1)));
5968   Node* bool_instof  = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
5969 
5970   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
5971 
5972   // for encryption, we are done
5973   if (!decrypting)
5974     return instof_false;  // even if it is NULL
5975 
5976   // for decryption, we need to add a further check to avoid
5977   // taking the intrinsic path when cipher and plain are the same
5978   // see the original java code for why.
5979   RegionNode* region = new RegionNode(3);
5980   region-&gt;init_req(1, instof_false);
5981   Node* src = argument(1);
5982   Node* dest = argument(4);
5983   Node* cmp_src_dest = _gvn.transform(new CmpPNode(src, dest));
5984   Node* bool_src_dest = _gvn.transform(new BoolNode(cmp_src_dest, BoolTest::eq));
5985   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
5986   region-&gt;init_req(2, src_dest_conjoint);
5987 
5988   record_for_igvn(region);
5989   return _gvn.transform(region);
5990 }
5991 
5992 //------------------------------inline_ghash_processBlocks
5993 bool LibraryCallKit::inline_ghash_processBlocks() {
5994   address stubAddr;
5995   const char *stubName;
5996   assert(UseGHASHIntrinsics, "need GHASH intrinsics support");
5997 
5998   stubAddr = StubRoutines::ghash_processBlocks();
5999   stubName = "ghash_processBlocks";
6000 
6001   Node* data           = argument(0);
6002   Node* offset         = argument(1);
6003   Node* len            = argument(2);
6004   Node* state          = argument(3);
6005   Node* subkeyH        = argument(4);
6006 
6007   Node* state_start  = array_element_address(state, intcon(0), T_LONG);
6008   assert(state_start, "state is NULL");
6009   Node* subkeyH_start  = array_element_address(subkeyH, intcon(0), T_LONG);
6010   assert(subkeyH_start, "subkeyH is NULL");
6011   Node* data_start  = array_element_address(data, offset, T_BYTE);
6012   assert(data_start, "data is NULL");
6013 
6014   Node* ghash = make_runtime_call(RC_LEAF|RC_NO_FP,
6015                                   OptoRuntime::ghash_processBlocks_Type(),
6016                                   stubAddr, stubName, TypePtr::BOTTOM,
6017                                   state_start, subkeyH_start, data_start, len);
6018   return true;
6019 }
6020 
6021 //------------------------------inline_sha_implCompress-----------------------
6022 //
6023 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6024 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6025 //
6026 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6027 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6028 //
6029 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6030 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6031 //
6032 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6033   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6034 
6035   Node* sha_obj = argument(0);
6036   Node* src     = argument(1); // type oop
6037   Node* ofs     = argument(2); // type int
6038 
6039   const Type* src_type = src-&gt;Value(&amp;_gvn);
6040   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6041   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6042     // failed array check
6043     return false;
6044   }
6045   // Figure out the size and type of the elements we will be copying.
6046   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6047   if (src_elem != T_BYTE) {
6048     return false;
6049   }
6050   // 'src_start' points to src array + offset
6051   Node* src_start = array_element_address(src, ofs, src_elem);
6052   Node* state = NULL;
6053   address stubAddr;
6054   const char *stubName;
6055 
6056   switch(id) {
6057   case vmIntrinsics::_sha_implCompress:
6058     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6059     state = get_state_from_sha_object(sha_obj);
6060     stubAddr = StubRoutines::sha1_implCompress();
6061     stubName = "sha1_implCompress";
6062     break;
6063   case vmIntrinsics::_sha2_implCompress:
6064     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6065     state = get_state_from_sha_object(sha_obj);
6066     stubAddr = StubRoutines::sha256_implCompress();
6067     stubName = "sha256_implCompress";
6068     break;
6069   case vmIntrinsics::_sha5_implCompress:
6070     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6071     state = get_state_from_sha5_object(sha_obj);
6072     stubAddr = StubRoutines::sha512_implCompress();
6073     stubName = "sha512_implCompress";
6074     break;
6075   default:
6076     fatal_unexpected_iid(id);
6077     return false;
6078   }
6079   if (state == NULL) return false;
6080 
6081   // Call the stub.
6082   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6083                                  stubAddr, stubName, TypePtr::BOTTOM,
6084                                  src_start, state);
6085 
6086   return true;
6087 }
6088 
6089 //------------------------------inline_digestBase_implCompressMB-----------------------
6090 //
6091 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6092 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6093 //
6094 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6095   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6096          "need SHA1/SHA256/SHA512 instruction support");
6097   assert((uint)predicate &lt; 3, "sanity");
6098   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6099 
6100   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6101   Node* src            = argument(1); // byte[] array
6102   Node* ofs            = argument(2); // type int
6103   Node* limit          = argument(3); // type int
6104 
6105   const Type* src_type = src-&gt;Value(&amp;_gvn);
6106   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6107   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6108     // failed array check
6109     return false;
6110   }
6111   // Figure out the size and type of the elements we will be copying.
6112   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6113   if (src_elem != T_BYTE) {
6114     return false;
6115   }
6116   // 'src_start' points to src array + offset
6117   Node* src_start = array_element_address(src, ofs, src_elem);
6118 
6119   const char* klass_SHA_name = NULL;
6120   const char* stub_name = NULL;
6121   address     stub_addr = NULL;
6122   bool        long_state = false;
6123 
6124   switch (predicate) {
6125   case 0:
6126     if (UseSHA1Intrinsics) {
6127       klass_SHA_name = "sun/security/provider/SHA";
6128       stub_name = "sha1_implCompressMB";
6129       stub_addr = StubRoutines::sha1_implCompressMB();
6130     }
6131     break;
6132   case 1:
6133     if (UseSHA256Intrinsics) {
6134       klass_SHA_name = "sun/security/provider/SHA2";
6135       stub_name = "sha256_implCompressMB";
6136       stub_addr = StubRoutines::sha256_implCompressMB();
6137     }
6138     break;
6139   case 2:
6140     if (UseSHA512Intrinsics) {
6141       klass_SHA_name = "sun/security/provider/SHA5";
6142       stub_name = "sha512_implCompressMB";
6143       stub_addr = StubRoutines::sha512_implCompressMB();
6144       long_state = true;
6145     }
6146     break;
6147   default:
6148     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6149   }
6150   if (klass_SHA_name != NULL) {
6151     // get DigestBase klass to lookup for SHA klass
6152     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6153     assert(tinst != NULL, "digestBase_obj is not instance???");
6154     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6155 
6156     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6157     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6158     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6159     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6160   }
6161   return false;
6162 }
6163 //------------------------------inline_sha_implCompressMB-----------------------
6164 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6165                                                bool long_state, address stubAddr, const char *stubName,
6166                                                Node* src_start, Node* ofs, Node* limit) {
6167   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6168   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6169   Node* sha_obj = new CheckCastPPNode(control(), digestBase_obj, xtype);
6170   sha_obj = _gvn.transform(sha_obj);
6171 
6172   Node* state;
6173   if (long_state) {
6174     state = get_state_from_sha5_object(sha_obj);
6175   } else {
6176     state = get_state_from_sha_object(sha_obj);
6177   }
6178   if (state == NULL) return false;
6179 
6180   // Call the stub.
6181   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6182                                  OptoRuntime::digestBase_implCompressMB_Type(),
6183                                  stubAddr, stubName, TypePtr::BOTTOM,
6184                                  src_start, state, ofs, limit);
6185   // return ofs (int)
6186   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
6187   set_result(result);
6188 
6189   return true;
6190 }
6191 
6192 //------------------------------get_state_from_sha_object-----------------------
6193 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6194   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6195   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6196   if (sha_state == NULL) return (Node *) NULL;
6197 
6198   // now have the array, need to get the start address of the state array
6199   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6200   return state;
6201 }
6202 
6203 //------------------------------get_state_from_sha5_object-----------------------
6204 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6205   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6206   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6207   if (sha_state == NULL) return (Node *) NULL;
6208 
6209   // now have the array, need to get the start address of the state array
6210   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6211   return state;
6212 }
6213 
6214 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6215 // Return node representing slow path of predicate check.
6216 // the pseudo code we want to emulate with this predicate is:
6217 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6218 //
6219 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6220   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6221          "need SHA1/SHA256/SHA512 instruction support");
6222   assert((uint)predicate &lt; 3, "sanity");
6223 
6224   // The receiver was checked for NULL already.
6225   Node* digestBaseObj = argument(0);
6226 
6227   // get DigestBase klass for instanceOf check
6228   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6229   assert(tinst != NULL, "digestBaseObj is null");
6230   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6231 
6232   const char* klass_SHA_name = NULL;
6233   switch (predicate) {
6234   case 0:
6235     if (UseSHA1Intrinsics) {
6236       // we want to do an instanceof comparison against the SHA class
6237       klass_SHA_name = "sun/security/provider/SHA";
6238     }
6239     break;
6240   case 1:
6241     if (UseSHA256Intrinsics) {
6242       // we want to do an instanceof comparison against the SHA2 class
6243       klass_SHA_name = "sun/security/provider/SHA2";
6244     }
6245     break;
6246   case 2:
6247     if (UseSHA512Intrinsics) {
6248       // we want to do an instanceof comparison against the SHA5 class
6249       klass_SHA_name = "sun/security/provider/SHA5";
6250     }
6251     break;
6252   default:
6253     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6254   }
6255 
6256   ciKlass* klass_SHA = NULL;
6257   if (klass_SHA_name != NULL) {
6258     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6259   }
6260   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6261     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6262     Node* ctrl = control();
6263     set_control(top()); // no intrinsic path
6264     return ctrl;
6265   }
6266   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6267 
6268   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6269   Node* cmp_instof = _gvn.transform(new CmpINode(instofSHA, intcon(1)));
6270   Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6271   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6272 
6273   return instof_false;  // even if it is NULL
6274 }
6275 
6276 bool LibraryCallKit::inline_profileBoolean() {
6277   Node* counts = argument(1);
6278   const TypeAryPtr* ary = NULL;
6279   ciArray* aobj = NULL;
6280   if (counts-&gt;is_Con()
6281       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL
6282       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL
6283       &amp;&amp; (aobj-&gt;length() == 2)) {
6284     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
6285     jint false_cnt = aobj-&gt;element_value(0).as_int();
6286     jint  true_cnt = aobj-&gt;element_value(1).as_int();
6287 
6288     if (C-&gt;log() != NULL) {
6289       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",
6290                      false_cnt, true_cnt);
6291     }
6292 
6293     if (false_cnt + true_cnt == 0) {
6294       // According to profile, never executed.
6295       uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6296                           Deoptimization::Action_reinterpret);
6297       return true;
6298     }
6299 
6300     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)
6301     // is a number of each value occurrences.
6302     Node* result = argument(0);
6303     if (false_cnt == 0 || true_cnt == 0) {
6304       // According to profile, one value has been never seen.
6305       int expected_val = (false_cnt == 0) ? 1 : 0;
6306 
6307       Node* cmp  = _gvn.transform(new CmpINode(result, intcon(expected_val)));
6308       Node* test = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
6309 
6310       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
6311       Node* fast_path = _gvn.transform(new IfTrueNode(check));
6312       Node* slow_path = _gvn.transform(new IfFalseNode(check));
6313 
6314       { // Slow path: uncommon trap for never seen value and then reexecute
6315         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
6316         // the value has been seen at least once.
6317         PreserveJVMState pjvms(this);
6318         PreserveReexecuteState preexecs(this);
6319         jvms()-&gt;set_should_reexecute(true);
6320 
6321         set_control(slow_path);
6322         set_i_o(i_o());
6323 
6324         uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6325                             Deoptimization::Action_reinterpret);
6326       }
6327       // The guard for never seen value enables sharpening of the result and
6328       // returning a constant. It allows to eliminate branches on the same value
6329       // later on.
6330       set_control(fast_path);
6331       result = intcon(expected_val);
6332     }
6333     // Stop profiling.
6334     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
6335     // By replacing method body with profile data (represented as ProfileBooleanNode
6336     // on IR level) we effectively disable profiling.
6337     // It enables full speed execution once optimized code is generated.
6338     Node* profile = _gvn.transform(new ProfileBooleanNode(result, false_cnt, true_cnt));
6339     C-&gt;record_for_igvn(profile);
6340     set_result(profile);
6341     return true;
6342   } else {
6343     // Continue profiling.
6344     // Profile data isn't available at the moment. So, execute method's bytecode version.
6345     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
6346     // is compiled and counters aren't available since corresponding MethodHandle
6347     // isn't a compile-time constant.
6348     return false;
6349   }
6350 }
6351 
6352 bool LibraryCallKit::inline_isCompileConstant() {
6353   Node* n = argument(0);
6354   set_result(n-&gt;is_Con() ? intcon(1) : intcon(0));
6355   return true;
6356 }
</pre></body></html>
