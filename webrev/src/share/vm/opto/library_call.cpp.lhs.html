<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "classfile/vmSymbols.hpp"
  29 #include "compiler/compileBroker.hpp"
  30 #include "compiler/compileLog.hpp"
  31 #include "oops/objArrayKlass.hpp"
  32 #include "opto/addnode.hpp"
  33 #include "opto/arraycopynode.hpp"
  34 #include "opto/c2compiler.hpp"
  35 #include "opto/callGenerator.hpp"
  36 #include "opto/castnode.hpp"
  37 #include "opto/cfgnode.hpp"
  38 #include "opto/convertnode.hpp"
  39 #include "opto/countbitsnode.hpp"
  40 #include "opto/intrinsicnode.hpp"
  41 #include "opto/idealKit.hpp"
  42 #include "opto/mathexactnode.hpp"
  43 #include "opto/movenode.hpp"
  44 #include "opto/mulnode.hpp"
  45 #include "opto/narrowptrnode.hpp"
  46 #include "opto/opaquenode.hpp"
  47 #include "opto/parse.hpp"
  48 #include "opto/runtime.hpp"
  49 #include "opto/subnode.hpp"
  50 #include "prims/nativeLookup.hpp"
  51 #include "runtime/sharedRuntime.hpp"
  52 #include "trace/traceMacros.hpp"
  53 
  54 class LibraryIntrinsic : public InlineCallGenerator {
  55   // Extend the set of intrinsics known to the runtime:
  56  public:
  57  private:
  58   bool             _is_virtual;
  59   bool             _does_virtual_dispatch;
  60   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  61   int8_t           _last_predicate; // Last generated predicate
  62   vmIntrinsics::ID _intrinsic_id;
  63 
  64  public:
  65   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  66     : InlineCallGenerator(m),
  67       _is_virtual(is_virtual),
  68       _does_virtual_dispatch(does_virtual_dispatch),
  69       _predicates_count((int8_t)predicates_count),
  70       _last_predicate((int8_t)-1),
  71       _intrinsic_id(id)
  72   {
  73   }
  74   virtual bool is_intrinsic() const { return true; }
  75   virtual bool is_virtual()   const { return _is_virtual; }
  76   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  77   virtual int  predicates_count() const { return _predicates_count; }
  78   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  79   virtual JVMState* generate(JVMState* jvms);
  80   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  81   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  82 };
  83 
  84 
  85 // Local helper class for LibraryIntrinsic:
  86 class LibraryCallKit : public GraphKit {
  87  private:
  88   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  89   Node*             _result;        // the result node, if any
  90   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  91 
  92   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  93 
  94  public:
  95   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  96     : GraphKit(jvms),
  97       _intrinsic(intrinsic),
  98       _result(NULL)
  99   {
 100     // Check if this is a root compile.  In that case we don't have a caller.
 101     if (!jvms-&gt;has_method()) {
 102       _reexecute_sp = sp();
 103     } else {
 104       // Find out how many arguments the interpreter needs when deoptimizing
 105       // and save the stack pointer value so it can used by uncommon_trap.
 106       // We find the argument count by looking at the declared signature.
 107       bool ignored_will_link;
 108       ciSignature* declared_signature = NULL;
 109       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 110       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 111       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 112     }
 113   }
 114 
 115   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 116 
 117   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 118   int               bci()       const    { return jvms()-&gt;bci(); }
 119   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 120   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 121   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 122 
 123   bool  try_to_inline(int predicate);
 124   Node* try_to_predicate(int predicate);
 125 
 126   void push_result() {
 127     // Push the result onto the stack.
 128     if (!stopped() &amp;&amp; result() != NULL) {
 129       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 130       push_node(bt, result());
 131     }
 132   }
 133 
 134  private:
 135   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 136     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 137   }
 138 
 139   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 140   void  set_result(RegionNode* region, PhiNode* value);
 141   Node*     result() { return _result; }
 142 
 143   virtual int reexecute_sp() { return _reexecute_sp; }
 144 
 145   // Helper functions to inline natives
 146   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 147   Node* generate_slow_guard(Node* test, RegionNode* region);
 148   Node* generate_fair_guard(Node* test, RegionNode* region);
 149   Node* generate_negative_guard(Node* index, RegionNode* region,
 150                                 // resulting CastII of index:
 151                                 Node* *pos_index = NULL);
 152   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 153                              Node* array_length,
 154                              RegionNode* region);
 155   Node* generate_current_thread(Node* &amp;tls_output);
 156   Node* load_mirror_from_klass(Node* klass);
 157   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 158                                       RegionNode* region, int null_path,
 159                                       int offset);
 160   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 161                                RegionNode* region, int null_path) {
 162     int offset = java_lang_Class::klass_offset_in_bytes();
 163     return load_klass_from_mirror_common(mirror, never_see_null,
 164                                          region, null_path,
 165                                          offset);
 166   }
 167   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 168                                      RegionNode* region, int null_path) {
 169     int offset = java_lang_Class::array_klass_offset_in_bytes();
 170     return load_klass_from_mirror_common(mirror, never_see_null,
 171                                          region, null_path,
 172                                          offset);
 173   }
 174   Node* generate_access_flags_guard(Node* kls,
 175                                     int modifier_mask, int modifier_bits,
 176                                     RegionNode* region);
 177   Node* generate_interface_guard(Node* kls, RegionNode* region);
 178   Node* generate_array_guard(Node* kls, RegionNode* region) {
 179     return generate_array_guard_common(kls, region, false, false);
 180   }
 181   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 182     return generate_array_guard_common(kls, region, false, true);
 183   }
 184   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 185     return generate_array_guard_common(kls, region, true, false);
 186   }
 187   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 188     return generate_array_guard_common(kls, region, true, true);
 189   }
 190   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 191                                     bool obj_array, bool not_array);
 192   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 193   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 194                                      bool is_virtual = false, bool is_static = false);
 195   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 196     return generate_method_call(method_id, false, true);
 197   }
 198   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 199     return generate_method_call(method_id, true, false);
 200   }
 201   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 202 
 203   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 204   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 205   bool inline_string_compareTo();
 206   bool inline_string_indexOf();
 207   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 208   bool inline_string_equals();
 209   Node* round_double_node(Node* n);
 210   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 211   bool inline_math_native(vmIntrinsics::ID id);
 212   bool inline_trig(vmIntrinsics::ID id);
 213   bool inline_math(vmIntrinsics::ID id);
 214   template &lt;typename OverflowOp&gt;
 215   bool inline_math_overflow(Node* arg1, Node* arg2);
 216   void inline_math_mathExact(Node* math, Node* test);
 217   bool inline_math_addExactI(bool is_increment);
 218   bool inline_math_addExactL(bool is_increment);
 219   bool inline_math_multiplyExactI();
 220   bool inline_math_multiplyExactL();
 221   bool inline_math_negateExactI();
 222   bool inline_math_negateExactL();
 223   bool inline_math_subtractExactI(bool is_decrement);
 224   bool inline_math_subtractExactL(bool is_decrement);
 225   bool inline_exp();
 226   bool inline_pow();
 227   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 228   bool inline_min_max(vmIntrinsics::ID id);
 229   bool inline_notify(vmIntrinsics::ID id);
 230   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 231   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 232   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 233   Node* make_unsafe_address(Node* base, Node* offset);
 234   // Helper for inline_unsafe_access.
 235   // Generates the guards that check whether the result of
 236   // Unsafe.getObject should be recorded in an SATB log buffer.
 237   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 238   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 239   static bool klass_needs_init_guard(Node* kls);
 240   bool inline_unsafe_allocate();
 241   bool inline_unsafe_copyMemory();
 242   bool inline_native_currentThread();
 243 #ifdef TRACE_HAVE_INTRINSICS
 244   bool inline_native_classID();
 245   bool inline_native_threadID();
 246 #endif
 247   bool inline_native_time_funcs(address method, const char* funcName);
 248   bool inline_native_isInterrupted();
 249   bool inline_native_Class_query(vmIntrinsics::ID id);
 250   bool inline_native_subtype_check();
 251 
 252   bool inline_native_newArray();
 253   bool inline_native_getLength();
 254   bool inline_array_copyOf(bool is_copyOfRange);
 255   bool inline_array_equals();
 256   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 257   bool inline_native_clone(bool is_virtual);
 258   bool inline_native_Reflection_getCallerClass();
 259   // Helper function for inlining native object hash method
 260   bool inline_native_hashcode(bool is_virtual, bool is_static);
 261   bool inline_native_getClass();
 262 
 263   // Helper functions for inlining arraycopy
 264   bool inline_arraycopy();
 265   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 266                                                 RegionNode* slow_region);
 267   JVMState* arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp);
 268   void arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp);
 269 
 270   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 271   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 272   bool inline_unsafe_ordered_store(BasicType type);
 273   bool inline_unsafe_fence(vmIntrinsics::ID id);
<a name="1" id="anc1"></a>
 274   bool inline_fp_conversions(vmIntrinsics::ID id);
 275   bool inline_number_methods(vmIntrinsics::ID id);
 276   bool inline_reference_get();
 277   bool inline_Class_cast();
 278   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 279   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 280   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 281   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 282   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 283   bool inline_ghash_processBlocks();
 284   bool inline_sha_implCompress(vmIntrinsics::ID id);
 285   bool inline_digestBase_implCompressMB(int predicate);
 286   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 287                                  bool long_state, address stubAddr, const char *stubName,
 288                                  Node* src_start, Node* ofs, Node* limit);
 289   Node* get_state_from_sha_object(Node *sha_object);
 290   Node* get_state_from_sha5_object(Node *sha_object);
 291   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 292   bool inline_encodeISOArray();
 293   bool inline_updateCRC32();
 294   bool inline_updateBytesCRC32();
 295   bool inline_updateByteBufferCRC32();
 296   Node* get_table_from_crc32c_class(ciInstanceKlass *crc32c_class);
 297   bool inline_updateBytesCRC32C();
 298   bool inline_updateDirectByteBufferCRC32C();
 299   bool inline_updateBytesAdler32();
 300   bool inline_updateByteBufferAdler32();
 301   bool inline_multiplyToLen();
 302   bool inline_squareToLen();
 303   bool inline_mulAdd();
 304   bool inline_montgomeryMultiply();
 305   bool inline_montgomerySquare();
 306 
 307   bool inline_profileBoolean();
 308   bool inline_isCompileConstant();
 309 };
 310 
 311 //---------------------------make_vm_intrinsic----------------------------
 312 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 313   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 314   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 315 
 316   if (!m-&gt;is_loaded()) {
 317     // Do not attempt to inline unloaded methods.
 318     return NULL;
 319   }
 320 
 321   C2Compiler* compiler = (C2Compiler*)CompileBroker::compiler(CompLevel_full_optimization);
 322   bool is_available = false;
 323 
 324   {
 325     // For calling is_intrinsic_supported and is_intrinsic_disabled_by_flag
 326     // the compiler must transition to '_thread_in_vm' state because both
 327     // methods access VM-internal data.
 328     VM_ENTRY_MARK;
 329     methodHandle mh(THREAD, m-&gt;get_Method());
 330     methodHandle ct(THREAD, method()-&gt;get_Method());
 331     is_available = compiler-&gt;is_intrinsic_supported(mh, is_virtual) &amp;&amp;
 332                    !vmIntrinsics::is_disabled_by_flags(mh, ct);
 333   }
 334 
 335   if (is_available) {
 336     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 337     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 338     return new LibraryIntrinsic(m, is_virtual,
 339                                 vmIntrinsics::predicates_needed(id),
 340                                 vmIntrinsics::does_virtual_dispatch(id),
 341                                 (vmIntrinsics::ID) id);
 342   } else {
 343     return NULL;
 344   }
 345 }
 346 
 347 //----------------------register_library_intrinsics-----------------------
 348 // Initialize this file's data structures, for each Compile instance.
 349 void Compile::register_library_intrinsics() {
 350   // Nothing to do here.
 351 }
 352 
 353 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 354   LibraryCallKit kit(jvms, this);
 355   Compile* C = kit.C;
 356   int nodes = C-&gt;unique();
 357 #ifndef PRODUCT
 358   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 359     char buf[1000];
 360     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 361     tty-&gt;print_cr("Intrinsic %s", str);
 362   }
 363 #endif
 364   ciMethod* callee = kit.callee();
 365   const int bci    = kit.bci();
 366 
 367   // Try to inline the intrinsic.
 368   if ((CheckIntrinsics ? callee-&gt;intrinsic_candidate() : true) &amp;&amp;
 369       kit.try_to_inline(_last_predicate)) {
 370     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 371       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 372     }
 373     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 374     if (C-&gt;log()) {
 375       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 376                      vmIntrinsics::name_at(intrinsic_id()),
 377                      (is_virtual() ? " virtual='1'" : ""),
 378                      C-&gt;unique() - nodes);
 379     }
 380     // Push the result from the inlined method onto the stack.
 381     kit.push_result();
 382     C-&gt;print_inlining_update(this);
 383     return kit.transfer_exceptions_into_jvms();
 384   }
 385 
 386   // The intrinsic bailed out
 387   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 388     if (jvms-&gt;has_method()) {
 389       // Not a root compile.
 390       const char* msg;
 391       if (callee-&gt;intrinsic_candidate()) {
 392         msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 393       } else {
 394         msg = is_virtual() ? "failed to inline (intrinsic, virtual), method not annotated"
 395                            : "failed to inline (intrinsic), method not annotated";
 396       }
 397       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 398     } else {
 399       // Root compile
 400       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 401                vmIntrinsics::name_at(intrinsic_id()),
 402                (is_virtual() ? " (virtual)" : ""), bci);
 403     }
 404   }
 405   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 406   C-&gt;print_inlining_update(this);
 407   return NULL;
 408 }
 409 
 410 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 411   LibraryCallKit kit(jvms, this);
 412   Compile* C = kit.C;
 413   int nodes = C-&gt;unique();
 414   _last_predicate = predicate;
 415 #ifndef PRODUCT
 416   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 417   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 418     char buf[1000];
 419     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 420     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 421   }
 422 #endif
 423   ciMethod* callee = kit.callee();
 424   const int bci    = kit.bci();
 425 
 426   Node* slow_ctl = kit.try_to_predicate(predicate);
 427   if (!kit.failing()) {
 428     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 429       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 430     }
 431     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 432     if (C-&gt;log()) {
 433       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 434                      vmIntrinsics::name_at(intrinsic_id()),
 435                      (is_virtual() ? " virtual='1'" : ""),
 436                      C-&gt;unique() - nodes);
 437     }
 438     return slow_ctl; // Could be NULL if the check folds.
 439   }
 440 
 441   // The intrinsic bailed out
 442   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 443     if (jvms-&gt;has_method()) {
 444       // Not a root compile.
 445       const char* msg = "failed to generate predicate for intrinsic";
 446       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 447     } else {
 448       // Root compile
 449       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 450                                         vmIntrinsics::name_at(intrinsic_id()),
 451                                         (is_virtual() ? " (virtual)" : ""), bci);
 452     }
 453   }
 454   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 455   return NULL;
 456 }
 457 
 458 bool LibraryCallKit::try_to_inline(int predicate) {
 459   // Handle symbolic names for otherwise undistinguished boolean switches:
 460   const bool is_store       = true;
 461   const bool is_native_ptr  = true;
 462   const bool is_static      = true;
 463   const bool is_volatile    = true;
 464 
 465   if (!jvms()-&gt;has_method()) {
 466     // Root JVMState has a null method.
 467     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 468     // Insert the memory aliasing node
 469     set_all_memory(reset_memory());
 470   }
 471   assert(merged_memory(), "");
 472 
 473 
 474   switch (intrinsic_id()) {
 475   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 476   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 477   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 478 
 479   case vmIntrinsics::_dsin:
 480   case vmIntrinsics::_dcos:
 481   case vmIntrinsics::_dtan:
 482   case vmIntrinsics::_dabs:
 483   case vmIntrinsics::_datan2:
 484   case vmIntrinsics::_dsqrt:
 485   case vmIntrinsics::_dexp:
 486   case vmIntrinsics::_dlog:
 487   case vmIntrinsics::_dlog10:
 488   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 489 
 490   case vmIntrinsics::_min:
 491   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 492 
 493   case vmIntrinsics::_notify:
 494   case vmIntrinsics::_notifyAll:
 495     if (InlineNotify) {
 496       return inline_notify(intrinsic_id());
 497     }
 498     return false;
 499 
 500   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 501   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 502   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 503   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 504   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 505   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 506   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 507   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 508   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 509   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 510   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 511   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 512 
 513   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 514 
 515   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 516   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 517   case vmIntrinsics::_equals:                   return inline_string_equals();
 518 
 519   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 520   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 521   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 522   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 523   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 524   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 525   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 526   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 527   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 528   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 529   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 530   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 531   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 532   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 533   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 534   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 535   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 536   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 537 
 538   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 539   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 540   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 541   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 542   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 543   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 544   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 545   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 546 
 547   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 548   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 549   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 550   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 551   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 552   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 553   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 554   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 555 
 556   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 557   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 558   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 559   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 560   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 561   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 562   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 563   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 564   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 565 
 566   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 567   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 568   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 569   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 570   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 571   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 572   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 573   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 574   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 575 
 576   case vmIntrinsics::_getShortUnaligned:        return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 577   case vmIntrinsics::_getCharUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 578   case vmIntrinsics::_getIntUnaligned:          return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 579   case vmIntrinsics::_getLongUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 580 
 581   case vmIntrinsics::_putShortUnaligned:        return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 582   case vmIntrinsics::_putCharUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 583   case vmIntrinsics::_putIntUnaligned:          return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 584   case vmIntrinsics::_putLongUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 585 
 586   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 587   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 588   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 589 
 590   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 591   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 592   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 593 
 594   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 595   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 596   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 597   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 598   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 599 
 600   case vmIntrinsics::_loadFence:
 601   case vmIntrinsics::_storeFence:
 602   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 603 
<a name="2" id="anc2"></a>

 604   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 605   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 606 
 607 #ifdef TRACE_HAVE_INTRINSICS
 608   case vmIntrinsics::_classID:                  return inline_native_classID();
 609   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 610   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 611 #endif
 612   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 613   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 614   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 615   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 616   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 617   case vmIntrinsics::_getLength:                return inline_native_getLength();
 618   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 619   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 620   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 621   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 622 
 623   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 624 
 625   case vmIntrinsics::_isInstance:
 626   case vmIntrinsics::_getModifiers:
 627   case vmIntrinsics::_isInterface:
 628   case vmIntrinsics::_isArray:
 629   case vmIntrinsics::_isPrimitive:
 630   case vmIntrinsics::_getSuperclass:
 631   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 632 
 633   case vmIntrinsics::_floatToRawIntBits:
 634   case vmIntrinsics::_floatToIntBits:
 635   case vmIntrinsics::_intBitsToFloat:
 636   case vmIntrinsics::_doubleToRawLongBits:
 637   case vmIntrinsics::_doubleToLongBits:
 638   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 639 
 640   case vmIntrinsics::_numberOfLeadingZeros_i:
 641   case vmIntrinsics::_numberOfLeadingZeros_l:
 642   case vmIntrinsics::_numberOfTrailingZeros_i:
 643   case vmIntrinsics::_numberOfTrailingZeros_l:
 644   case vmIntrinsics::_bitCount_i:
 645   case vmIntrinsics::_bitCount_l:
 646   case vmIntrinsics::_reverseBytes_i:
 647   case vmIntrinsics::_reverseBytes_l:
 648   case vmIntrinsics::_reverseBytes_s:
 649   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 650 
 651   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 652 
 653   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 654 
 655   case vmIntrinsics::_Class_cast:               return inline_Class_cast();
 656 
 657   case vmIntrinsics::_aescrypt_encryptBlock:
 658   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 659 
 660   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 661   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 662     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 663 
 664   case vmIntrinsics::_sha_implCompress:
 665   case vmIntrinsics::_sha2_implCompress:
 666   case vmIntrinsics::_sha5_implCompress:
 667     return inline_sha_implCompress(intrinsic_id());
 668 
 669   case vmIntrinsics::_digestBase_implCompressMB:
 670     return inline_digestBase_implCompressMB(predicate);
 671 
 672   case vmIntrinsics::_multiplyToLen:
 673     return inline_multiplyToLen();
 674 
 675   case vmIntrinsics::_squareToLen:
 676     return inline_squareToLen();
 677 
 678   case vmIntrinsics::_mulAdd:
 679     return inline_mulAdd();
 680 
 681   case vmIntrinsics::_montgomeryMultiply:
 682     return inline_montgomeryMultiply();
 683   case vmIntrinsics::_montgomerySquare:
 684     return inline_montgomerySquare();
 685 
 686   case vmIntrinsics::_ghash_processBlocks:
 687     return inline_ghash_processBlocks();
 688 
 689   case vmIntrinsics::_encodeISOArray:
 690     return inline_encodeISOArray();
 691 
 692   case vmIntrinsics::_updateCRC32:
 693     return inline_updateCRC32();
 694   case vmIntrinsics::_updateBytesCRC32:
 695     return inline_updateBytesCRC32();
 696   case vmIntrinsics::_updateByteBufferCRC32:
 697     return inline_updateByteBufferCRC32();
 698 
 699   case vmIntrinsics::_updateBytesCRC32C:
 700     return inline_updateBytesCRC32C();
 701   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 702     return inline_updateDirectByteBufferCRC32C();
 703 
 704   case vmIntrinsics::_updateBytesAdler32:
 705     return inline_updateBytesAdler32();
 706   case vmIntrinsics::_updateByteBufferAdler32:
 707     return inline_updateByteBufferAdler32();
 708 
 709   case vmIntrinsics::_profileBoolean:
 710     return inline_profileBoolean();
 711   case vmIntrinsics::_isCompileConstant:
 712     return inline_isCompileConstant();
 713 
 714   default:
 715     // If you get here, it may be that someone has added a new intrinsic
 716     // to the list in vmSymbols.hpp without implementing it here.
 717 #ifndef PRODUCT
 718     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 719       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 720                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 721     }
 722 #endif
 723     return false;
 724   }
 725 }
 726 
 727 Node* LibraryCallKit::try_to_predicate(int predicate) {
 728   if (!jvms()-&gt;has_method()) {
 729     // Root JVMState has a null method.
 730     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 731     // Insert the memory aliasing node
 732     set_all_memory(reset_memory());
 733   }
 734   assert(merged_memory(), "");
 735 
 736   switch (intrinsic_id()) {
 737   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 738     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 739   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 740     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 741   case vmIntrinsics::_digestBase_implCompressMB:
 742     return inline_digestBase_implCompressMB_predicate(predicate);
 743 
 744   default:
 745     // If you get here, it may be that someone has added a new intrinsic
 746     // to the list in vmSymbols.hpp without implementing it here.
 747 #ifndef PRODUCT
 748     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 749       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 750                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 751     }
 752 #endif
 753     Node* slow_ctl = control();
 754     set_control(top()); // No fast path instrinsic
 755     return slow_ctl;
 756   }
 757 }
 758 
 759 //------------------------------set_result-------------------------------
 760 // Helper function for finishing intrinsics.
 761 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 762   record_for_igvn(region);
 763   set_control(_gvn.transform(region));
 764   set_result( _gvn.transform(value));
 765   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
 766 }
 767 
 768 //------------------------------generate_guard---------------------------
 769 // Helper function for generating guarded fast-slow graph structures.
 770 // The given 'test', if true, guards a slow path.  If the test fails
 771 // then a fast path can be taken.  (We generally hope it fails.)
 772 // In all cases, GraphKit::control() is updated to the fast path.
 773 // The returned value represents the control for the slow path.
 774 // The return value is never 'top'; it is either a valid control
 775 // or NULL if it is obvious that the slow path can never be taken.
 776 // Also, if region and the slow control are not NULL, the slow edge
 777 // is appended to the region.
 778 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
 779   if (stopped()) {
 780     // Already short circuited.
 781     return NULL;
 782   }
 783 
 784   // Build an if node and its projections.
 785   // If test is true we take the slow path, which we assume is uncommon.
 786   if (_gvn.type(test) == TypeInt::ZERO) {
 787     // The slow branch is never taken.  No need to build this guard.
 788     return NULL;
 789   }
 790 
 791   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
 792 
 793   Node* if_slow = _gvn.transform(new IfTrueNode(iff));
 794   if (if_slow == top()) {
 795     // The slow branch is never taken.  No need to build this guard.
 796     return NULL;
 797   }
 798 
 799   if (region != NULL)
 800     region-&gt;add_req(if_slow);
 801 
 802   Node* if_fast = _gvn.transform(new IfFalseNode(iff));
 803   set_control(if_fast);
 804 
 805   return if_slow;
 806 }
 807 
 808 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
 809   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
 810 }
 811 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
 812   return generate_guard(test, region, PROB_FAIR);
 813 }
 814 
 815 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
 816                                                      Node* *pos_index) {
 817   if (stopped())
 818     return NULL;                // already stopped
 819   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
 820     return NULL;                // index is already adequately typed
 821   Node* cmp_lt = _gvn.transform(new CmpINode(index, intcon(0)));
 822   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 823   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
 824   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
 825     // Emulate effect of Parse::adjust_map_after_if.
 826     Node* ccast = new CastIINode(index, TypeInt::POS);
 827     ccast-&gt;set_req(0, control());
 828     (*pos_index) = _gvn.transform(ccast);
 829   }
 830   return is_neg;
 831 }
 832 
 833 // Make sure that 'position' is a valid limit index, in [0..length].
 834 // There are two equivalent plans for checking this:
 835 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
 836 //   B. offset  &lt;=  (arrayLength - copyLength)
 837 // We require that all of the values above, except for the sum and
 838 // difference, are already known to be non-negative.
 839 // Plan A is robust in the face of overflow, if offset and copyLength
 840 // are both hugely positive.
 841 //
 842 // Plan B is less direct and intuitive, but it does not overflow at
 843 // all, since the difference of two non-negatives is always
 844 // representable.  Whenever Java methods must perform the equivalent
 845 // check they generally use Plan B instead of Plan A.
 846 // For the moment we use Plan A.
 847 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
 848                                                   Node* subseq_length,
 849                                                   Node* array_length,
 850                                                   RegionNode* region) {
 851   if (stopped())
 852     return NULL;                // already stopped
 853   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
 854   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
 855     return NULL;                // common case of whole-array copy
 856   Node* last = subseq_length;
 857   if (!zero_offset)             // last += offset
 858     last = _gvn.transform(new AddINode(last, offset));
 859   Node* cmp_lt = _gvn.transform(new CmpUNode(array_length, last));
 860   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 861   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
 862   return is_over;
 863 }
 864 
 865 
 866 //--------------------------generate_current_thread--------------------
 867 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
 868   ciKlass*    thread_klass = env()-&gt;Thread_klass();
 869   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
 870   Node* thread = _gvn.transform(new ThreadLocalNode());
 871   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
 872   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
 873   tls_output = thread;
 874   return threadObj;
 875 }
 876 
 877 
 878 //------------------------------make_string_method_node------------------------
 879 // Helper method for String intrinsic functions. This version is called
 880 // with str1 and str2 pointing to String object nodes.
 881 //
 882 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
 883   Node* no_ctrl = NULL;
 884 
 885   // Get start addr of string
 886   Node* str1_value   = load_String_value(no_ctrl, str1);
 887   Node* str1_offset  = load_String_offset(no_ctrl, str1);
 888   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
 889 
 890   // Get length of string 1
 891   Node* str1_len  = load_String_length(no_ctrl, str1);
 892 
 893   Node* str2_value   = load_String_value(no_ctrl, str2);
 894   Node* str2_offset  = load_String_offset(no_ctrl, str2);
 895   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
 896 
 897   Node* str2_len = NULL;
 898   Node* result = NULL;
 899 
 900   switch (opcode) {
 901   case Op_StrIndexOf:
 902     // Get length of string 2
 903     str2_len = load_String_length(no_ctrl, str2);
 904 
 905     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
 906                                 str1_start, str1_len, str2_start, str2_len);
 907     break;
 908   case Op_StrComp:
 909     // Get length of string 2
 910     str2_len = load_String_length(no_ctrl, str2);
 911 
 912     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
 913                              str1_start, str1_len, str2_start, str2_len);
 914     break;
 915   case Op_StrEquals:
 916     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
 917                                str1_start, str2_start, str1_len);
 918     break;
 919   default:
 920     ShouldNotReachHere();
 921     return NULL;
 922   }
 923 
 924   // All these intrinsics have checks.
 925   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
 926 
 927   return _gvn.transform(result);
 928 }
 929 
 930 // Helper method for String intrinsic functions. This version is called
 931 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
 932 // to Int nodes containing the lenghts of str1 and str2.
 933 //
 934 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
 935   Node* result = NULL;
 936   switch (opcode) {
 937   case Op_StrIndexOf:
 938     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
 939                                 str1_start, cnt1, str2_start, cnt2);
 940     break;
 941   case Op_StrComp:
 942     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
 943                              str1_start, cnt1, str2_start, cnt2);
 944     break;
 945   case Op_StrEquals:
 946     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
 947                                str1_start, str2_start, cnt1);
 948     break;
 949   default:
 950     ShouldNotReachHere();
 951     return NULL;
 952   }
 953 
 954   // All these intrinsics have checks.
 955   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
 956 
 957   return _gvn.transform(result);
 958 }
 959 
 960 //------------------------------inline_string_compareTo------------------------
 961 // public int java.lang.String.compareTo(String anotherString);
 962 bool LibraryCallKit::inline_string_compareTo() {
 963   Node* receiver = null_check(argument(0));
 964   Node* arg      = null_check(argument(1));
 965   if (stopped()) {
 966     return true;
 967   }
 968   set_result(make_string_method_node(Op_StrComp, receiver, arg));
 969   return true;
 970 }
 971 
 972 //------------------------------inline_string_equals------------------------
 973 bool LibraryCallKit::inline_string_equals() {
 974   Node* receiver = null_check_receiver();
 975   // NOTE: Do not null check argument for String.equals() because spec
 976   // allows to specify NULL as argument.
 977   Node* argument = this-&gt;argument(1);
 978   if (stopped()) {
 979     return true;
 980   }
 981 
 982   // paths (plus control) merge
 983   RegionNode* region = new RegionNode(5);
 984   Node* phi = new PhiNode(region, TypeInt::BOOL);
 985 
 986   // does source == target string?
 987   Node* cmp = _gvn.transform(new CmpPNode(receiver, argument));
 988   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
 989 
 990   Node* if_eq = generate_slow_guard(bol, NULL);
 991   if (if_eq != NULL) {
 992     // receiver == argument
 993     phi-&gt;init_req(2, intcon(1));
 994     region-&gt;init_req(2, if_eq);
 995   }
 996 
 997   // get String klass for instanceOf
 998   ciInstanceKlass* klass = env()-&gt;String_klass();
 999 
1000   if (!stopped()) {
1001     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1002     Node* cmp  = _gvn.transform(new CmpINode(inst, intcon(1)));
1003     Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1004 
1005     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1006     //instanceOf == true, fallthrough
1007 
1008     if (inst_false != NULL) {
1009       phi-&gt;init_req(3, intcon(0));
1010       region-&gt;init_req(3, inst_false);
1011     }
1012   }
1013 
1014   if (!stopped()) {
1015     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1016 
1017     // Properly cast the argument to String
1018     argument = _gvn.transform(new CheckCastPPNode(control(), argument, string_type));
1019     // This path is taken only when argument's type is String:NotNull.
1020     argument = cast_not_null(argument, false);
1021 
1022     Node* no_ctrl = NULL;
1023 
1024     // Get start addr of receiver
1025     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1026     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1027     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1028 
1029     // Get length of receiver
1030     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1031 
1032     // Get start addr of argument
1033     Node* argument_val    = load_String_value(no_ctrl, argument);
1034     Node* argument_offset = load_String_offset(no_ctrl, argument);
1035     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1036 
1037     // Get length of argument
1038     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1039 
1040     // Check for receiver count != argument count
1041     Node* cmp = _gvn.transform(new CmpINode(receiver_cnt, argument_cnt));
1042     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1043     Node* if_ne = generate_slow_guard(bol, NULL);
1044     if (if_ne != NULL) {
1045       phi-&gt;init_req(4, intcon(0));
1046       region-&gt;init_req(4, if_ne);
1047     }
1048 
1049     // Check for count == 0 is done by assembler code for StrEquals.
1050 
1051     if (!stopped()) {
1052       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1053       phi-&gt;init_req(1, equals);
1054       region-&gt;init_req(1, control());
1055     }
1056   }
1057 
1058   // post merge
1059   set_control(_gvn.transform(region));
1060   record_for_igvn(region);
1061 
1062   set_result(_gvn.transform(phi));
1063   return true;
1064 }
1065 
1066 //------------------------------inline_array_equals----------------------------
1067 bool LibraryCallKit::inline_array_equals() {
1068   Node* arg1 = argument(0);
1069   Node* arg2 = argument(1);
1070   set_result(_gvn.transform(new AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1071   return true;
1072 }
1073 
1074 // Java version of String.indexOf(constant string)
1075 // class StringDecl {
1076 //   StringDecl(char[] ca) {
1077 //     offset = 0;
1078 //     count = ca.length;
1079 //     value = ca;
1080 //   }
1081 //   int offset;
1082 //   int count;
1083 //   char[] value;
1084 // }
1085 //
1086 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1087 //                             int targetOffset, int cache_i, int md2) {
1088 //   int cache = cache_i;
1089 //   int sourceOffset = string_object.offset;
1090 //   int sourceCount = string_object.count;
1091 //   int targetCount = target_object.length;
1092 //
1093 //   int targetCountLess1 = targetCount - 1;
1094 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1095 //
1096 //   char[] source = string_object.value;
1097 //   char[] target = target_object;
1098 //   int lastChar = target[targetCountLess1];
1099 //
1100 //  outer_loop:
1101 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1102 //     int src = source[i + targetCountLess1];
1103 //     if (src == lastChar) {
1104 //       // With random strings and a 4-character alphabet,
1105 //       // reverse matching at this point sets up 0.8% fewer
1106 //       // frames, but (paradoxically) makes 0.3% more probes.
1107 //       // Since those probes are nearer the lastChar probe,
1108 //       // there is may be a net D$ win with reverse matching.
1109 //       // But, reversing loop inhibits unroll of inner loop
1110 //       // for unknown reason.  So, does running outer loop from
1111 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1112 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1113 //         if (target[targetOffset + j] != source[i+j]) {
1114 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1115 //             if (md2 &lt; j+1) {
1116 //               i += j+1;
1117 //               continue outer_loop;
1118 //             }
1119 //           }
1120 //           i += md2;
1121 //           continue outer_loop;
1122 //         }
1123 //       }
1124 //       return i - sourceOffset;
1125 //     }
1126 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1127 //       i += targetCountLess1;
1128 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1129 //     i++;
1130 //   }
1131 //   return -1;
1132 // }
1133 
1134 //------------------------------string_indexOf------------------------
1135 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1136                                      jint cache_i, jint md2_i) {
1137 
1138   Node* no_ctrl  = NULL;
1139   float likely   = PROB_LIKELY(0.9);
1140   float unlikely = PROB_UNLIKELY(0.9);
1141 
1142   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1143 
1144   Node* source        = load_String_value(no_ctrl, string_object);
1145   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1146   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1147 
1148   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1149   jint target_length = target_array-&gt;length();
1150   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1151   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1152 
1153   // String.value field is known to be @Stable.
1154   if (UseImplicitStableValues) {
1155     target = cast_array_to_stable(target, target_type);
1156   }
1157 
1158   IdealKit kit(this, false, true);
1159 #define __ kit.
1160   Node* zero             = __ ConI(0);
1161   Node* one              = __ ConI(1);
1162   Node* cache            = __ ConI(cache_i);
1163   Node* md2              = __ ConI(md2_i);
1164   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1165   Node* targetCountLess1 = __ ConI(target_length - 1);
1166   Node* targetOffset     = __ ConI(targetOffset_i);
1167   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1168 
1169   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1170   Node* outer_loop = __ make_label(2 /* goto */);
1171   Node* return_    = __ make_label(1);
1172 
1173   __ set(rtn,__ ConI(-1));
1174   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1175        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1176        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1177        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1178        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1179          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1180               Node* tpj = __ AddI(targetOffset, __ value(j));
1181               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1182               Node* ipj  = __ AddI(__ value(i), __ value(j));
1183               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1184               __ if_then(targ, BoolTest::ne, src2); {
1185                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1186                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1187                     __ increment(i, __ AddI(__ value(j), one));
1188                     __ goto_(outer_loop);
1189                   } __ end_if(); __ dead(j);
1190                 }__ end_if(); __ dead(j);
1191                 __ increment(i, md2);
1192                 __ goto_(outer_loop);
1193               }__ end_if();
1194               __ increment(j, one);
1195          }__ end_loop(); __ dead(j);
1196          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1197          __ goto_(return_);
1198        }__ end_if();
1199        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1200          __ increment(i, targetCountLess1);
1201        }__ end_if();
1202        __ increment(i, one);
1203        __ bind(outer_loop);
1204   }__ end_loop(); __ dead(i);
1205   __ bind(return_);
1206 
1207   // Final sync IdealKit and GraphKit.
1208   final_sync(kit);
1209   Node* result = __ value(rtn);
1210 #undef __
1211   C-&gt;set_has_loops(true);
1212   return result;
1213 }
1214 
1215 //------------------------------inline_string_indexOf------------------------
1216 bool LibraryCallKit::inline_string_indexOf() {
1217   Node* receiver = argument(0);
1218   Node* arg      = argument(1);
1219 
1220   Node* result;
1221   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1222       UseSSE42Intrinsics) {
1223     // Generate SSE4.2 version of indexOf
1224     // We currently only have match rules that use SSE4.2
1225 
1226     receiver = null_check(receiver);
1227     arg      = null_check(arg);
1228     if (stopped()) {
1229       return true;
1230     }
1231 
1232     // Make the merge point
1233     RegionNode* result_rgn = new RegionNode(4);
1234     Node*       result_phi = new PhiNode(result_rgn, TypeInt::INT);
1235     Node* no_ctrl  = NULL;
1236 
1237     // Get start addr of source string
1238     Node* source = load_String_value(no_ctrl, receiver);
1239     Node* source_offset = load_String_offset(no_ctrl, receiver);
1240     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1241 
1242     // Get length of source string
1243     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1244 
1245     // Get start addr of substring
1246     Node* substr = load_String_value(no_ctrl, arg);
1247     Node* substr_offset = load_String_offset(no_ctrl, arg);
1248     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1249 
1250     // Get length of source string
1251     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1252 
1253     // Check for substr count &gt; string count
1254     Node* cmp = _gvn.transform(new CmpINode(substr_cnt, source_cnt));
1255     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::gt));
1256     Node* if_gt = generate_slow_guard(bol, NULL);
1257     if (if_gt != NULL) {
1258       result_phi-&gt;init_req(2, intcon(-1));
1259       result_rgn-&gt;init_req(2, if_gt);
1260     }
1261 
1262     if (!stopped()) {
1263       // Check for substr count == 0
1264       cmp = _gvn.transform(new CmpINode(substr_cnt, intcon(0)));
1265       bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1266       Node* if_zero = generate_slow_guard(bol, NULL);
1267       if (if_zero != NULL) {
1268         result_phi-&gt;init_req(3, intcon(0));
1269         result_rgn-&gt;init_req(3, if_zero);
1270       }
1271     }
1272 
1273     if (!stopped()) {
1274       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1275       result_phi-&gt;init_req(1, result);
1276       result_rgn-&gt;init_req(1, control());
1277     }
1278     set_control(_gvn.transform(result_rgn));
1279     record_for_igvn(result_rgn);
1280     result = _gvn.transform(result_phi);
1281 
1282   } else { // Use LibraryCallKit::string_indexOf
1283     // don't intrinsify if argument isn't a constant string.
1284     if (!arg-&gt;is_Con()) {
1285      return false;
1286     }
1287     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1288     if (str_type == NULL) {
1289       return false;
1290     }
1291     ciInstanceKlass* klass = env()-&gt;String_klass();
1292     ciObject* str_const = str_type-&gt;const_oop();
1293     if (str_const == NULL || str_const-&gt;klass() != klass) {
1294       return false;
1295     }
1296     ciInstance* str = str_const-&gt;as_instance();
1297     assert(str != NULL, "must be instance");
1298 
1299     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1300     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1301 
1302     int o;
1303     int c;
1304     if (java_lang_String::has_offset_field()) {
1305       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1306       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1307     } else {
1308       o = 0;
1309       c = pat-&gt;length();
1310     }
1311 
1312     // constant strings have no offset and count == length which
1313     // simplifies the resulting code somewhat so lets optimize for that.
1314     if (o != 0 || c != pat-&gt;length()) {
1315      return false;
1316     }
1317 
1318     receiver = null_check(receiver, T_OBJECT);
1319     // NOTE: No null check on the argument is needed since it's a constant String oop.
1320     if (stopped()) {
1321       return true;
1322     }
1323 
1324     // The null string as a pattern always returns 0 (match at beginning of string)
1325     if (c == 0) {
1326       set_result(intcon(0));
1327       return true;
1328     }
1329 
1330     // Generate default indexOf
1331     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1332     int cache = 0;
1333     int i;
1334     for (i = 0; i &lt; c - 1; i++) {
1335       assert(i &lt; pat-&gt;length(), "out of range");
1336       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1337     }
1338 
1339     int md2 = c;
1340     for (i = 0; i &lt; c - 1; i++) {
1341       assert(i &lt; pat-&gt;length(), "out of range");
1342       if (pat-&gt;char_at(o + i) == lastChar) {
1343         md2 = (c - 1) - i;
1344       }
1345     }
1346 
1347     result = string_indexOf(receiver, pat, o, cache, md2);
1348   }
1349   set_result(result);
1350   return true;
1351 }
1352 
1353 //--------------------------round_double_node--------------------------------
1354 // Round a double node if necessary.
1355 Node* LibraryCallKit::round_double_node(Node* n) {
1356   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1357     n = _gvn.transform(new RoundDoubleNode(0, n));
1358   return n;
1359 }
1360 
1361 //------------------------------inline_math-----------------------------------
1362 // public static double Math.abs(double)
1363 // public static double Math.sqrt(double)
1364 // public static double Math.log(double)
1365 // public static double Math.log10(double)
1366 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1367   Node* arg = round_double_node(argument(0));
1368   Node* n;
1369   switch (id) {
1370   case vmIntrinsics::_dabs:   n = new AbsDNode(                arg);  break;
1371   case vmIntrinsics::_dsqrt:  n = new SqrtDNode(C, control(),  arg);  break;
1372   case vmIntrinsics::_dlog:   n = new LogDNode(C, control(),   arg);  break;
1373   case vmIntrinsics::_dlog10: n = new Log10DNode(C, control(), arg);  break;
1374   default:  fatal_unexpected_iid(id);  break;
1375   }
1376   set_result(_gvn.transform(n));
1377   return true;
1378 }
1379 
1380 //------------------------------inline_trig----------------------------------
1381 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1382 // argument reduction which will turn into a fast/slow diamond.
1383 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1384   Node* arg = round_double_node(argument(0));
1385   Node* n = NULL;
1386 
1387   switch (id) {
1388   case vmIntrinsics::_dsin:  n = new SinDNode(C, control(), arg);  break;
1389   case vmIntrinsics::_dcos:  n = new CosDNode(C, control(), arg);  break;
1390   case vmIntrinsics::_dtan:  n = new TanDNode(C, control(), arg);  break;
1391   default:  fatal_unexpected_iid(id);  break;
1392   }
1393   n = _gvn.transform(n);
1394 
1395   // Rounding required?  Check for argument reduction!
1396   if (Matcher::strict_fp_requires_explicit_rounding) {
1397     static const double     pi_4 =  0.7853981633974483;
1398     static const double neg_pi_4 = -0.7853981633974483;
1399     // pi/2 in 80-bit extended precision
1400     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1401     // -pi/2 in 80-bit extended precision
1402     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1403     // Cutoff value for using this argument reduction technique
1404     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1405     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1406 
1407     // Pseudocode for sin:
1408     // if (x &lt;= Math.PI / 4.0) {
1409     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1410     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1411     // } else {
1412     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1413     // }
1414     // return StrictMath.sin(x);
1415 
1416     // Pseudocode for cos:
1417     // if (x &lt;= Math.PI / 4.0) {
1418     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1419     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1420     // } else {
1421     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1422     // }
1423     // return StrictMath.cos(x);
1424 
1425     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1426     // requires a special machine instruction to load it.  Instead we'll try
1427     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1428     // probably do the math inside the SIN encoding.
1429 
1430     // Make the merge point
1431     RegionNode* r = new RegionNode(3);
1432     Node* phi = new PhiNode(r, Type::DOUBLE);
1433 
1434     // Flatten arg so we need only 1 test
1435     Node *abs = _gvn.transform(new AbsDNode(arg));
1436     // Node for PI/4 constant
1437     Node *pi4 = makecon(TypeD::make(pi_4));
1438     // Check PI/4 : abs(arg)
1439     Node *cmp = _gvn.transform(new CmpDNode(pi4,abs));
1440     // Check: If PI/4 &lt; abs(arg) then go slow
1441     Node *bol = _gvn.transform(new BoolNode( cmp, BoolTest::lt ));
1442     // Branch either way
1443     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1444     set_control(opt_iff(r,iff));
1445 
1446     // Set fast path result
1447     phi-&gt;init_req(2, n);
1448 
1449     // Slow path - non-blocking leaf call
1450     Node* call = NULL;
1451     switch (id) {
1452     case vmIntrinsics::_dsin:
1453       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1454                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1455                                "Sin", NULL, arg, top());
1456       break;
1457     case vmIntrinsics::_dcos:
1458       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1459                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1460                                "Cos", NULL, arg, top());
1461       break;
1462     case vmIntrinsics::_dtan:
1463       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1464                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1465                                "Tan", NULL, arg, top());
1466       break;
1467     }
1468     assert(control()-&gt;in(0) == call, "");
1469     Node* slow_result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1470     r-&gt;init_req(1, control());
1471     phi-&gt;init_req(1, slow_result);
1472 
1473     // Post-merge
1474     set_control(_gvn.transform(r));
1475     record_for_igvn(r);
1476     n = _gvn.transform(phi);
1477 
1478     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1479   }
1480   set_result(n);
1481   return true;
1482 }
1483 
1484 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1485   //-------------------
1486   //result=(result.isNaN())? funcAddr():result;
1487   // Check: If isNaN() by checking result!=result? then either trap
1488   // or go to runtime
1489   Node* cmpisnan = _gvn.transform(new CmpDNode(result, result));
1490   // Build the boolean node
1491   Node* bolisnum = _gvn.transform(new BoolNode(cmpisnan, BoolTest::eq));
1492 
1493   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1494     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1495       // The pow or exp intrinsic returned a NaN, which requires a call
1496       // to the runtime.  Recompile with the runtime call.
1497       uncommon_trap(Deoptimization::Reason_intrinsic,
1498                     Deoptimization::Action_make_not_entrant);
1499     }
1500     return result;
1501   } else {
1502     // If this inlining ever returned NaN in the past, we compile a call
1503     // to the runtime to properly handle corner cases
1504 
1505     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1506     Node* if_slow = _gvn.transform(new IfFalseNode(iff));
1507     Node* if_fast = _gvn.transform(new IfTrueNode(iff));
1508 
1509     if (!if_slow-&gt;is_top()) {
1510       RegionNode* result_region = new RegionNode(3);
1511       PhiNode*    result_val = new PhiNode(result_region, Type::DOUBLE);
1512 
1513       result_region-&gt;init_req(1, if_fast);
1514       result_val-&gt;init_req(1, result);
1515 
1516       set_control(if_slow);
1517 
1518       const TypePtr* no_memory_effects = NULL;
1519       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1520                                    no_memory_effects,
1521                                    x, top(), y, y ? top() : NULL);
1522       Node* value = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+0));
1523 #ifdef ASSERT
1524       Node* value_top = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+1));
1525       assert(value_top == top(), "second value must be top");
1526 #endif
1527 
1528       result_region-&gt;init_req(2, control());
1529       result_val-&gt;init_req(2, value);
1530       set_control(_gvn.transform(result_region));
1531       return _gvn.transform(result_val);
1532     } else {
1533       return result;
1534     }
1535   }
1536 }
1537 
1538 //------------------------------inline_exp-------------------------------------
1539 // Inline exp instructions, if possible.  The Intel hardware only misses
1540 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1541 bool LibraryCallKit::inline_exp() {
1542   Node* arg = round_double_node(argument(0));
1543   Node* n   = _gvn.transform(new ExpDNode(C, control(), arg));
1544 
1545   n = finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1546   set_result(n);
1547 
1548   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1549   return true;
1550 }
1551 
1552 //------------------------------inline_pow-------------------------------------
1553 // Inline power instructions, if possible.
1554 bool LibraryCallKit::inline_pow() {
1555   // Pseudocode for pow
1556   // if (y == 2) {
1557   //   return x * x;
1558   // } else {
1559   //   if (x &lt;= 0.0) {
1560   //     long longy = (long)y;
1561   //     if ((double)longy == y) { // if y is long
1562   //       if (y + 1 == y) longy = 0; // huge number: even
1563   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1564   //     } else {
1565   //       result = NaN;
1566   //     }
1567   //   } else {
1568   //     result = DPow(x,y);
1569   //   }
1570   //   if (result != result)?  {
1571   //     result = uncommon_trap() or runtime_call();
1572   //   }
1573   //   return result;
1574   // }
1575 
1576   Node* x = round_double_node(argument(0));
1577   Node* y = round_double_node(argument(2));
1578 
1579   Node* result = NULL;
1580 
1581   Node*   const_two_node = makecon(TypeD::make(2.0));
1582   Node*   cmp_node       = _gvn.transform(new CmpDNode(y, const_two_node));
1583   Node*   bool_node      = _gvn.transform(new BoolNode(cmp_node, BoolTest::eq));
1584   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1585   Node*   if_true        = _gvn.transform(new IfTrueNode(if_node));
1586   Node*   if_false       = _gvn.transform(new IfFalseNode(if_node));
1587 
1588   RegionNode* region_node = new RegionNode(3);
1589   region_node-&gt;init_req(1, if_true);
1590 
1591   Node* phi_node = new PhiNode(region_node, Type::DOUBLE);
1592   // special case for x^y where y == 2, we can convert it to x * x
1593   phi_node-&gt;init_req(1, _gvn.transform(new MulDNode(x, x)));
1594 
1595   // set control to if_false since we will now process the false branch
1596   set_control(if_false);
1597 
1598   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1599     // Short form: skip the fancy tests and just check for NaN result.
1600     result = _gvn.transform(new PowDNode(C, control(), x, y));
1601   } else {
1602     // If this inlining ever returned NaN in the past, include all
1603     // checks + call to the runtime.
1604 
1605     // Set the merge point for If node with condition of (x &lt;= 0.0)
1606     // There are four possible paths to region node and phi node
1607     RegionNode *r = new RegionNode(4);
1608     Node *phi = new PhiNode(r, Type::DOUBLE);
1609 
1610     // Build the first if node: if (x &lt;= 0.0)
1611     // Node for 0 constant
1612     Node *zeronode = makecon(TypeD::ZERO);
1613     // Check x:0
1614     Node *cmp = _gvn.transform(new CmpDNode(x, zeronode));
1615     // Check: If (x&lt;=0) then go complex path
1616     Node *bol1 = _gvn.transform(new BoolNode( cmp, BoolTest::le ));
1617     // Branch either way
1618     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1619     // Fast path taken; set region slot 3
1620     Node *fast_taken = _gvn.transform(new IfFalseNode(if1));
1621     r-&gt;init_req(3,fast_taken); // Capture fast-control
1622 
1623     // Fast path not-taken, i.e. slow path
1624     Node *complex_path = _gvn.transform(new IfTrueNode(if1));
1625 
1626     // Set fast path result
1627     Node *fast_result = _gvn.transform(new PowDNode(C, control(), x, y));
1628     phi-&gt;init_req(3, fast_result);
1629 
1630     // Complex path
1631     // Build the second if node (if y is long)
1632     // Node for (long)y
1633     Node *longy = _gvn.transform(new ConvD2LNode(y));
1634     // Node for (double)((long) y)
1635     Node *doublelongy= _gvn.transform(new ConvL2DNode(longy));
1636     // Check (double)((long) y) : y
1637     Node *cmplongy= _gvn.transform(new CmpDNode(doublelongy, y));
1638     // Check if (y isn't long) then go to slow path
1639 
1640     Node *bol2 = _gvn.transform(new BoolNode( cmplongy, BoolTest::ne ));
1641     // Branch either way
1642     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1643     Node* ylong_path = _gvn.transform(new IfFalseNode(if2));
1644 
1645     Node *slow_path = _gvn.transform(new IfTrueNode(if2));
1646 
1647     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1648     // Node for constant 1
1649     Node *conone = longcon(1);
1650     // 1&amp; (long)y
1651     Node *signnode= _gvn.transform(new AndLNode(conone, longy));
1652 
1653     // A huge number is always even. Detect a huge number by checking
1654     // if y + 1 == y and set integer to be tested for parity to 0.
1655     // Required for corner case:
1656     // (long)9.223372036854776E18 = max_jlong
1657     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1658     // max_jlong is odd but 9.223372036854776E18 is even
1659     Node* yplus1 = _gvn.transform(new AddDNode(y, makecon(TypeD::make(1))));
1660     Node *cmpyplus1= _gvn.transform(new CmpDNode(yplus1, y));
1661     Node *bolyplus1 = _gvn.transform(new BoolNode( cmpyplus1, BoolTest::eq ));
1662     Node* correctedsign = NULL;
1663     if (ConditionalMoveLimit != 0) {
1664       correctedsign = _gvn.transform(CMoveNode::make(NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1665     } else {
1666       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1667       RegionNode *r = new RegionNode(3);
1668       Node *phi = new PhiNode(r, TypeLong::LONG);
1669       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyplus1)));
1670       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyplus1)));
1671       phi-&gt;init_req(1, signnode);
1672       phi-&gt;init_req(2, longcon(0));
1673       correctedsign = _gvn.transform(phi);
1674       ylong_path = _gvn.transform(r);
1675       record_for_igvn(r);
1676     }
1677 
1678     // zero node
1679     Node *conzero = longcon(0);
1680     // Check (1&amp;(long)y)==0?
1681     Node *cmpeq1 = _gvn.transform(new CmpLNode(correctedsign, conzero));
1682     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1683     Node *bol3 = _gvn.transform(new BoolNode( cmpeq1, BoolTest::ne ));
1684     // abs(x)
1685     Node *absx=_gvn.transform(new AbsDNode(x));
1686     // abs(x)^y
1687     Node *absxpowy = _gvn.transform(new PowDNode(C, control(), absx, y));
1688     // -abs(x)^y
1689     Node *negabsxpowy = _gvn.transform(new NegDNode (absxpowy));
1690     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1691     Node *signresult = NULL;
1692     if (ConditionalMoveLimit != 0) {
1693       signresult = _gvn.transform(CMoveNode::make(NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1694     } else {
1695       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1696       RegionNode *r = new RegionNode(3);
1697       Node *phi = new PhiNode(r, Type::DOUBLE);
1698       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyeven)));
1699       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyeven)));
1700       phi-&gt;init_req(1, absxpowy);
1701       phi-&gt;init_req(2, negabsxpowy);
1702       signresult = _gvn.transform(phi);
1703       ylong_path = _gvn.transform(r);
1704       record_for_igvn(r);
1705     }
1706     // Set complex path fast result
1707     r-&gt;init_req(2, ylong_path);
1708     phi-&gt;init_req(2, signresult);
1709 
1710     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1711     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1712     r-&gt;init_req(1,slow_path);
1713     phi-&gt;init_req(1,slow_result);
1714 
1715     // Post merge
1716     set_control(_gvn.transform(r));
1717     record_for_igvn(r);
1718     result = _gvn.transform(phi);
1719   }
1720 
1721   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1722 
1723   // control from finish_pow_exp is now input to the region node
1724   region_node-&gt;set_req(2, control());
1725   // the result from finish_pow_exp is now input to the phi node
1726   phi_node-&gt;init_req(2, result);
1727   set_control(_gvn.transform(region_node));
1728   record_for_igvn(region_node);
1729   set_result(_gvn.transform(phi_node));
1730 
1731   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1732   return true;
1733 }
1734 
1735 //------------------------------runtime_math-----------------------------
1736 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1737   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1738          "must be (DD)D or (D)D type");
1739 
1740   // Inputs
1741   Node* a = round_double_node(argument(0));
1742   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1743 
1744   const TypePtr* no_memory_effects = NULL;
1745   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1746                                  no_memory_effects,
1747                                  a, top(), b, b ? top() : NULL);
1748   Node* value = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+0));
1749 #ifdef ASSERT
1750   Node* value_top = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+1));
1751   assert(value_top == top(), "second value must be top");
1752 #endif
1753 
1754   set_result(value);
1755   return true;
1756 }
1757 
1758 //------------------------------inline_math_native-----------------------------
1759 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
1760 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
1761   switch (id) {
1762     // These intrinsics are not properly supported on all hardware
1763   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
1764     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
1765   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
1766     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
1767   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
1768     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
1769 
1770   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
1771     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
1772   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
1773     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
1774 
1775     // These intrinsics are supported on all hardware
1776   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
1777   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
1778 
1779   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
1780     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
1781   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
1782     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
1783 #undef FN_PTR
1784 
1785    // These intrinsics are not yet correctly implemented
1786   case vmIntrinsics::_datan2:
1787     return false;
1788 
1789   default:
1790     fatal_unexpected_iid(id);
1791     return false;
1792   }
1793 }
1794 
1795 static bool is_simple_name(Node* n) {
1796   return (n-&gt;req() == 1         // constant
1797           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
1798           || n-&gt;is_Proj()       // parameter or return value
1799           || n-&gt;is_Phi()        // local of some sort
1800           );
1801 }
1802 
1803 //----------------------------inline_notify-----------------------------------*
1804 bool LibraryCallKit::inline_notify(vmIntrinsics::ID id) {
1805   const TypeFunc* ftype = OptoRuntime::monitor_notify_Type();
1806   address func;
1807   if (id == vmIntrinsics::_notify) {
1808     func = OptoRuntime::monitor_notify_Java();
1809   } else {
1810     func = OptoRuntime::monitor_notifyAll_Java();
1811   }
1812   Node* call = make_runtime_call(RC_NO_LEAF, ftype, func, NULL, TypeRawPtr::BOTTOM, argument(0));
1813   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
1814   return true;
1815 }
1816 
1817 
1818 //----------------------------inline_min_max-----------------------------------
1819 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
1820   set_result(generate_min_max(id, argument(0), argument(1)));
1821   return true;
1822 }
1823 
1824 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
1825   Node* bol = _gvn.transform( new BoolNode(test, BoolTest::overflow) );
1826   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
1827   Node* fast_path = _gvn.transform( new IfFalseNode(check));
1828   Node* slow_path = _gvn.transform( new IfTrueNode(check) );
1829 
1830   {
1831     PreserveJVMState pjvms(this);
1832     PreserveReexecuteState preexecs(this);
1833     jvms()-&gt;set_should_reexecute(true);
1834 
1835     set_control(slow_path);
1836     set_i_o(i_o());
1837 
1838     uncommon_trap(Deoptimization::Reason_intrinsic,
1839                   Deoptimization::Action_none);
1840   }
1841 
1842   set_control(fast_path);
1843   set_result(math);
1844 }
1845 
1846 template &lt;typename OverflowOp&gt;
1847 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
1848   typedef typename OverflowOp::MathOp MathOp;
1849 
1850   MathOp* mathOp = new MathOp(arg1, arg2);
1851   Node* operation = _gvn.transform( mathOp );
1852   Node* ofcheck = _gvn.transform( new OverflowOp(arg1, arg2) );
1853   inline_math_mathExact(operation, ofcheck);
1854   return true;
1855 }
1856 
1857 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
1858   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
1859 }
1860 
1861 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
1862   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
1863 }
1864 
1865 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
1866   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
1867 }
1868 
1869 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
1870   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
1871 }
1872 
1873 bool LibraryCallKit::inline_math_negateExactI() {
1874   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
1875 }
1876 
1877 bool LibraryCallKit::inline_math_negateExactL() {
1878   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
1879 }
1880 
1881 bool LibraryCallKit::inline_math_multiplyExactI() {
1882   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
1883 }
1884 
1885 bool LibraryCallKit::inline_math_multiplyExactL() {
1886   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
1887 }
1888 
1889 Node*
1890 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
1891   // These are the candidate return value:
1892   Node* xvalue = x0;
1893   Node* yvalue = y0;
1894 
1895   if (xvalue == yvalue) {
1896     return xvalue;
1897   }
1898 
1899   bool want_max = (id == vmIntrinsics::_max);
1900 
1901   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
1902   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
1903   if (txvalue == NULL || tyvalue == NULL)  return top();
1904   // This is not really necessary, but it is consistent with a
1905   // hypothetical MaxINode::Value method:
1906   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
1907 
1908   // %%% This folding logic should (ideally) be in a different place.
1909   // Some should be inside IfNode, and there to be a more reliable
1910   // transformation of ?: style patterns into cmoves.  We also want
1911   // more powerful optimizations around cmove and min/max.
1912 
1913   // Try to find a dominating comparison of these guys.
1914   // It can simplify the index computation for Arrays.copyOf
1915   // and similar uses of System.arraycopy.
1916   // First, compute the normalized version of CmpI(x, y).
1917   int   cmp_op = Op_CmpI;
1918   Node* xkey = xvalue;
1919   Node* ykey = yvalue;
1920   Node* ideal_cmpxy = _gvn.transform(new CmpINode(xkey, ykey));
1921   if (ideal_cmpxy-&gt;is_Cmp()) {
1922     // E.g., if we have CmpI(length - offset, count),
1923     // it might idealize to CmpI(length, count + offset)
1924     cmp_op = ideal_cmpxy-&gt;Opcode();
1925     xkey = ideal_cmpxy-&gt;in(1);
1926     ykey = ideal_cmpxy-&gt;in(2);
1927   }
1928 
1929   // Start by locating any relevant comparisons.
1930   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
1931   Node* cmpxy = NULL;
1932   Node* cmpyx = NULL;
1933   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
1934     Node* cmp = start_from-&gt;fast_out(k);
1935     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
1936         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
1937         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
1938       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
1939       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
1940     }
1941   }
1942 
1943   const int NCMPS = 2;
1944   Node* cmps[NCMPS] = { cmpxy, cmpyx };
1945   int cmpn;
1946   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
1947     if (cmps[cmpn] != NULL)  break;     // find a result
1948   }
1949   if (cmpn &lt; NCMPS) {
1950     // Look for a dominating test that tells us the min and max.
1951     int depth = 0;                // Limit search depth for speed
1952     Node* dom = control();
1953     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
1954       if (++depth &gt;= 100)  break;
1955       Node* ifproj = dom;
1956       if (!ifproj-&gt;is_Proj())  continue;
1957       Node* iff = ifproj-&gt;in(0);
1958       if (!iff-&gt;is_If())  continue;
1959       Node* bol = iff-&gt;in(1);
1960       if (!bol-&gt;is_Bool())  continue;
1961       Node* cmp = bol-&gt;in(1);
1962       if (cmp == NULL)  continue;
1963       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
1964         if (cmps[cmpn] == cmp)  break;
1965       if (cmpn == NCMPS)  continue;
1966       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
1967       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
1968       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
1969       // At this point, we know that 'x btest y' is true.
1970       switch (btest) {
1971       case BoolTest::eq:
1972         // They are proven equal, so we can collapse the min/max.
1973         // Either value is the answer.  Choose the simpler.
1974         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
1975           return yvalue;
1976         return xvalue;
1977       case BoolTest::lt:          // x &lt; y
1978       case BoolTest::le:          // x &lt;= y
1979         return (want_max ? yvalue : xvalue);
1980       case BoolTest::gt:          // x &gt; y
1981       case BoolTest::ge:          // x &gt;= y
1982         return (want_max ? xvalue : yvalue);
1983       }
1984     }
1985   }
1986 
1987   // We failed to find a dominating test.
1988   // Let's pick a test that might GVN with prior tests.
1989   Node*          best_bol   = NULL;
1990   BoolTest::mask best_btest = BoolTest::illegal;
1991   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
1992     Node* cmp = cmps[cmpn];
1993     if (cmp == NULL)  continue;
1994     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
1995       Node* bol = cmp-&gt;fast_out(j);
1996       if (!bol-&gt;is_Bool())  continue;
1997       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
1998       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
1999       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2000       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2001         best_bol   = bol-&gt;as_Bool();
2002         best_btest = btest;
2003       }
2004     }
2005   }
2006 
2007   Node* answer_if_true  = NULL;
2008   Node* answer_if_false = NULL;
2009   switch (best_btest) {
2010   default:
2011     if (cmpxy == NULL)
2012       cmpxy = ideal_cmpxy;
2013     best_bol = _gvn.transform(new BoolNode(cmpxy, BoolTest::lt));
2014     // and fall through:
2015   case BoolTest::lt:          // x &lt; y
2016   case BoolTest::le:          // x &lt;= y
2017     answer_if_true  = (want_max ? yvalue : xvalue);
2018     answer_if_false = (want_max ? xvalue : yvalue);
2019     break;
2020   case BoolTest::gt:          // x &gt; y
2021   case BoolTest::ge:          // x &gt;= y
2022     answer_if_true  = (want_max ? xvalue : yvalue);
2023     answer_if_false = (want_max ? yvalue : xvalue);
2024     break;
2025   }
2026 
2027   jint hi, lo;
2028   if (want_max) {
2029     // We can sharpen the minimum.
2030     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2031     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2032   } else {
2033     // We can sharpen the maximum.
2034     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2035     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2036   }
2037 
2038   // Use a flow-free graph structure, to avoid creating excess control edges
2039   // which could hinder other optimizations.
2040   // Since Math.min/max is often used with arraycopy, we want
2041   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2042   Node* cmov = CMoveNode::make(NULL, best_bol,
2043                                answer_if_false, answer_if_true,
2044                                TypeInt::make(lo, hi, widen));
2045 
2046   return _gvn.transform(cmov);
2047 
2048   /*
2049   // This is not as desirable as it may seem, since Min and Max
2050   // nodes do not have a full set of optimizations.
2051   // And they would interfere, anyway, with 'if' optimizations
2052   // and with CMoveI canonical forms.
2053   switch (id) {
2054   case vmIntrinsics::_min:
2055     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2056   case vmIntrinsics::_max:
2057     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2058   default:
2059     ShouldNotReachHere();
2060   }
2061   */
2062 }
2063 
2064 inline int
2065 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2066   const TypePtr* base_type = TypePtr::NULL_PTR;
2067   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2068   if (base_type == NULL) {
2069     // Unknown type.
2070     return Type::AnyPtr;
2071   } else if (base_type == TypePtr::NULL_PTR) {
2072     // Since this is a NULL+long form, we have to switch to a rawptr.
2073     base   = _gvn.transform(new CastX2PNode(offset));
2074     offset = MakeConX(0);
2075     return Type::RawPtr;
2076   } else if (base_type-&gt;base() == Type::RawPtr) {
2077     return Type::RawPtr;
2078   } else if (base_type-&gt;isa_oopptr()) {
2079     // Base is never null =&gt; always a heap address.
2080     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2081       return Type::OopPtr;
2082     }
2083     // Offset is small =&gt; always a heap address.
2084     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2085     if (offset_type != NULL &amp;&amp;
2086         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2087         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2088         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2089       return Type::OopPtr;
2090     }
2091     // Otherwise, it might either be oop+off or NULL+addr.
2092     return Type::AnyPtr;
2093   } else {
2094     // No information:
2095     return Type::AnyPtr;
2096   }
2097 }
2098 
2099 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2100   int kind = classify_unsafe_addr(base, offset);
2101   if (kind == Type::RawPtr) {
2102     return basic_plus_adr(top(), base, offset);
2103   } else {
2104     return basic_plus_adr(base, offset);
2105   }
2106 }
2107 
2108 //--------------------------inline_number_methods-----------------------------
2109 // inline int     Integer.numberOfLeadingZeros(int)
2110 // inline int        Long.numberOfLeadingZeros(long)
2111 //
2112 // inline int     Integer.numberOfTrailingZeros(int)
2113 // inline int        Long.numberOfTrailingZeros(long)
2114 //
2115 // inline int     Integer.bitCount(int)
2116 // inline int        Long.bitCount(long)
2117 //
2118 // inline char  Character.reverseBytes(char)
2119 // inline short     Short.reverseBytes(short)
2120 // inline int     Integer.reverseBytes(int)
2121 // inline long       Long.reverseBytes(long)
2122 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2123   Node* arg = argument(0);
2124   Node* n;
2125   switch (id) {
2126   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new CountLeadingZerosINode( arg);  break;
2127   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new CountLeadingZerosLNode( arg);  break;
2128   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new CountTrailingZerosINode(arg);  break;
2129   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new CountTrailingZerosLNode(arg);  break;
2130   case vmIntrinsics::_bitCount_i:               n = new PopCountINode(          arg);  break;
2131   case vmIntrinsics::_bitCount_l:               n = new PopCountLNode(          arg);  break;
2132   case vmIntrinsics::_reverseBytes_c:           n = new ReverseBytesUSNode(0,   arg);  break;
2133   case vmIntrinsics::_reverseBytes_s:           n = new ReverseBytesSNode( 0,   arg);  break;
2134   case vmIntrinsics::_reverseBytes_i:           n = new ReverseBytesINode( 0,   arg);  break;
2135   case vmIntrinsics::_reverseBytes_l:           n = new ReverseBytesLNode( 0,   arg);  break;
2136   default:  fatal_unexpected_iid(id);  break;
2137   }
2138   set_result(_gvn.transform(n));
2139   return true;
2140 }
2141 
2142 //----------------------------inline_unsafe_access----------------------------
2143 
2144 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2145 
2146 // Helper that guards and inserts a pre-barrier.
2147 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2148                                         Node* pre_val, bool need_mem_bar) {
2149   // We could be accessing the referent field of a reference object. If so, when G1
2150   // is enabled, we need to log the value in the referent field in an SATB buffer.
2151   // This routine performs some compile time filters and generates suitable
2152   // runtime filters that guard the pre-barrier code.
2153   // Also add memory barrier for non volatile load from the referent field
2154   // to prevent commoning of loads across safepoint.
2155   if (!UseG1GC &amp;&amp; !need_mem_bar)
2156     return;
2157 
2158   // Some compile time checks.
2159 
2160   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2161   const TypeX* otype = offset-&gt;find_intptr_t_type();
2162   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2163       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2164     // Constant offset but not the reference_offset so just return
2165     return;
2166   }
2167 
2168   // We only need to generate the runtime guards for instances.
2169   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2170   if (btype != NULL) {
2171     if (btype-&gt;isa_aryptr()) {
2172       // Array type so nothing to do
2173       return;
2174     }
2175 
2176     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2177     if (itype != NULL) {
2178       // Can the klass of base_oop be statically determined to be
2179       // _not_ a sub-class of Reference and _not_ Object?
2180       ciKlass* klass = itype-&gt;klass();
2181       if ( klass-&gt;is_loaded() &amp;&amp;
2182           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2183           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2184         return;
2185       }
2186     }
2187   }
2188 
2189   // The compile time filters did not reject base_oop/offset so
2190   // we need to generate the following runtime filters
2191   //
2192   // if (offset == java_lang_ref_Reference::_reference_offset) {
2193   //   if (instance_of(base, java.lang.ref.Reference)) {
2194   //     pre_barrier(_, pre_val, ...);
2195   //   }
2196   // }
2197 
2198   float likely   = PROB_LIKELY(  0.999);
2199   float unlikely = PROB_UNLIKELY(0.999);
2200 
2201   IdealKit ideal(this);
2202 #define __ ideal.
2203 
2204   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2205 
2206   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2207       // Update graphKit memory and control from IdealKit.
2208       sync_kit(ideal);
2209 
2210       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2211       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2212 
2213       // Update IdealKit memory and control from graphKit.
2214       __ sync_kit(this);
2215 
2216       Node* one = __ ConI(1);
2217       // is_instof == 0 if base_oop == NULL
2218       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2219 
2220         // Update graphKit from IdeakKit.
2221         sync_kit(ideal);
2222 
2223         // Use the pre-barrier to record the value in the referent field
2224         pre_barrier(false /* do_load */,
2225                     __ ctrl(),
2226                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2227                     pre_val /* pre_val */,
2228                     T_OBJECT);
2229         if (need_mem_bar) {
2230           // Add memory barrier to prevent commoning reads from this field
2231           // across safepoint since GC can change its value.
2232           insert_mem_bar(Op_MemBarCPUOrder);
2233         }
2234         // Update IdealKit from graphKit.
2235         __ sync_kit(this);
2236 
2237       } __ end_if(); // _ref_type != ref_none
2238   } __ end_if(); // offset == referent_offset
2239 
2240   // Final sync IdealKit and GraphKit.
2241   final_sync(ideal);
2242 #undef __
2243 }
2244 
2245 
2246 // Interpret Unsafe.fieldOffset cookies correctly:
2247 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2248 
2249 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2250   // Attempt to infer a sharper value type from the offset and base type.
2251   ciKlass* sharpened_klass = NULL;
2252 
2253   // See if it is an instance field, with an object type.
2254   if (alias_type-&gt;field() != NULL) {
2255     assert(!is_native_ptr, "native pointer op cannot use a java address");
2256     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2257       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2258     }
2259   }
2260 
2261   // See if it is a narrow oop array.
2262   if (adr_type-&gt;isa_aryptr()) {
2263     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2264       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2265       if (elem_type != NULL) {
2266         sharpened_klass = elem_type-&gt;klass();
2267       }
2268     }
2269   }
2270 
2271   // The sharpened class might be unloaded if there is no class loader
2272   // contraint in place.
2273   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2274     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2275 
2276 #ifndef PRODUCT
2277     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2278       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2279       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2280     }
2281 #endif
2282     // Sharpen the value type.
2283     return tjp;
2284   }
2285   return NULL;
2286 }
2287 
2288 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2289   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2290 
2291 #ifndef PRODUCT
2292   {
2293     ResourceMark rm;
2294     // Check the signatures.
2295     ciSignature* sig = callee()-&gt;signature();
2296 #ifdef ASSERT
2297     if (!is_store) {
2298       // Object getObject(Object base, int/long offset), etc.
2299       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2300       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2301           rtype = T_ADDRESS;  // it is really a C void*
2302       assert(rtype == type, "getter must return the expected value");
2303       if (!is_native_ptr) {
2304         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2305         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2306         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2307       } else {
2308         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2309         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2310       }
2311     } else {
2312       // void putObject(Object base, int/long offset, Object x), etc.
2313       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2314       if (!is_native_ptr) {
2315         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2316         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2317         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2318       } else {
2319         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2320         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2321       }
2322       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2323       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2324         vtype = T_ADDRESS;  // it is really a C void*
2325       assert(vtype == type, "putter must accept the expected value");
2326     }
2327 #endif // ASSERT
2328  }
2329 #endif //PRODUCT
2330 
2331   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2332 
2333   Node* receiver = argument(0);  // type: oop
2334 
2335   // Build address expression.
2336   Node* adr;
2337   Node* heap_base_oop = top();
2338   Node* offset = top();
2339   Node* val;
2340 
2341   if (!is_native_ptr) {
2342     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2343     Node* base = argument(1);  // type: oop
2344     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2345     offset = argument(2);  // type: long
2346     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2347     // to be plain byte offsets, which are also the same as those accepted
2348     // by oopDesc::field_base.
2349     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2350            "fieldOffset must be byte-scaled");
2351     // 32-bit machines ignore the high half!
2352     offset = ConvL2X(offset);
2353     adr = make_unsafe_address(base, offset);
2354     heap_base_oop = base;
2355     val = is_store ? argument(4) : NULL;
2356   } else {
2357     Node* ptr = argument(1);  // type: long
2358     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2359     adr = make_unsafe_address(NULL, ptr);
2360     val = is_store ? argument(3) : NULL;
2361   }
2362 
2363   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2364 
2365   // First guess at the value type.
2366   const Type *value_type = Type::get_const_basic_type(type);
2367 
2368   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2369   // there was not enough information to nail it down.
2370   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2371   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2372 
2373   // We will need memory barriers unless we can determine a unique
2374   // alias category for this reference.  (Note:  If for some reason
2375   // the barriers get omitted and the unsafe reference begins to "pollute"
2376   // the alias analysis of the rest of the graph, either Compile::can_alias
2377   // or Compile::must_alias will throw a diagnostic assert.)
2378   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2379 
2380   // If we are reading the value of the referent field of a Reference
2381   // object (either by using Unsafe directly or through reflection)
2382   // then, if G1 is enabled, we need to record the referent in an
2383   // SATB log buffer using the pre-barrier mechanism.
2384   // Also we need to add memory barrier to prevent commoning reads
2385   // from this field across safepoint since GC can change its value.
2386   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2387                            offset != top() &amp;&amp; heap_base_oop != top();
2388 
2389   if (!is_store &amp;&amp; type == T_OBJECT) {
2390     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2391     if (tjp != NULL) {
2392       value_type = tjp;
2393     }
2394   }
2395 
2396   receiver = null_check(receiver);
2397   if (stopped()) {
2398     return true;
2399   }
2400   // Heap pointers get a null-check from the interpreter,
2401   // as a courtesy.  However, this is not guaranteed by Unsafe,
2402   // and it is not possible to fully distinguish unintended nulls
2403   // from intended ones in this API.
2404 
2405   if (is_volatile) {
2406     // We need to emit leading and trailing CPU membars (see below) in
2407     // addition to memory membars when is_volatile. This is a little
2408     // too strong, but avoids the need to insert per-alias-type
2409     // volatile membars (for stores; compare Parse::do_put_xxx), which
2410     // we cannot do effectively here because we probably only have a
2411     // rough approximation of type.
2412     need_mem_bar = true;
2413     // For Stores, place a memory ordering barrier now.
2414     if (is_store) {
2415       insert_mem_bar(Op_MemBarRelease);
2416     } else {
2417       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2418         insert_mem_bar(Op_MemBarVolatile);
2419       }
2420     }
2421   }
2422 
2423   // Memory barrier to prevent normal and 'unsafe' accesses from
2424   // bypassing each other.  Happens after null checks, so the
2425   // exception paths do not take memory state from the memory barrier,
2426   // so there's no problems making a strong assert about mixing users
2427   // of safe &amp; unsafe memory.
2428   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2429 
2430    if (!is_store) {
2431     Node* p = NULL;
2432     // Try to constant fold a load from a constant field
2433     ciField* field = alias_type-&gt;field();
2434     if (heap_base_oop != top() &amp;&amp;
2435         field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; field-&gt;layout_type() == type) {
2436       // final or stable field
2437       const Type* con_type = Type::make_constant(alias_type-&gt;field(), heap_base_oop);
2438       if (con_type != NULL) {
2439         p = makecon(con_type);
2440       }
2441     }
2442     if (p == NULL) {
2443       MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2444       // To be valid, unsafe loads may depend on other conditions than
2445       // the one that guards them: pin the Load node
2446       p = make_load(control(), adr, value_type, type, adr_type, mo, LoadNode::Pinned, is_volatile);
2447       // load value
2448       switch (type) {
2449       case T_BOOLEAN:
2450       case T_CHAR:
2451       case T_BYTE:
2452       case T_SHORT:
2453       case T_INT:
2454       case T_LONG:
2455       case T_FLOAT:
2456       case T_DOUBLE:
2457         break;
2458       case T_OBJECT:
2459         if (need_read_barrier) {
2460           insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2461         }
2462         break;
2463       case T_ADDRESS:
2464         // Cast to an int type.
2465         p = _gvn.transform(new CastP2XNode(NULL, p));
2466         p = ConvX2UL(p);
2467         break;
2468       default:
2469         fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2470         break;
2471       }
2472     }
2473     // The load node has the control of the preceding MemBarCPUOrder.  All
2474     // following nodes will have the control of the MemBarCPUOrder inserted at
2475     // the end of this method.  So, pushing the load onto the stack at a later
2476     // point is fine.
2477     set_result(p);
2478   } else {
2479     // place effect of store into memory
2480     switch (type) {
2481     case T_DOUBLE:
2482       val = dstore_rounding(val);
2483       break;
2484     case T_ADDRESS:
2485       // Repackage the long as a pointer.
2486       val = ConvL2X(val);
2487       val = _gvn.transform(new CastX2PNode(val));
2488       break;
2489     }
2490 
2491     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2492     if (type != T_OBJECT ) {
2493       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2494     } else {
2495       // Possibly an oop being stored to Java heap or native memory
2496       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2497         // oop to Java heap.
2498         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2499       } else {
2500         // We can't tell at compile time if we are storing in the Java heap or outside
2501         // of it. So we need to emit code to conditionally do the proper type of
2502         // store.
2503 
2504         IdealKit ideal(this);
2505 #define __ ideal.
2506         // QQQ who knows what probability is here??
2507         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2508           // Sync IdealKit and graphKit.
2509           sync_kit(ideal);
2510           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2511           // Update IdealKit memory.
2512           __ sync_kit(this);
2513         } __ else_(); {
2514           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2515         } __ end_if();
2516         // Final sync IdealKit and GraphKit.
2517         final_sync(ideal);
2518 #undef __
2519       }
2520     }
2521   }
2522 
2523   if (is_volatile) {
2524     if (!is_store) {
2525       insert_mem_bar(Op_MemBarAcquire);
2526     } else {
2527       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2528         insert_mem_bar(Op_MemBarVolatile);
2529       }
2530     }
2531   }
2532 
2533   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2534 
2535   return true;
2536 }
2537 
2538 //----------------------------inline_unsafe_load_store----------------------------
2539 // This method serves a couple of different customers (depending on LoadStoreKind):
2540 //
2541 // LS_cmpxchg:
2542 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2543 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2544 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2545 //
2546 // LS_xadd:
2547 //   public int  getAndAddInt( Object o, long offset, int  delta)
2548 //   public long getAndAddLong(Object o, long offset, long delta)
2549 //
2550 // LS_xchg:
2551 //   int    getAndSet(Object o, long offset, int    newValue)
2552 //   long   getAndSet(Object o, long offset, long   newValue)
2553 //   Object getAndSet(Object o, long offset, Object newValue)
2554 //
2555 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2556   // This basic scheme here is the same as inline_unsafe_access, but
2557   // differs in enough details that combining them would make the code
2558   // overly confusing.  (This is a true fact! I originally combined
2559   // them, but even I was confused by it!) As much code/comments as
2560   // possible are retained from inline_unsafe_access though to make
2561   // the correspondences clearer. - dl
2562 
2563   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2564 
2565 #ifndef PRODUCT
2566   BasicType rtype;
2567   {
2568     ResourceMark rm;
2569     // Check the signatures.
2570     ciSignature* sig = callee()-&gt;signature();
2571     rtype = sig-&gt;return_type()-&gt;basic_type();
2572     if (kind == LS_xadd || kind == LS_xchg) {
2573       // Check the signatures.
2574 #ifdef ASSERT
2575       assert(rtype == type, "get and set must return the expected type");
2576       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2577       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2578       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2579       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2580 #endif // ASSERT
2581     } else if (kind == LS_cmpxchg) {
2582       // Check the signatures.
2583 #ifdef ASSERT
2584       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2585       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2586       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2587       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2588 #endif // ASSERT
2589     } else {
2590       ShouldNotReachHere();
2591     }
2592   }
2593 #endif //PRODUCT
2594 
2595   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2596 
2597   // Get arguments:
2598   Node* receiver = NULL;
2599   Node* base     = NULL;
2600   Node* offset   = NULL;
2601   Node* oldval   = NULL;
2602   Node* newval   = NULL;
2603   if (kind == LS_cmpxchg) {
2604     const bool two_slot_type = type2size[type] == 2;
2605     receiver = argument(0);  // type: oop
2606     base     = argument(1);  // type: oop
2607     offset   = argument(2);  // type: long
2608     oldval   = argument(4);  // type: oop, int, or long
2609     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2610   } else if (kind == LS_xadd || kind == LS_xchg){
2611     receiver = argument(0);  // type: oop
2612     base     = argument(1);  // type: oop
2613     offset   = argument(2);  // type: long
2614     oldval   = NULL;
2615     newval   = argument(4);  // type: oop, int, or long
2616   }
2617 
2618   // Null check receiver.
2619   receiver = null_check(receiver);
2620   if (stopped()) {
2621     return true;
2622   }
2623 
2624   // Build field offset expression.
2625   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2626   // to be plain byte offsets, which are also the same as those accepted
2627   // by oopDesc::field_base.
2628   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2629   // 32-bit machines ignore the high half of long offsets
2630   offset = ConvL2X(offset);
2631   Node* adr = make_unsafe_address(base, offset);
2632   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2633 
2634   // For CAS, unlike inline_unsafe_access, there seems no point in
2635   // trying to refine types. Just use the coarse types here.
2636   const Type *value_type = Type::get_const_basic_type(type);
2637   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2638   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2639 
2640   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2641     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2642     if (tjp != NULL) {
2643       value_type = tjp;
2644     }
2645   }
2646 
2647   int alias_idx = C-&gt;get_alias_index(adr_type);
2648 
2649   // Memory-model-wise, a LoadStore acts like a little synchronized
2650   // block, so needs barriers on each side.  These don't translate
2651   // into actual barriers on most machines, but we still need rest of
2652   // compiler to respect ordering.
2653 
2654   insert_mem_bar(Op_MemBarRelease);
2655   insert_mem_bar(Op_MemBarCPUOrder);
2656 
2657   // 4984716: MemBars must be inserted before this
2658   //          memory node in order to avoid a false
2659   //          dependency which will confuse the scheduler.
2660   Node *mem = memory(alias_idx);
2661 
2662   // For now, we handle only those cases that actually exist: ints,
2663   // longs, and Object. Adding others should be straightforward.
2664   Node* load_store;
2665   switch(type) {
2666   case T_INT:
2667     if (kind == LS_xadd) {
2668       load_store = _gvn.transform(new GetAndAddINode(control(), mem, adr, newval, adr_type));
2669     } else if (kind == LS_xchg) {
2670       load_store = _gvn.transform(new GetAndSetINode(control(), mem, adr, newval, adr_type));
2671     } else if (kind == LS_cmpxchg) {
2672       load_store = _gvn.transform(new CompareAndSwapINode(control(), mem, adr, newval, oldval));
2673     } else {
2674       ShouldNotReachHere();
2675     }
2676     break;
2677   case T_LONG:
2678     if (kind == LS_xadd) {
2679       load_store = _gvn.transform(new GetAndAddLNode(control(), mem, adr, newval, adr_type));
2680     } else if (kind == LS_xchg) {
2681       load_store = _gvn.transform(new GetAndSetLNode(control(), mem, adr, newval, adr_type));
2682     } else if (kind == LS_cmpxchg) {
2683       load_store = _gvn.transform(new CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2684     } else {
2685       ShouldNotReachHere();
2686     }
2687     break;
2688   case T_OBJECT:
2689     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2690     // could be delayed during Parse (for example, in adjust_map_after_if()).
2691     // Execute transformation here to avoid barrier generation in such case.
2692     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2693       newval = _gvn.makecon(TypePtr::NULL_PTR);
2694 
2695     // Reference stores need a store barrier.
2696     if (kind == LS_xchg) {
2697       // If pre-barrier must execute before the oop store, old value will require do_load here.
2698       if (!can_move_pre_barrier()) {
2699         pre_barrier(true /* do_load*/,
2700                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2701                     NULL /* pre_val*/,
2702                     T_OBJECT);
2703       } // Else move pre_barrier to use load_store value, see below.
2704     } else if (kind == LS_cmpxchg) {
2705       // Same as for newval above:
2706       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2707         oldval = _gvn.makecon(TypePtr::NULL_PTR);
2708       }
2709       // The only known value which might get overwritten is oldval.
2710       pre_barrier(false /* do_load */,
2711                   control(), NULL, NULL, max_juint, NULL, NULL,
2712                   oldval /* pre_val */,
2713                   T_OBJECT);
2714     } else {
2715       ShouldNotReachHere();
2716     }
2717 
2718 #ifdef _LP64
2719     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2720       Node *newval_enc = _gvn.transform(new EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2721       if (kind == LS_xchg) {
2722         load_store = _gvn.transform(new GetAndSetNNode(control(), mem, adr,
2723                                                        newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2724       } else {
2725         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2726         Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2727         load_store = _gvn.transform(new CompareAndSwapNNode(control(), mem, adr,
2728                                                                 newval_enc, oldval_enc));
2729       }
2730     } else
2731 #endif
2732     {
2733       if (kind == LS_xchg) {
2734         load_store = _gvn.transform(new GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2735       } else {
2736         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2737         load_store = _gvn.transform(new CompareAndSwapPNode(control(), mem, adr, newval, oldval));
2738       }
2739     }
2740     if (kind == LS_cmpxchg) {
2741       // Emit the post barrier only when the actual store happened.
2742       // This makes sense to check only for compareAndSet that can fail to set the value.
2743       // CAS success path is marked more likely since we anticipate this is a performance
2744       // critical path, while CAS failure path can use the penalty for going through unlikely
2745       // path as backoff. Which is still better than doing a store barrier there.
2746       IdealKit ideal(this);
2747       ideal.if_then(load_store, BoolTest::ne, ideal.ConI(0), PROB_STATIC_FREQUENT); {
2748         sync_kit(ideal);
2749         post_barrier(ideal.ctrl(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2750         ideal.sync_kit(this);
2751       } ideal.end_if();
2752       final_sync(ideal);
2753     } else {
2754       post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2755     }
2756     break;
2757   default:
2758     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2759     break;
2760   }
2761 
2762   // SCMemProjNodes represent the memory state of a LoadStore. Their
2763   // main role is to prevent LoadStore nodes from being optimized away
2764   // when their results aren't used.
2765   Node* proj = _gvn.transform(new SCMemProjNode(load_store));
2766   set_memory(proj, alias_idx);
2767 
2768   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
2769 #ifdef _LP64
2770     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2771       load_store = _gvn.transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
2772     }
2773 #endif
2774     if (can_move_pre_barrier()) {
2775       // Don't need to load pre_val. The old value is returned by load_store.
2776       // The pre_barrier can execute after the xchg as long as no safepoint
2777       // gets inserted between them.
2778       pre_barrier(false /* do_load */,
2779                   control(), NULL, NULL, max_juint, NULL, NULL,
2780                   load_store /* pre_val */,
2781                   T_OBJECT);
2782     }
2783   }
2784 
2785   // Add the trailing membar surrounding the access
2786   insert_mem_bar(Op_MemBarCPUOrder);
2787   insert_mem_bar(Op_MemBarAcquire);
2788 
2789   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
2790   set_result(load_store);
2791   return true;
2792 }
2793 
2794 //----------------------------inline_unsafe_ordered_store----------------------
2795 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
2796 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
2797 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
2798 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
2799   // This is another variant of inline_unsafe_access, differing in
2800   // that it always issues store-store ("release") barrier and ensures
2801   // store-atomicity (which only matters for "long").
2802 
2803   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2804 
2805 #ifndef PRODUCT
2806   {
2807     ResourceMark rm;
2808     // Check the signatures.
2809     ciSignature* sig = callee()-&gt;signature();
2810 #ifdef ASSERT
2811     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2812     assert(rtype == T_VOID, "must return void");
2813     assert(sig-&gt;count() == 3, "has 3 arguments");
2814     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
2815     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
2816 #endif // ASSERT
2817   }
2818 #endif //PRODUCT
2819 
2820   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2821 
2822   // Get arguments:
2823   Node* receiver = argument(0);  // type: oop
2824   Node* base     = argument(1);  // type: oop
2825   Node* offset   = argument(2);  // type: long
2826   Node* val      = argument(4);  // type: oop, int, or long
2827 
2828   // Null check receiver.
2829   receiver = null_check(receiver);
2830   if (stopped()) {
2831     return true;
2832   }
2833 
2834   // Build field offset expression.
2835   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2836   // 32-bit machines ignore the high half of long offsets
2837   offset = ConvL2X(offset);
2838   Node* adr = make_unsafe_address(base, offset);
2839   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2840   const Type *value_type = Type::get_const_basic_type(type);
2841   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2842 
2843   insert_mem_bar(Op_MemBarRelease);
2844   insert_mem_bar(Op_MemBarCPUOrder);
2845   // Ensure that the store is atomic for longs:
2846   const bool require_atomic_access = true;
2847   Node* store;
2848   if (type == T_OBJECT) // reference stores need a store barrier.
2849     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
2850   else {
2851     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
2852   }
2853   insert_mem_bar(Op_MemBarCPUOrder);
2854   return true;
2855 }
2856 
2857 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
2858   // Regardless of form, don't allow previous ld/st to move down,
2859   // then issue acquire, release, or volatile mem_bar.
2860   insert_mem_bar(Op_MemBarCPUOrder);
2861   switch(id) {
2862     case vmIntrinsics::_loadFence:
2863       insert_mem_bar(Op_LoadFence);
2864       return true;
2865     case vmIntrinsics::_storeFence:
2866       insert_mem_bar(Op_StoreFence);
2867       return true;
2868     case vmIntrinsics::_fullFence:
2869       insert_mem_bar(Op_MemBarVolatile);
2870       return true;
2871     default:
2872       fatal_unexpected_iid(id);
2873       return false;
2874   }
2875 }
2876 
<a name="3" id="anc3"></a>




2877 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
2878   if (!kls-&gt;is_Con()) {
2879     return true;
2880   }
2881   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
2882   if (klsptr == NULL) {
2883     return true;
2884   }
2885   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
2886   // don't need a guard for a klass that is already initialized
2887   return !ik-&gt;is_initialized();
2888 }
2889 
2890 //----------------------------inline_unsafe_allocate---------------------------
2891 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
2892 bool LibraryCallKit::inline_unsafe_allocate() {
2893   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2894 
2895   null_check_receiver();  // null-check, then ignore
2896   Node* cls = null_check(argument(1));
2897   if (stopped())  return true;
2898 
2899   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
2900   kls = null_check(kls);
2901   if (stopped())  return true;  // argument was like int.class
2902 
2903   Node* test = NULL;
2904   if (LibraryCallKit::klass_needs_init_guard(kls)) {
2905     // Note:  The argument might still be an illegal value like
2906     // Serializable.class or Object[].class.   The runtime will handle it.
2907     // But we must make an explicit check for initialization.
2908     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
2909     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
2910     // can generate code to load it as unsigned byte.
2911     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
2912     Node* bits = intcon(InstanceKlass::fully_initialized);
2913     test = _gvn.transform(new SubINode(inst, bits));
2914     // The 'test' is non-zero if we need to take a slow path.
2915   }
2916 
2917   Node* obj = new_instance(kls, test);
2918   set_result(obj);
2919   return true;
2920 }
2921 
2922 #ifdef TRACE_HAVE_INTRINSICS
2923 /*
2924  * oop -&gt; myklass
2925  * myklass-&gt;trace_id |= USED
2926  * return myklass-&gt;trace_id &amp; ~0x3
2927  */
2928 bool LibraryCallKit::inline_native_classID() {
2929   null_check_receiver();  // null-check, then ignore
2930   Node* cls = null_check(argument(1), T_OBJECT);
2931   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
2932   kls = null_check(kls, T_OBJECT);
2933   ByteSize offset = TRACE_ID_OFFSET;
2934   Node* insp = basic_plus_adr(kls, in_bytes(offset));
2935   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
2936   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
2937   Node* andl = _gvn.transform(new AndLNode(tvalue, bits));
2938   Node* clsused = longcon(0x01l); // set the class bit
2939   Node* orl = _gvn.transform(new OrLNode(tvalue, clsused));
2940 
2941   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
2942   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
2943   set_result(andl);
2944   return true;
2945 }
2946 
2947 bool LibraryCallKit::inline_native_threadID() {
2948   Node* tls_ptr = NULL;
2949   Node* cur_thr = generate_current_thread(tls_ptr);
2950   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
2951   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
2952   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
2953 
2954   Node* threadid = NULL;
2955   size_t thread_id_size = OSThread::thread_id_size();
2956   if (thread_id_size == (size_t) BytesPerLong) {
2957     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
2958   } else if (thread_id_size == (size_t) BytesPerInt) {
2959     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
2960   } else {
2961     ShouldNotReachHere();
2962   }
2963   set_result(threadid);
2964   return true;
2965 }
2966 #endif
2967 
2968 //------------------------inline_native_time_funcs--------------
2969 // inline code for System.currentTimeMillis() and System.nanoTime()
2970 // these have the same type and signature
2971 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
2972   const TypeFunc* tf = OptoRuntime::void_long_Type();
2973   const TypePtr* no_memory_effects = NULL;
2974   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
2975   Node* value = _gvn.transform(new ProjNode(time, TypeFunc::Parms+0));
2976 #ifdef ASSERT
2977   Node* value_top = _gvn.transform(new ProjNode(time, TypeFunc::Parms+1));
2978   assert(value_top == top(), "second value must be top");
2979 #endif
2980   set_result(value);
2981   return true;
2982 }
2983 
2984 //------------------------inline_native_currentThread------------------
2985 bool LibraryCallKit::inline_native_currentThread() {
2986   Node* junk = NULL;
2987   set_result(generate_current_thread(junk));
2988   return true;
2989 }
2990 
2991 //------------------------inline_native_isInterrupted------------------
2992 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
2993 bool LibraryCallKit::inline_native_isInterrupted() {
2994   // Add a fast path to t.isInterrupted(clear_int):
2995   //   (t == Thread.current() &amp;&amp;
2996   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
2997   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
2998   // So, in the common case that the interrupt bit is false,
2999   // we avoid making a call into the VM.  Even if the interrupt bit
3000   // is true, if the clear_int argument is false, we avoid the VM call.
3001   // However, if the receiver is not currentThread, we must call the VM,
3002   // because there must be some locking done around the operation.
3003 
3004   // We only go to the fast case code if we pass two guards.
3005   // Paths which do not pass are accumulated in the slow_region.
3006 
3007   enum {
3008     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3009     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3010     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3011     PATH_LIMIT
3012   };
3013 
3014   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3015   // out of the function.
3016   insert_mem_bar(Op_MemBarCPUOrder);
3017 
3018   RegionNode* result_rgn = new RegionNode(PATH_LIMIT);
3019   PhiNode*    result_val = new PhiNode(result_rgn, TypeInt::BOOL);
3020 
3021   RegionNode* slow_region = new RegionNode(1);
3022   record_for_igvn(slow_region);
3023 
3024   // (a) Receiving thread must be the current thread.
3025   Node* rec_thr = argument(0);
3026   Node* tls_ptr = NULL;
3027   Node* cur_thr = generate_current_thread(tls_ptr);
3028   Node* cmp_thr = _gvn.transform(new CmpPNode(cur_thr, rec_thr));
3029   Node* bol_thr = _gvn.transform(new BoolNode(cmp_thr, BoolTest::ne));
3030 
3031   generate_slow_guard(bol_thr, slow_region);
3032 
3033   // (b) Interrupt bit on TLS must be false.
3034   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3035   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3036   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3037 
3038   // Set the control input on the field _interrupted read to prevent it floating up.
3039   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3040   Node* cmp_bit = _gvn.transform(new CmpINode(int_bit, intcon(0)));
3041   Node* bol_bit = _gvn.transform(new BoolNode(cmp_bit, BoolTest::ne));
3042 
3043   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3044 
3045   // First fast path:  if (!TLS._interrupted) return false;
3046   Node* false_bit = _gvn.transform(new IfFalseNode(iff_bit));
3047   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3048   result_val-&gt;init_req(no_int_result_path, intcon(0));
3049 
3050   // drop through to next case
3051   set_control( _gvn.transform(new IfTrueNode(iff_bit)));
3052 
3053 #ifndef TARGET_OS_FAMILY_windows
3054   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3055   Node* clr_arg = argument(1);
3056   Node* cmp_arg = _gvn.transform(new CmpINode(clr_arg, intcon(0)));
3057   Node* bol_arg = _gvn.transform(new BoolNode(cmp_arg, BoolTest::ne));
3058   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3059 
3060   // Second fast path:  ... else if (!clear_int) return true;
3061   Node* false_arg = _gvn.transform(new IfFalseNode(iff_arg));
3062   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3063   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3064 
3065   // drop through to next case
3066   set_control( _gvn.transform(new IfTrueNode(iff_arg)));
3067 #else
3068   // To return true on Windows you must read the _interrupted field
3069   // and check the the event state i.e. take the slow path.
3070 #endif // TARGET_OS_FAMILY_windows
3071 
3072   // (d) Otherwise, go to the slow path.
3073   slow_region-&gt;add_req(control());
3074   set_control( _gvn.transform(slow_region));
3075 
3076   if (stopped()) {
3077     // There is no slow path.
3078     result_rgn-&gt;init_req(slow_result_path, top());
3079     result_val-&gt;init_req(slow_result_path, top());
3080   } else {
3081     // non-virtual because it is a private non-static
3082     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3083 
3084     Node* slow_val = set_results_for_java_call(slow_call);
3085     // this-&gt;control() comes from set_results_for_java_call
3086 
3087     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3088     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3089 
3090     // These two phis are pre-filled with copies of of the fast IO and Memory
3091     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3092     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3093 
3094     result_rgn-&gt;init_req(slow_result_path, control());
3095     result_io -&gt;init_req(slow_result_path, i_o());
3096     result_mem-&gt;init_req(slow_result_path, reset_memory());
3097     result_val-&gt;init_req(slow_result_path, slow_val);
3098 
3099     set_all_memory(_gvn.transform(result_mem));
3100     set_i_o(       _gvn.transform(result_io));
3101   }
3102 
3103   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3104   set_result(result_rgn, result_val);
3105   return true;
3106 }
3107 
3108 //---------------------------load_mirror_from_klass----------------------------
3109 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3110 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3111   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3112   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3113 }
3114 
3115 //-----------------------load_klass_from_mirror_common-------------------------
3116 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3117 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3118 // and branch to the given path on the region.
3119 // If never_see_null, take an uncommon trap on null, so we can optimistically
3120 // compile for the non-null case.
3121 // If the region is NULL, force never_see_null = true.
3122 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3123                                                     bool never_see_null,
3124                                                     RegionNode* region,
3125                                                     int null_path,
3126                                                     int offset) {
3127   if (region == NULL)  never_see_null = true;
3128   Node* p = basic_plus_adr(mirror, offset);
3129   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3130   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3131   Node* null_ctl = top();
3132   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3133   if (region != NULL) {
3134     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3135     region-&gt;init_req(null_path, null_ctl);
3136   } else {
3137     assert(null_ctl == top(), "no loose ends");
3138   }
3139   return kls;
3140 }
3141 
3142 //--------------------(inline_native_Class_query helpers)---------------------
3143 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3144 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3145 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3146   // Branch around if the given klass has the given modifier bit set.
3147   // Like generate_guard, adds a new path onto the region.
3148   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3149   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3150   Node* mask = intcon(modifier_mask);
3151   Node* bits = intcon(modifier_bits);
3152   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3153   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3154   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3155   return generate_fair_guard(bol, region);
3156 }
3157 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3158   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3159 }
3160 
3161 //-------------------------inline_native_Class_query-------------------
3162 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3163   const Type* return_type = TypeInt::BOOL;
3164   Node* prim_return_value = top();  // what happens if it's a primitive class?
3165   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3166   bool expect_prim = false;     // most of these guys expect to work on refs
3167 
3168   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3169 
3170   Node* mirror = argument(0);
3171   Node* obj    = top();
3172 
3173   switch (id) {
3174   case vmIntrinsics::_isInstance:
3175     // nothing is an instance of a primitive type
3176     prim_return_value = intcon(0);
3177     obj = argument(1);
3178     break;
3179   case vmIntrinsics::_getModifiers:
3180     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3181     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3182     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3183     break;
3184   case vmIntrinsics::_isInterface:
3185     prim_return_value = intcon(0);
3186     break;
3187   case vmIntrinsics::_isArray:
3188     prim_return_value = intcon(0);
3189     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3190     break;
3191   case vmIntrinsics::_isPrimitive:
3192     prim_return_value = intcon(1);
3193     expect_prim = true;  // obviously
3194     break;
3195   case vmIntrinsics::_getSuperclass:
3196     prim_return_value = null();
3197     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3198     break;
3199   case vmIntrinsics::_getClassAccessFlags:
3200     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3201     return_type = TypeInt::INT;  // not bool!  6297094
3202     break;
3203   default:
3204     fatal_unexpected_iid(id);
3205     break;
3206   }
3207 
3208   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3209   if (mirror_con == NULL)  return false;  // cannot happen?
3210 
3211 #ifndef PRODUCT
3212   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3213     ciType* k = mirror_con-&gt;java_mirror_type();
3214     if (k) {
3215       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3216       k-&gt;print_name();
3217       tty-&gt;cr();
3218     }
3219   }
3220 #endif
3221 
3222   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3223   RegionNode* region = new RegionNode(PATH_LIMIT);
3224   record_for_igvn(region);
3225   PhiNode* phi = new PhiNode(region, return_type);
3226 
3227   // The mirror will never be null of Reflection.getClassAccessFlags, however
3228   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3229   // if it is. See bug 4774291.
3230 
3231   // For Reflection.getClassAccessFlags(), the null check occurs in
3232   // the wrong place; see inline_unsafe_access(), above, for a similar
3233   // situation.
3234   mirror = null_check(mirror);
3235   // If mirror or obj is dead, only null-path is taken.
3236   if (stopped())  return true;
3237 
3238   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3239 
3240   // Now load the mirror's klass metaobject, and null-check it.
3241   // Side-effects region with the control path if the klass is null.
3242   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3243   // If kls is null, we have a primitive mirror.
3244   phi-&gt;init_req(_prim_path, prim_return_value);
3245   if (stopped()) { set_result(region, phi); return true; }
3246   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3247 
3248   Node* p;  // handy temp
3249   Node* null_ctl;
3250 
3251   // Now that we have the non-null klass, we can perform the real query.
3252   // For constant classes, the query will constant-fold in LoadNode::Value.
3253   Node* query_value = top();
3254   switch (id) {
3255   case vmIntrinsics::_isInstance:
3256     // nothing is an instance of a primitive type
3257     query_value = gen_instanceof(obj, kls, safe_for_replace);
3258     break;
3259 
<a name="4" id="anc4"></a>


3260   case vmIntrinsics::_getModifiers:
3261     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3262     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3263     break;
3264 
3265   case vmIntrinsics::_isInterface:
3266     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3267     if (generate_interface_guard(kls, region) != NULL)
3268       // A guard was added.  If the guard is taken, it was an interface.
3269       phi-&gt;add_req(intcon(1));
3270     // If we fall through, it's a plain class.
3271     query_value = intcon(0);
3272     break;
3273 
3274   case vmIntrinsics::_isArray:
3275     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3276     if (generate_array_guard(kls, region) != NULL)
3277       // A guard was added.  If the guard is taken, it was an array.
3278       phi-&gt;add_req(intcon(1));
3279     // If we fall through, it's a plain class.
3280     query_value = intcon(0);
3281     break;
3282 
3283   case vmIntrinsics::_isPrimitive:
3284     query_value = intcon(0); // "normal" path produces false
3285     break;
3286 
3287   case vmIntrinsics::_getSuperclass:
3288     // The rules here are somewhat unfortunate, but we can still do better
3289     // with random logic than with a JNI call.
3290     // Interfaces store null or Object as _super, but must report null.
3291     // Arrays store an intermediate super as _super, but must report Object.
3292     // Other types can report the actual _super.
3293     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3294     if (generate_interface_guard(kls, region) != NULL)
3295       // A guard was added.  If the guard is taken, it was an interface.
3296       phi-&gt;add_req(null());
3297     if (generate_array_guard(kls, region) != NULL)
3298       // A guard was added.  If the guard is taken, it was an array.
3299       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3300     // If we fall through, it's a plain class.  Get its _super.
3301     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3302     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3303     null_ctl = top();
3304     kls = null_check_oop(kls, &amp;null_ctl);
3305     if (null_ctl != top()) {
3306       // If the guard is taken, Object.superClass is null (both klass and mirror).
3307       region-&gt;add_req(null_ctl);
3308       phi   -&gt;add_req(null());
3309     }
3310     if (!stopped()) {
3311       query_value = load_mirror_from_klass(kls);
3312     }
3313     break;
3314 
3315   case vmIntrinsics::_getClassAccessFlags:
3316     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3317     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3318     break;
3319 
3320   default:
3321     fatal_unexpected_iid(id);
3322     break;
3323   }
3324 
3325   // Fall-through is the normal case of a query to a real class.
3326   phi-&gt;init_req(1, query_value);
3327   region-&gt;init_req(1, control());
3328 
3329   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3330   set_result(region, phi);
3331   return true;
3332 }
3333 
3334 //-------------------------inline_Class_cast-------------------
3335 bool LibraryCallKit::inline_Class_cast() {
3336   Node* mirror = argument(0); // Class
3337   Node* obj    = argument(1);
3338   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3339   if (mirror_con == NULL) {
3340     return false;  // dead path (mirror-&gt;is_top()).
3341   }
3342   if (obj == NULL || obj-&gt;is_top()) {
3343     return false;  // dead path
3344   }
3345   const TypeOopPtr* tp = _gvn.type(obj)-&gt;isa_oopptr();
3346 
3347   // First, see if Class.cast() can be folded statically.
3348   // java_mirror_type() returns non-null for compile-time Class constants.
3349   ciType* tm = mirror_con-&gt;java_mirror_type();
3350   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp;
3351       tp != NULL &amp;&amp; tp-&gt;klass() != NULL) {
3352     if (!tp-&gt;klass()-&gt;is_loaded()) {
3353       // Don't use intrinsic when class is not loaded.
3354       return false;
3355     } else {
3356       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), tp-&gt;klass());
3357       if (static_res == Compile::SSC_always_true) {
3358         // isInstance() is true - fold the code.
3359         set_result(obj);
3360         return true;
3361       } else if (static_res == Compile::SSC_always_false) {
3362         // Don't use intrinsic, have to throw ClassCastException.
3363         // If the reference is null, the non-intrinsic bytecode will
3364         // be optimized appropriately.
3365         return false;
3366       }
3367     }
3368   }
3369 
3370   // Bailout intrinsic and do normal inlining if exception path is frequent.
3371   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3372     return false;
3373   }
3374 
3375   // Generate dynamic checks.
3376   // Class.cast() is java implementation of _checkcast bytecode.
3377   // Do checkcast (Parse::do_checkcast()) optimizations here.
3378 
3379   mirror = null_check(mirror);
3380   // If mirror is dead, only null-path is taken.
3381   if (stopped()) {
3382     return true;
3383   }
3384 
3385   // Not-subtype or the mirror's klass ptr is NULL (in case it is a primitive).
3386   enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };
3387   RegionNode* region = new RegionNode(PATH_LIMIT);
3388   record_for_igvn(region);
3389 
3390   // Now load the mirror's klass metaobject, and null-check it.
3391   // If kls is null, we have a primitive mirror and
3392   // nothing is an instance of a primitive type.
3393   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3394 
3395   Node* res = top();
3396   if (!stopped()) {
3397     Node* bad_type_ctrl = top();
3398     // Do checkcast optimizations.
3399     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3400     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3401   }
3402   if (region-&gt;in(_prim_path) != top() ||
3403       region-&gt;in(_bad_type_path) != top()) {
3404     // Let Interpreter throw ClassCastException.
3405     PreserveJVMState pjvms(this);
3406     set_control(_gvn.transform(region));
3407     uncommon_trap(Deoptimization::Reason_intrinsic,
3408                   Deoptimization::Action_maybe_recompile);
3409   }
3410   if (!stopped()) {
3411     set_result(res);
3412   }
3413   return true;
3414 }
3415 
3416 
3417 //--------------------------inline_native_subtype_check------------------------
3418 // This intrinsic takes the JNI calls out of the heart of
3419 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3420 bool LibraryCallKit::inline_native_subtype_check() {
3421   // Pull both arguments off the stack.
3422   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3423   args[0] = argument(0);
3424   args[1] = argument(1);
3425   Node* klasses[2];             // corresponding Klasses: superk, subk
3426   klasses[0] = klasses[1] = top();
3427 
3428   enum {
3429     // A full decision tree on {superc is prim, subc is prim}:
3430     _prim_0_path = 1,           // {P,N} =&gt; false
3431                                 // {P,P} &amp; superc!=subc =&gt; false
3432     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3433     _prim_1_path,               // {N,P} =&gt; false
3434     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3435     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3436     PATH_LIMIT
3437   };
3438 
3439   RegionNode* region = new RegionNode(PATH_LIMIT);
3440   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3441   record_for_igvn(region);
3442 
3443   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3444   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3445   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3446 
3447   // First null-check both mirrors and load each mirror's klass metaobject.
3448   int which_arg;
3449   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3450     Node* arg = args[which_arg];
3451     arg = null_check(arg);
3452     if (stopped())  break;
3453     args[which_arg] = arg;
3454 
3455     Node* p = basic_plus_adr(arg, class_klass_offset);
3456     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3457     klasses[which_arg] = _gvn.transform(kls);
3458   }
3459 
3460   // Having loaded both klasses, test each for null.
3461   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3462   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3463     Node* kls = klasses[which_arg];
3464     Node* null_ctl = top();
3465     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3466     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3467     region-&gt;init_req(prim_path, null_ctl);
3468     if (stopped())  break;
3469     klasses[which_arg] = kls;
3470   }
3471 
3472   if (!stopped()) {
3473     // now we have two reference types, in klasses[0..1]
3474     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3475     Node* superk = klasses[0];  // the receiver
3476     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3477     // now we have a successful reference subtype check
3478     region-&gt;set_req(_ref_subtype_path, control());
3479   }
3480 
3481   // If both operands are primitive (both klasses null), then
3482   // we must return true when they are identical primitives.
3483   // It is convenient to test this after the first null klass check.
3484   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3485   if (!stopped()) {
3486     // Since superc is primitive, make a guard for the superc==subc case.
3487     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3488     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
3489     generate_guard(bol_eq, region, PROB_FAIR);
3490     if (region-&gt;req() == PATH_LIMIT+1) {
3491       // A guard was added.  If the added guard is taken, superc==subc.
3492       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3493       region-&gt;del_req(PATH_LIMIT);
3494     }
3495     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3496   }
3497 
3498   // these are the only paths that produce 'true':
3499   phi-&gt;set_req(_prim_same_path,   intcon(1));
3500   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3501 
3502   // pull together the cases:
3503   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3504   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3505     Node* ctl = region-&gt;in(i);
3506     if (ctl == NULL || ctl == top()) {
3507       region-&gt;set_req(i, top());
3508       phi   -&gt;set_req(i, top());
3509     } else if (phi-&gt;in(i) == NULL) {
3510       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3511     }
3512   }
3513 
3514   set_control(_gvn.transform(region));
3515   set_result(_gvn.transform(phi));
3516   return true;
3517 }
3518 
3519 //---------------------generate_array_guard_common------------------------
3520 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3521                                                   bool obj_array, bool not_array) {
3522 
3523   if (stopped()) {
3524     return NULL;
3525   }
3526 
3527   // If obj_array/non_array==false/false:
3528   // Branch around if the given klass is in fact an array (either obj or prim).
3529   // If obj_array/non_array==false/true:
3530   // Branch around if the given klass is not an array klass of any kind.
3531   // If obj_array/non_array==true/true:
3532   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3533   // If obj_array/non_array==true/false:
3534   // Branch around if the kls is an oop array (Object[] or subtype)
3535   //
3536   // Like generate_guard, adds a new path onto the region.
3537   jint  layout_con = 0;
3538   Node* layout_val = get_layout_helper(kls, layout_con);
3539   if (layout_val == NULL) {
3540     bool query = (obj_array
3541                   ? Klass::layout_helper_is_objArray(layout_con)
3542                   : Klass::layout_helper_is_array(layout_con));
3543     if (query == not_array) {
3544       return NULL;                       // never a branch
3545     } else {                             // always a branch
3546       Node* always_branch = control();
3547       if (region != NULL)
3548         region-&gt;add_req(always_branch);
3549       set_control(top());
3550       return always_branch;
3551     }
3552   }
3553   // Now test the correct condition.
3554   jint  nval = (obj_array
3555                 ? ((jint)Klass::_lh_array_tag_type_value
3556                    &lt;&lt;    Klass::_lh_array_tag_shift)
3557                 : Klass::_lh_neutral_value);
3558   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3559   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3560   // invert the test if we are looking for a non-array
3561   if (not_array)  btest = BoolTest(btest).negate();
3562   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3563   return generate_fair_guard(bol, region);
3564 }
3565 
3566 
3567 //-----------------------inline_native_newArray--------------------------
3568 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3569 bool LibraryCallKit::inline_native_newArray() {
3570   Node* mirror    = argument(0);
3571   Node* count_val = argument(1);
3572 
3573   mirror = null_check(mirror);
3574   // If mirror or obj is dead, only null-path is taken.
3575   if (stopped())  return true;
3576 
3577   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3578   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3579   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3580   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3581   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3582 
3583   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3584   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3585                                                   result_reg, _slow_path);
3586   Node* normal_ctl   = control();
3587   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3588 
3589   // Generate code for the slow case.  We make a call to newArray().
3590   set_control(no_array_ctl);
3591   if (!stopped()) {
3592     // Either the input type is void.class, or else the
3593     // array klass has not yet been cached.  Either the
3594     // ensuing call will throw an exception, or else it
3595     // will cache the array klass for next time.
3596     PreserveJVMState pjvms(this);
3597     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3598     Node* slow_result = set_results_for_java_call(slow_call);
3599     // this-&gt;control() comes from set_results_for_java_call
3600     result_reg-&gt;set_req(_slow_path, control());
3601     result_val-&gt;set_req(_slow_path, slow_result);
3602     result_io -&gt;set_req(_slow_path, i_o());
3603     result_mem-&gt;set_req(_slow_path, reset_memory());
3604   }
3605 
3606   set_control(normal_ctl);
3607   if (!stopped()) {
3608     // Normal case:  The array type has been cached in the java.lang.Class.
3609     // The following call works fine even if the array type is polymorphic.
3610     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3611     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3612     result_reg-&gt;init_req(_normal_path, control());
3613     result_val-&gt;init_req(_normal_path, obj);
3614     result_io -&gt;init_req(_normal_path, i_o());
3615     result_mem-&gt;init_req(_normal_path, reset_memory());
3616   }
3617 
3618   // Return the combined state.
3619   set_i_o(        _gvn.transform(result_io)  );
3620   set_all_memory( _gvn.transform(result_mem));
3621 
3622   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3623   set_result(result_reg, result_val);
3624   return true;
3625 }
3626 
3627 //----------------------inline_native_getLength--------------------------
3628 // public static native int java.lang.reflect.Array.getLength(Object array);
3629 bool LibraryCallKit::inline_native_getLength() {
3630   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3631 
3632   Node* array = null_check(argument(0));
3633   // If array is dead, only null-path is taken.
3634   if (stopped())  return true;
3635 
3636   // Deoptimize if it is a non-array.
3637   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3638 
3639   if (non_array != NULL) {
3640     PreserveJVMState pjvms(this);
3641     set_control(non_array);
3642     uncommon_trap(Deoptimization::Reason_intrinsic,
3643                   Deoptimization::Action_maybe_recompile);
3644   }
3645 
3646   // If control is dead, only non-array-path is taken.
3647   if (stopped())  return true;
3648 
3649   // The works fine even if the array type is polymorphic.
3650   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3651   Node* result = load_array_length(array);
3652 
3653   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3654   set_result(result);
3655   return true;
3656 }
3657 
3658 //------------------------inline_array_copyOf----------------------------
3659 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3660 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3661 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3662   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3663 
3664   // Get the arguments.
3665   Node* original          = argument(0);
3666   Node* start             = is_copyOfRange? argument(1): intcon(0);
3667   Node* end               = is_copyOfRange? argument(2): argument(1);
3668   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3669 
3670   Node* newcopy;
3671 
3672   // Set the original stack and the reexecute bit for the interpreter to reexecute
3673   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3674   { PreserveReexecuteState preexecs(this);
3675     jvms()-&gt;set_should_reexecute(true);
3676 
3677     array_type_mirror = null_check(array_type_mirror);
3678     original          = null_check(original);
3679 
3680     // Check if a null path was taken unconditionally.
3681     if (stopped())  return true;
3682 
3683     Node* orig_length = load_array_length(original);
3684 
3685     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3686     klass_node = null_check(klass_node);
3687 
3688     RegionNode* bailout = new RegionNode(1);
3689     record_for_igvn(bailout);
3690 
3691     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3692     // Bail out if that is so.
3693     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3694     if (not_objArray != NULL) {
3695       // Improve the klass node's type from the new optimistic assumption:
3696       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3697       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3698       Node* cast = new CastPPNode(klass_node, akls);
3699       cast-&gt;init_req(0, control());
3700       klass_node = _gvn.transform(cast);
3701     }
3702 
3703     // Bail out if either start or end is negative.
3704     generate_negative_guard(start, bailout, &amp;start);
3705     generate_negative_guard(end,   bailout, &amp;end);
3706 
3707     Node* length = end;
3708     if (_gvn.type(start) != TypeInt::ZERO) {
3709       length = _gvn.transform(new SubINode(end, start));
3710     }
3711 
3712     // Bail out if length is negative.
3713     // Without this the new_array would throw
3714     // NegativeArraySizeException but IllegalArgumentException is what
3715     // should be thrown
3716     generate_negative_guard(length, bailout, &amp;length);
3717 
3718     if (bailout-&gt;req() &gt; 1) {
3719       PreserveJVMState pjvms(this);
3720       set_control(_gvn.transform(bailout));
3721       uncommon_trap(Deoptimization::Reason_intrinsic,
3722                     Deoptimization::Action_maybe_recompile);
3723     }
3724 
3725     if (!stopped()) {
3726       // How many elements will we copy from the original?
3727       // The answer is MinI(orig_length - start, length).
3728       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
3729       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3730 
3731       // Generate a direct call to the right arraycopy function(s).
3732       // We know the copy is disjoint but we might not know if the
3733       // oop stores need checking.
3734       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3735       // This will fail a store-check if x contains any non-nulls.
3736 
3737       // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3738       // loads/stores but it is legal only if we're sure the
3739       // Arrays.copyOf would succeed. So we need all input arguments
3740       // to the copyOf to be validated, including that the copy to the
3741       // new array won't trigger an ArrayStoreException. That subtype
3742       // check can be optimized if we know something on the type of
3743       // the input array from type speculation.
3744       if (_gvn.type(klass_node)-&gt;singleton()) {
3745         ciKlass* subk   = _gvn.type(load_object_klass(original))-&gt;is_klassptr()-&gt;klass();
3746         ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3747 
3748         int test = C-&gt;static_subtype_check(superk, subk);
3749         if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3750           const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3751           if (t_original-&gt;speculative_type() != NULL) {
3752             original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3753           }
3754         }
3755       }
3756 
3757       bool validated = false;
3758       // Reason_class_check rather than Reason_intrinsic because we
3759       // want to intrinsify even if this traps.
3760       if (!too_many_traps(Deoptimization::Reason_class_check)) {
3761         Node* not_subtype_ctrl = gen_subtype_check(load_object_klass(original),
3762                                                    klass_node);
3763 
3764         if (not_subtype_ctrl != top()) {
3765           PreserveJVMState pjvms(this);
3766           set_control(not_subtype_ctrl);
3767           uncommon_trap(Deoptimization::Reason_class_check,
3768                         Deoptimization::Action_make_not_entrant);
3769           assert(stopped(), "Should be stopped");
3770         }
3771         validated = true;
3772       }
3773 
3774       if (!stopped()) {
3775         newcopy = new_array(klass_node, length, 0);  // no arguments to push
3776 
3777         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true,
3778                                                 load_object_klass(original), klass_node);
3779         if (!is_copyOfRange) {
3780           ac-&gt;set_copyof(validated);
3781         } else {
3782           ac-&gt;set_copyofrange(validated);
3783         }
3784         Node* n = _gvn.transform(ac);
3785         if (n == ac) {
3786           ac-&gt;connect_outputs(this);
3787         } else {
3788           assert(validated, "shouldn't transform if all arguments not validated");
3789           set_all_memory(n);
3790         }
3791       }
3792     }
3793   } // original reexecute is set back here
3794 
3795   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3796   if (!stopped()) {
3797     set_result(newcopy);
3798   }
3799   return true;
3800 }
3801 
3802 
3803 //----------------------generate_virtual_guard---------------------------
3804 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
3805 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
3806                                              RegionNode* slow_region) {
3807   ciMethod* method = callee();
3808   int vtable_index = method-&gt;vtable_index();
3809   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3810          err_msg_res("bad index %d", vtable_index));
3811   // Get the Method* out of the appropriate vtable entry.
3812   int entry_offset  = (InstanceKlass::vtable_start_offset() +
3813                      vtable_index*vtableEntry::size()) * wordSize +
3814                      vtableEntry::method_offset_in_bytes();
3815   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
3816   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3817 
3818   // Compare the target method with the expected method (e.g., Object.hashCode).
3819   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
3820 
3821   Node* native_call = makecon(native_call_addr);
3822   Node* chk_native  = _gvn.transform(new CmpPNode(target_call, native_call));
3823   Node* test_native = _gvn.transform(new BoolNode(chk_native, BoolTest::ne));
3824 
3825   return generate_slow_guard(test_native, slow_region);
3826 }
3827 
3828 //-----------------------generate_method_call----------------------------
3829 // Use generate_method_call to make a slow-call to the real
3830 // method if the fast path fails.  An alternative would be to
3831 // use a stub like OptoRuntime::slow_arraycopy_Java.
3832 // This only works for expanding the current library call,
3833 // not another intrinsic.  (E.g., don't use this for making an
3834 // arraycopy call inside of the copyOf intrinsic.)
3835 CallJavaNode*
3836 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
3837   // When compiling the intrinsic method itself, do not use this technique.
3838   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
3839 
3840   ciMethod* method = callee();
3841   // ensure the JVMS we have will be correct for this call
3842   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
3843 
3844   const TypeFunc* tf = TypeFunc::make(method);
3845   CallJavaNode* slow_call;
3846   if (is_static) {
3847     assert(!is_virtual, "");
3848     slow_call = new CallStaticJavaNode(C, tf,
3849                            SharedRuntime::get_resolve_static_call_stub(),
3850                            method, bci());
3851   } else if (is_virtual) {
3852     null_check_receiver();
3853     int vtable_index = Method::invalid_vtable_index;
3854     if (UseInlineCaches) {
3855       // Suppress the vtable call
3856     } else {
3857       // hashCode and clone are not a miranda methods,
3858       // so the vtable index is fixed.
3859       // No need to use the linkResolver to get it.
3860        vtable_index = method-&gt;vtable_index();
3861        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3862               err_msg_res("bad index %d", vtable_index));
3863     }
3864     slow_call = new CallDynamicJavaNode(tf,
3865                           SharedRuntime::get_resolve_virtual_call_stub(),
3866                           method, vtable_index, bci());
3867   } else {  // neither virtual nor static:  opt_virtual
3868     null_check_receiver();
3869     slow_call = new CallStaticJavaNode(C, tf,
3870                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
3871                                 method, bci());
3872     slow_call-&gt;set_optimized_virtual(true);
3873   }
3874   set_arguments_for_java_call(slow_call);
3875   set_edges_for_java_call(slow_call);
3876   return slow_call;
3877 }
3878 
3879 
3880 /**
3881  * Build special case code for calls to hashCode on an object. This call may
3882  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
3883  * slightly different code.
3884  */
3885 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
3886   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
3887   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
3888 
3889   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
3890 
3891   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3892   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
3893   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3894   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3895   Node* obj = NULL;
3896   if (!is_static) {
3897     // Check for hashing null object
3898     obj = null_check_receiver();
3899     if (stopped())  return true;        // unconditionally null
3900     result_reg-&gt;init_req(_null_path, top());
3901     result_val-&gt;init_req(_null_path, top());
3902   } else {
3903     // Do a null check, and return zero if null.
3904     // System.identityHashCode(null) == 0
3905     obj = argument(0);
3906     Node* null_ctl = top();
3907     obj = null_check_oop(obj, &amp;null_ctl);
3908     result_reg-&gt;init_req(_null_path, null_ctl);
3909     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
3910   }
3911 
3912   // Unconditionally null?  Then return right away.
3913   if (stopped()) {
3914     set_control( result_reg-&gt;in(_null_path));
3915     if (!stopped())
3916       set_result(result_val-&gt;in(_null_path));
3917     return true;
3918   }
3919 
3920   // We only go to the fast case code if we pass a number of guards.  The
3921   // paths which do not pass are accumulated in the slow_region.
3922   RegionNode* slow_region = new RegionNode(1);
3923   record_for_igvn(slow_region);
3924 
3925   // If this is a virtual call, we generate a funny guard.  We pull out
3926   // the vtable entry corresponding to hashCode() from the target object.
3927   // If the target method which we are calling happens to be the native
3928   // Object hashCode() method, we pass the guard.  We do not need this
3929   // guard for non-virtual calls -- the caller is known to be the native
3930   // Object hashCode().
3931   if (is_virtual) {
3932     // After null check, get the object's klass.
3933     Node* obj_klass = load_object_klass(obj);
3934     generate_virtual_guard(obj_klass, slow_region);
3935   }
3936 
3937   // Get the header out of the object, use LoadMarkNode when available
3938   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
3939   // The control of the load must be NULL. Otherwise, the load can move before
3940   // the null check after castPP removal.
3941   Node* no_ctrl = NULL;
3942   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
3943 
3944   // Test the header to see if it is unlocked.
3945   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
3946   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
3947   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
3948   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
3949   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
3950 
3951   generate_slow_guard(test_unlocked, slow_region);
3952 
3953   // Get the hash value and check to see that it has been properly assigned.
3954   // We depend on hash_mask being at most 32 bits and avoid the use of
3955   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
3956   // vm: see markOop.hpp.
3957   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
3958   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
3959   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
3960   // This hack lets the hash bits live anywhere in the mark object now, as long
3961   // as the shift drops the relevant bits into the low 32 bits.  Note that
3962   // Java spec says that HashCode is an int so there's no point in capturing
3963   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
3964   hshifted_header      = ConvX2I(hshifted_header);
3965   Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));
3966 
3967   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
3968   Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));
3969   Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));
3970 
3971   generate_slow_guard(test_assigned, slow_region);
3972 
3973   Node* init_mem = reset_memory();
3974   // fill in the rest of the null path:
3975   result_io -&gt;init_req(_null_path, i_o());
3976   result_mem-&gt;init_req(_null_path, init_mem);
3977 
3978   result_val-&gt;init_req(_fast_path, hash_val);
3979   result_reg-&gt;init_req(_fast_path, control());
3980   result_io -&gt;init_req(_fast_path, i_o());
3981   result_mem-&gt;init_req(_fast_path, init_mem);
3982 
3983   // Generate code for the slow case.  We make a call to hashCode().
3984   set_control(_gvn.transform(slow_region));
3985   if (!stopped()) {
3986     // No need for PreserveJVMState, because we're using up the present state.
3987     set_all_memory(init_mem);
3988     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
3989     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
3990     Node* slow_result = set_results_for_java_call(slow_call);
3991     // this-&gt;control() comes from set_results_for_java_call
3992     result_reg-&gt;init_req(_slow_path, control());
3993     result_val-&gt;init_req(_slow_path, slow_result);
3994     result_io  -&gt;set_req(_slow_path, i_o());
3995     result_mem -&gt;set_req(_slow_path, reset_memory());
3996   }
3997 
3998   // Return the combined state.
3999   set_i_o(        _gvn.transform(result_io)  );
4000   set_all_memory( _gvn.transform(result_mem));
4001 
4002   set_result(result_reg, result_val);
4003   return true;
4004 }
4005 
4006 //---------------------------inline_native_getClass----------------------------
4007 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4008 //
4009 // Build special case code for calls to getClass on an object.
4010 bool LibraryCallKit::inline_native_getClass() {
4011   Node* obj = null_check_receiver();
4012   if (stopped())  return true;
4013   set_result(load_mirror_from_klass(load_object_klass(obj)));
4014   return true;
4015 }
4016 
4017 //-----------------inline_native_Reflection_getCallerClass---------------------
4018 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4019 //
4020 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4021 //
4022 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4023 // in that it must skip particular security frames and checks for
4024 // caller sensitive methods.
4025 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4026 #ifndef PRODUCT
4027   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4028     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4029   }
4030 #endif
4031 
4032   if (!jvms()-&gt;has_method()) {
4033 #ifndef PRODUCT
4034     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4035       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4036     }
4037 #endif
4038     return false;
4039   }
4040 
4041   // Walk back up the JVM state to find the caller at the required
4042   // depth.
4043   JVMState* caller_jvms = jvms();
4044 
4045   // Cf. JVM_GetCallerClass
4046   // NOTE: Start the loop at depth 1 because the current JVM state does
4047   // not include the Reflection.getCallerClass() frame.
4048   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4049     ciMethod* m = caller_jvms-&gt;method();
4050     switch (n) {
4051     case 0:
4052       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4053       break;
4054     case 1:
4055       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4056       if (!m-&gt;caller_sensitive()) {
4057 #ifndef PRODUCT
4058         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4059           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4060         }
4061 #endif
4062         return false;  // bail-out; let JVM_GetCallerClass do the work
4063       }
4064       break;
4065     default:
4066       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4067         // We have reached the desired frame; return the holder class.
4068         // Acquire method holder as java.lang.Class and push as constant.
4069         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4070         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4071         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4072 
4073 #ifndef PRODUCT
4074         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4075           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4076           tty-&gt;print_cr("  JVM state at this point:");
4077           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4078             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4079             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4080           }
4081         }
4082 #endif
4083         return true;
4084       }
4085       break;
4086     }
4087   }
4088 
4089 #ifndef PRODUCT
4090   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4091     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4092     tty-&gt;print_cr("  JVM state at this point:");
4093     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4094       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4095       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4096     }
4097   }
4098 #endif
4099 
4100   return false;  // bail-out; let JVM_GetCallerClass do the work
4101 }
4102 
4103 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4104   Node* arg = argument(0);
4105   Node* result;
4106 
4107   switch (id) {
4108   case vmIntrinsics::_floatToRawIntBits:    result = new MoveF2INode(arg);  break;
4109   case vmIntrinsics::_intBitsToFloat:       result = new MoveI2FNode(arg);  break;
4110   case vmIntrinsics::_doubleToRawLongBits:  result = new MoveD2LNode(arg);  break;
4111   case vmIntrinsics::_longBitsToDouble:     result = new MoveL2DNode(arg);  break;
4112 
4113   case vmIntrinsics::_doubleToLongBits: {
4114     // two paths (plus control) merge in a wood
4115     RegionNode *r = new RegionNode(3);
4116     Node *phi = new PhiNode(r, TypeLong::LONG);
4117 
4118     Node *cmpisnan = _gvn.transform(new CmpDNode(arg, arg));
4119     // Build the boolean node
4120     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4121 
4122     // Branch either way.
4123     // NaN case is less traveled, which makes all the difference.
4124     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4125     Node *opt_isnan = _gvn.transform(ifisnan);
4126     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4127     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4128     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4129 
4130     set_control(iftrue);
4131 
4132     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4133     Node *slow_result = longcon(nan_bits); // return NaN
4134     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4135     r-&gt;init_req(1, iftrue);
4136 
4137     // Else fall through
4138     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4139     set_control(iffalse);
4140 
4141     phi-&gt;init_req(2, _gvn.transform(new MoveD2LNode(arg)));
4142     r-&gt;init_req(2, iffalse);
4143 
4144     // Post merge
4145     set_control(_gvn.transform(r));
4146     record_for_igvn(r);
4147 
4148     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4149     result = phi;
4150     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4151     break;
4152   }
4153 
4154   case vmIntrinsics::_floatToIntBits: {
4155     // two paths (plus control) merge in a wood
4156     RegionNode *r = new RegionNode(3);
4157     Node *phi = new PhiNode(r, TypeInt::INT);
4158 
4159     Node *cmpisnan = _gvn.transform(new CmpFNode(arg, arg));
4160     // Build the boolean node
4161     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4162 
4163     // Branch either way.
4164     // NaN case is less traveled, which makes all the difference.
4165     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4166     Node *opt_isnan = _gvn.transform(ifisnan);
4167     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4168     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4169     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4170 
4171     set_control(iftrue);
4172 
4173     static const jint nan_bits = 0x7fc00000;
4174     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4175     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4176     r-&gt;init_req(1, iftrue);
4177 
4178     // Else fall through
4179     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4180     set_control(iffalse);
4181 
4182     phi-&gt;init_req(2, _gvn.transform(new MoveF2INode(arg)));
4183     r-&gt;init_req(2, iffalse);
4184 
4185     // Post merge
4186     set_control(_gvn.transform(r));
4187     record_for_igvn(r);
4188 
4189     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4190     result = phi;
4191     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4192     break;
4193   }
4194 
4195   default:
4196     fatal_unexpected_iid(id);
4197     break;
4198   }
4199   set_result(_gvn.transform(result));
4200   return true;
4201 }
4202 
4203 #ifdef _LP64
4204 #define XTOP ,top() /*additional argument*/
4205 #else  //_LP64
4206 #define XTOP        /*no additional argument*/
4207 #endif //_LP64
4208 
4209 //----------------------inline_unsafe_copyMemory-------------------------
4210 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4211 bool LibraryCallKit::inline_unsafe_copyMemory() {
4212   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4213   null_check_receiver();  // null-check receiver
4214   if (stopped())  return true;
4215 
4216   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4217 
4218   Node* src_ptr =         argument(1);   // type: oop
4219   Node* src_off = ConvL2X(argument(2));  // type: long
4220   Node* dst_ptr =         argument(4);   // type: oop
4221   Node* dst_off = ConvL2X(argument(5));  // type: long
4222   Node* size    = ConvL2X(argument(7));  // type: long
4223 
4224   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4225          "fieldOffset must be byte-scaled");
4226 
4227   Node* src = make_unsafe_address(src_ptr, src_off);
4228   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4229 
4230   // Conservatively insert a memory barrier on all memory slices.
4231   // Do not let writes of the copy source or destination float below the copy.
4232   insert_mem_bar(Op_MemBarCPUOrder);
4233 
4234   // Call it.  Note that the length argument is not scaled.
4235   make_runtime_call(RC_LEAF|RC_NO_FP,
4236                     OptoRuntime::fast_arraycopy_Type(),
4237                     StubRoutines::unsafe_arraycopy(),
4238                     "unsafe_arraycopy",
4239                     TypeRawPtr::BOTTOM,
4240                     src, dst, size XTOP);
4241 
4242   // Do not let reads of the copy destination float above the copy.
4243   insert_mem_bar(Op_MemBarCPUOrder);
4244 
4245   return true;
4246 }
4247 
4248 //------------------------clone_coping-----------------------------------
4249 // Helper function for inline_native_clone.
4250 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4251   assert(obj_size != NULL, "");
4252   Node* raw_obj = alloc_obj-&gt;in(1);
4253   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4254 
4255   AllocateNode* alloc = NULL;
4256   if (ReduceBulkZeroing) {
4257     // We will be completely responsible for initializing this object -
4258     // mark Initialize node as complete.
4259     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4260     // The object was just allocated - there should be no any stores!
4261     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4262     // Mark as complete_with_arraycopy so that on AllocateNode
4263     // expansion, we know this AllocateNode is initialized by an array
4264     // copy and a StoreStore barrier exists after the array copy.
4265     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4266   }
4267 
4268   // Copy the fastest available way.
4269   // TODO: generate fields copies for small objects instead.
4270   Node* src  = obj;
4271   Node* dest = alloc_obj;
4272   Node* size = _gvn.transform(obj_size);
4273 
4274   // Exclude the header but include array length to copy by 8 bytes words.
4275   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4276   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4277                             instanceOopDesc::base_offset_in_bytes();
4278   // base_off:
4279   // 8  - 32-bit VM
4280   // 12 - 64-bit VM, compressed klass
4281   // 16 - 64-bit VM, normal klass
4282   if (base_off % BytesPerLong != 0) {
4283     assert(UseCompressedClassPointers, "");
4284     if (is_array) {
4285       // Exclude length to copy by 8 bytes words.
4286       base_off += sizeof(int);
4287     } else {
4288       // Include klass to copy by 8 bytes words.
4289       base_off = instanceOopDesc::klass_offset_in_bytes();
4290     }
4291     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4292   }
4293   src  = basic_plus_adr(src,  base_off);
4294   dest = basic_plus_adr(dest, base_off);
4295 
4296   // Compute the length also, if needed:
4297   Node* countx = size;
4298   countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
4299   countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong) ));
4300 
4301   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4302 
4303   ArrayCopyNode* ac = ArrayCopyNode::make(this, false, src, NULL, dest, NULL, countx, false);
4304   ac-&gt;set_clonebasic();
4305   Node* n = _gvn.transform(ac);
4306   if (n == ac) {
4307     set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
4308   } else {
4309     set_all_memory(n);
4310   }
4311 
4312   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4313   if (card_mark) {
4314     assert(!is_array, "");
4315     // Put in store barrier for any and all oops we are sticking
4316     // into this object.  (We could avoid this if we could prove
4317     // that the object type contains no oop fields at all.)
4318     Node* no_particular_value = NULL;
4319     Node* no_particular_field = NULL;
4320     int raw_adr_idx = Compile::AliasIdxRaw;
4321     post_barrier(control(),
4322                  memory(raw_adr_type),
4323                  alloc_obj,
4324                  no_particular_field,
4325                  raw_adr_idx,
4326                  no_particular_value,
4327                  T_OBJECT,
4328                  false);
4329   }
4330 
4331   // Do not let reads from the cloned object float above the arraycopy.
4332   if (alloc != NULL) {
4333     // Do not let stores that initialize this object be reordered with
4334     // a subsequent store that would make this object accessible by
4335     // other threads.
4336     // Record what AllocateNode this StoreStore protects so that
4337     // escape analysis can go from the MemBarStoreStoreNode to the
4338     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4339     // based on the escape status of the AllocateNode.
4340     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4341   } else {
4342     insert_mem_bar(Op_MemBarCPUOrder);
4343   }
4344 }
4345 
4346 //------------------------inline_native_clone----------------------------
4347 // protected native Object java.lang.Object.clone();
4348 //
4349 // Here are the simple edge cases:
4350 //  null receiver =&gt; normal trap
4351 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4352 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4353 //
4354 // The general case has two steps, allocation and copying.
4355 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4356 //
4357 // Copying also has two cases, oop arrays and everything else.
4358 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4359 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4360 //
4361 // These steps fold up nicely if and when the cloned object's klass
4362 // can be sharply typed as an object array, a type array, or an instance.
4363 //
4364 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4365   PhiNode* result_val;
4366 
4367   // Set the reexecute bit for the interpreter to reexecute
4368   // the bytecode that invokes Object.clone if deoptimization happens.
4369   { PreserveReexecuteState preexecs(this);
4370     jvms()-&gt;set_should_reexecute(true);
4371 
4372     Node* obj = null_check_receiver();
4373     if (stopped())  return true;
4374 
4375     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4376 
4377     // If we are going to clone an instance, we need its exact type to
4378     // know the number and types of fields to convert the clone to
4379     // loads/stores. Maybe a speculative type can help us.
4380     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4381         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4382         obj_type-&gt;speculative_type()-&gt;is_instance_klass()) {
4383       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4384       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4385           !spec_ik-&gt;has_injected_fields()) {
4386         ciKlass* k = obj_type-&gt;klass();
4387         if (!k-&gt;is_instance_klass() ||
4388             k-&gt;as_instance_klass()-&gt;is_interface() ||
4389             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4390           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4391         }
4392       }
4393     }
4394 
4395     Node* obj_klass = load_object_klass(obj);
4396     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4397     const TypeOopPtr*   toop   = ((tklass != NULL)
4398                                 ? tklass-&gt;as_instance_type()
4399                                 : TypeInstPtr::NOTNULL);
4400 
4401     // Conservatively insert a memory barrier on all memory slices.
4402     // Do not let writes into the original float below the clone.
4403     insert_mem_bar(Op_MemBarCPUOrder);
4404 
4405     // paths into result_reg:
4406     enum {
4407       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4408       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4409       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4410       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4411       PATH_LIMIT
4412     };
4413     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4414     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4415     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4416     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4417     record_for_igvn(result_reg);
4418 
4419     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4420     int raw_adr_idx = Compile::AliasIdxRaw;
4421 
4422     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4423     if (array_ctl != NULL) {
4424       // It's an array.
4425       PreserveJVMState pjvms(this);
4426       set_control(array_ctl);
4427       Node* obj_length = load_array_length(obj);
4428       Node* obj_size  = NULL;
4429       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4430 
4431       if (!use_ReduceInitialCardMarks()) {
4432         // If it is an oop array, it requires very special treatment,
4433         // because card marking is required on each card of the array.
4434         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4435         if (is_obja != NULL) {
4436           PreserveJVMState pjvms2(this);
4437           set_control(is_obja);
4438           // Generate a direct call to the right arraycopy function(s).
4439           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4440           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL);
4441           ac-&gt;set_cloneoop();
4442           Node* n = _gvn.transform(ac);
4443           assert(n == ac, "cannot disappear");
4444           ac-&gt;connect_outputs(this);
4445 
4446           result_reg-&gt;init_req(_objArray_path, control());
4447           result_val-&gt;init_req(_objArray_path, alloc_obj);
4448           result_i_o -&gt;set_req(_objArray_path, i_o());
4449           result_mem -&gt;set_req(_objArray_path, reset_memory());
4450         }
4451       }
4452       // Otherwise, there are no card marks to worry about.
4453       // (We can dispense with card marks if we know the allocation
4454       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4455       //  causes the non-eden paths to take compensating steps to
4456       //  simulate a fresh allocation, so that no further
4457       //  card marks are required in compiled code to initialize
4458       //  the object.)
4459 
4460       if (!stopped()) {
4461         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4462 
4463         // Present the results of the copy.
4464         result_reg-&gt;init_req(_array_path, control());
4465         result_val-&gt;init_req(_array_path, alloc_obj);
4466         result_i_o -&gt;set_req(_array_path, i_o());
4467         result_mem -&gt;set_req(_array_path, reset_memory());
4468       }
4469     }
4470 
4471     // We only go to the instance fast case code if we pass a number of guards.
4472     // The paths which do not pass are accumulated in the slow_region.
4473     RegionNode* slow_region = new RegionNode(1);
4474     record_for_igvn(slow_region);
4475     if (!stopped()) {
4476       // It's an instance (we did array above).  Make the slow-path tests.
4477       // If this is a virtual call, we generate a funny guard.  We grab
4478       // the vtable entry corresponding to clone() from the target object.
4479       // If the target method which we are calling happens to be the
4480       // Object clone() method, we pass the guard.  We do not need this
4481       // guard for non-virtual calls; the caller is known to be the native
4482       // Object clone().
4483       if (is_virtual) {
4484         generate_virtual_guard(obj_klass, slow_region);
4485       }
4486 
4487       // The object must be cloneable and must not have a finalizer.
4488       // Both of these conditions may be checked in a single test.
4489       // We could optimize the cloneable test further, but we don't care.
4490       generate_access_flags_guard(obj_klass,
4491                                   // Test both conditions:
4492                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4493                                   // Must be cloneable but not finalizer:
4494                                   JVM_ACC_IS_CLONEABLE,
4495                                   slow_region);
4496     }
4497 
4498     if (!stopped()) {
4499       // It's an instance, and it passed the slow-path tests.
4500       PreserveJVMState pjvms(this);
4501       Node* obj_size  = NULL;
4502       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4503       // is reexecuted if deoptimization occurs and there could be problems when merging
4504       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4505       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4506 
4507       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4508 
4509       // Present the results of the slow call.
4510       result_reg-&gt;init_req(_instance_path, control());
4511       result_val-&gt;init_req(_instance_path, alloc_obj);
4512       result_i_o -&gt;set_req(_instance_path, i_o());
4513       result_mem -&gt;set_req(_instance_path, reset_memory());
4514     }
4515 
4516     // Generate code for the slow case.  We make a call to clone().
4517     set_control(_gvn.transform(slow_region));
4518     if (!stopped()) {
4519       PreserveJVMState pjvms(this);
4520       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4521       Node* slow_result = set_results_for_java_call(slow_call);
4522       // this-&gt;control() comes from set_results_for_java_call
4523       result_reg-&gt;init_req(_slow_path, control());
4524       result_val-&gt;init_req(_slow_path, slow_result);
4525       result_i_o -&gt;set_req(_slow_path, i_o());
4526       result_mem -&gt;set_req(_slow_path, reset_memory());
4527     }
4528 
4529     // Return the combined state.
4530     set_control(    _gvn.transform(result_reg));
4531     set_i_o(        _gvn.transform(result_i_o));
4532     set_all_memory( _gvn.transform(result_mem));
4533   } // original reexecute is set back here
4534 
4535   set_result(_gvn.transform(result_val));
4536   return true;
4537 }
4538 
4539 // If we have a tighly coupled allocation, the arraycopy may take care
4540 // of the array initialization. If one of the guards we insert between
4541 // the allocation and the arraycopy causes a deoptimization, an
4542 // unitialized array will escape the compiled method. To prevent that
4543 // we set the JVM state for uncommon traps between the allocation and
4544 // the arraycopy to the state before the allocation so, in case of
4545 // deoptimization, we'll reexecute the allocation and the
4546 // initialization.
4547 JVMState* LibraryCallKit::arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp) {
4548   if (alloc != NULL) {
4549     ciMethod* trap_method = alloc-&gt;jvms()-&gt;method();
4550     int trap_bci = alloc-&gt;jvms()-&gt;bci();
4551 
4552     if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;
4553           !C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
4554       // Make sure there's no store between the allocation and the
4555       // arraycopy otherwise visible side effects could be rexecuted
4556       // in case of deoptimization and cause incorrect execution.
4557       bool no_interfering_store = true;
4558       Node* mem = alloc-&gt;in(TypeFunc::Memory);
4559       if (mem-&gt;is_MergeMem()) {
4560         for (MergeMemStream mms(merged_memory(), mem-&gt;as_MergeMem()); mms.next_non_empty2(); ) {
4561           Node* n = mms.memory();
4562           if (n != mms.memory2() &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4563             assert(n-&gt;is_Store(), "what else?");
4564             no_interfering_store = false;
4565             break;
4566           }
4567         }
4568       } else {
4569         for (MergeMemStream mms(merged_memory()); mms.next_non_empty(); ) {
4570           Node* n = mms.memory();
4571           if (n != mem &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4572             assert(n-&gt;is_Store(), "what else?");
4573             no_interfering_store = false;
4574             break;
4575           }
4576         }
4577       }
4578 
4579       if (no_interfering_store) {
4580         JVMState* old_jvms = alloc-&gt;jvms()-&gt;clone_shallow(C);
4581         uint size = alloc-&gt;req();
4582         SafePointNode* sfpt = new SafePointNode(size, old_jvms);
4583         old_jvms-&gt;set_map(sfpt);
4584         for (uint i = 0; i &lt; size; i++) {
4585           sfpt-&gt;init_req(i, alloc-&gt;in(i));
4586         }
4587         // re-push array length for deoptimization
4588         sfpt-&gt;ins_req(old_jvms-&gt;stkoff() + old_jvms-&gt;sp(), alloc-&gt;in(AllocateNode::ALength));
4589         old_jvms-&gt;set_sp(old_jvms-&gt;sp()+1);
4590         old_jvms-&gt;set_monoff(old_jvms-&gt;monoff()+1);
4591         old_jvms-&gt;set_scloff(old_jvms-&gt;scloff()+1);
4592         old_jvms-&gt;set_endoff(old_jvms-&gt;endoff()+1);
4593         old_jvms-&gt;set_should_reexecute(true);
4594 
4595         sfpt-&gt;set_i_o(map()-&gt;i_o());
4596         sfpt-&gt;set_memory(map()-&gt;memory());
4597         sfpt-&gt;set_control(map()-&gt;control());
4598 
4599         JVMState* saved_jvms = jvms();
4600         saved_reexecute_sp = _reexecute_sp;
4601 
4602         set_jvms(sfpt-&gt;jvms());
4603         _reexecute_sp = jvms()-&gt;sp();
4604 
4605         return saved_jvms;
4606       }
4607     }
4608   }
4609   return NULL;
4610 }
4611 
4612 // In case of a deoptimization, we restart execution at the
4613 // allocation, allocating a new array. We would leave an uninitialized
4614 // array in the heap that GCs wouldn't expect. Move the allocation
4615 // after the traps so we don't allocate the array if we
4616 // deoptimize. This is possible because tightly_coupled_allocation()
4617 // guarantees there's no observer of the allocated array at this point
4618 // and the control flow is simple enough.
4619 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp) {
4620   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4621     assert(alloc != NULL, "only with a tightly coupled allocation");
4622     // restore JVM state to the state at the arraycopy
4623     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4624     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), "memory state changed?");
4625     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), "IO state changed?");
4626     // If we've improved the types of some nodes (null check) while
4627     // emitting the guards, propagate them to the current state
4628     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map());
4629     set_jvms(saved_jvms);
4630     _reexecute_sp = saved_reexecute_sp;
4631 
4632     // Remove the allocation from above the guards
4633     CallProjections callprojs;
4634     alloc-&gt;extract_projections(&amp;callprojs, true);
4635     InitializeNode* init = alloc-&gt;initialization();
4636     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
4637     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));
4638     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4639     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4640 
4641     // move the allocation here (after the guards)
4642     _gvn.hash_delete(alloc);
4643     alloc-&gt;set_req(TypeFunc::Control, control());
4644     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4645     Node *mem = reset_memory();
4646     set_all_memory(mem);
4647     alloc-&gt;set_req(TypeFunc::Memory, mem);
4648     set_control(init-&gt;proj_out(TypeFunc::Control));
4649     set_i_o(callprojs.fallthrough_ioproj);
4650 
4651     // Update memory as done in GraphKit::set_output_for_allocation()
4652     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4653     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4654     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4655       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4656     }
4657     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4658     int            elemidx  = C-&gt;get_alias_index(telemref);
4659     set_memory(init-&gt;proj_out(TypeFunc::Memory), Compile::AliasIdxRaw);
4660     set_memory(init-&gt;proj_out(TypeFunc::Memory), elemidx);
4661 
4662     Node* allocx = _gvn.transform(alloc);
4663     assert(allocx == alloc, "where has the allocation gone?");
4664     assert(dest-&gt;is_CheckCastPP(), "not an allocation result?");
4665 
4666     _gvn.hash_delete(dest);
4667     dest-&gt;set_req(0, control());
4668     Node* destx = _gvn.transform(dest);
4669     assert(destx == dest, "where has the allocation result gone?");
4670   }
4671 }
4672 
4673 
4674 //------------------------------inline_arraycopy-----------------------
4675 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4676 //                                                      Object dest, int destPos,
4677 //                                                      int length);
4678 bool LibraryCallKit::inline_arraycopy() {
4679   // Get the arguments.
4680   Node* src         = argument(0);  // type: oop
4681   Node* src_offset  = argument(1);  // type: int
4682   Node* dest        = argument(2);  // type: oop
4683   Node* dest_offset = argument(3);  // type: int
4684   Node* length      = argument(4);  // type: int
4685 
4686 
4687   // Check for allocation before we add nodes that would confuse
4688   // tightly_coupled_allocation()
4689   AllocateArrayNode* alloc = tightly_coupled_allocation(dest, NULL);
4690 
4691   int saved_reexecute_sp = -1;
4692   JVMState* saved_jvms = arraycopy_restore_alloc_state(alloc, saved_reexecute_sp);
4693   // See arraycopy_restore_alloc_state() comment
4694   // if alloc == NULL we don't have to worry about a tightly coupled allocation so we can emit all needed guards
4695   // if saved_jvms != NULL (then alloc != NULL) then we can handle guards and a tightly coupled allocation
4696   // if saved_jvms == NULL and alloc != NULL, we cant emit any guards
4697   bool can_emit_guards = (alloc == NULL || saved_jvms != NULL);
4698 
4699   // The following tests must be performed
4700   // (1) src and dest are arrays.
4701   // (2) src and dest arrays must have elements of the same BasicType
4702   // (3) src and dest must not be null.
4703   // (4) src_offset must not be negative.
4704   // (5) dest_offset must not be negative.
4705   // (6) length must not be negative.
4706   // (7) src_offset + length must not exceed length of src.
4707   // (8) dest_offset + length must not exceed length of dest.
4708   // (9) each element of an oop array must be assignable
4709 
4710   // (3) src and dest must not be null.
4711   // always do this here because we need the JVM state for uncommon traps
4712   Node* null_ctl = top();
4713   src  = saved_jvms != NULL ? null_check_oop(src, &amp;null_ctl, true, true) : null_check(src,  T_ARRAY);
4714   assert(null_ctl-&gt;is_top(), "no null control here");
4715   dest = null_check(dest, T_ARRAY);
4716 
4717   if (!can_emit_guards) {
4718     // if saved_jvms == NULL and alloc != NULL, we don't emit any
4719     // guards but the arraycopy node could still take advantage of a
4720     // tightly allocated allocation. tightly_coupled_allocation() is
4721     // called again to make sure it takes the null check above into
4722     // account: the null check is mandatory and if it caused an
4723     // uncommon trap to be emitted then the allocation can't be
4724     // considered tightly coupled in this context.
4725     alloc = tightly_coupled_allocation(dest, NULL);
4726   }
4727 
4728   bool validated = false;
4729 
4730   const Type* src_type  = _gvn.type(src);
4731   const Type* dest_type = _gvn.type(dest);
4732   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4733   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4734 
4735   // Do we have the type of src?
4736   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4737   // Do we have the type of dest?
4738   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4739   // Is the type for src from speculation?
4740   bool src_spec = false;
4741   // Is the type for dest from speculation?
4742   bool dest_spec = false;
4743 
4744   if ((!has_src || !has_dest) &amp;&amp; can_emit_guards) {
4745     // We don't have sufficient type information, let's see if
4746     // speculative types can help. We need to have types for both src
4747     // and dest so that it pays off.
4748 
4749     // Do we already have or could we have type information for src
4750     bool could_have_src = has_src;
4751     // Do we already have or could we have type information for dest
4752     bool could_have_dest = has_dest;
4753 
4754     ciKlass* src_k = NULL;
4755     if (!has_src) {
4756       src_k = src_type-&gt;speculative_type_not_null();
4757       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4758         could_have_src = true;
4759       }
4760     }
4761 
4762     ciKlass* dest_k = NULL;
4763     if (!has_dest) {
4764       dest_k = dest_type-&gt;speculative_type_not_null();
4765       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4766         could_have_dest = true;
4767       }
4768     }
4769 
4770     if (could_have_src &amp;&amp; could_have_dest) {
4771       // This is going to pay off so emit the required guards
4772       if (!has_src) {
4773         src = maybe_cast_profiled_obj(src, src_k, true);
4774         src_type  = _gvn.type(src);
4775         top_src  = src_type-&gt;isa_aryptr();
4776         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4777         src_spec = true;
4778       }
4779       if (!has_dest) {
4780         dest = maybe_cast_profiled_obj(dest, dest_k, true);
4781         dest_type  = _gvn.type(dest);
4782         top_dest  = dest_type-&gt;isa_aryptr();
4783         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4784         dest_spec = true;
4785       }
4786     }
4787   }
4788 
4789   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
4790     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4791     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4792     if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
4793     if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
4794 
4795     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
4796       // If both arrays are object arrays then having the exact types
4797       // for both will remove the need for a subtype check at runtime
4798       // before the call and may make it possible to pick a faster copy
4799       // routine (without a subtype check on every element)
4800       // Do we have the exact type of src?
4801       bool could_have_src = src_spec;
4802       // Do we have the exact type of dest?
4803       bool could_have_dest = dest_spec;
4804       ciKlass* src_k = top_src-&gt;klass();
4805       ciKlass* dest_k = top_dest-&gt;klass();
4806       if (!src_spec) {
4807         src_k = src_type-&gt;speculative_type_not_null();
4808         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4809           could_have_src = true;
4810         }
4811       }
4812       if (!dest_spec) {
4813         dest_k = dest_type-&gt;speculative_type_not_null();
4814         if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4815           could_have_dest = true;
4816         }
4817       }
4818       if (could_have_src &amp;&amp; could_have_dest) {
4819         // If we can have both exact types, emit the missing guards
4820         if (could_have_src &amp;&amp; !src_spec) {
4821           src = maybe_cast_profiled_obj(src, src_k, true);
4822         }
4823         if (could_have_dest &amp;&amp; !dest_spec) {
4824           dest = maybe_cast_profiled_obj(dest, dest_k, true);
4825         }
4826       }
4827     }
4828   }
4829 
4830   ciMethod* trap_method = method();
4831   int trap_bci = bci();
4832   if (saved_jvms != NULL) {
4833     trap_method = alloc-&gt;jvms()-&gt;method();
4834     trap_bci = alloc-&gt;jvms()-&gt;bci();
4835   }
4836 
4837   if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;&amp;
4838       can_emit_guards &amp;&amp;
4839       !src-&gt;is_top() &amp;&amp; !dest-&gt;is_top()) {
4840     // validate arguments: enables transformation the ArrayCopyNode
4841     validated = true;
4842 
4843     RegionNode* slow_region = new RegionNode(1);
4844     record_for_igvn(slow_region);
4845 
4846     // (1) src and dest are arrays.
4847     generate_non_array_guard(load_object_klass(src), slow_region);
4848     generate_non_array_guard(load_object_klass(dest), slow_region);
4849 
4850     // (2) src and dest arrays must have elements of the same BasicType
4851     // done at macro expansion or at Ideal transformation time
4852 
4853     // (4) src_offset must not be negative.
4854     generate_negative_guard(src_offset, slow_region);
4855 
4856     // (5) dest_offset must not be negative.
4857     generate_negative_guard(dest_offset, slow_region);
4858 
4859     // (7) src_offset + length must not exceed length of src.
4860     generate_limit_guard(src_offset, length,
4861                          load_array_length(src),
4862                          slow_region);
4863 
4864     // (8) dest_offset + length must not exceed length of dest.
4865     generate_limit_guard(dest_offset, length,
4866                          load_array_length(dest),
4867                          slow_region);
4868 
4869     // (9) each element of an oop array must be assignable
4870     Node* src_klass  = load_object_klass(src);
4871     Node* dest_klass = load_object_klass(dest);
4872     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
4873 
4874     if (not_subtype_ctrl != top()) {
4875       PreserveJVMState pjvms(this);
4876       set_control(not_subtype_ctrl);
4877       uncommon_trap(Deoptimization::Reason_intrinsic,
4878                     Deoptimization::Action_make_not_entrant);
4879       assert(stopped(), "Should be stopped");
4880     }
4881     {
4882       PreserveJVMState pjvms(this);
4883       set_control(_gvn.transform(slow_region));
4884       uncommon_trap(Deoptimization::Reason_intrinsic,
4885                     Deoptimization::Action_make_not_entrant);
4886       assert(stopped(), "Should be stopped");
4887     }
4888   }
4889 
4890   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp);
4891 
4892   if (stopped()) {
4893     return true;
4894   }
4895 
4896   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL,
4897                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
4898                                           // so the compiler has a chance to eliminate them: during macro expansion,
4899                                           // we have to set their control (CastPP nodes are eliminated).
4900                                           load_object_klass(src), load_object_klass(dest),
4901                                           load_array_length(src), load_array_length(dest));
4902 
4903   ac-&gt;set_arraycopy(validated);
4904 
4905   Node* n = _gvn.transform(ac);
4906   if (n == ac) {
4907     ac-&gt;connect_outputs(this);
4908   } else {
4909     assert(validated, "shouldn't transform if all arguments not validated");
4910     set_all_memory(n);
4911   }
4912 
4913   return true;
4914 }
4915 
4916 
4917 // Helper function which determines if an arraycopy immediately follows
4918 // an allocation, with no intervening tests or other escapes for the object.
4919 AllocateArrayNode*
4920 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
4921                                            RegionNode* slow_region) {
4922   if (stopped())             return NULL;  // no fast path
4923   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
4924 
4925   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
4926   if (alloc == NULL)  return NULL;
4927 
4928   Node* rawmem = memory(Compile::AliasIdxRaw);
4929   // Is the allocation's memory state untouched?
4930   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
4931     // Bail out if there have been raw-memory effects since the allocation.
4932     // (Example:  There might have been a call or safepoint.)
4933     return NULL;
4934   }
4935   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
4936   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
4937     return NULL;
4938   }
4939 
4940   // There must be no unexpected observers of this allocation.
4941   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
4942     Node* obs = ptr-&gt;fast_out(i);
4943     if (obs != this-&gt;map()) {
4944       return NULL;
4945     }
4946   }
4947 
4948   // This arraycopy must unconditionally follow the allocation of the ptr.
4949   Node* alloc_ctl = ptr-&gt;in(0);
4950   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
4951 
4952   Node* ctl = control();
4953   while (ctl != alloc_ctl) {
4954     // There may be guards which feed into the slow_region.
4955     // Any other control flow means that we might not get a chance
4956     // to finish initializing the allocated object.
4957     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
4958       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
4959       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
4960       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
4961       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
4962         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
4963         continue;
4964       }
4965       // One more try:  Various low-level checks bottom out in
4966       // uncommon traps.  If the debug-info of the trap omits
4967       // any reference to the allocation, as we've already
4968       // observed, then there can be no objection to the trap.
4969       bool found_trap = false;
4970       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
4971         Node* obs = not_ctl-&gt;fast_out(j);
4972         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
4973             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
4974           found_trap = true; break;
4975         }
4976       }
4977       if (found_trap) {
4978         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
4979         continue;
4980       }
4981     }
4982     return NULL;
4983   }
4984 
4985   // If we get this far, we have an allocation which immediately
4986   // precedes the arraycopy, and we can take over zeroing the new object.
4987   // The arraycopy will finish the initialization, and provide
4988   // a new control state to which we will anchor the destination pointer.
4989 
4990   return alloc;
4991 }
4992 
4993 //-------------inline_encodeISOArray-----------------------------------
4994 // encode char[] to byte[] in ISO_8859_1
4995 bool LibraryCallKit::inline_encodeISOArray() {
4996   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
4997   // no receiver since it is static method
4998   Node *src         = argument(0);
4999   Node *src_offset  = argument(1);
5000   Node *dst         = argument(2);
5001   Node *dst_offset  = argument(3);
5002   Node *length      = argument(4);
5003 
5004   const Type* src_type = src-&gt;Value(&amp;_gvn);
5005   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5006   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5007   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5008   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5009       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5010     // failed array check
5011     return false;
5012   }
5013 
5014   // Figure out the size and type of the elements we will be copying.
5015   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5016   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5017   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5018     return false;
5019   }
5020   Node* src_start = array_element_address(src, src_offset, src_elem);
5021   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5022   // 'src_start' points to src array + scaled offset
5023   // 'dst_start' points to dst array + scaled offset
5024 
5025   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5026   Node* enc = new EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5027   enc = _gvn.transform(enc);
5028   Node* res_mem = _gvn.transform(new SCMemProjNode(enc));
5029   set_memory(res_mem, mtype);
5030   set_result(enc);
5031   return true;
5032 }
5033 
5034 //-------------inline_multiplyToLen-----------------------------------
5035 bool LibraryCallKit::inline_multiplyToLen() {
5036   assert(UseMultiplyToLenIntrinsic, "not implemented on this platform");
5037 
5038   address stubAddr = StubRoutines::multiplyToLen();
5039   if (stubAddr == NULL) {
5040     return false; // Intrinsic's stub is not implemented on this platform
5041   }
5042   const char* stubName = "multiplyToLen";
5043 
5044   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5045 
5046   // no receiver because it is a static method
5047   Node* x    = argument(0);
5048   Node* xlen = argument(1);
5049   Node* y    = argument(2);
5050   Node* ylen = argument(3);
5051   Node* z    = argument(4);
5052 
5053   const Type* x_type = x-&gt;Value(&amp;_gvn);
5054   const Type* y_type = y-&gt;Value(&amp;_gvn);
5055   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5056   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5057   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5058       top_y == NULL || top_y-&gt;klass() == NULL) {
5059     // failed array check
5060     return false;
5061   }
5062 
5063   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5064   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5065   if (x_elem != T_INT || y_elem != T_INT) {
5066     return false;
5067   }
5068 
5069   // Set the original stack and the reexecute bit for the interpreter to reexecute
5070   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5071   // on the return from z array allocation in runtime.
5072   { PreserveReexecuteState preexecs(this);
5073     jvms()-&gt;set_should_reexecute(true);
5074 
5075     Node* x_start = array_element_address(x, intcon(0), x_elem);
5076     Node* y_start = array_element_address(y, intcon(0), y_elem);
5077     // 'x_start' points to x array + scaled xlen
5078     // 'y_start' points to y array + scaled ylen
5079 
5080     // Allocate the result array
5081     Node* zlen = _gvn.transform(new AddINode(xlen, ylen));
5082     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5083     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5084 
5085     IdealKit ideal(this);
5086 
5087 #define __ ideal.
5088      Node* one = __ ConI(1);
5089      Node* zero = __ ConI(0);
5090      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5091      __ set(need_alloc, zero);
5092      __ set(z_alloc, z);
5093      __ if_then(z, BoolTest::eq, null()); {
5094        __ increment (need_alloc, one);
5095      } __ else_(); {
5096        // Update graphKit memory and control from IdealKit.
5097        sync_kit(ideal);
5098        Node* zlen_arg = load_array_length(z);
5099        // Update IdealKit memory and control from graphKit.
5100        __ sync_kit(this);
5101        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5102          __ increment (need_alloc, one);
5103        } __ end_if();
5104      } __ end_if();
5105 
5106      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5107        // Update graphKit memory and control from IdealKit.
5108        sync_kit(ideal);
5109        Node * narr = new_array(klass_node, zlen, 1);
5110        // Update IdealKit memory and control from graphKit.
5111        __ sync_kit(this);
5112        __ set(z_alloc, narr);
5113      } __ end_if();
5114 
5115      sync_kit(ideal);
5116      z = __ value(z_alloc);
5117      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5118      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5119      // Final sync IdealKit and GraphKit.
5120      final_sync(ideal);
5121 #undef __
5122 
5123     Node* z_start = array_element_address(z, intcon(0), T_INT);
5124 
5125     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5126                                    OptoRuntime::multiplyToLen_Type(),
5127                                    stubAddr, stubName, TypePtr::BOTTOM,
5128                                    x_start, xlen, y_start, ylen, z_start, zlen);
5129   } // original reexecute is set back here
5130 
5131   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5132   set_result(z);
5133   return true;
5134 }
5135 
5136 //-------------inline_squareToLen------------------------------------
5137 bool LibraryCallKit::inline_squareToLen() {
5138   assert(UseSquareToLenIntrinsic, "not implementated on this platform");
5139 
5140   address stubAddr = StubRoutines::squareToLen();
5141   if (stubAddr == NULL) {
5142     return false; // Intrinsic's stub is not implemented on this platform
5143   }
5144   const char* stubName = "squareToLen";
5145 
5146   assert(callee()-&gt;signature()-&gt;size() == 4, "implSquareToLen has 4 parameters");
5147 
5148   Node* x    = argument(0);
5149   Node* len  = argument(1);
5150   Node* z    = argument(2);
5151   Node* zlen = argument(3);
5152 
5153   const Type* x_type = x-&gt;Value(&amp;_gvn);
5154   const Type* z_type = z-&gt;Value(&amp;_gvn);
5155   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5156   const TypeAryPtr* top_z = z_type-&gt;isa_aryptr();
5157   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5158       top_z  == NULL || top_z-&gt;klass()  == NULL) {
5159     // failed array check
5160     return false;
5161   }
5162 
5163   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5164   BasicType z_elem = z_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5165   if (x_elem != T_INT || z_elem != T_INT) {
5166     return false;
5167   }
5168 
5169 
5170   Node* x_start = array_element_address(x, intcon(0), x_elem);
5171   Node* z_start = array_element_address(z, intcon(0), z_elem);
5172 
5173   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5174                                   OptoRuntime::squareToLen_Type(),
5175                                   stubAddr, stubName, TypePtr::BOTTOM,
5176                                   x_start, len, z_start, zlen);
5177 
5178   set_result(z);
5179   return true;
5180 }
5181 
5182 //-------------inline_mulAdd------------------------------------------
5183 bool LibraryCallKit::inline_mulAdd() {
5184   assert(UseMulAddIntrinsic, "not implementated on this platform");
5185 
5186   address stubAddr = StubRoutines::mulAdd();
5187   if (stubAddr == NULL) {
5188     return false; // Intrinsic's stub is not implemented on this platform
5189   }
5190   const char* stubName = "mulAdd";
5191 
5192   assert(callee()-&gt;signature()-&gt;size() == 5, "mulAdd has 5 parameters");
5193 
5194   Node* out      = argument(0);
5195   Node* in       = argument(1);
5196   Node* offset   = argument(2);
5197   Node* len      = argument(3);
5198   Node* k        = argument(4);
5199 
5200   const Type* out_type = out-&gt;Value(&amp;_gvn);
5201   const Type* in_type = in-&gt;Value(&amp;_gvn);
5202   const TypeAryPtr* top_out = out_type-&gt;isa_aryptr();
5203   const TypeAryPtr* top_in = in_type-&gt;isa_aryptr();
5204   if (top_out  == NULL || top_out-&gt;klass()  == NULL ||
5205       top_in == NULL || top_in-&gt;klass() == NULL) {
5206     // failed array check
5207     return false;
5208   }
5209 
5210   BasicType out_elem = out_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5211   BasicType in_elem = in_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5212   if (out_elem != T_INT || in_elem != T_INT) {
5213     return false;
5214   }
5215 
5216   Node* outlen = load_array_length(out);
5217   Node* new_offset = _gvn.transform(new SubINode(outlen, offset));
5218   Node* out_start = array_element_address(out, intcon(0), out_elem);
5219   Node* in_start = array_element_address(in, intcon(0), in_elem);
5220 
5221   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5222                                   OptoRuntime::mulAdd_Type(),
5223                                   stubAddr, stubName, TypePtr::BOTTOM,
5224                                   out_start,in_start, new_offset, len, k);
5225   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5226   set_result(result);
5227   return true;
5228 }
5229 
5230 //-------------inline_montgomeryMultiply-----------------------------------
5231 bool LibraryCallKit::inline_montgomeryMultiply() {
5232   address stubAddr = StubRoutines::montgomeryMultiply();
5233   if (stubAddr == NULL) {
5234     return false; // Intrinsic's stub is not implemented on this platform
5235   }
5236 
5237   assert(UseMontgomeryMultiplyIntrinsic, "not implemented on this platform");
5238   const char* stubName = "montgomery_square";
5239 
5240   assert(callee()-&gt;signature()-&gt;size() == 7, "montgomeryMultiply has 7 parameters");
5241 
5242   Node* a    = argument(0);
5243   Node* b    = argument(1);
5244   Node* n    = argument(2);
5245   Node* len  = argument(3);
5246   Node* inv  = argument(4);
5247   Node* m    = argument(6);
5248 
5249   const Type* a_type = a-&gt;Value(&amp;_gvn);
5250   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5251   const Type* b_type = b-&gt;Value(&amp;_gvn);
5252   const TypeAryPtr* top_b = b_type-&gt;isa_aryptr();
5253   const Type* n_type = a-&gt;Value(&amp;_gvn);
5254   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5255   const Type* m_type = a-&gt;Value(&amp;_gvn);
5256   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5257   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5258       top_b == NULL || top_b-&gt;klass()  == NULL ||
5259       top_n == NULL || top_n-&gt;klass()  == NULL ||
5260       top_m == NULL || top_m-&gt;klass()  == NULL) {
5261     // failed array check
5262     return false;
5263   }
5264 
5265   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5266   BasicType b_elem = b_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5267   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5268   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5269   if (a_elem != T_INT || b_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5270     return false;
5271   }
5272 
5273   // Make the call
5274   {
5275     Node* a_start = array_element_address(a, intcon(0), a_elem);
5276     Node* b_start = array_element_address(b, intcon(0), b_elem);
5277     Node* n_start = array_element_address(n, intcon(0), n_elem);
5278     Node* m_start = array_element_address(m, intcon(0), m_elem);
5279 
5280     Node* call = make_runtime_call(RC_LEAF,
5281                                    OptoRuntime::montgomeryMultiply_Type(),
5282                                    stubAddr, stubName, TypePtr::BOTTOM,
5283                                    a_start, b_start, n_start, len, inv, top(),
5284                                    m_start);
5285     set_result(m);
5286   }
5287 
5288   return true;
5289 }
5290 
5291 bool LibraryCallKit::inline_montgomerySquare() {
5292   address stubAddr = StubRoutines::montgomerySquare();
5293   if (stubAddr == NULL) {
5294     return false; // Intrinsic's stub is not implemented on this platform
5295   }
5296 
5297   assert(UseMontgomerySquareIntrinsic, "not implemented on this platform");
5298   const char* stubName = "montgomery_square";
5299 
5300   assert(callee()-&gt;signature()-&gt;size() == 6, "montgomerySquare has 6 parameters");
5301 
5302   Node* a    = argument(0);
5303   Node* n    = argument(1);
5304   Node* len  = argument(2);
5305   Node* inv  = argument(3);
5306   Node* m    = argument(5);
5307 
5308   const Type* a_type = a-&gt;Value(&amp;_gvn);
5309   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5310   const Type* n_type = a-&gt;Value(&amp;_gvn);
5311   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5312   const Type* m_type = a-&gt;Value(&amp;_gvn);
5313   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5314   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5315       top_n == NULL || top_n-&gt;klass()  == NULL ||
5316       top_m == NULL || top_m-&gt;klass()  == NULL) {
5317     // failed array check
5318     return false;
5319   }
5320 
5321   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5322   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5323   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5324   if (a_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5325     return false;
5326   }
5327 
5328   // Make the call
5329   {
5330     Node* a_start = array_element_address(a, intcon(0), a_elem);
5331     Node* n_start = array_element_address(n, intcon(0), n_elem);
5332     Node* m_start = array_element_address(m, intcon(0), m_elem);
5333 
5334     Node* call = make_runtime_call(RC_LEAF,
5335                                    OptoRuntime::montgomerySquare_Type(),
5336                                    stubAddr, stubName, TypePtr::BOTTOM,
5337                                    a_start, n_start, len, inv, top(),
5338                                    m_start);
5339     set_result(m);
5340   }
5341 
5342   return true;
5343 }
5344 
5345 
5346 /**
5347  * Calculate CRC32 for byte.
5348  * int java.util.zip.CRC32.update(int crc, int b)
5349  */
5350 bool LibraryCallKit::inline_updateCRC32() {
5351   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5352   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5353   // no receiver since it is static method
5354   Node* crc  = argument(0); // type: int
5355   Node* b    = argument(1); // type: int
5356 
5357   /*
5358    *    int c = ~ crc;
5359    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5360    *    b = b ^ (c &gt;&gt;&gt; 8);
5361    *    crc = ~b;
5362    */
5363 
5364   Node* M1 = intcon(-1);
5365   crc = _gvn.transform(new XorINode(crc, M1));
5366   Node* result = _gvn.transform(new XorINode(crc, b));
5367   result = _gvn.transform(new AndINode(result, intcon(0xFF)));
5368 
5369   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5370   Node* offset = _gvn.transform(new LShiftINode(result, intcon(0x2)));
5371   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5372   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5373 
5374   crc = _gvn.transform(new URShiftINode(crc, intcon(8)));
5375   result = _gvn.transform(new XorINode(crc, result));
5376   result = _gvn.transform(new XorINode(result, M1));
5377   set_result(result);
5378   return true;
5379 }
5380 
5381 /**
5382  * Calculate CRC32 for byte[] array.
5383  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5384  */
5385 bool LibraryCallKit::inline_updateBytesCRC32() {
5386   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5387   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5388   // no receiver since it is static method
5389   Node* crc     = argument(0); // type: int
5390   Node* src     = argument(1); // type: oop
5391   Node* offset  = argument(2); // type: int
5392   Node* length  = argument(3); // type: int
5393 
5394   const Type* src_type = src-&gt;Value(&amp;_gvn);
5395   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5396   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5397     // failed array check
5398     return false;
5399   }
5400 
5401   // Figure out the size and type of the elements we will be copying.
5402   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5403   if (src_elem != T_BYTE) {
5404     return false;
5405   }
5406 
5407   // 'src_start' points to src array + scaled offset
5408   Node* src_start = array_element_address(src, offset, src_elem);
5409 
5410   // We assume that range check is done by caller.
5411   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5412 
5413   // Call the stub.
5414   address stubAddr = StubRoutines::updateBytesCRC32();
5415   const char *stubName = "updateBytesCRC32";
5416 
5417   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5418                                  stubAddr, stubName, TypePtr::BOTTOM,
5419                                  crc, src_start, length);
5420   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5421   set_result(result);
5422   return true;
5423 }
5424 
5425 /**
5426  * Calculate CRC32 for ByteBuffer.
5427  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5428  */
5429 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5430   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5431   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5432   // no receiver since it is static method
5433   Node* crc     = argument(0); // type: int
5434   Node* src     = argument(1); // type: long
5435   Node* offset  = argument(3); // type: int
5436   Node* length  = argument(4); // type: int
5437 
5438   src = ConvL2X(src);  // adjust Java long to machine word
5439   Node* base = _gvn.transform(new CastX2PNode(src));
5440   offset = ConvI2X(offset);
5441 
5442   // 'src_start' points to src array + scaled offset
5443   Node* src_start = basic_plus_adr(top(), base, offset);
5444 
5445   // Call the stub.
5446   address stubAddr = StubRoutines::updateBytesCRC32();
5447   const char *stubName = "updateBytesCRC32";
5448 
5449   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5450                                  stubAddr, stubName, TypePtr::BOTTOM,
5451                                  crc, src_start, length);
5452   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5453   set_result(result);
5454   return true;
5455 }
5456 
5457 //------------------------------get_table_from_crc32c_class-----------------------
5458 Node * LibraryCallKit::get_table_from_crc32c_class(ciInstanceKlass *crc32c_class) {
5459   Node* table = load_field_from_object(NULL, "byteTable", "[I", /*is_exact*/ false, /*is_static*/ true, crc32c_class);
5460   assert (table != NULL, "wrong version of java.util.zip.CRC32C");
5461 
5462   return table;
5463 }
5464 
5465 //------------------------------inline_updateBytesCRC32C-----------------------
5466 //
5467 // Calculate CRC32C for byte[] array.
5468 // int java.util.zip.CRC32C.updateBytes(int crc, byte[] buf, int off, int end)
5469 //
5470 bool LibraryCallKit::inline_updateBytesCRC32C() {
5471   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5472   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5473   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5474   // no receiver since it is a static method
5475   Node* crc     = argument(0); // type: int
5476   Node* src     = argument(1); // type: oop
5477   Node* offset  = argument(2); // type: int
5478   Node* end     = argument(3); // type: int
5479 
5480   Node* length = _gvn.transform(new SubINode(end, offset));
5481 
5482   const Type* src_type = src-&gt;Value(&amp;_gvn);
5483   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5484   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5485     // failed array check
5486     return false;
5487   }
5488 
5489   // Figure out the size and type of the elements we will be copying.
5490   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5491   if (src_elem != T_BYTE) {
5492     return false;
5493   }
5494 
5495   // 'src_start' points to src array + scaled offset
5496   Node* src_start = array_element_address(src, offset, src_elem);
5497 
5498   // static final int[] byteTable in class CRC32C
5499   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5500   Node* table_start = array_element_address(table, intcon(0), T_INT);
5501 
5502   // We assume that range check is done by caller.
5503   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5504 
5505   // Call the stub.
5506   address stubAddr = StubRoutines::updateBytesCRC32C();
5507   const char *stubName = "updateBytesCRC32C";
5508 
5509   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5510                                  stubAddr, stubName, TypePtr::BOTTOM,
5511                                  crc, src_start, length, table_start);
5512   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5513   set_result(result);
5514   return true;
5515 }
5516 
5517 //------------------------------inline_updateDirectByteBufferCRC32C-----------------------
5518 //
5519 // Calculate CRC32C for DirectByteBuffer.
5520 // int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)
5521 //
5522 bool LibraryCallKit::inline_updateDirectByteBufferCRC32C() {
5523   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5524   assert(callee()-&gt;signature()-&gt;size() == 5, "updateDirectByteBuffer has 4 parameters and one is long");
5525   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5526   // no receiver since it is a static method
5527   Node* crc     = argument(0); // type: int
5528   Node* src     = argument(1); // type: long
5529   Node* offset  = argument(3); // type: int
5530   Node* end     = argument(4); // type: int
5531 
5532   Node* length = _gvn.transform(new SubINode(end, offset));
5533 
5534   src = ConvL2X(src);  // adjust Java long to machine word
5535   Node* base = _gvn.transform(new CastX2PNode(src));
5536   offset = ConvI2X(offset);
5537 
5538   // 'src_start' points to src array + scaled offset
5539   Node* src_start = basic_plus_adr(top(), base, offset);
5540 
5541   // static final int[] byteTable in class CRC32C
5542   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5543   Node* table_start = array_element_address(table, intcon(0), T_INT);
5544 
5545   // Call the stub.
5546   address stubAddr = StubRoutines::updateBytesCRC32C();
5547   const char *stubName = "updateBytesCRC32C";
5548 
5549   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5550                                  stubAddr, stubName, TypePtr::BOTTOM,
5551                                  crc, src_start, length, table_start);
5552   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5553   set_result(result);
5554   return true;
5555 }
5556 
5557 //------------------------------inline_updateBytesAdler32----------------------
5558 //
5559 // Calculate Adler32 checksum for byte[] array.
5560 // int java.util.zip.Adler32.updateBytes(int crc, byte[] buf, int off, int len)
5561 //
5562 bool LibraryCallKit::inline_updateBytesAdler32() {
5563   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5564   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5565   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5566   // no receiver since it is static method
5567   Node* crc     = argument(0); // type: int
5568   Node* src     = argument(1); // type: oop
5569   Node* offset  = argument(2); // type: int
5570   Node* length  = argument(3); // type: int
5571 
5572   const Type* src_type = src-&gt;Value(&amp;_gvn);
5573   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5574   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5575     // failed array check
5576     return false;
5577   }
5578 
5579   // Figure out the size and type of the elements we will be copying.
5580   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5581   if (src_elem != T_BYTE) {
5582     return false;
5583   }
5584 
5585   // 'src_start' points to src array + scaled offset
5586   Node* src_start = array_element_address(src, offset, src_elem);
5587 
5588   // We assume that range check is done by caller.
5589   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5590 
5591   // Call the stub.
5592   address stubAddr = StubRoutines::updateBytesAdler32();
5593   const char *stubName = "updateBytesAdler32";
5594 
5595   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5596                                  stubAddr, stubName, TypePtr::BOTTOM,
5597                                  crc, src_start, length);
5598   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5599   set_result(result);
5600   return true;
5601 }
5602 
5603 //------------------------------inline_updateByteBufferAdler32---------------
5604 //
5605 // Calculate Adler32 checksum for DirectByteBuffer.
5606 // int java.util.zip.Adler32.updateByteBuffer(int crc, long buf, int off, int len)
5607 //
5608 bool LibraryCallKit::inline_updateByteBufferAdler32() {
5609   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5610   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5611   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5612   // no receiver since it is static method
5613   Node* crc     = argument(0); // type: int
5614   Node* src     = argument(1); // type: long
5615   Node* offset  = argument(3); // type: int
5616   Node* length  = argument(4); // type: int
5617 
5618   src = ConvL2X(src);  // adjust Java long to machine word
5619   Node* base = _gvn.transform(new CastX2PNode(src));
5620   offset = ConvI2X(offset);
5621 
5622   // 'src_start' points to src array + scaled offset
5623   Node* src_start = basic_plus_adr(top(), base, offset);
5624 
5625   // Call the stub.
5626   address stubAddr = StubRoutines::updateBytesAdler32();
5627   const char *stubName = "updateBytesAdler32";
5628 
5629   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5630                                  stubAddr, stubName, TypePtr::BOTTOM,
5631                                  crc, src_start, length);
5632 
5633   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5634   set_result(result);
5635   return true;
5636 }
5637 
5638 //----------------------------inline_reference_get----------------------------
5639 // public T java.lang.ref.Reference.get();
5640 bool LibraryCallKit::inline_reference_get() {
5641   const int referent_offset = java_lang_ref_Reference::referent_offset;
5642   guarantee(referent_offset &gt; 0, "should have already been set");
5643 
5644   // Get the argument:
5645   Node* reference_obj = null_check_receiver();
5646   if (stopped()) return true;
5647 
5648   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5649 
5650   ciInstanceKlass* klass = env()-&gt;Object_klass();
5651   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5652 
5653   Node* no_ctrl = NULL;
5654   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5655 
5656   // Use the pre-barrier to record the value in the referent field
5657   pre_barrier(false /* do_load */,
5658               control(),
5659               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5660               result /* pre_val */,
5661               T_OBJECT);
5662 
5663   // Add memory barrier to prevent commoning reads from this field
5664   // across safepoint since GC can change its value.
5665   insert_mem_bar(Op_MemBarCPUOrder);
5666 
5667   set_result(result);
5668   return true;
5669 }
5670 
5671 
5672 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5673                                               bool is_exact=true, bool is_static=false,
5674                                               ciInstanceKlass * fromKls=NULL) {
5675   if (fromKls == NULL) {
5676     const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5677     assert(tinst != NULL, "obj is null");
5678     assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5679     assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5680     fromKls = tinst-&gt;klass()-&gt;as_instance_klass();
5681   } else {
5682     assert(is_static, "only for static field access");
5683   }
5684   ciField* field = fromKls-&gt;get_field_by_name(ciSymbol::make(fieldName),
5685                                               ciSymbol::make(fieldTypeString),
5686                                               is_static);
5687 
5688   assert (field != NULL, "undefined field");
5689   if (field == NULL) return (Node *) NULL;
5690 
5691   if (is_static) {
5692     const TypeInstPtr* tip = TypeInstPtr::make(fromKls-&gt;java_mirror());
5693     fromObj = makecon(tip);
5694   }
5695 
5696   // Next code  copied from Parse::do_get_xxx():
5697 
5698   // Compute address and memory type.
5699   int offset  = field-&gt;offset_in_bytes();
5700   bool is_vol = field-&gt;is_volatile();
5701   ciType* field_klass = field-&gt;type();
5702   assert(field_klass-&gt;is_loaded(), "should be loaded");
5703   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5704   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5705   BasicType bt = field-&gt;layout_type();
5706 
5707   // Build the resultant type of the load
5708   const Type *type;
5709   if (bt == T_OBJECT) {
5710     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5711   } else {
5712     type = Type::get_const_basic_type(bt);
5713   }
5714 
5715   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
5716     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
5717   }
5718   // Build the load.
5719   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
5720   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, LoadNode::DependsOnlyOnTest, is_vol);
5721   // If reference is volatile, prevent following memory ops from
5722   // floating up past the volatile read.  Also prevents commoning
5723   // another volatile read.
5724   if (is_vol) {
5725     // Memory barrier includes bogus read of value to force load BEFORE membar
5726     insert_mem_bar(Op_MemBarAcquire, loadedField);
5727   }
5728   return loadedField;
5729 }
5730 
5731 
5732 //------------------------------inline_aescrypt_Block-----------------------
5733 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
5734   address stubAddr;
5735   const char *stubName;
5736   assert(UseAES, "need AES instruction support");
5737 
5738   switch(id) {
5739   case vmIntrinsics::_aescrypt_encryptBlock:
5740     stubAddr = StubRoutines::aescrypt_encryptBlock();
5741     stubName = "aescrypt_encryptBlock";
5742     break;
5743   case vmIntrinsics::_aescrypt_decryptBlock:
5744     stubAddr = StubRoutines::aescrypt_decryptBlock();
5745     stubName = "aescrypt_decryptBlock";
5746     break;
5747   }
5748   if (stubAddr == NULL) return false;
5749 
5750   Node* aescrypt_object = argument(0);
5751   Node* src             = argument(1);
5752   Node* src_offset      = argument(2);
5753   Node* dest            = argument(3);
5754   Node* dest_offset     = argument(4);
5755 
5756   // (1) src and dest are arrays.
5757   const Type* src_type = src-&gt;Value(&amp;_gvn);
5758   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5759   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5760   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5761   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5762 
5763   // for the quick and dirty code we will skip all the checks.
5764   // we are just trying to get the call to be generated.
5765   Node* src_start  = src;
5766   Node* dest_start = dest;
5767   if (src_offset != NULL || dest_offset != NULL) {
5768     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5769     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5770     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5771   }
5772 
5773   // now need to get the start of its expanded key array
5774   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5775   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5776   if (k_start == NULL) return false;
5777 
5778   if (Matcher::pass_original_key_for_aes()) {
5779     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5780     // compatibility issues between Java key expansion and SPARC crypto instructions
5781     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5782     if (original_k_start == NULL) return false;
5783 
5784     // Call the stub.
5785     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5786                       stubAddr, stubName, TypePtr::BOTTOM,
5787                       src_start, dest_start, k_start, original_k_start);
5788   } else {
5789     // Call the stub.
5790     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5791                       stubAddr, stubName, TypePtr::BOTTOM,
5792                       src_start, dest_start, k_start);
5793   }
5794 
5795   return true;
5796 }
5797 
5798 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
5799 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
5800   address stubAddr;
5801   const char *stubName;
5802 
5803   assert(UseAES, "need AES instruction support");
5804 
5805   switch(id) {
5806   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
5807     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
5808     stubName = "cipherBlockChaining_encryptAESCrypt";
5809     break;
5810   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
5811     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
5812     stubName = "cipherBlockChaining_decryptAESCrypt";
5813     break;
5814   }
5815   if (stubAddr == NULL) return false;
5816 
5817   Node* cipherBlockChaining_object = argument(0);
5818   Node* src                        = argument(1);
5819   Node* src_offset                 = argument(2);
5820   Node* len                        = argument(3);
5821   Node* dest                       = argument(4);
5822   Node* dest_offset                = argument(5);
5823 
5824   // (1) src and dest are arrays.
5825   const Type* src_type = src-&gt;Value(&amp;_gvn);
5826   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5827   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5828   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5829   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
5830           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5831 
5832   // checks are the responsibility of the caller
5833   Node* src_start  = src;
5834   Node* dest_start = dest;
5835   if (src_offset != NULL || dest_offset != NULL) {
5836     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5837     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5838     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5839   }
5840 
5841   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
5842   // (because of the predicated logic executed earlier).
5843   // so we cast it here safely.
5844   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5845 
5846   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5847   if (embeddedCipherObj == NULL) return false;
5848 
5849   // cast it to what we know it will be at runtime
5850   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
5851   assert(tinst != NULL, "CBC obj is null");
5852   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
5853   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5854   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
5855 
5856   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5857   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
5858   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
5859   Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
5860   aescrypt_object = _gvn.transform(aescrypt_object);
5861 
5862   // we need to get the start of the aescrypt_object's expanded key array
5863   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5864   if (k_start == NULL) return false;
5865 
5866   // similarly, get the start address of the r vector
5867   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
5868   if (objRvec == NULL) return false;
5869   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
5870 
5871   Node* cbcCrypt;
5872   if (Matcher::pass_original_key_for_aes()) {
5873     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5874     // compatibility issues between Java key expansion and SPARC crypto instructions
5875     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5876     if (original_k_start == NULL) return false;
5877 
5878     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
5879     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5880                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5881                                  stubAddr, stubName, TypePtr::BOTTOM,
5882                                  src_start, dest_start, k_start, r_start, len, original_k_start);
5883   } else {
5884     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
5885     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5886                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5887                                  stubAddr, stubName, TypePtr::BOTTOM,
5888                                  src_start, dest_start, k_start, r_start, len);
5889   }
5890 
5891   // return cipher length (int)
5892   Node* retvalue = _gvn.transform(new ProjNode(cbcCrypt, TypeFunc::Parms));
5893   set_result(retvalue);
5894   return true;
5895 }
5896 
5897 //------------------------------get_key_start_from_aescrypt_object-----------------------
5898 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
5899   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
5900   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5901   if (objAESCryptKey == NULL) return (Node *) NULL;
5902 
5903   // now have the array, need to get the start address of the K array
5904   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
5905   return k_start;
5906 }
5907 
5908 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
5909 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
5910   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
5911   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5912   if (objAESCryptKey == NULL) return (Node *) NULL;
5913 
5914   // now have the array, need to get the start address of the lastKey array
5915   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
5916   return original_k_start;
5917 }
5918 
5919 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
5920 // Return node representing slow path of predicate check.
5921 // the pseudo code we want to emulate with this predicate is:
5922 // for encryption:
5923 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
5924 // for decryption:
5925 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
5926 //    note cipher==plain is more conservative than the original java code but that's OK
5927 //
5928 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
5929   // The receiver was checked for NULL already.
5930   Node* objCBC = argument(0);
5931 
5932   // Load embeddedCipher field of CipherBlockChaining object.
5933   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5934 
5935   // get AESCrypt klass for instanceOf check
5936   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
5937   // will have same classloader as CipherBlockChaining object
5938   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
5939   assert(tinst != NULL, "CBCobj is null");
5940   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
5941 
5942   // we want to do an instanceof comparison against the AESCrypt class
5943   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5944   if (!klass_AESCrypt-&gt;is_loaded()) {
5945     // if AESCrypt is not even loaded, we never take the intrinsic fast path
5946     Node* ctrl = control();
5947     set_control(top()); // no regular fast path
5948     return ctrl;
5949   }
5950   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5951 
5952   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
5953   Node* cmp_instof  = _gvn.transform(new CmpINode(instof, intcon(1)));
5954   Node* bool_instof  = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
5955 
5956   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
5957 
5958   // for encryption, we are done
5959   if (!decrypting)
5960     return instof_false;  // even if it is NULL
5961 
5962   // for decryption, we need to add a further check to avoid
5963   // taking the intrinsic path when cipher and plain are the same
5964   // see the original java code for why.
5965   RegionNode* region = new RegionNode(3);
5966   region-&gt;init_req(1, instof_false);
5967   Node* src = argument(1);
5968   Node* dest = argument(4);
5969   Node* cmp_src_dest = _gvn.transform(new CmpPNode(src, dest));
5970   Node* bool_src_dest = _gvn.transform(new BoolNode(cmp_src_dest, BoolTest::eq));
5971   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
5972   region-&gt;init_req(2, src_dest_conjoint);
5973 
5974   record_for_igvn(region);
5975   return _gvn.transform(region);
5976 }
5977 
5978 //------------------------------inline_ghash_processBlocks
5979 bool LibraryCallKit::inline_ghash_processBlocks() {
5980   address stubAddr;
5981   const char *stubName;
5982   assert(UseGHASHIntrinsics, "need GHASH intrinsics support");
5983 
5984   stubAddr = StubRoutines::ghash_processBlocks();
5985   stubName = "ghash_processBlocks";
5986 
5987   Node* data           = argument(0);
5988   Node* offset         = argument(1);
5989   Node* len            = argument(2);
5990   Node* state          = argument(3);
5991   Node* subkeyH        = argument(4);
5992 
5993   Node* state_start  = array_element_address(state, intcon(0), T_LONG);
5994   assert(state_start, "state is NULL");
5995   Node* subkeyH_start  = array_element_address(subkeyH, intcon(0), T_LONG);
5996   assert(subkeyH_start, "subkeyH is NULL");
5997   Node* data_start  = array_element_address(data, offset, T_BYTE);
5998   assert(data_start, "data is NULL");
5999 
6000   Node* ghash = make_runtime_call(RC_LEAF|RC_NO_FP,
6001                                   OptoRuntime::ghash_processBlocks_Type(),
6002                                   stubAddr, stubName, TypePtr::BOTTOM,
6003                                   state_start, subkeyH_start, data_start, len);
6004   return true;
6005 }
6006 
6007 //------------------------------inline_sha_implCompress-----------------------
6008 //
6009 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6010 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6011 //
6012 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6013 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6014 //
6015 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6016 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6017 //
6018 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6019   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6020 
6021   Node* sha_obj = argument(0);
6022   Node* src     = argument(1); // type oop
6023   Node* ofs     = argument(2); // type int
6024 
6025   const Type* src_type = src-&gt;Value(&amp;_gvn);
6026   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6027   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6028     // failed array check
6029     return false;
6030   }
6031   // Figure out the size and type of the elements we will be copying.
6032   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6033   if (src_elem != T_BYTE) {
6034     return false;
6035   }
6036   // 'src_start' points to src array + offset
6037   Node* src_start = array_element_address(src, ofs, src_elem);
6038   Node* state = NULL;
6039   address stubAddr;
6040   const char *stubName;
6041 
6042   switch(id) {
6043   case vmIntrinsics::_sha_implCompress:
6044     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6045     state = get_state_from_sha_object(sha_obj);
6046     stubAddr = StubRoutines::sha1_implCompress();
6047     stubName = "sha1_implCompress";
6048     break;
6049   case vmIntrinsics::_sha2_implCompress:
6050     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6051     state = get_state_from_sha_object(sha_obj);
6052     stubAddr = StubRoutines::sha256_implCompress();
6053     stubName = "sha256_implCompress";
6054     break;
6055   case vmIntrinsics::_sha5_implCompress:
6056     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6057     state = get_state_from_sha5_object(sha_obj);
6058     stubAddr = StubRoutines::sha512_implCompress();
6059     stubName = "sha512_implCompress";
6060     break;
6061   default:
6062     fatal_unexpected_iid(id);
6063     return false;
6064   }
6065   if (state == NULL) return false;
6066 
6067   // Call the stub.
6068   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6069                                  stubAddr, stubName, TypePtr::BOTTOM,
6070                                  src_start, state);
6071 
6072   return true;
6073 }
6074 
6075 //------------------------------inline_digestBase_implCompressMB-----------------------
6076 //
6077 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6078 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6079 //
6080 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6081   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6082          "need SHA1/SHA256/SHA512 instruction support");
6083   assert((uint)predicate &lt; 3, "sanity");
6084   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6085 
6086   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6087   Node* src            = argument(1); // byte[] array
6088   Node* ofs            = argument(2); // type int
6089   Node* limit          = argument(3); // type int
6090 
6091   const Type* src_type = src-&gt;Value(&amp;_gvn);
6092   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6093   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6094     // failed array check
6095     return false;
6096   }
6097   // Figure out the size and type of the elements we will be copying.
6098   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6099   if (src_elem != T_BYTE) {
6100     return false;
6101   }
6102   // 'src_start' points to src array + offset
6103   Node* src_start = array_element_address(src, ofs, src_elem);
6104 
6105   const char* klass_SHA_name = NULL;
6106   const char* stub_name = NULL;
6107   address     stub_addr = NULL;
6108   bool        long_state = false;
6109 
6110   switch (predicate) {
6111   case 0:
6112     if (UseSHA1Intrinsics) {
6113       klass_SHA_name = "sun/security/provider/SHA";
6114       stub_name = "sha1_implCompressMB";
6115       stub_addr = StubRoutines::sha1_implCompressMB();
6116     }
6117     break;
6118   case 1:
6119     if (UseSHA256Intrinsics) {
6120       klass_SHA_name = "sun/security/provider/SHA2";
6121       stub_name = "sha256_implCompressMB";
6122       stub_addr = StubRoutines::sha256_implCompressMB();
6123     }
6124     break;
6125   case 2:
6126     if (UseSHA512Intrinsics) {
6127       klass_SHA_name = "sun/security/provider/SHA5";
6128       stub_name = "sha512_implCompressMB";
6129       stub_addr = StubRoutines::sha512_implCompressMB();
6130       long_state = true;
6131     }
6132     break;
6133   default:
6134     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6135   }
6136   if (klass_SHA_name != NULL) {
6137     // get DigestBase klass to lookup for SHA klass
6138     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6139     assert(tinst != NULL, "digestBase_obj is not instance???");
6140     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6141 
6142     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6143     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6144     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6145     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6146   }
6147   return false;
6148 }
6149 //------------------------------inline_sha_implCompressMB-----------------------
6150 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6151                                                bool long_state, address stubAddr, const char *stubName,
6152                                                Node* src_start, Node* ofs, Node* limit) {
6153   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6154   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6155   Node* sha_obj = new CheckCastPPNode(control(), digestBase_obj, xtype);
6156   sha_obj = _gvn.transform(sha_obj);
6157 
6158   Node* state;
6159   if (long_state) {
6160     state = get_state_from_sha5_object(sha_obj);
6161   } else {
6162     state = get_state_from_sha_object(sha_obj);
6163   }
6164   if (state == NULL) return false;
6165 
6166   // Call the stub.
6167   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6168                                  OptoRuntime::digestBase_implCompressMB_Type(),
6169                                  stubAddr, stubName, TypePtr::BOTTOM,
6170                                  src_start, state, ofs, limit);
6171   // return ofs (int)
6172   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
6173   set_result(result);
6174 
6175   return true;
6176 }
6177 
6178 //------------------------------get_state_from_sha_object-----------------------
6179 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6180   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6181   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6182   if (sha_state == NULL) return (Node *) NULL;
6183 
6184   // now have the array, need to get the start address of the state array
6185   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6186   return state;
6187 }
6188 
6189 //------------------------------get_state_from_sha5_object-----------------------
6190 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6191   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6192   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6193   if (sha_state == NULL) return (Node *) NULL;
6194 
6195   // now have the array, need to get the start address of the state array
6196   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6197   return state;
6198 }
6199 
6200 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6201 // Return node representing slow path of predicate check.
6202 // the pseudo code we want to emulate with this predicate is:
6203 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6204 //
6205 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6206   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6207          "need SHA1/SHA256/SHA512 instruction support");
6208   assert((uint)predicate &lt; 3, "sanity");
6209 
6210   // The receiver was checked for NULL already.
6211   Node* digestBaseObj = argument(0);
6212 
6213   // get DigestBase klass for instanceOf check
6214   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6215   assert(tinst != NULL, "digestBaseObj is null");
6216   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6217 
6218   const char* klass_SHA_name = NULL;
6219   switch (predicate) {
6220   case 0:
6221     if (UseSHA1Intrinsics) {
6222       // we want to do an instanceof comparison against the SHA class
6223       klass_SHA_name = "sun/security/provider/SHA";
6224     }
6225     break;
6226   case 1:
6227     if (UseSHA256Intrinsics) {
6228       // we want to do an instanceof comparison against the SHA2 class
6229       klass_SHA_name = "sun/security/provider/SHA2";
6230     }
6231     break;
6232   case 2:
6233     if (UseSHA512Intrinsics) {
6234       // we want to do an instanceof comparison against the SHA5 class
6235       klass_SHA_name = "sun/security/provider/SHA5";
6236     }
6237     break;
6238   default:
6239     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6240   }
6241 
6242   ciKlass* klass_SHA = NULL;
6243   if (klass_SHA_name != NULL) {
6244     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6245   }
6246   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6247     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6248     Node* ctrl = control();
6249     set_control(top()); // no intrinsic path
6250     return ctrl;
6251   }
6252   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6253 
6254   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6255   Node* cmp_instof = _gvn.transform(new CmpINode(instofSHA, intcon(1)));
6256   Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6257   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6258 
6259   return instof_false;  // even if it is NULL
6260 }
6261 
6262 bool LibraryCallKit::inline_profileBoolean() {
6263   Node* counts = argument(1);
6264   const TypeAryPtr* ary = NULL;
6265   ciArray* aobj = NULL;
6266   if (counts-&gt;is_Con()
6267       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL
6268       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL
6269       &amp;&amp; (aobj-&gt;length() == 2)) {
6270     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
6271     jint false_cnt = aobj-&gt;element_value(0).as_int();
6272     jint  true_cnt = aobj-&gt;element_value(1).as_int();
6273 
6274     if (C-&gt;log() != NULL) {
6275       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",
6276                      false_cnt, true_cnt);
6277     }
6278 
6279     if (false_cnt + true_cnt == 0) {
6280       // According to profile, never executed.
6281       uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6282                           Deoptimization::Action_reinterpret);
6283       return true;
6284     }
6285 
6286     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)
6287     // is a number of each value occurrences.
6288     Node* result = argument(0);
6289     if (false_cnt == 0 || true_cnt == 0) {
6290       // According to profile, one value has been never seen.
6291       int expected_val = (false_cnt == 0) ? 1 : 0;
6292 
6293       Node* cmp  = _gvn.transform(new CmpINode(result, intcon(expected_val)));
6294       Node* test = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
6295 
6296       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
6297       Node* fast_path = _gvn.transform(new IfTrueNode(check));
6298       Node* slow_path = _gvn.transform(new IfFalseNode(check));
6299 
6300       { // Slow path: uncommon trap for never seen value and then reexecute
6301         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
6302         // the value has been seen at least once.
6303         PreserveJVMState pjvms(this);
6304         PreserveReexecuteState preexecs(this);
6305         jvms()-&gt;set_should_reexecute(true);
6306 
6307         set_control(slow_path);
6308         set_i_o(i_o());
6309 
6310         uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6311                             Deoptimization::Action_reinterpret);
6312       }
6313       // The guard for never seen value enables sharpening of the result and
6314       // returning a constant. It allows to eliminate branches on the same value
6315       // later on.
6316       set_control(fast_path);
6317       result = intcon(expected_val);
6318     }
6319     // Stop profiling.
6320     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
6321     // By replacing method body with profile data (represented as ProfileBooleanNode
6322     // on IR level) we effectively disable profiling.
6323     // It enables full speed execution once optimized code is generated.
6324     Node* profile = _gvn.transform(new ProfileBooleanNode(result, false_cnt, true_cnt));
6325     C-&gt;record_for_igvn(profile);
6326     set_result(profile);
6327     return true;
6328   } else {
6329     // Continue profiling.
6330     // Profile data isn't available at the moment. So, execute method's bytecode version.
6331     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
6332     // is compiled and counters aren't available since corresponding MethodHandle
6333     // isn't a compile-time constant.
6334     return false;
6335   }
6336 }
6337 
6338 bool LibraryCallKit::inline_isCompileConstant() {
6339   Node* n = argument(0);
6340   set_result(n-&gt;is_Con() ? intcon(1) : intcon(0));
6341   return true;
6342 }
<a name="5" id="anc5"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="5" type="hidden" /></form></body></html>
