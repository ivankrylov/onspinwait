<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/opto/memnode.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "compiler/compileLog.hpp"
  28 #include "memory/allocation.inline.hpp"
  29 #include "oops/objArrayKlass.hpp"
  30 #include "opto/addnode.hpp"
  31 #include "opto/arraycopynode.hpp"
  32 #include "opto/cfgnode.hpp"
  33 #include "opto/compile.hpp"
  34 #include "opto/connode.hpp"
  35 #include "opto/convertnode.hpp"
  36 #include "opto/loopnode.hpp"
  37 #include "opto/machnode.hpp"
  38 #include "opto/matcher.hpp"
  39 #include "opto/memnode.hpp"
  40 #include "opto/mulnode.hpp"
  41 #include "opto/narrowptrnode.hpp"
  42 #include "opto/phaseX.hpp"
  43 #include "opto/regmask.hpp"
  44 #include "utilities/copy.hpp"
  45 
  46 // Portions of code courtesy of Clifford Click
  47 
  48 // Optimization - Graph Style
  49 
  50 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  51 
  52 //=============================================================================
  53 uint MemNode::size_of() const { return sizeof(*this); }
  54 
  55 const TypePtr *MemNode::adr_type() const {
  56   Node* adr = in(Address);
  57   if (adr == NULL)  return NULL; // node is dead
  58   const TypePtr* cross_check = NULL;
  59   DEBUG_ONLY(cross_check = _adr_type);
  60   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  61 }
  62 
  63 #ifndef PRODUCT
  64 void MemNode::dump_spec(outputStream *st) const {
  65   if (in(Address) == NULL)  return; // node is dead
  66 #ifndef ASSERT
  67   // fake the missing field
  68   const TypePtr* _adr_type = NULL;
  69   if (in(Address) != NULL)
  70     _adr_type = in(Address)-&gt;bottom_type()-&gt;isa_ptr();
  71 #endif
  72   dump_adr_type(this, _adr_type, st);
  73 
  74   Compile* C = Compile::current();
  75   if( C-&gt;alias_type(_adr_type)-&gt;is_volatile() )
  76     st-&gt;print(" Volatile!");
  77 }
  78 
  79 void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {
  80   st-&gt;print(" @");
  81   if (adr_type == NULL) {
  82     st-&gt;print("NULL");
  83   } else {
  84     adr_type-&gt;dump_on(st);
  85     Compile* C = Compile::current();
  86     Compile::AliasType* atp = NULL;
  87     if (C-&gt;have_alias_type(adr_type))  atp = C-&gt;alias_type(adr_type);
  88     if (atp == NULL)
  89       st-&gt;print(", idx=?\?;");
  90     else if (atp-&gt;index() == Compile::AliasIdxBot)
  91       st-&gt;print(", idx=Bot;");
  92     else if (atp-&gt;index() == Compile::AliasIdxTop)
  93       st-&gt;print(", idx=Top;");
  94     else if (atp-&gt;index() == Compile::AliasIdxRaw)
  95       st-&gt;print(", idx=Raw;");
  96     else {
  97       ciField* field = atp-&gt;field();
  98       if (field) {
  99         st-&gt;print(", name=");
 100         field-&gt;print_name_on(st);
 101       }
 102       st-&gt;print(", idx=%d;", atp-&gt;index());
 103     }
 104   }
 105 }
 106 
 107 extern void print_alias_types();
 108 
 109 #endif
 110 
 111 static bool membar_for_arraycopy_helper(const TypeOopPtr *t_oop, Node* n, PhaseTransform *phase) {
 112   if (n-&gt;is_Proj()) {
 113     n = n-&gt;in(0);
 114     if (n-&gt;is_Call() &amp;&amp; n-&gt;as_Call()-&gt;may_modify(t_oop, phase)) {
 115       return true;
 116     }
 117   }
 118   return false;
 119 }
 120 
 121 static bool membar_for_arraycopy(const TypeOopPtr *t_oop, MemBarNode* mb, PhaseTransform *phase) {
 122   Node* mem = mb-&gt;in(TypeFunc::Memory);
 123 
 124   if (mem-&gt;is_MergeMem()) {
 125     Node* n = mem-&gt;as_MergeMem()-&gt;memory_at(Compile::AliasIdxRaw);
 126     if (membar_for_arraycopy_helper(t_oop, n, phase)) {
 127       return true;
 128     } else if (n-&gt;is_Phi()) {
 129       for (uint i = 1; i &lt; n-&gt;req(); i++) {
 130         if (n-&gt;in(i) != NULL) {
 131           if (membar_for_arraycopy_helper(t_oop, n-&gt;in(i), phase)) {
 132             return true;
 133           }
 134         }
 135       }
 136     }
 137   }
 138 
 139   return false;
 140 }
 141 
 142 Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {
 143   assert((t_oop != NULL), "sanity");
 144   bool is_instance = t_oop-&gt;is_known_instance_field();
 145   bool is_boxed_value_load = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp;
 146                              (load != NULL) &amp;&amp; load-&gt;is_Load() &amp;&amp;
 147                              (phase-&gt;is_IterGVN() != NULL);
 148   if (!(is_instance || is_boxed_value_load))
 149     return mchain;  // don't try to optimize non-instance types
 150   uint instance_id = t_oop-&gt;instance_id();
 151   Node *start_mem = phase-&gt;C-&gt;start()-&gt;proj_out(TypeFunc::Memory);
 152   Node *prev = NULL;
 153   Node *result = mchain;
 154   while (prev != result) {
 155     prev = result;
 156     if (result == start_mem)
 157       break;  // hit one of our sentinels
 158     // skip over a call which does not affect this memory slice
 159     if (result-&gt;is_Proj() &amp;&amp; result-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
 160       Node *proj_in = result-&gt;in(0);
 161       if (proj_in-&gt;is_Allocate() &amp;&amp; proj_in-&gt;_idx == instance_id) {
 162         break;  // hit one of our sentinels
 163       } else if (proj_in-&gt;is_Call()) {
 164         // ArrayCopyNodes processed here as well
 165         CallNode *call = proj_in-&gt;as_Call();
 166         if (!call-&gt;may_modify(t_oop, phase)) { // returns false for instances
 167           result = call-&gt;in(TypeFunc::Memory);
 168         }
 169       } else if (proj_in-&gt;is_Initialize()) {
 170         AllocateNode* alloc = proj_in-&gt;as_Initialize()-&gt;allocation();
 171         // Stop if this is the initialization for the object instance which
 172         // contains this memory slice, otherwise skip over it.
 173         if ((alloc == NULL) || (alloc-&gt;_idx == instance_id)) {
 174           break;
 175         }
 176         if (is_instance) {
 177           result = proj_in-&gt;in(TypeFunc::Memory);
 178         } else if (is_boxed_value_load) {
 179           Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
 180           const TypeKlassPtr* tklass = phase-&gt;type(klass)-&gt;is_klassptr();
 181           if (tklass-&gt;klass_is_exact() &amp;&amp; !tklass-&gt;klass()-&gt;equals(t_oop-&gt;klass())) {
 182             result = proj_in-&gt;in(TypeFunc::Memory); // not related allocation
 183           }
 184         }
 185       } else if (proj_in-&gt;is_MemBar()) {
 186         if (membar_for_arraycopy(t_oop, proj_in-&gt;as_MemBar(), phase)) {
 187           break;
 188         }
 189         result = proj_in-&gt;in(TypeFunc::Memory);
 190       } else {
 191         assert(false, "unexpected projection");
 192       }
 193     } else if (result-&gt;is_ClearArray()) {
 194       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 195         // Can not bypass initialization of the instance
 196         // we are looking for.
 197         break;
 198       }
 199       // Otherwise skip it (the call updated 'result' value).
 200     } else if (result-&gt;is_MergeMem()) {
 201       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 202     }
 203   }
 204   return result;
 205 }
 206 
 207 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 208   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 209   if (t_oop == NULL)
 210     return mchain;  // don't try to optimize non-oop types
 211   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 212   bool is_instance = t_oop-&gt;is_known_instance_field();
 213   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 214   if (is_instance &amp;&amp; igvn != NULL  &amp;&amp; result-&gt;is_Phi()) {
 215     PhiNode *mphi = result-&gt;as_Phi();
 216     assert(mphi-&gt;bottom_type() == Type::MEMORY, "memory phi required");
 217     const TypePtr *t = mphi-&gt;adr_type();
 218     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 219         t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 220         t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 221          -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 222          -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop) {
 223       // clone the Phi with our address type
 224       result = mphi-&gt;split_out_instance(t_adr, igvn);
 225     } else {
 226       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), "correct memory chain");
 227     }
 228   }
 229   return result;
 230 }
 231 
 232 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 233   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 234   Node *mem = mmem;
 235 #ifdef ASSERT
 236   {
 237     // Check that current type is consistent with the alias index used during graph construction
 238     assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not be a bad alias_idx");
 239     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 240                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 241     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 242     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
 243                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;
 244         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 245         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 246           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 247           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 248       // don't assert if it is dead code.
 249       consistent = true;
 250     }
 251     if( !consistent ) {
 252       st-&gt;print("alias_idx==%d, adr_check==", alias_idx);
 253       if( adr_check == NULL ) {
 254         st-&gt;print("NULL");
 255       } else {
 256         adr_check-&gt;dump();
 257       }
 258       st-&gt;cr();
 259       print_alias_types();
 260       assert(consistent, "adr_check must match alias idx");
 261     }
 262   }
 263 #endif
 264   // TypeOopPtr::NOTNULL+any is an OOP with unknown offset - generally
 265   // means an array I have not precisely typed yet.  Do not do any
 266   // alias stuff with it any time soon.
 267   const TypeOopPtr *toop = tp-&gt;isa_oopptr();
 268   if( tp-&gt;base() != Type::AnyPtr &amp;&amp;
 269       !(toop &amp;&amp;
 270         toop-&gt;klass() != NULL &amp;&amp;
 271         toop-&gt;klass()-&gt;is_java_lang_Object() &amp;&amp;
 272         toop-&gt;offset() == Type::OffsetBot) ) {
 273     // compress paths and change unreachable cycles to TOP
 274     // If not, we can update the input infinitely along a MergeMem cycle
 275     // Equivalent code in PhiNode::Ideal
 276     Node* m  = phase-&gt;transform(mmem);
 277     // If transformed to a MergeMem, get the desired slice
 278     // Otherwise the returned node represents memory for every slice
 279     mem = (m-&gt;is_MergeMem())? m-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : m;
 280     // Update input if it is progress over what we have now
 281   }
 282   return mem;
 283 }
 284 
 285 //--------------------------Ideal_common---------------------------------------
 286 // Look for degenerate control and memory inputs.  Bypass MergeMem inputs.
 287 // Unhook non-raw memories from complete (macro-expanded) initializations.
 288 Node *MemNode::Ideal_common(PhaseGVN *phase, bool can_reshape) {
 289   // If our control input is a dead region, kill all below the region
 290   Node *ctl = in(MemNode::Control);
 291   if (ctl &amp;&amp; remove_dead_region(phase, can_reshape))
 292     return this;
 293   ctl = in(MemNode::Control);
 294   // Don't bother trying to transform a dead node
 295   if (ctl &amp;&amp; ctl-&gt;is_top())  return NodeSentinel;
 296 
 297   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 298   // Wait if control on the worklist.
 299   if (ctl &amp;&amp; can_reshape &amp;&amp; igvn != NULL) {
 300     Node* bol = NULL;
 301     Node* cmp = NULL;
 302     if (ctl-&gt;in(0)-&gt;is_If()) {
 303       assert(ctl-&gt;is_IfTrue() || ctl-&gt;is_IfFalse(), "sanity");
 304       bol = ctl-&gt;in(0)-&gt;in(1);
 305       if (bol-&gt;is_Bool())
 306         cmp = ctl-&gt;in(0)-&gt;in(1)-&gt;in(1);
 307     }
 308     if (igvn-&gt;_worklist.member(ctl) ||
 309         (bol != NULL &amp;&amp; igvn-&gt;_worklist.member(bol)) ||
 310         (cmp != NULL &amp;&amp; igvn-&gt;_worklist.member(cmp)) ) {
 311       // This control path may be dead.
 312       // Delay this memory node transformation until the control is processed.
 313       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 314       return NodeSentinel; // caller will return NULL
 315     }
 316   }
 317   // Ignore if memory is dead, or self-loop
 318   Node *mem = in(MemNode::Memory);
 319   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 320   assert(mem != this, "dead loop in MemNode::Ideal");
 321 
 322   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 323     // This memory slice may be dead.
 324     // Delay this mem node transformation until the memory is processed.
 325     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 326     return NodeSentinel; // caller will return NULL
 327   }
 328 
 329   Node *address = in(MemNode::Address);
 330   const Type *t_adr = phase-&gt;type(address);
 331   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 332 
 333   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 334       (igvn-&gt;_worklist.member(address) ||
 335        igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; (t_adr != adr_type())) ) {
 336     // The address's base and type may change when the address is processed.
 337     // Delay this mem node transformation until the address is processed.
 338     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 339     return NodeSentinel; // caller will return NULL
 340   }
 341 
 342   // Do NOT remove or optimize the next lines: ensure a new alias index
 343   // is allocated for an oop pointer type before Escape Analysis.
 344   // Note: C++ will not remove it since the call has side effect.
 345   if (t_adr-&gt;isa_oopptr()) {
 346     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 347   }
 348 
 349   Node* base = NULL;
 350   if (address-&gt;is_AddP()) {
 351     base = address-&gt;in(AddPNode::Base);
 352   }
 353   if (base != NULL &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR) &amp;&amp;
 354       !t_adr-&gt;isa_rawptr()) {
 355     // Note: raw address has TOP base and top-&gt;higher_equal(TypePtr::NULL_PTR) is true.
 356     // Skip this node optimization if its address has TOP base.
 357     return NodeSentinel; // caller will return NULL
 358   }
 359 
 360   // Avoid independent memory operations
 361   Node* old_mem = mem;
 362 
 363   // The code which unhooks non-raw memories from complete (macro-expanded)
 364   // initializations was removed. After macro-expansion all stores catched
 365   // by Initialize node became raw stores and there is no information
 366   // which memory slices they modify. So it is unsafe to move any memory
 367   // operation above these stores. Also in most cases hooked non-raw memories
 368   // were already unhooked by using information from detect_ptr_independence()
 369   // and find_previous_store().
 370 
 371   if (mem-&gt;is_MergeMem()) {
 372     MergeMemNode* mmem = mem-&gt;as_MergeMem();
 373     const TypePtr *tp = t_adr-&gt;is_ptr();
 374 
 375     mem = step_through_mergemem(phase, mmem, tp, adr_type(), tty);
 376   }
 377 
 378   if (mem != old_mem) {
 379     set_req(MemNode::Memory, mem);
 380     if (can_reshape &amp;&amp; old_mem-&gt;outcnt() == 0) {
 381         igvn-&gt;_worklist.push(old_mem);
 382     }
 383     if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel;
 384     return this;
 385   }
 386 
 387   // let the subclass continue analyzing...
 388   return NULL;
 389 }
 390 
 391 // Helper function for proving some simple control dominations.
 392 // Attempt to prove that all control inputs of 'dom' dominate 'sub'.
 393 // Already assumes that 'dom' is available at 'sub', and that 'sub'
 394 // is not a constant (dominated by the method's StartNode).
 395 // Used by MemNode::find_previous_store to prove that the
 396 // control input of a memory operation predates (dominates)
 397 // an allocation it wants to look past.
 398 bool MemNode::all_controls_dominate(Node* dom, Node* sub) {
 399   if (dom == NULL || dom-&gt;is_top() || sub == NULL || sub-&gt;is_top())
 400     return false; // Conservative answer for dead code
 401 
 402   // Check 'dom'. Skip Proj and CatchProj nodes.
 403   dom = dom-&gt;find_exact_control(dom);
 404   if (dom == NULL || dom-&gt;is_top())
 405     return false; // Conservative answer for dead code
 406 
 407   if (dom == sub) {
 408     // For the case when, for example, 'sub' is Initialize and the original
 409     // 'dom' is Proj node of the 'sub'.
 410     return false;
 411   }
 412 
 413   if (dom-&gt;is_Con() || dom-&gt;is_Start() || dom-&gt;is_Root() || dom == sub)
 414     return true;
 415 
 416   // 'dom' dominates 'sub' if its control edge and control edges
 417   // of all its inputs dominate or equal to sub's control edge.
 418 
 419   // Currently 'sub' is either Allocate, Initialize or Start nodes.
 420   // Or Region for the check in LoadNode::Ideal();
 421   // 'sub' should have sub-&gt;in(0) != NULL.
 422   assert(sub-&gt;is_Allocate() || sub-&gt;is_Initialize() || sub-&gt;is_Start() ||
 423          sub-&gt;is_Region() || sub-&gt;is_Call(), "expecting only these nodes");
 424 
 425   // Get control edge of 'sub'.
 426   Node* orig_sub = sub;
 427   sub = sub-&gt;find_exact_control(sub-&gt;in(0));
 428   if (sub == NULL || sub-&gt;is_top())
 429     return false; // Conservative answer for dead code
 430 
 431   assert(sub-&gt;is_CFG(), "expecting control");
 432 
 433   if (sub == dom)
 434     return true;
 435 
 436   if (sub-&gt;is_Start() || sub-&gt;is_Root())
 437     return false;
 438 
 439   {
 440     // Check all control edges of 'dom'.
 441 
 442     ResourceMark rm;
 443     Arena* arena = Thread::current()-&gt;resource_area();
 444     Node_List nlist(arena);
 445     Unique_Node_List dom_list(arena);
 446 
 447     dom_list.push(dom);
 448     bool only_dominating_controls = false;
 449 
 450     for (uint next = 0; next &lt; dom_list.size(); next++) {
 451       Node* n = dom_list.at(next);
 452       if (n == orig_sub)
 453         return false; // One of dom's inputs dominated by sub.
 454       if (!n-&gt;is_CFG() &amp;&amp; n-&gt;pinned()) {
 455         // Check only own control edge for pinned non-control nodes.
 456         n = n-&gt;find_exact_control(n-&gt;in(0));
 457         if (n == NULL || n-&gt;is_top())
 458           return false; // Conservative answer for dead code
 459         assert(n-&gt;is_CFG(), "expecting control");
 460         dom_list.push(n);
 461       } else if (n-&gt;is_Con() || n-&gt;is_Start() || n-&gt;is_Root()) {
 462         only_dominating_controls = true;
 463       } else if (n-&gt;is_CFG()) {
 464         if (n-&gt;dominates(sub, nlist))
 465           only_dominating_controls = true;
 466         else
 467           return false;
 468       } else {
 469         // First, own control edge.
 470         Node* m = n-&gt;find_exact_control(n-&gt;in(0));
 471         if (m != NULL) {
 472           if (m-&gt;is_top())
 473             return false; // Conservative answer for dead code
 474           dom_list.push(m);
 475         }
 476         // Now, the rest of edges.
 477         uint cnt = n-&gt;req();
 478         for (uint i = 1; i &lt; cnt; i++) {
 479           m = n-&gt;find_exact_control(n-&gt;in(i));
 480           if (m == NULL || m-&gt;is_top())
 481             continue;
 482           dom_list.push(m);
 483         }
 484       }
 485     }
 486     return only_dominating_controls;
 487   }
 488 }
 489 
 490 //---------------------detect_ptr_independence---------------------------------
 491 // Used by MemNode::find_previous_store to prove that two base
 492 // pointers are never equal.
 493 // The pointers are accompanied by their associated allocations,
 494 // if any, which have been previously discovered by the caller.
 495 bool MemNode::detect_ptr_independence(Node* p1, AllocateNode* a1,
 496                                       Node* p2, AllocateNode* a2,
 497                                       PhaseTransform* phase) {
 498   // Attempt to prove that these two pointers cannot be aliased.
 499   // They may both manifestly be allocations, and they should differ.
 500   // Or, if they are not both allocations, they can be distinct constants.
 501   // Otherwise, one is an allocation and the other a pre-existing value.
 502   if (a1 == NULL &amp;&amp; a2 == NULL) {           // neither an allocation
 503     return (p1 != p2) &amp;&amp; p1-&gt;is_Con() &amp;&amp; p2-&gt;is_Con();
 504   } else if (a1 != NULL &amp;&amp; a2 != NULL) {    // both allocations
 505     return (a1 != a2);
 506   } else if (a1 != NULL) {                  // one allocation a1
 507     // (Note:  p2-&gt;is_Con implies p2-&gt;in(0)-&gt;is_Root, which dominates.)
 508     return all_controls_dominate(p2, a1);
 509   } else { //(a2 != NULL)                   // one allocation a2
 510     return all_controls_dominate(p1, a2);
 511   }
 512   return false;
 513 }
 514 
 515 
 516 // Find an arraycopy that must have set (can_see_stored_value=true) or
 517 // could have set (can_see_stored_value=false) the value for this load
 518 Node* LoadNode::find_previous_arraycopy(PhaseTransform* phase, Node* ld_alloc, Node*&amp; mem, bool can_see_stored_value) const {
 519   if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) != NULL &amp;&amp; (mem-&gt;in(0)-&gt;Opcode() == Op_MemBarStoreStore ||
 520                                                mem-&gt;in(0)-&gt;Opcode() == Op_MemBarCPUOrder)) {
 521     Node* mb = mem-&gt;in(0);
 522     if (mb-&gt;in(0) != NULL &amp;&amp; mb-&gt;in(0)-&gt;is_Proj() &amp;&amp;
 523         mb-&gt;in(0)-&gt;in(0) != NULL &amp;&amp; mb-&gt;in(0)-&gt;in(0)-&gt;is_ArrayCopy()) {
 524       ArrayCopyNode* ac = mb-&gt;in(0)-&gt;in(0)-&gt;as_ArrayCopy();
 525       if (ac-&gt;is_clonebasic()) {
 526         intptr_t offset;
 527         AllocateNode* alloc = AllocateNode::Ideal_allocation(ac-&gt;in(ArrayCopyNode::Dest), phase, offset);
 528         assert(alloc != NULL &amp;&amp; alloc-&gt;initialization()-&gt;is_complete_with_arraycopy(), "broken allocation");
 529         if (alloc == ld_alloc) {
 530           return ac;
 531         }
 532       }
 533     }
 534   } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) != NULL &amp;&amp; mem-&gt;in(0)-&gt;is_ArrayCopy()) {
 535     ArrayCopyNode* ac = mem-&gt;in(0)-&gt;as_ArrayCopy();
 536 
 537     if (ac-&gt;is_arraycopy_validated() ||
 538         ac-&gt;is_copyof_validated() ||
 539         ac-&gt;is_copyofrange_validated()) {
 540       Node* ld_addp = in(MemNode::Address);
 541       if (ld_addp-&gt;is_AddP()) {
 542         Node* ld_base = ld_addp-&gt;in(AddPNode::Address);
 543         Node* ld_offs = ld_addp-&gt;in(AddPNode::Offset);
 544 
 545         Node* dest = ac-&gt;in(ArrayCopyNode::Dest);
 546 
 547         if (dest == ld_base) {
 548           Node* src_pos = ac-&gt;in(ArrayCopyNode::SrcPos);
 549           Node* dest_pos = ac-&gt;in(ArrayCopyNode::DestPos);
 550           Node* len = ac-&gt;in(ArrayCopyNode::Length);
 551 
 552           const TypeInt *dest_pos_t = phase-&gt;type(dest_pos)-&gt;isa_int();
 553           const TypeX *ld_offs_t = phase-&gt;type(ld_offs)-&gt;isa_intptr_t();
 554           const TypeInt *len_t = phase-&gt;type(len)-&gt;isa_int();
 555           const TypeAryPtr* ary_t = phase-&gt;type(dest)-&gt;isa_aryptr();
 556 
 557           if (dest_pos_t != NULL &amp;&amp; ld_offs_t != NULL &amp;&amp; len_t != NULL &amp;&amp; ary_t != NULL) {
 558             BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
 559             uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 560             uint elemsize = type2aelembytes(ary_elem);
 561 
 562             intptr_t dest_pos_plus_len_lo = (((intptr_t)dest_pos_t-&gt;_lo) + len_t-&gt;_lo) * elemsize + header;
 563             intptr_t dest_pos_plus_len_hi = (((intptr_t)dest_pos_t-&gt;_hi) + len_t-&gt;_hi) * elemsize + header;
 564             intptr_t dest_pos_lo = ((intptr_t)dest_pos_t-&gt;_lo) * elemsize + header;
 565             intptr_t dest_pos_hi = ((intptr_t)dest_pos_t-&gt;_hi) * elemsize + header;
 566 
 567             if (can_see_stored_value) {
 568               if (ld_offs_t-&gt;_lo &gt;= dest_pos_hi &amp;&amp; ld_offs_t-&gt;_hi &lt; dest_pos_plus_len_lo) {
 569                 return ac;
 570               }
 571             } else {
 572               if (ld_offs_t-&gt;_hi &lt; dest_pos_lo || ld_offs_t-&gt;_lo &gt;= dest_pos_plus_len_hi) {
 573                 mem = ac-&gt;in(TypeFunc::Memory);
 574               }
 575               return ac;
 576             }
 577           }
 578         }
 579       }
 580     }
 581   }
 582   return NULL;
 583 }
 584 
 585 // The logic for reordering loads and stores uses four steps:
 586 // (a) Walk carefully past stores and initializations which we
 587 //     can prove are independent of this load.
 588 // (b) Observe that the next memory state makes an exact match
 589 //     with self (load or store), and locate the relevant store.
 590 // (c) Ensure that, if we were to wire self directly to the store,
 591 //     the optimizer would fold it up somehow.
 592 // (d) Do the rewiring, and return, depending on some other part of
 593 //     the optimizer to fold up the load.
 594 // This routine handles steps (a) and (b).  Steps (c) and (d) are
 595 // specific to loads and stores, so they are handled by the callers.
 596 // (Currently, only LoadNode::Ideal has steps (c), (d).  More later.)
 597 //
 598 Node* MemNode::find_previous_store(PhaseTransform* phase) {
 599   Node*         ctrl   = in(MemNode::Control);
 600   Node*         adr    = in(MemNode::Address);
 601   intptr_t      offset = 0;
 602   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
 603   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
 604 
 605   if (offset == Type::OffsetBot)
 606     return NULL;            // cannot unalias unless there are precise offsets
 607 
 608   const TypeOopPtr *addr_t = adr-&gt;bottom_type()-&gt;isa_oopptr();
 609 
 610   intptr_t size_in_bytes = memory_size();
 611 
 612   Node* mem = in(MemNode::Memory);   // start searching here...
 613 
 614   int cnt = 50;             // Cycle limiter
 615   for (;;) {                // While we can dance past unrelated stores...
 616     if (--cnt &lt; 0)  break;  // Caught in cycle or a complicated dance?
 617 
 618     Node* prev = mem;
 619     if (mem-&gt;is_Store()) {
 620       Node* st_adr = mem-&gt;in(MemNode::Address);
 621       intptr_t st_offset = 0;
 622       Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);
 623       if (st_base == NULL)
 624         break;              // inscrutable pointer
 625       if (st_offset != offset &amp;&amp; st_offset != Type::OffsetBot) {
 626         const int MAX_STORE = BytesPerLong;
 627         if (st_offset &gt;= offset + size_in_bytes ||
 628             st_offset &lt;= offset - MAX_STORE ||
 629             st_offset &lt;= offset - mem-&gt;as_Store()-&gt;memory_size()) {
 630           // Success:  The offsets are provably independent.
 631           // (You may ask, why not just test st_offset != offset and be done?
 632           // The answer is that stores of different sizes can co-exist
 633           // in the same sequence of RawMem effects.  We sometimes initialize
 634           // a whole 'tile' of array elements with a single jint or jlong.)
 635           mem = mem-&gt;in(MemNode::Memory);
 636           continue;           // (a) advance through independent store memory
 637         }
 638       }
 639       if (st_base != base &amp;&amp;
 640           detect_ptr_independence(base, alloc,
 641                                   st_base,
 642                                   AllocateNode::Ideal_allocation(st_base, phase),
 643                                   phase)) {
 644         // Success:  The bases are provably independent.
 645         mem = mem-&gt;in(MemNode::Memory);
 646         continue;           // (a) advance through independent store memory
 647       }
 648 
 649       // (b) At this point, if the bases or offsets do not agree, we lose,
 650       // since we have not managed to prove 'this' and 'mem' independent.
 651       if (st_base == base &amp;&amp; st_offset == offset) {
 652         return mem;         // let caller handle steps (c), (d)
 653       }
 654 
 655     } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
 656       InitializeNode* st_init = mem-&gt;in(0)-&gt;as_Initialize();
 657       AllocateNode*  st_alloc = st_init-&gt;allocation();
 658       if (st_alloc == NULL)
 659         break;              // something degenerated
 660       bool known_identical = false;
 661       bool known_independent = false;
 662       if (alloc == st_alloc)
 663         known_identical = true;
 664       else if (alloc != NULL)
 665         known_independent = true;
 666       else if (all_controls_dominate(this, st_alloc))
 667         known_independent = true;
 668 
 669       if (known_independent) {
 670         // The bases are provably independent: Either they are
 671         // manifestly distinct allocations, or else the control
 672         // of this load dominates the store's allocation.
 673         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 674         if (alias_idx == Compile::AliasIdxRaw) {
 675           mem = st_alloc-&gt;in(TypeFunc::Memory);
 676         } else {
 677           mem = st_init-&gt;memory(alias_idx);
 678         }
 679         continue;           // (a) advance through independent store memory
 680       }
 681 
 682       // (b) at this point, if we are not looking at a store initializing
 683       // the same allocation we are loading from, we lose.
 684       if (known_identical) {
 685         // From caller, can_see_stored_value will consult find_captured_store.
 686         return mem;         // let caller handle steps (c), (d)
 687       }
 688 
 689     } else if (find_previous_arraycopy(phase, alloc, mem, false) != NULL) {
 690       if (prev != mem) {
 691         // Found an arraycopy but it doesn't affect that load
 692         continue;
 693       }
 694       // Found an arraycopy that may affect that load
 695       return mem;
 696     } else if (addr_t != NULL &amp;&amp; addr_t-&gt;is_known_instance_field()) {
 697       // Can't use optimize_simple_memory_chain() since it needs PhaseGVN.
 698       if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Call()) {
 699         // ArrayCopyNodes processed here as well.
 700         CallNode *call = mem-&gt;in(0)-&gt;as_Call();
 701         if (!call-&gt;may_modify(addr_t, phase)) {
 702           mem = call-&gt;in(TypeFunc::Memory);
 703           continue;         // (a) advance through independent call memory
 704         }
 705       } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_MemBar()) {
 706         if (membar_for_arraycopy(addr_t, mem-&gt;in(0)-&gt;as_MemBar(), phase)) {
 707           break;
 708         }
 709         mem = mem-&gt;in(0)-&gt;in(TypeFunc::Memory);
 710         continue;           // (a) advance through independent MemBar memory
 711       } else if (mem-&gt;is_ClearArray()) {
 712         if (ClearArrayNode::step_through(&amp;mem, (uint)addr_t-&gt;instance_id(), phase)) {
 713           // (the call updated 'mem' value)
 714           continue;         // (a) advance through independent allocation memory
 715         } else {
 716           // Can not bypass initialization of the instance
 717           // we are looking for.
 718           return mem;
 719         }
 720       } else if (mem-&gt;is_MergeMem()) {
 721         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 722         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 723         continue;           // (a) advance through independent MergeMem memory
 724       }
 725     }
 726 
 727     // Unless there is an explicit 'continue', we must bail out here,
 728     // because 'mem' is an inscrutable memory state (e.g., a call).
 729     break;
 730   }
 731 
 732   return NULL;              // bail out
 733 }
 734 
 735 //----------------------calculate_adr_type-------------------------------------
 736 // Helper function.  Notices when the given type of address hits top or bottom.
 737 // Also, asserts a cross-check of the type against the expected address type.
 738 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 739   if (t == Type::TOP)  return NULL; // does not touch memory any more?
 740   #ifdef PRODUCT
 741   cross_check = NULL;
 742   #else
 743   if (!VerifyAliases || is_error_reported() || Node::in_dump())  cross_check = NULL;
 744   #endif
 745   const TypePtr* tp = t-&gt;isa_ptr();
 746   if (tp == NULL) {
 747     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, "expected memory type must be wide");
 748     return TypePtr::BOTTOM;           // touches lots of memory
 749   } else {
 750     #ifdef ASSERT
 751     // %%%% [phh] We don't check the alias index if cross_check is
 752     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 753     if (cross_check != NULL &amp;&amp;
 754         cross_check != TypePtr::BOTTOM &amp;&amp;
 755         cross_check != TypeRawPtr::BOTTOM) {
 756       // Recheck the alias index, to see if it has changed (due to a bug).
 757       Compile* C = Compile::current();
 758       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 759              "must stay in the original alias category");
 760       // The type of the address must be contained in the adr_type,
 761       // disregarding "null"-ness.
 762       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 763       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 764       assert(cross_check-&gt;meet(tp_notnull) == cross_check-&gt;remove_speculative(),
 765              "real address must not escape from expected memory type");
 766     }
 767     #endif
 768     return tp;
 769   }
 770 }
 771 
 772 //=============================================================================
 773 // Should LoadNode::Ideal() attempt to remove control edges?
 774 bool LoadNode::can_remove_control() const {
 775   return true;
 776 }
 777 uint LoadNode::size_of() const { return sizeof(*this); }
 778 uint LoadNode::cmp( const Node &amp;n ) const
 779 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 780 const Type *LoadNode::bottom_type() const { return _type; }
 781 uint LoadNode::ideal_reg() const {
 782   return _type-&gt;ideal_reg();
 783 }
 784 
 785 #ifndef PRODUCT
 786 void LoadNode::dump_spec(outputStream *st) const {
 787   MemNode::dump_spec(st);
 788   if( !Verbose &amp;&amp; !WizardMode ) {
 789     // standard dump does this in Verbose and WizardMode
 790     st-&gt;print(" #"); _type-&gt;dump_on(st);
 791   }
 792   if (!_depends_only_on_test) {
 793     st-&gt;print(" (does not depend only on test)");
 794   }
 795 }
 796 #endif
 797 
 798 #ifdef ASSERT
 799 //----------------------------is_immutable_value-------------------------------
 800 // Helper function to allow a raw load without control edge for some cases
 801 bool LoadNode::is_immutable_value(Node* adr) {
 802   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 803           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 804           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 805            in_bytes(JavaThread::osthread_offset())));
 806 }
 807 #endif
 808 
 809 //----------------------------LoadNode::make-----------------------------------
 810 // Polymorphic factory method:
 811 Node *LoadNode::make(PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt, MemOrd mo, ControlDependency control_dependency) {
 812   Compile* C = gvn.C;
 813 
 814   // sanity check the alias category against the created node type
 815   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 816            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 817          "use LoadKlassNode instead");
 818   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 819            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 820          "use LoadRangeNode instead");
 821   // Check control edge of raw loads
 822   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 823           // oop will be recorded in oop map if load crosses safepoint
 824           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 825           "raw memory operations should have control edge");
 826   switch (bt) {
 827   case T_BOOLEAN: return new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency);
 828   case T_BYTE:    return new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency);
 829   case T_INT:     return new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency);
 830   case T_CHAR:    return new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency);
 831   case T_SHORT:   return new LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency);
 832   case T_LONG:    return new LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency);
 833   case T_FLOAT:   return new LoadFNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency);
 834   case T_DOUBLE:  return new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency);
 835   case T_ADDRESS: return new LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo, control_dependency);
 836   case T_OBJECT:
 837 #ifdef _LP64
 838     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 839       Node* load  = gvn.transform(new LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo, control_dependency));
 840       return new DecodeNNode(load, load-&gt;bottom_type()-&gt;make_ptr());
 841     } else
 842 #endif
 843     {
 844       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), "should have got back a narrow oop");
 845       return new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_oopptr(), mo, control_dependency);
 846     }
 847   }
 848   ShouldNotReachHere();
 849   return (LoadNode*)NULL;
 850 }
 851 
 852 LoadLNode* LoadLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo, ControlDependency control_dependency) {
 853   bool require_atomic = true;
 854   return new LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency, require_atomic);
 855 }
 856 
 857 LoadDNode* LoadDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo, ControlDependency control_dependency) {
 858   bool require_atomic = true;
 859   return new LoadDNode(ctl, mem, adr, adr_type, rt, mo, control_dependency, require_atomic);
 860 }
 861 
 862 
 863 
 864 //------------------------------hash-------------------------------------------
 865 uint LoadNode::hash() const {
 866   // unroll addition of interesting fields
 867   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 868 }
 869 
 870 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 871   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 872     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 873     bool is_stable_ary = FoldStableValues &amp;&amp;
 874                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 875                          tp-&gt;isa_aryptr()-&gt;is_stable();
 876 
 877     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 878   }
 879 
 880   return false;
 881 }
 882 
 883 // Is the value loaded previously stored by an arraycopy? If so return
 884 // a load node that reads from the source array so we may be able to
 885 // optimize out the ArrayCopy node later.
 886 Node* MemNode::can_see_arraycopy_value(Node* st, PhaseTransform* phase) const {
 887   Node* ld_adr = in(MemNode::Address);
 888   intptr_t ld_off = 0;
 889   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 890   Node* ac = find_previous_arraycopy(phase, ld_alloc, st, true);
 891   if (ac != NULL) {
 892     assert(ac-&gt;is_ArrayCopy(), "what kind of node can this be?");
 893     assert(is_Load(), "only for loads");
 894 
 895     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 896       assert(ld_alloc != NULL, "need an alloc");
 897       Node* ld = clone();
 898       Node* addp = in(MemNode::Address)-&gt;clone();
 899       assert(addp-&gt;is_AddP(), "address must be addp");
 900       assert(addp-&gt;in(AddPNode::Base) == ac-&gt;in(ArrayCopyNode::Dest)-&gt;in(AddPNode::Base), "strange pattern");
 901       assert(addp-&gt;in(AddPNode::Address) == ac-&gt;in(ArrayCopyNode::Dest)-&gt;in(AddPNode::Address), "strange pattern");
 902       addp-&gt;set_req(AddPNode::Base, ac-&gt;in(ArrayCopyNode::Src)-&gt;in(AddPNode::Base));
 903       addp-&gt;set_req(AddPNode::Address, ac-&gt;in(ArrayCopyNode::Src)-&gt;in(AddPNode::Address));
 904       ld-&gt;set_req(MemNode::Address, phase-&gt;transform(addp));
 905       if (in(0) != NULL) {
 906         assert(ld_alloc-&gt;in(0) != NULL, "alloc must have control");
 907         ld-&gt;set_req(0, ld_alloc-&gt;in(0));
 908       }
 909       return ld;
 910     } else {
 911       Node* ld = clone();
 912       Node* addp = in(MemNode::Address)-&gt;clone();
 913       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), "should be");
 914       addp-&gt;set_req(AddPNode::Base, ac-&gt;in(ArrayCopyNode::Src));
 915       addp-&gt;set_req(AddPNode::Address, ac-&gt;in(ArrayCopyNode::Src));
 916 
 917       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
 918       BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
 919       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 920       uint shift  = exact_log2(type2aelembytes(ary_elem));
 921 
 922       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 923 #ifdef _LP64
 924       diff = phase-&gt;transform(new ConvI2LNode(diff));
 925 #endif
 926       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 927 
 928       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 929       addp-&gt;set_req(AddPNode::Offset, offset);
 930       ld-&gt;set_req(MemNode::Address, phase-&gt;transform(addp));
 931 
 932       if (in(0) != NULL) {
 933         assert(ac-&gt;in(0) != NULL, "alloc must have control");
 934         ld-&gt;set_req(0, ac-&gt;in(0));
 935       }
 936       return ld;
 937     }
 938   }
 939   return NULL;
 940 }
 941 
 942 
 943 //---------------------------can_see_stored_value------------------------------
 944 // This routine exists to make sure this set of tests is done the same
 945 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 946 // will change the graph shape in a way which makes memory alive twice at the
 947 // same time (uses the Oracle model of aliasing), then some
 948 // LoadXNode::Identity will fold things back to the equivalence-class model
 949 // of aliasing.
 950 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
 951   Node* ld_adr = in(MemNode::Address);
 952   intptr_t ld_off = 0;
 953   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 954   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
 955   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
 956   // This is more general than load from boxing objects.
 957   if (skip_through_membars(atp, tp, phase-&gt;C-&gt;eliminate_boxing())) {
 958     uint alias_idx = atp-&gt;index();
 959     bool final = !atp-&gt;is_rewritable();
 960     Node* result = NULL;
 961     Node* current = st;
 962     // Skip through chains of MemBarNodes checking the MergeMems for
 963     // new states for the slice of this load.  Stop once any other
 964     // kind of node is encountered.  Loads from final memory can skip
 965     // through any kind of MemBar but normal loads shouldn't skip
 966     // through MemBarAcquire since the could allow them to move out of
 967     // a synchronized region.
 968     while (current-&gt;is_Proj()) {
 969       int opc = current-&gt;in(0)-&gt;Opcode();
 970       if ((final &amp;&amp; (opc == Op_MemBarAcquire ||
 971                      opc == Op_MemBarAcquireLock ||
 972                      opc == Op_LoadFence)) ||
 973           opc == Op_MemBarRelease ||
 974           opc == Op_StoreFence ||
 975           opc == Op_MemBarReleaseLock ||
 976           opc == Op_MemBarStoreStore ||
 977           opc == Op_MemBarCPUOrder) {
 978         Node* mem = current-&gt;in(0)-&gt;in(TypeFunc::Memory);
 979         if (mem-&gt;is_MergeMem()) {
 980           MergeMemNode* merge = mem-&gt;as_MergeMem();
 981           Node* new_st = merge-&gt;memory_at(alias_idx);
 982           if (new_st == merge-&gt;base_memory()) {
 983             // Keep searching
 984             current = new_st;
 985             continue;
 986           }
 987           // Save the new memory state for the slice and fall through
 988           // to exit.
 989           result = new_st;
 990         }
 991       }
 992       break;
 993     }
 994     if (result != NULL) {
 995       st = result;
 996     }
 997   }
 998 
 999   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
1000   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
1001   for (int trip = 0; trip &lt;= 1; trip++) {
1002 
1003     if (st-&gt;is_Store()) {
1004       Node* st_adr = st-&gt;in(MemNode::Address);
1005       if (!phase-&gt;eqv(st_adr, ld_adr)) {
1006         // Try harder before giving up...  Match raw and non-raw pointers.
1007         intptr_t st_off = 0;
1008         AllocateNode* alloc = AllocateNode::Ideal_allocation(st_adr, phase, st_off);
1009         if (alloc == NULL)       return NULL;
1010         if (alloc != ld_alloc)   return NULL;
1011         if (ld_off != st_off)    return NULL;
1012         // At this point we have proven something like this setup:
1013         //  A = Allocate(...)
1014         //  L = LoadQ(,  AddP(CastPP(, A.Parm),, #Off))
1015         //  S = StoreQ(, AddP(,        A.Parm  , #Off), V)
1016         // (Actually, we haven't yet proven the Q's are the same.)
1017         // In other words, we are loading from a casted version of
1018         // the same pointer-and-offset that we stored to.
1019         // Thus, we are able to replace L by V.
1020       }
1021       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1022       if (store_Opcode() != st-&gt;Opcode())
1023         return NULL;
1024       return st-&gt;in(MemNode::ValueIn);
1025     }
1026 
1027     // A load from a freshly-created object always returns zero.
1028     // (This can happen after LoadNode::Ideal resets the load's memory input
1029     // to find_captured_store, which returned InitializeNode::zero_memory.)
1030     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1031         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1032         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1033       // return a zero value for the load's basic type
1034       // (This is one of the few places where a generic PhaseTransform
1035       // can create new nodes.  Think of it as lazily manifesting
1036       // virtually pre-existing constants.)
1037       return phase-&gt;zerocon(memory_type());
1038     }
1039 
1040     // A load from an initialization barrier can match a captured store.
1041     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1042       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1043       AllocateNode* alloc = init-&gt;allocation();
1044       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1045         // examine a captured store value
1046         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1047         if (st != NULL) {
1048           continue;             // take one more trip around
1049         }
1050       }
1051     }
1052 
1053     // Load boxed value from result of valueOf() call is input parameter.
1054     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1055         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1056       intptr_t ignore = 0;
1057       Node* base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ignore);
1058       if (base != NULL &amp;&amp; base-&gt;is_Proj() &amp;&amp;
1059           base-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp;
1060           base-&gt;in(0)-&gt;is_CallStaticJava() &amp;&amp;
1061           base-&gt;in(0)-&gt;as_CallStaticJava()-&gt;is_boxing_method()) {
1062         return base-&gt;in(0)-&gt;in(TypeFunc::Parms);
1063       }
1064     }
1065 
1066     break;
1067   }
1068 
1069   return NULL;
1070 }
1071 
1072 //----------------------is_instance_field_load_with_local_phi------------------
1073 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1074   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1075       in(Address)-&gt;is_AddP() ) {
1076     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1077     // Only instances and boxed values.
1078     if( t_oop != NULL &amp;&amp;
1079         (t_oop-&gt;is_ptr_to_boxed_value() ||
1080          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1081         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1082         t_oop-&gt;offset() != Type::OffsetTop) {
1083       return true;
1084     }
1085   }
1086   return false;
1087 }
1088 
1089 //------------------------------Identity---------------------------------------
1090 // Loads are identity if previous store is to same address
1091 Node *LoadNode::Identity( PhaseTransform *phase ) {
1092   // If the previous store-maker is the right kind of Store, and the store is
1093   // to the same address, then we are equal to the value stored.
1094   Node* mem = in(Memory);
1095   Node* value = can_see_stored_value(mem, phase);
1096   if( value ) {
1097     // byte, short &amp; char stores truncate naturally.
1098     // A load has to load the truncated value which requires
1099     // some sort of masking operation and that requires an
1100     // Ideal call instead of an Identity call.
1101     if (memory_size() &lt; BytesPerInt) {
1102       // If the input to the store does not fit with the load's result type,
1103       // it must be truncated via an Ideal call.
1104       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1105         return this;
1106     }
1107     // (This works even when value is a Con, but LoadNode::Value
1108     // usually runs first, producing the singleton type of the Con.)
1109     return value;
1110   }
1111 
1112   // Search for an existing data phi which was generated before for the same
1113   // instance's field to avoid infinite generation of phis in a loop.
1114   Node *region = mem-&gt;in(0);
1115   if (is_instance_field_load_with_local_phi(region)) {
1116     const TypeOopPtr *addr_t = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1117     int this_index  = phase-&gt;C-&gt;get_alias_index(addr_t);
1118     int this_offset = addr_t-&gt;offset();
1119     int this_iid    = addr_t-&gt;instance_id();
1120     if (!addr_t-&gt;is_known_instance() &amp;&amp;
1121          addr_t-&gt;is_ptr_to_boxed_value()) {
1122       // Use _idx of address base (could be Phi node) for boxed values.
1123       intptr_t   ignore = 0;
1124       Node*      base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1125       this_iid = base-&gt;_idx;
1126     }
1127     const Type* this_type = bottom_type();
1128     for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1129       Node* phi = region-&gt;fast_out(i);
1130       if (phi-&gt;is_Phi() &amp;&amp; phi != mem &amp;&amp;
1131           phi-&gt;as_Phi()-&gt;is_same_inst_field(this_type, this_iid, this_index, this_offset)) {
1132         return phi;
1133       }
1134     }
1135   }
1136 
1137   return this;
1138 }
1139 
1140 // We're loading from an object which has autobox behaviour.
1141 // If this object is result of a valueOf call we'll have a phi
1142 // merging a newly allocated object and a load from the cache.
1143 // We want to replace this load with the original incoming
1144 // argument to the valueOf call.
1145 Node* LoadNode::eliminate_autobox(PhaseGVN* phase) {
1146   assert(phase-&gt;C-&gt;eliminate_boxing(), "sanity");
1147   intptr_t ignore = 0;
1148   Node* base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1149   if ((base == NULL) || base-&gt;is_Phi()) {
1150     // Push the loads from the phi that comes from valueOf up
1151     // through it to allow elimination of the loads and the recovery
1152     // of the original value. It is done in split_through_phi().
1153     return NULL;
1154   } else if (base-&gt;is_Load() ||
1155              base-&gt;is_DecodeN() &amp;&amp; base-&gt;in(1)-&gt;is_Load()) {
1156     // Eliminate the load of boxed value for integer types from the cache
1157     // array by deriving the value from the index into the array.
1158     // Capture the offset of the load and then reverse the computation.
1159 
1160     // Get LoadN node which loads a boxing object from 'cache' array.
1161     if (base-&gt;is_DecodeN()) {
1162       base = base-&gt;in(1);
1163     }
1164     if (!base-&gt;in(Address)-&gt;is_AddP()) {
1165       return NULL; // Complex address
1166     }
1167     AddPNode* address = base-&gt;in(Address)-&gt;as_AddP();
1168     Node* cache_base = address-&gt;in(AddPNode::Base);
1169     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_DecodeN()) {
1170       // Get ConP node which is static 'cache' field.
1171       cache_base = cache_base-&gt;in(1);
1172     }
1173     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_Con()) {
1174       const TypeAryPtr* base_type = cache_base-&gt;bottom_type()-&gt;isa_aryptr();
1175       if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1176         Node* elements[4];
1177         int shift = exact_log2(type2aelembytes(T_OBJECT));
1178         int count = address-&gt;unpack_offsets(elements, ARRAY_SIZE(elements));
1179         if ((count &gt;  0) &amp;&amp; elements[0]-&gt;is_Con() &amp;&amp;
1180             ((count == 1) ||
1181              (count == 2) &amp;&amp; elements[1]-&gt;Opcode() == Op_LShiftX &amp;&amp;
1182                              elements[1]-&gt;in(2) == phase-&gt;intcon(shift))) {
1183           ciObjArray* array = base_type-&gt;const_oop()-&gt;as_obj_array();
1184           // Fetch the box object cache[0] at the base of the array and get its value
1185           ciInstance* box = array-&gt;obj_at(0)-&gt;as_instance();
1186           ciInstanceKlass* ik = box-&gt;klass()-&gt;as_instance_klass();
1187           assert(ik-&gt;is_box_klass(), "sanity");
1188           assert(ik-&gt;nof_nonstatic_fields() == 1, "change following code");
1189           if (ik-&gt;nof_nonstatic_fields() == 1) {
1190             // This should be true nonstatic_field_at requires calling
1191             // nof_nonstatic_fields so check it anyway
1192             ciConstant c = box-&gt;field_value(ik-&gt;nonstatic_field_at(0));
1193             BasicType bt = c.basic_type();
1194             // Only integer types have boxing cache.
1195             assert(bt == T_BOOLEAN || bt == T_CHAR  ||
1196                    bt == T_BYTE    || bt == T_SHORT ||
1197                    bt == T_INT     || bt == T_LONG, err_msg_res("wrong type = %s", type2name(bt)));
1198             jlong cache_low = (bt == T_LONG) ? c.as_long() : c.as_int();
1199             if (cache_low != (int)cache_low) {
1200               return NULL; // should not happen since cache is array indexed by value
1201             }
1202             jlong offset = arrayOopDesc::base_offset_in_bytes(T_OBJECT) - (cache_low &lt;&lt; shift);
1203             if (offset != (int)offset) {
1204               return NULL; // should not happen since cache is array indexed by value
1205             }
1206            // Add up all the offsets making of the address of the load
1207             Node* result = elements[0];
1208             for (int i = 1; i &lt; count; i++) {
1209               result = phase-&gt;transform(new AddXNode(result, elements[i]));
1210             }
1211             // Remove the constant offset from the address and then
1212             result = phase-&gt;transform(new AddXNode(result, phase-&gt;MakeConX(-(int)offset)));
1213             // remove the scaling of the offset to recover the original index.
1214             if (result-&gt;Opcode() == Op_LShiftX &amp;&amp; result-&gt;in(2) == phase-&gt;intcon(shift)) {
1215               // Peel the shift off directly but wrap it in a dummy node
1216               // since Ideal can't return existing nodes
1217               result = new RShiftXNode(result-&gt;in(1), phase-&gt;intcon(0));
1218             } else if (result-&gt;is_Add() &amp;&amp; result-&gt;in(2)-&gt;is_Con() &amp;&amp;
1219                        result-&gt;in(1)-&gt;Opcode() == Op_LShiftX &amp;&amp;
1220                        result-&gt;in(1)-&gt;in(2) == phase-&gt;intcon(shift)) {
1221               // We can't do general optimization: ((X&lt;&lt;Z) + Y) &gt;&gt; Z ==&gt; X + (Y&gt;&gt;Z)
1222               // but for boxing cache access we know that X&lt;&lt;Z will not overflow
1223               // (there is range check) so we do this optimizatrion by hand here.
1224               Node* add_con = new RShiftXNode(result-&gt;in(2), phase-&gt;intcon(shift));
1225               result = new AddXNode(result-&gt;in(1)-&gt;in(1), phase-&gt;transform(add_con));
1226             } else {
1227               result = new RShiftXNode(result, phase-&gt;intcon(shift));
1228             }
1229 #ifdef _LP64
1230             if (bt != T_LONG) {
1231               result = new ConvL2INode(phase-&gt;transform(result));
1232             }
1233 #else
1234             if (bt == T_LONG) {
1235               result = new ConvI2LNode(phase-&gt;transform(result));
1236             }
1237 #endif
1238             // Boxing/unboxing can be done from signed &amp; unsigned loads (e.g. LoadUB -&gt; ... -&gt; LoadB pair).
1239             // Need to preserve unboxing load type if it is unsigned.
1240             switch(this-&gt;Opcode()) {
1241               case Op_LoadUB:
1242                 result = new AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFF));
1243                 break;
1244               case Op_LoadUS:
1245                 result = new AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFFFF));
1246                 break;
1247             }
1248             return result;
1249           }
1250         }
1251       }
1252     }
1253   }
1254   return NULL;
1255 }
1256 
1257 static bool stable_phi(PhiNode* phi, PhaseGVN *phase) {
1258   Node* region = phi-&gt;in(0);
1259   if (region == NULL) {
1260     return false; // Wait stable graph
1261   }
1262   uint cnt = phi-&gt;req();
1263   for (uint i = 1; i &lt; cnt; i++) {
1264     Node* rc = region-&gt;in(i);
1265     if (rc == NULL || phase-&gt;type(rc) == Type::TOP)
1266       return false; // Wait stable graph
1267     Node* in = phi-&gt;in(i);
1268     if (in == NULL || phase-&gt;type(in) == Type::TOP)
1269       return false; // Wait stable graph
1270   }
1271   return true;
1272 }
1273 //------------------------------split_through_phi------------------------------
1274 // Split instance or boxed field load through Phi.
1275 Node *LoadNode::split_through_phi(PhaseGVN *phase) {
1276   Node* mem     = in(Memory);
1277   Node* address = in(Address);
1278   const TypeOopPtr *t_oop = phase-&gt;type(address)-&gt;isa_oopptr();
1279 
1280   assert((t_oop != NULL) &amp;&amp;
1281          (t_oop-&gt;is_known_instance_field() ||
1282           t_oop-&gt;is_ptr_to_boxed_value()), "invalide conditions");
1283 
1284   Compile* C = phase-&gt;C;
1285   intptr_t ignore = 0;
1286   Node*    base = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1287   bool base_is_phi = (base != NULL) &amp;&amp; base-&gt;is_Phi();
1288   bool load_boxed_values = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp; C-&gt;aggressive_unboxing() &amp;&amp;
1289                            (base != NULL) &amp;&amp; (base == address-&gt;in(AddPNode::Base)) &amp;&amp;
1290                            phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL);
1291 
1292   if (!((mem-&gt;is_Phi() || base_is_phi) &amp;&amp;
1293         (load_boxed_values || t_oop-&gt;is_known_instance_field()))) {
1294     return NULL; // memory is not Phi
1295   }
1296 
1297   if (mem-&gt;is_Phi()) {
1298     if (!stable_phi(mem-&gt;as_Phi(), phase)) {
1299       return NULL; // Wait stable graph
1300     }
1301     uint cnt = mem-&gt;req();
1302     // Check for loop invariant memory.
1303     if (cnt == 3) {
1304       for (uint i = 1; i &lt; cnt; i++) {
1305         Node* in = mem-&gt;in(i);
1306         Node*  m = optimize_memory_chain(in, t_oop, this, phase);
1307         if (m == mem) {
1308           set_req(Memory, mem-&gt;in(cnt - i));
1309           return this; // made change
1310         }
1311       }
1312     }
1313   }
1314   if (base_is_phi) {
1315     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1316       return NULL; // Wait stable graph
1317     }
1318     uint cnt = base-&gt;req();
1319     // Check for loop invariant memory.
1320     if (cnt == 3) {
1321       for (uint i = 1; i &lt; cnt; i++) {
1322         if (base-&gt;in(i) == base) {
1323           return NULL; // Wait stable graph
1324         }
1325       }
1326     }
1327   }
1328 
1329   bool load_boxed_phi = load_boxed_values &amp;&amp; base_is_phi &amp;&amp; (base-&gt;in(0) == mem-&gt;in(0));
1330 
1331   // Split through Phi (see original code in loopopts.cpp).
1332   assert(C-&gt;have_alias_type(t_oop), "instance should have alias type");
1333 
1334   // Do nothing here if Identity will find a value
1335   // (to avoid infinite chain of value phis generation).
1336   if (!phase-&gt;eqv(this, this-&gt;Identity(phase)))
1337     return NULL;
1338 
1339   // Select Region to split through.
1340   Node* region;
1341   if (!base_is_phi) {
1342     assert(mem-&gt;is_Phi(), "sanity");
1343     region = mem-&gt;in(0);
1344     // Skip if the region dominates some control edge of the address.
1345     if (!MemNode::all_controls_dominate(address, region))
1346       return NULL;
1347   } else if (!mem-&gt;is_Phi()) {
1348     assert(base_is_phi, "sanity");
1349     region = base-&gt;in(0);
1350     // Skip if the region dominates some control edge of the memory.
1351     if (!MemNode::all_controls_dominate(mem, region))
1352       return NULL;
1353   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1354     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), "sanity");
1355     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1356       region = base-&gt;in(0);
1357     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
1358       region = mem-&gt;in(0);
1359     } else {
1360       return NULL; // complex graph
1361     }
1362   } else {
1363     assert(base-&gt;in(0) == mem-&gt;in(0), "sanity");
1364     region = mem-&gt;in(0);
1365   }
1366 
1367   const Type* this_type = this-&gt;bottom_type();
1368   int this_index  = C-&gt;get_alias_index(t_oop);
1369   int this_offset = t_oop-&gt;offset();
1370   int this_iid    = t_oop-&gt;instance_id();
1371   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1372     // Use _idx of address base for boxed values.
1373     this_iid = base-&gt;_idx;
1374   }
1375   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1376   Node* phi = new PhiNode(region, this_type, NULL, this_iid, this_index, this_offset);
1377   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1378     Node* x;
1379     Node* the_clone = NULL;
1380     if (region-&gt;in(i) == C-&gt;top()) {
1381       x = C-&gt;top();      // Dead path?  Use a dead data op
1382     } else {
1383       x = this-&gt;clone();        // Else clone up the data op
1384       the_clone = x;            // Remember for possible deletion.
1385       // Alter data node to use pre-phi inputs
1386       if (this-&gt;in(0) == region) {
1387         x-&gt;set_req(0, region-&gt;in(i));
1388       } else {
1389         x-&gt;set_req(0, NULL);
1390       }
1391       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1392         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1393       }
1394       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1395         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1396       }
1397       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1398         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1399         Node* adr_x = phase-&gt;transform(new AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1400         x-&gt;set_req(Address, adr_x);
1401       }
1402     }
1403     // Check for a 'win' on some paths
1404     const Type *t = x-&gt;Value(igvn);
1405 
1406     bool singleton = t-&gt;singleton();
1407 
1408     // See comments in PhaseIdealLoop::split_thru_phi().
1409     if (singleton &amp;&amp; t == Type::TOP) {
1410       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1411     }
1412 
1413     if (singleton) {
1414       x = igvn-&gt;makecon(t);
1415     } else {
1416       // We now call Identity to try to simplify the cloned node.
1417       // Note that some Identity methods call phase-&gt;type(this).
1418       // Make sure that the type array is big enough for
1419       // our new node, even though we may throw the node away.
1420       // (This tweaking with igvn only works because x is a new node.)
1421       igvn-&gt;set_type(x, t);
1422       // If x is a TypeNode, capture any more-precise type permanently into Node
1423       // otherwise it will be not updated during igvn-&gt;transform since
1424       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1425       x-&gt;raise_bottom_type(t);
1426       Node *y = x-&gt;Identity(igvn);
1427       if (y != x) {
1428         x = y;
1429       } else {
1430         y = igvn-&gt;hash_find_insert(x);
1431         if (y) {
1432           x = y;
1433         } else {
1434           // Else x is a new node we are keeping
1435           // We do not need register_new_node_with_optimizer
1436           // because set_type has already been called.
1437           igvn-&gt;_worklist.push(x);
1438         }
1439       }
1440     }
1441     if (x != the_clone &amp;&amp; the_clone != NULL) {
1442       igvn-&gt;remove_dead_node(the_clone);
1443     }
1444     phi-&gt;set_req(i, x);
1445   }
1446   // Record Phi
1447   igvn-&gt;register_new_node_with_optimizer(phi);
1448   return phi;
1449 }
1450 
1451 //------------------------------Ideal------------------------------------------
1452 // If the load is from Field memory and the pointer is non-null, it might be possible to
1453 // zero out the control input.
1454 // If the offset is constant and the base is an object allocation,
1455 // try to hook me up to the exact initializing store.
1456 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1457   Node* p = MemNode::Ideal_common(phase, can_reshape);
1458   if (p)  return (p == NodeSentinel) ? NULL : p;
1459 
1460   Node* ctrl    = in(MemNode::Control);
1461   Node* address = in(MemNode::Address);
1462   bool progress = false;
1463 
1464   // Skip up past a SafePoint control.  Cannot do this for Stores because
1465   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1466   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1467       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw ) {
1468     ctrl = ctrl-&gt;in(0);
1469     set_req(MemNode::Control,ctrl);
1470     progress = true;
1471   }
1472 
1473   intptr_t ignore = 0;
1474   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1475   if (base != NULL
1476       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1477     // Check for useless control edge in some common special cases
1478     if (in(MemNode::Control) != NULL
1479         &amp;&amp; can_remove_control()
1480         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1481         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1482       // A method-invariant, non-null address (constant or 'this' argument).
1483       set_req(MemNode::Control, NULL);
1484       progress = true;
1485     }
1486   }
1487 
1488   Node* mem = in(MemNode::Memory);
1489   const TypePtr *addr_t = phase-&gt;type(address)-&gt;isa_ptr();
1490 
1491   if (can_reshape &amp;&amp; (addr_t != NULL)) {
1492     // try to optimize our memory input
1493     Node* opt_mem = MemNode::optimize_memory_chain(mem, addr_t, this, phase);
1494     if (opt_mem != mem) {
1495       set_req(MemNode::Memory, opt_mem);
1496       if (phase-&gt;type( opt_mem ) == Type::TOP) return NULL;
1497       return this;
1498     }
1499     const TypeOopPtr *t_oop = addr_t-&gt;isa_oopptr();
1500     if ((t_oop != NULL) &amp;&amp;
1501         (t_oop-&gt;is_known_instance_field() ||
1502          t_oop-&gt;is_ptr_to_boxed_value())) {
1503       PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
1504       if (igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(opt_mem)) {
1505         // Delay this transformation until memory Phi is processed.
1506         phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
1507         return NULL;
1508       }
1509       // Split instance field load through Phi.
1510       Node* result = split_through_phi(phase);
1511       if (result != NULL) return result;
1512 
1513       if (t_oop-&gt;is_ptr_to_boxed_value()) {
1514         Node* result = eliminate_autobox(phase);
1515         if (result != NULL) return result;
1516       }
1517     }
1518   }
1519 
1520   // Is there a dominating load that loads the same value?  Leave
1521   // anything that is not a load of a field/array element (like
1522   // barriers etc.) alone
1523   if (in(0) != NULL &amp;&amp; adr_type() != TypeRawPtr::BOTTOM &amp;&amp; can_reshape) {
1524     for (DUIterator_Fast imax, i = mem-&gt;fast_outs(imax); i &lt; imax; i++) {
1525       Node *use = mem-&gt;fast_out(i);
1526       if (use != this &amp;&amp;
1527           use-&gt;Opcode() == Opcode() &amp;&amp;
1528           use-&gt;in(0) != NULL &amp;&amp;
1529           use-&gt;in(0) != in(0) &amp;&amp;
1530           use-&gt;in(Address) == in(Address)) {
1531         Node* ctl = in(0);
1532         for (int i = 0; i &lt; 10 &amp;&amp; ctl != NULL; i++) {
1533           ctl = IfNode::up_one_dom(ctl);
1534           if (ctl == use-&gt;in(0)) {
1535             set_req(0, use-&gt;in(0));
1536             return this;
1537           }
1538         }
1539       }
1540     }
1541   }
1542 
1543   // Check for prior store with a different base or offset; make Load
1544   // independent.  Skip through any number of them.  Bail out if the stores
1545   // are in an endless dead cycle and report no progress.  This is a key
1546   // transform for Reflection.  However, if after skipping through the Stores
1547   // we can't then fold up against a prior store do NOT do the transform as
1548   // this amounts to using the 'Oracle' model of aliasing.  It leaves the same
1549   // array memory alive twice: once for the hoisted Load and again after the
1550   // bypassed Store.  This situation only works if EVERYBODY who does
1551   // anti-dependence work knows how to bypass.  I.e. we need all
1552   // anti-dependence checks to ask the same Oracle.  Right now, that Oracle is
1553   // the alias index stuff.  So instead, peek through Stores and IFF we can
1554   // fold up, do so.
1555   Node* prev_mem = find_previous_store(phase);
1556   if (prev_mem != NULL) {
1557     Node* value = can_see_arraycopy_value(prev_mem, phase);
1558     if (value != NULL) {
1559       return value;
1560     }
1561   }
1562   // Steps (a), (b):  Walk past independent stores to find an exact match.
1563   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1564     // (c) See if we can fold up on the spot, but don't fold up here.
1565     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1566     // just return a prior value, which is done by Identity calls.
1567     if (can_see_stored_value(prev_mem, phase)) {
1568       // Make ready for step (d):
1569       set_req(MemNode::Memory, prev_mem);
1570       return this;
1571     }
1572   }
1573 
1574   return progress ? this : NULL;
1575 }
1576 
1577 // Helper to recognize certain Klass fields which are invariant across
1578 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1579 const Type*
1580 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1581                                  ciKlass* klass) const {
1582   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1583     // The field is Klass::_modifier_flags.  Return its (constant) value.
1584     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1585     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _modifier_flags");
1586     return TypeInt::make(klass-&gt;modifier_flags());
1587   }
1588   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1589     // The field is Klass::_access_flags.  Return its (constant) value.
1590     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1591     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _access_flags");
1592     return TypeInt::make(klass-&gt;access_flags());
1593   }
1594   if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())) {
1595     // The field is Klass::_layout_helper.  Return its constant value if known.
1596     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _layout_helper");
1597     return TypeInt::make(klass-&gt;layout_helper());
1598   }
1599 
1600   // No match.
1601   return NULL;
1602 }
1603 
1604 // Try to constant-fold a stable array element.
1605 static const Type* fold_stable_ary_elem(const TypeAryPtr* ary, int off, BasicType loadbt) {
1606   assert(ary-&gt;const_oop(), "array should be constant");
1607   assert(ary-&gt;is_stable(), "array should be stable");
1608 
1609   // Decode the results of GraphKit::array_element_address.
1610   ciArray* aobj = ary-&gt;const_oop()-&gt;as_array();
1611   ciConstant con = aobj-&gt;element_value_by_offset(off);
1612 
1613   if (con.basic_type() != T_ILLEGAL &amp;&amp; !con.is_null_or_zero()) {
1614     const Type* con_type = Type::make_from_constant(con);
1615     if (con_type != NULL) {
1616       if (con_type-&gt;isa_aryptr()) {
1617         // Join with the array element type, in case it is also stable.
1618         int dim = ary-&gt;stable_dimension();
1619         con_type = con_type-&gt;is_aryptr()-&gt;cast_to_stable(true, dim-1);
1620       }
1621       if (loadbt == T_NARROWOOP &amp;&amp; con_type-&gt;isa_oopptr()) {
1622         con_type = con_type-&gt;make_narrowoop();
1623       }
1624 #ifndef PRODUCT
1625       if (TraceIterativeGVN) {
1626         tty-&gt;print("FoldStableValues: array element [off=%d]: con_type=", off);
1627         con_type-&gt;dump(); tty-&gt;cr();
1628       }
1629 #endif //PRODUCT
1630       return con_type;
1631     }
1632   }
1633   return NULL;
1634 }
1635 
1636 //------------------------------Value-----------------------------------------
1637 const Type *LoadNode::Value( PhaseTransform *phase ) const {
1638   // Either input is TOP ==&gt; the result is TOP
1639   Node* mem = in(MemNode::Memory);
1640   const Type *t1 = phase-&gt;type(mem);
1641   if (t1 == Type::TOP)  return Type::TOP;
1642   Node* adr = in(MemNode::Address);
1643   const TypePtr* tp = phase-&gt;type(adr)-&gt;isa_ptr();
1644   if (tp == NULL || tp-&gt;empty())  return Type::TOP;
1645   int off = tp-&gt;offset();
1646   assert(off != Type::OffsetTop, "case covered by TypePtr::empty");
1647   Compile* C = phase-&gt;C;
1648 
1649   // Try to guess loaded type from pointer type
1650   if (tp-&gt;isa_aryptr()) {
1651     const TypeAryPtr* ary = tp-&gt;is_aryptr();
1652     const Type* t = ary-&gt;elem();
1653 
1654     // Determine whether the reference is beyond the header or not, by comparing
1655     // the offset against the offset of the start of the array's data.
1656     // Different array types begin at slightly different offsets (12 vs. 16).
1657     // We choose T_BYTE as an example base type that is least restrictive
1658     // as to alignment, which will therefore produce the smallest
1659     // possible base offset.
1660     const int min_base_off = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1661     const bool off_beyond_header = ((uint)off &gt;= (uint)min_base_off);
1662 
1663     // Try to constant-fold a stable array element.
1664     if (FoldStableValues &amp;&amp; ary-&gt;is_stable() &amp;&amp; ary-&gt;const_oop() != NULL) {
1665       // Make sure the reference is not into the header and the offset is constant
1666       if (off_beyond_header &amp;&amp; adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1667         const Type* con_type = fold_stable_ary_elem(ary, off, memory_type());
1668         if (con_type != NULL) {
1669           return con_type;
1670         }
1671       }
1672     }
1673 
1674     // Don't do this for integer types. There is only potential profit if
1675     // the element type t is lower than _type; that is, for int types, if _type is
1676     // more restrictive than t.  This only happens here if one is short and the other
1677     // char (both 16 bits), and in those cases we've made an intentional decision
1678     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1679     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1680     //
1681     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1682     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1683     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1684     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1685     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1686     // In fact, that could have been the original type of p1, and p1 could have
1687     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1688     // expression (LShiftL quux 3) independently optimized to the constant 8.
1689     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1690         &amp;&amp; (_type-&gt;isa_vect() == NULL)
1691         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1692       // t might actually be lower than _type, if _type is a unique
1693       // concrete subclass of abstract class t.
1694       if (off_beyond_header) {  // is the offset beyond the header?
1695         const Type* jt = t-&gt;join_speculative(_type);
1696         // In any case, do not allow the join, per se, to empty out the type.
1697         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1698           // This can happen if a interface-typed array narrows to a class type.
1699           jt = _type;
1700         }
1701 #ifdef ASSERT
1702         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1703           // The pointers in the autobox arrays are always non-null
1704           Node* base = adr-&gt;in(AddPNode::Base);
1705           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1706             // Get LoadN node which loads IntegerCache.cache field
1707             base = base-&gt;in(1);
1708           }
1709           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1710             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1711             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1712               // It could be narrow oop
1713               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,"sanity");
1714             }
1715           }
1716         }
1717 #endif
1718         return jt;
1719       }
1720     }
1721   } else if (tp-&gt;base() == Type::InstPtr) {
1722     ciEnv* env = C-&gt;env();
1723     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1724     ciKlass* klass = tinst-&gt;klass();
1725     assert( off != Type::OffsetBot ||
1726             // arrays can be cast to Objects
1727             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1728             // unsafe field access may not have a constant offset
1729             C-&gt;has_unsafe_access(),
1730             "Field accesses must be precise" );
1731     // For oop loads, we expect the _type to be precise
1732     if (klass == env-&gt;String_klass() &amp;&amp;
1733         adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1734       // For constant Strings treat the final fields as compile time constants.
1735       Node* base = adr-&gt;in(AddPNode::Base);
1736       const TypeOopPtr* t = phase-&gt;type(base)-&gt;isa_oopptr();
1737       if (t != NULL &amp;&amp; t-&gt;singleton()) {
1738         ciField* field = env-&gt;String_klass()-&gt;get_field_by_offset(off, false);
1739         if (field != NULL &amp;&amp; field-&gt;is_final()) {
1740           ciObject* string = t-&gt;const_oop();
1741           ciConstant constant = string-&gt;as_instance()-&gt;field_value(field);
1742           if (constant.basic_type() == T_INT) {
1743             return TypeInt::make(constant.as_int());
1744           } else if (constant.basic_type() == T_ARRAY) {
1745             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1746               return TypeNarrowOop::make_from_constant(constant.as_object(), true);
1747             } else {
1748               return TypeOopPtr::make_from_constant(constant.as_object(), true);
1749             }
1750           }
1751         }
1752       }
1753     }
1754     // Optimizations for constant objects
1755     ciObject* const_oop = tinst-&gt;const_oop();
1756     if (const_oop != NULL) {
1757       // For constant Boxed value treat the target field as a compile time constant.
1758       if (tinst-&gt;is_ptr_to_boxed_value()) {
1759         return tinst-&gt;get_const_boxed_value();
1760       } else
1761       // For constant CallSites treat the target field as a compile time constant.
1762       if (const_oop-&gt;is_call_site()) {
1763         ciCallSite* call_site = const_oop-&gt;as_call_site();
1764         ciField* field = call_site-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_offset(off, /*is_static=*/ false);
1765         if (field != NULL &amp;&amp; field-&gt;is_call_site_target()) {
1766           ciMethodHandle* target = call_site-&gt;get_target();
1767           if (target != NULL) {  // just in case
1768             ciConstant constant(T_OBJECT, target);
1769             const Type* t;
1770             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1771               t = TypeNarrowOop::make_from_constant(constant.as_object(), true);
1772             } else {
1773               t = TypeOopPtr::make_from_constant(constant.as_object(), true);
1774             }
1775             // Add a dependence for invalidation of the optimization.
1776             if (!call_site-&gt;is_constant_call_site()) {
1777               C-&gt;dependencies()-&gt;assert_call_site_target_value(call_site, target);
1778             }
1779             return t;
1780           }
1781         }
1782       }
1783     }
1784   } else if (tp-&gt;base() == Type::KlassPtr) {
1785     assert( off != Type::OffsetBot ||
1786             // arrays can be cast to Objects
1787             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1788             // also allow array-loading from the primary supertype
1789             // array during subtype checks
1790             Opcode() == Op_LoadKlass,
1791             "Field accesses must be precise" );
1792     // For klass/static loads, we expect the _type to be precise
1793   }
1794 
1795   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1796   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1797     ciKlass* klass = tkls-&gt;klass();
1798     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1799       // We are loading a field from a Klass metaobject whose identity
1800       // is known at compile time (the type is "exact" or "precise").
1801       // Check for fields we know are maintained as constants by the VM.
1802       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1803         // The field is Klass::_super_check_offset.  Return its (constant) value.
1804         // (Folds up type checking code.)
1805         assert(Opcode() == Op_LoadI, "must load an int from _super_check_offset");
1806         return TypeInt::make(klass-&gt;super_check_offset());
1807       }
1808       // Compute index into primary_supers array
1809       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1810       // Check for overflowing; use unsigned compare to handle the negative case.
1811       if( depth &lt; ciKlass::primary_super_limit() ) {
1812         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1813         // (Folds up type checking code.)
1814         assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1815         ciKlass *ss = klass-&gt;super_of_depth(depth);
1816         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1817       }
1818       const Type* aift = load_array_final_field(tkls, klass);
1819       if (aift != NULL)  return aift;
1820       if (tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1821         // The field is Klass::_java_mirror.  Return its (constant) value.
1822         // (Folds up the 2nd indirection in anObjConstant.getClass().)
1823         assert(Opcode() == Op_LoadP, "must load an oop from _java_mirror");
1824         return TypeInstPtr::make(klass-&gt;java_mirror());
1825       }
1826     }
1827 
1828     // We can still check if we are loading from the primary_supers array at a
1829     // shallow enough depth.  Even though the klass is not exact, entries less
1830     // than or equal to its super depth are correct.
1831     if (klass-&gt;is_loaded() ) {
1832       ciType *inner = klass;
1833       while( inner-&gt;is_obj_array_klass() )
1834         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1835       if( inner-&gt;is_instance_klass() &amp;&amp;
1836           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1837         // Compute index into primary_supers array
1838         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1839         // Check for overflowing; use unsigned compare to handle the negative case.
1840         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1841             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1842           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1843           // (Folds up type checking code.)
1844           assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1845           ciKlass *ss = klass-&gt;super_of_depth(depth);
1846           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1847         }
1848       }
1849     }
1850 
1851     // If the type is enough to determine that the thing is not an array,
1852     // we can give the layout_helper a positive interval type.
1853     // This will help short-circuit some reflective code.
1854     if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())
1855         &amp;&amp; !klass-&gt;is_array_klass() // not directly typed as an array
1856         &amp;&amp; !klass-&gt;is_interface()  // specifically not Serializable &amp; Cloneable
1857         &amp;&amp; !klass-&gt;is_java_lang_Object()   // not the supertype of all T[]
1858         ) {
1859       // Note:  When interfaces are reliable, we can narrow the interface
1860       // test to (klass != Serializable &amp;&amp; klass != Cloneable).
1861       assert(Opcode() == Op_LoadI, "must load an int from _layout_helper");
1862       jint min_size = Klass::instance_layout_helper(oopDesc::header_size(), false);
1863       // The key property of this type is that it folds up tests
1864       // for array-ness, since it proves that the layout_helper is positive.
1865       // Thus, a generic value like the basic object layout helper works fine.
1866       return TypeInt::make(min_size, max_jint, Type::WidenMin);
1867     }
1868   }
1869 
1870   // If we are loading from a freshly-allocated object, produce a zero,
1871   // if the load is provably beyond the header of the object.
1872   // (Also allow a variable load from a fresh array to produce zero.)
1873   const TypeOopPtr *tinst = tp-&gt;isa_oopptr();
1874   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1875   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1876   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1877     Node* value = can_see_stored_value(mem,phase);
1878     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1879       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),"sanity");
1880       return value-&gt;bottom_type();
1881     }
1882   }
1883 
1884   if (is_instance) {
1885     // If we have an instance type and our memory input is the
1886     // programs's initial memory state, there is no matching store,
1887     // so just return a zero of the appropriate type
1888     Node *mem = in(MemNode::Memory);
1889     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1890       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, "must be memory Parm");
1891       return Type::get_zero_type(_type-&gt;basic_type());
1892     }
1893   }
1894   return _type;
1895 }
1896 
1897 //------------------------------match_edge-------------------------------------
1898 // Do we Match on this edge index or not?  Match only the address.
1899 uint LoadNode::match_edge(uint idx) const {
1900   return idx == MemNode::Address;
1901 }
1902 
1903 //--------------------------LoadBNode::Ideal--------------------------------------
1904 //
1905 //  If the previous store is to the same address as this load,
1906 //  and the value stored was larger than a byte, replace this load
1907 //  with the value stored truncated to a byte.  If no truncation is
1908 //  needed, the replacement is done in LoadNode::Identity().
1909 //
1910 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1911   Node* mem = in(MemNode::Memory);
1912   Node* value = can_see_stored_value(mem,phase);
1913   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
1914     Node *result = phase-&gt;transform( new LShiftINode(value, phase-&gt;intcon(24)) );
1915     return new RShiftINode(result, phase-&gt;intcon(24));
1916   }
1917   // Identity call will handle the case where truncation is not needed.
1918   return LoadNode::Ideal(phase, can_reshape);
1919 }
1920 
1921 const Type* LoadBNode::Value(PhaseTransform *phase) const {
1922   Node* mem = in(MemNode::Memory);
1923   Node* value = can_see_stored_value(mem,phase);
1924   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1925       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1926     // If the input to the store does not fit with the load's result type,
1927     // it must be truncated. We can't delay until Ideal call since
1928     // a singleton Value is needed for split_thru_phi optimization.
1929     int con = value-&gt;get_int();
1930     return TypeInt::make((con &lt;&lt; 24) &gt;&gt; 24);
1931   }
1932   return LoadNode::Value(phase);
1933 }
1934 
1935 //--------------------------LoadUBNode::Ideal-------------------------------------
1936 //
1937 //  If the previous store is to the same address as this load,
1938 //  and the value stored was larger than a byte, replace this load
1939 //  with the value stored truncated to a byte.  If no truncation is
1940 //  needed, the replacement is done in LoadNode::Identity().
1941 //
1942 Node* LoadUBNode::Ideal(PhaseGVN* phase, bool can_reshape) {
1943   Node* mem = in(MemNode::Memory);
1944   Node* value = can_see_stored_value(mem, phase);
1945   if (value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal(_type))
1946     return new AndINode(value, phase-&gt;intcon(0xFF));
1947   // Identity call will handle the case where truncation is not needed.
1948   return LoadNode::Ideal(phase, can_reshape);
1949 }
1950 
1951 const Type* LoadUBNode::Value(PhaseTransform *phase) const {
1952   Node* mem = in(MemNode::Memory);
1953   Node* value = can_see_stored_value(mem,phase);
1954   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1955       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1956     // If the input to the store does not fit with the load's result type,
1957     // it must be truncated. We can't delay until Ideal call since
1958     // a singleton Value is needed for split_thru_phi optimization.
1959     int con = value-&gt;get_int();
1960     return TypeInt::make(con &amp; 0xFF);
1961   }
1962   return LoadNode::Value(phase);
1963 }
1964 
1965 //--------------------------LoadUSNode::Ideal-------------------------------------
1966 //
1967 //  If the previous store is to the same address as this load,
1968 //  and the value stored was larger than a char, replace this load
1969 //  with the value stored truncated to a char.  If no truncation is
1970 //  needed, the replacement is done in LoadNode::Identity().
1971 //
1972 Node *LoadUSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1973   Node* mem = in(MemNode::Memory);
1974   Node* value = can_see_stored_value(mem,phase);
1975   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) )
1976     return new AndINode(value,phase-&gt;intcon(0xFFFF));
1977   // Identity call will handle the case where truncation is not needed.
1978   return LoadNode::Ideal(phase, can_reshape);
1979 }
1980 
1981 const Type* LoadUSNode::Value(PhaseTransform *phase) const {
1982   Node* mem = in(MemNode::Memory);
1983   Node* value = can_see_stored_value(mem,phase);
1984   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1985       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1986     // If the input to the store does not fit with the load's result type,
1987     // it must be truncated. We can't delay until Ideal call since
1988     // a singleton Value is needed for split_thru_phi optimization.
1989     int con = value-&gt;get_int();
1990     return TypeInt::make(con &amp; 0xFFFF);
1991   }
1992   return LoadNode::Value(phase);
1993 }
1994 
1995 //--------------------------LoadSNode::Ideal--------------------------------------
1996 //
1997 //  If the previous store is to the same address as this load,
1998 //  and the value stored was larger than a short, replace this load
1999 //  with the value stored truncated to a short.  If no truncation is
2000 //  needed, the replacement is done in LoadNode::Identity().
2001 //
2002 Node *LoadSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2003   Node* mem = in(MemNode::Memory);
2004   Node* value = can_see_stored_value(mem,phase);
2005   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
2006     Node *result = phase-&gt;transform( new LShiftINode(value, phase-&gt;intcon(16)) );
2007     return new RShiftINode(result, phase-&gt;intcon(16));
2008   }
2009   // Identity call will handle the case where truncation is not needed.
2010   return LoadNode::Ideal(phase, can_reshape);
2011 }
2012 
2013 const Type* LoadSNode::Value(PhaseTransform *phase) const {
2014   Node* mem = in(MemNode::Memory);
2015   Node* value = can_see_stored_value(mem,phase);
2016   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2017       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2018     // If the input to the store does not fit with the load's result type,
2019     // it must be truncated. We can't delay until Ideal call since
2020     // a singleton Value is needed for split_thru_phi optimization.
2021     int con = value-&gt;get_int();
2022     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2023   }
2024   return LoadNode::Value(phase);
2025 }
2026 
2027 //=============================================================================
2028 //----------------------------LoadKlassNode::make------------------------------
2029 // Polymorphic factory method:
2030 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {
2031   // sanity check the alias category against the created node type
2032   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2033   assert(adr_type != NULL, "expecting TypeKlassPtr");
2034 #ifdef _LP64
2035   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2036     assert(UseCompressedClassPointers, "no compressed klasses");
2037     Node* load_klass = gvn.transform(new LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2038     return new DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2039   }
2040 #endif
2041   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), "should have got back a narrow oop");
2042   return new LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2043 }
2044 
2045 //------------------------------Value------------------------------------------
2046 const Type *LoadKlassNode::Value( PhaseTransform *phase ) const {
2047   return klass_value_common(phase);
2048 }
2049 
2050 // In most cases, LoadKlassNode does not have the control input set. If the control
2051 // input is set, it must not be removed (by LoadNode::Ideal()).
2052 bool LoadKlassNode::can_remove_control() const {
2053   return false;
2054 }
2055 
2056 const Type *LoadNode::klass_value_common( PhaseTransform *phase ) const {
2057   // Either input is TOP ==&gt; the result is TOP
2058   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2059   if (t1 == Type::TOP)  return Type::TOP;
2060   Node *adr = in(MemNode::Address);
2061   const Type *t2 = phase-&gt;type( adr );
2062   if (t2 == Type::TOP)  return Type::TOP;
2063   const TypePtr *tp = t2-&gt;is_ptr();
2064   if (TypePtr::above_centerline(tp-&gt;ptr()) ||
2065       tp-&gt;ptr() == TypePtr::Null)  return Type::TOP;
2066 
2067   // Return a more precise klass, if possible
2068   const TypeInstPtr *tinst = tp-&gt;isa_instptr();
2069   if (tinst != NULL) {
2070     ciInstanceKlass* ik = tinst-&gt;klass()-&gt;as_instance_klass();
2071     int offset = tinst-&gt;offset();
2072     if (ik == phase-&gt;C-&gt;env()-&gt;Class_klass()
2073         &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2074             offset == java_lang_Class::array_klass_offset_in_bytes())) {
2075       // We are loading a special hidden field from a Class mirror object,
2076       // the field which points to the VM's Klass metaobject.
2077       ciType* t = tinst-&gt;java_mirror_type();
2078       // java_mirror_type returns non-null for compile-time Class constants.
2079       if (t != NULL) {
2080         // constant oop =&gt; constant klass
2081         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2082           if (t-&gt;is_void()) {
2083             // We cannot create a void array.  Since void is a primitive type return null
2084             // klass.  Users of this result need to do a null check on the returned klass.
2085             return TypePtr::NULL_PTR;
2086           }
2087           return TypeKlassPtr::make(ciArrayKlass::make(t));
2088         }
2089         if (!t-&gt;is_klass()) {
2090           // a primitive Class (e.g., int.class) has NULL for a klass field
2091           return TypePtr::NULL_PTR;
2092         }
2093         // (Folds up the 1st indirection in aClassConstant.getModifiers().)
2094         return TypeKlassPtr::make(t-&gt;as_klass());
2095       }
2096       // non-constant mirror, so we can't tell what's going on
2097     }
2098     if( !ik-&gt;is_loaded() )
2099       return _type;             // Bail out if not loaded
2100     if (offset == oopDesc::klass_offset_in_bytes()) {
2101       if (tinst-&gt;klass_is_exact()) {
2102         return TypeKlassPtr::make(ik);
2103       }
2104       // See if we can become precise: no subklasses and no interface
2105       // (Note:  We need to support verified interfaces.)
2106       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2107         //assert(!UseExactTypes, "this code should be useless with exact types");
2108         // Add a dependence; if any subclass added we need to recompile
2109         if (!ik-&gt;is_final()) {
2110           // %%% should use stronger assert_unique_concrete_subtype instead
2111           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2112         }
2113         // Return precise klass
2114         return TypeKlassPtr::make(ik);
2115       }
2116 
2117       // Return root of possible klass
2118       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);
2119     }
2120   }
2121 
2122   // Check for loading klass from an array
2123   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
2124   if( tary != NULL ) {
2125     ciKlass *tary_klass = tary-&gt;klass();
2126     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2127         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2128       if (tary-&gt;klass_is_exact()) {
2129         return TypeKlassPtr::make(tary_klass);
2130       }
2131       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();
2132       // If the klass is an object array, we defer the question to the
2133       // array component klass.
2134       if( ak-&gt;is_obj_array_klass() ) {
2135         assert( ak-&gt;is_loaded(), "" );
2136         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
2137         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {
2138           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();
2139           // See if we can become precise: no subklasses and no interface
2140           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2141             //assert(!UseExactTypes, "this code should be useless with exact types");
2142             // Add a dependence; if any subclass added we need to recompile
2143             if (!ik-&gt;is_final()) {
2144               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2145             }
2146             // Return precise array klass
2147             return TypeKlassPtr::make(ak);
2148           }
2149         }
2150         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
2151       } else {                  // Found a type-array?
2152         //assert(!UseExactTypes, "this code should be useless with exact types");
2153         assert( ak-&gt;is_type_array_klass(), "" );
2154         return TypeKlassPtr::make(ak); // These are always precise
2155       }
2156     }
2157   }
2158 
2159   // Check for loading klass from an array klass
2160   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2161   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2162     ciKlass* klass = tkls-&gt;klass();
2163     if( !klass-&gt;is_loaded() )
2164       return _type;             // Bail out if not loaded
2165     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2166         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2167       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2168       // // Always returning precise element type is incorrect,
2169       // // e.g., element type could be object and array may contain strings
2170       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2171 
2172       // The array's TypeKlassPtr was declared 'precise' or 'not precise'
2173       // according to the element type's subclassing.
2174       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);
2175     }
2176     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2177         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2178       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2179       // The field is Klass::_super.  Return its (constant) value.
2180       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2181       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2182     }
2183   }
2184 
2185   // Bailout case
2186   return LoadNode::Value(phase);
2187 }
2188 
2189 //------------------------------Identity---------------------------------------
2190 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2191 // Also feed through the klass in Allocate(...klass...)._klass.
2192 Node* LoadKlassNode::Identity( PhaseTransform *phase ) {
2193   return klass_identity_common(phase);
2194 }
2195 
2196 Node* LoadNode::klass_identity_common(PhaseTransform *phase ) {
2197   Node* x = LoadNode::Identity(phase);
2198   if (x != this)  return x;
2199 
2200   // Take apart the address into an oop and and offset.
2201   // Return 'this' if we cannot.
2202   Node*    adr    = in(MemNode::Address);
2203   intptr_t offset = 0;
2204   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2205   if (base == NULL)     return this;
2206   const TypeOopPtr* toop = phase-&gt;type(adr)-&gt;isa_oopptr();
2207   if (toop == NULL)     return this;
2208 
2209   // We can fetch the klass directly through an AllocateNode.
2210   // This works even if the klass is not constant (clone or newArray).
2211   if (offset == oopDesc::klass_offset_in_bytes()) {
2212     Node* allocated_klass = AllocateNode::Ideal_klass(base, phase);
2213     if (allocated_klass != NULL) {
2214       return allocated_klass;
2215     }
2216   }
2217 
2218   // Simplify k.java_mirror.as_klass to plain k, where k is a Klass*.
2219   // See inline_native_Class_query for occurrences of these patterns.
2220   // Java Example:  x.getClass().isAssignableFrom(y)
2221   //
2222   // This improves reflective code, often making the Class
2223   // mirror go completely dead.  (Current exception:  Class
2224   // mirrors may appear in debug info, but we could clean them out by
2225   // introducing a new debug info operator for Klass*.java_mirror).
2226   if (toop-&gt;isa_instptr() &amp;&amp; toop-&gt;klass() == phase-&gt;C-&gt;env()-&gt;Class_klass()
2227       &amp;&amp; offset == java_lang_Class::klass_offset_in_bytes()) {
2228     // We are loading a special hidden field from a Class mirror,
2229     // the field which points to its Klass or ArrayKlass metaobject.
2230     if (base-&gt;is_Load()) {
2231       Node* adr2 = base-&gt;in(MemNode::Address);
2232       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
2233       if (tkls != NULL &amp;&amp; !tkls-&gt;empty()
2234           &amp;&amp; (tkls-&gt;klass()-&gt;is_instance_klass() ||
2235               tkls-&gt;klass()-&gt;is_array_klass())
2236           &amp;&amp; adr2-&gt;is_AddP()
2237           ) {
2238         int mirror_field = in_bytes(Klass::java_mirror_offset());
2239         if (tkls-&gt;offset() == mirror_field) {
2240           return adr2-&gt;in(AddPNode::Base);
2241         }
2242       }
2243     }
2244   }
2245 
2246   return this;
2247 }
2248 
2249 
2250 //------------------------------Value------------------------------------------
2251 const Type *LoadNKlassNode::Value( PhaseTransform *phase ) const {
2252   const Type *t = klass_value_common(phase);
2253   if (t == Type::TOP)
2254     return t;
2255 
2256   return t-&gt;make_narrowklass();
2257 }
2258 
2259 //------------------------------Identity---------------------------------------
2260 // To clean up reflective code, simplify k.java_mirror.as_klass to narrow k.
2261 // Also feed through the klass in Allocate(...klass...)._klass.
2262 Node* LoadNKlassNode::Identity( PhaseTransform *phase ) {
2263   Node *x = klass_identity_common(phase);
2264 
2265   const Type *t = phase-&gt;type( x );
2266   if( t == Type::TOP ) return x;
2267   if( t-&gt;isa_narrowklass()) return x;
2268   assert (!t-&gt;isa_narrowoop(), "no narrow oop here");
2269 
2270   return phase-&gt;transform(new EncodePKlassNode(x, t-&gt;make_narrowklass()));
2271 }
2272 
2273 //------------------------------Value-----------------------------------------
2274 const Type *LoadRangeNode::Value( PhaseTransform *phase ) const {
2275   // Either input is TOP ==&gt; the result is TOP
2276   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2277   if( t1 == Type::TOP ) return Type::TOP;
2278   Node *adr = in(MemNode::Address);
2279   const Type *t2 = phase-&gt;type( adr );
2280   if( t2 == Type::TOP ) return Type::TOP;
2281   const TypePtr *tp = t2-&gt;is_ptr();
2282   if (TypePtr::above_centerline(tp-&gt;ptr()))  return Type::TOP;
2283   const TypeAryPtr *tap = tp-&gt;isa_aryptr();
2284   if( !tap ) return _type;
2285   return tap-&gt;size();
2286 }
2287 
2288 //-------------------------------Ideal---------------------------------------
2289 // Feed through the length in AllocateArray(...length...)._length.
2290 Node *LoadRangeNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2291   Node* p = MemNode::Ideal_common(phase, can_reshape);
2292   if (p)  return (p == NodeSentinel) ? NULL : p;
2293 
2294   // Take apart the address into an oop and and offset.
2295   // Return 'this' if we cannot.
2296   Node*    adr    = in(MemNode::Address);
2297   intptr_t offset = 0;
2298   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase,  offset);
2299   if (base == NULL)     return NULL;
2300   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2301   if (tary == NULL)     return NULL;
2302 
2303   // We can fetch the length directly through an AllocateArrayNode.
2304   // This works even if the length is not constant (clone or newArray).
2305   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2306     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2307     if (alloc != NULL) {
2308       Node* allocated_length = alloc-&gt;Ideal_length();
2309       Node* len = alloc-&gt;make_ideal_length(tary, phase);
2310       if (allocated_length != len) {
2311         // New CastII improves on this.
2312         return len;
2313       }
2314     }
2315   }
2316 
2317   return NULL;
2318 }
2319 
2320 //------------------------------Identity---------------------------------------
2321 // Feed through the length in AllocateArray(...length...)._length.
2322 Node* LoadRangeNode::Identity( PhaseTransform *phase ) {
2323   Node* x = LoadINode::Identity(phase);
2324   if (x != this)  return x;
2325 
2326   // Take apart the address into an oop and and offset.
2327   // Return 'this' if we cannot.
2328   Node*    adr    = in(MemNode::Address);
2329   intptr_t offset = 0;
2330   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2331   if (base == NULL)     return this;
2332   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2333   if (tary == NULL)     return this;
2334 
2335   // We can fetch the length directly through an AllocateArrayNode.
2336   // This works even if the length is not constant (clone or newArray).
2337   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2338     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2339     if (alloc != NULL) {
2340       Node* allocated_length = alloc-&gt;Ideal_length();
2341       // Do not allow make_ideal_length to allocate a CastII node.
2342       Node* len = alloc-&gt;make_ideal_length(tary, phase, false);
2343       if (allocated_length == len) {
2344         // Return allocated_length only if it would not be improved by a CastII.
2345         return allocated_length;
2346       }
2347     }
2348   }
2349 
2350   return this;
2351 
2352 }
2353 
2354 //=============================================================================
2355 //---------------------------StoreNode::make-----------------------------------
2356 // Polymorphic factory method:
2357 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2358   assert((mo == unordered || mo == release), "unexpected");
2359   Compile* C = gvn.C;
2360   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2361          ctl != NULL, "raw memory operations should have control edge");
2362 
2363   switch (bt) {
2364   case T_BOOLEAN:
2365   case T_BYTE:    return new StoreBNode(ctl, mem, adr, adr_type, val, mo);
2366   case T_INT:     return new StoreINode(ctl, mem, adr, adr_type, val, mo);
2367   case T_CHAR:
2368   case T_SHORT:   return new StoreCNode(ctl, mem, adr, adr_type, val, mo);
2369   case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo);
2370   case T_FLOAT:   return new StoreFNode(ctl, mem, adr, adr_type, val, mo);
2371   case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo);
2372   case T_METADATA:
2373   case T_ADDRESS:
2374   case T_OBJECT:
2375 #ifdef _LP64
2376     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2377       val = gvn.transform(new EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2378       return new StoreNNode(ctl, mem, adr, adr_type, val, mo);
2379     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2380                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2381                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2382       val = gvn.transform(new EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2383       return new StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2384     }
2385 #endif
2386     {
2387       return new StorePNode(ctl, mem, adr, adr_type, val, mo);
2388     }
2389   }
2390   ShouldNotReachHere();
2391   return (StoreNode*)NULL;
2392 }
2393 
2394 StoreLNode* StoreLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {
2395   bool require_atomic = true;
2396   return new StoreLNode(ctl, mem, adr, adr_type, val, mo, require_atomic);
2397 }
2398 
2399 StoreDNode* StoreDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {
2400   bool require_atomic = true;
2401   return new StoreDNode(ctl, mem, adr, adr_type, val, mo, require_atomic);
2402 }
2403 
2404 
2405 //--------------------------bottom_type----------------------------------------
2406 const Type *StoreNode::bottom_type() const {
2407   return Type::MEMORY;
2408 }
2409 
2410 //------------------------------hash-------------------------------------------
2411 uint StoreNode::hash() const {
2412   // unroll addition of interesting fields
2413   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2414 
2415   // Since they are not commoned, do not hash them:
2416   return NO_HASH;
2417 }
2418 
2419 //------------------------------Ideal------------------------------------------
2420 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2421 // When a store immediately follows a relevant allocation/initialization,
2422 // try to capture it into the initialization, or hoist it above.
2423 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2424   Node* p = MemNode::Ideal_common(phase, can_reshape);
2425   if (p)  return (p == NodeSentinel) ? NULL : p;
2426 
2427   Node* mem     = in(MemNode::Memory);
2428   Node* address = in(MemNode::Address);
2429 
2430   // Back-to-back stores to same address?  Fold em up.  Generally
2431   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2432   // since they must follow each StoreP operation.  Redundant StoreCMs
2433   // are eliminated just before matching in final_graph_reshape.
2434   if (mem-&gt;is_Store() &amp;&amp; mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2435       mem-&gt;Opcode() != Op_StoreCM) {
2436     // Looking at a dead closed cycle of memory?
2437     assert(mem != mem-&gt;in(MemNode::Memory), "dead loop in StoreNode::Ideal");
2438 
2439     assert(Opcode() == mem-&gt;Opcode() ||
2440            phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw,
2441            "no mismatched stores, except on raw memory");
2442 
2443     if (mem-&gt;outcnt() == 1 &amp;&amp;           // check for intervening uses
2444         mem-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2445       // If anybody other than 'this' uses 'mem', we cannot fold 'mem' away.
2446       // For example, 'mem' might be the final state at a conditional return.
2447       // Or, 'mem' might be used by some node which is live at the same time
2448       // 'this' is live, which might be unschedulable.  So, require exactly
2449       // ONE user, the 'this' store, until such time as we clone 'mem' for
2450       // each of 'mem's uses (thus making the exactly-1-user-rule hold true).
2451       if (can_reshape) {  // (%%% is this an anachronism?)
2452         set_req_X(MemNode::Memory, mem-&gt;in(MemNode::Memory),
2453                   phase-&gt;is_IterGVN());
2454       } else {
2455         // It's OK to do this in the parser, since DU info is always accurate,
2456         // and the parser always refers to nodes via SafePointNode maps.
2457         set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2458       }
2459       return this;
2460     }
2461   }
2462 
2463   // Capture an unaliased, unconditional, simple store into an initializer.
2464   // Or, if it is independent of the allocation, hoist it above the allocation.
2465   if (ReduceFieldZeroing &amp;&amp; /*can_reshape &amp;&amp;*/
2466       mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
2467     InitializeNode* init = mem-&gt;in(0)-&gt;as_Initialize();
2468     intptr_t offset = init-&gt;can_capture_store(this, phase, can_reshape);
2469     if (offset &gt; 0) {
2470       Node* moved = init-&gt;capture_store(this, offset, phase, can_reshape);
2471       // If the InitializeNode captured me, it made a raw copy of me,
2472       // and I need to disappear.
2473       if (moved != NULL) {
2474         // %%% hack to ensure that Ideal returns a new node:
2475         mem = MergeMemNode::make(mem);
2476         return mem;             // fold me away
2477       }
2478     }
2479   }
2480 
2481   return NULL;                  // No further progress
2482 }
2483 
2484 //------------------------------Value-----------------------------------------
2485 const Type *StoreNode::Value( PhaseTransform *phase ) const {
2486   // Either input is TOP ==&gt; the result is TOP
2487   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2488   if( t1 == Type::TOP ) return Type::TOP;
2489   const Type *t2 = phase-&gt;type( in(MemNode::Address) );
2490   if( t2 == Type::TOP ) return Type::TOP;
2491   const Type *t3 = phase-&gt;type( in(MemNode::ValueIn) );
2492   if( t3 == Type::TOP ) return Type::TOP;
2493   return Type::MEMORY;
2494 }
2495 
2496 //------------------------------Identity---------------------------------------
2497 // Remove redundant stores:
2498 //   Store(m, p, Load(m, p)) changes to m.
2499 //   Store(, p, x) -&gt; Store(m, p, x) changes to Store(m, p, x).
2500 Node *StoreNode::Identity( PhaseTransform *phase ) {
2501   Node* mem = in(MemNode::Memory);
2502   Node* adr = in(MemNode::Address);
2503   Node* val = in(MemNode::ValueIn);
2504 
2505   // Load then Store?  Then the Store is useless
2506   if (val-&gt;is_Load() &amp;&amp;
2507       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2508       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2509       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2510     return mem;
2511   }
2512 
2513   // Two stores in a row of the same value?
2514   if (mem-&gt;is_Store() &amp;&amp;
2515       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2516       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2517       mem-&gt;Opcode() == Opcode()) {
2518     return mem;
2519   }
2520 
2521   // Store of zero anywhere into a freshly-allocated object?
2522   // Then the store is useless.
2523   // (It must already have been captured by the InitializeNode.)
2524   if (ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {
2525     // a newly allocated object is already all-zeroes everywhere
2526     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {
2527       return mem;
2528     }
2529 
2530     // the store may also apply to zero-bits in an earlier object
2531     Node* prev_mem = find_previous_store(phase);
2532     // Steps (a), (b):  Walk past independent stores to find an exact match.
2533     if (prev_mem != NULL) {
2534       Node* prev_val = can_see_stored_value(prev_mem, phase);
2535       if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2536         // prev_val and val might differ by a cast; it would be good
2537         // to keep the more informative of the two.
2538         return mem;
2539       }
2540     }
2541   }
2542 
2543   return this;
2544 }
2545 
2546 //------------------------------match_edge-------------------------------------
2547 // Do we Match on this edge index or not?  Match only memory &amp; value
2548 uint StoreNode::match_edge(uint idx) const {
2549   return idx == MemNode::Address || idx == MemNode::ValueIn;
2550 }
2551 
2552 //------------------------------cmp--------------------------------------------
2553 // Do not common stores up together.  They generally have to be split
2554 // back up anyways, so do not bother.
2555 uint StoreNode::cmp( const Node &amp;n ) const {
2556   return (&amp;n == this);          // Always fail except on self
2557 }
2558 
2559 //------------------------------Ideal_masked_input-----------------------------
2560 // Check for a useless mask before a partial-word store
2561 // (StoreB ... (AndI valIn conIa) )
2562 // If (conIa &amp; mask == mask) this simplifies to
2563 // (StoreB ... (valIn) )
2564 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2565   Node *val = in(MemNode::ValueIn);
2566   if( val-&gt;Opcode() == Op_AndI ) {
2567     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2568     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2569       set_req(MemNode::ValueIn, val-&gt;in(1));
2570       return this;
2571     }
2572   }
2573   return NULL;
2574 }
2575 
2576 
2577 //------------------------------Ideal_sign_extended_input----------------------
2578 // Check for useless sign-extension before a partial-word store
2579 // (StoreB ... (RShiftI _ (LShiftI _ valIn conIL ) conIR) )
2580 // If (conIL == conIR &amp;&amp; conIR &lt;= num_bits)  this simplifies to
2581 // (StoreB ... (valIn) )
2582 Node *StoreNode::Ideal_sign_extended_input(PhaseGVN *phase, int num_bits) {
2583   Node *val = in(MemNode::ValueIn);
2584   if( val-&gt;Opcode() == Op_RShiftI ) {
2585     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2586     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &lt;= num_bits) ) {
2587       Node *shl = val-&gt;in(1);
2588       if( shl-&gt;Opcode() == Op_LShiftI ) {
2589         const TypeInt *t2 = phase-&gt;type( shl-&gt;in(2) )-&gt;isa_int();
2590         if( t2 &amp;&amp; t2-&gt;is_con() &amp;&amp; (t2-&gt;get_con() == t-&gt;get_con()) ) {
2591           set_req(MemNode::ValueIn, shl-&gt;in(1));
2592           return this;
2593         }
2594       }
2595     }
2596   }
2597   return NULL;
2598 }
2599 
2600 //------------------------------value_never_loaded-----------------------------------
2601 // Determine whether there are any possible loads of the value stored.
2602 // For simplicity, we actually check if there are any loads from the
2603 // address stored to, not just for loads of the value stored by this node.
2604 //
2605 bool StoreNode::value_never_loaded( PhaseTransform *phase) const {
2606   Node *adr = in(Address);
2607   const TypeOopPtr *adr_oop = phase-&gt;type(adr)-&gt;isa_oopptr();
2608   if (adr_oop == NULL)
2609     return false;
2610   if (!adr_oop-&gt;is_known_instance_field())
2611     return false; // if not a distinct instance, there may be aliases of the address
2612   for (DUIterator_Fast imax, i = adr-&gt;fast_outs(imax); i &lt; imax; i++) {
2613     Node *use = adr-&gt;fast_out(i);
2614     if (use-&gt;is_Load() || use-&gt;is_LoadStore()) {
2615       return false;
2616     }
2617   }
2618   return true;
2619 }
2620 
2621 //=============================================================================
2622 //------------------------------Ideal------------------------------------------
2623 // If the store is from an AND mask that leaves the low bits untouched, then
2624 // we can skip the AND operation.  If the store is from a sign-extension
2625 // (a left shift, then right shift) we can skip both.
2626 Node *StoreBNode::Ideal(PhaseGVN *phase, bool can_reshape){
2627   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFF);
2628   if( progress != NULL ) return progress;
2629 
2630   progress = StoreNode::Ideal_sign_extended_input(phase, 24);
2631   if( progress != NULL ) return progress;
2632 
2633   // Finally check the default case
2634   return StoreNode::Ideal(phase, can_reshape);
2635 }
2636 
2637 //=============================================================================
2638 //------------------------------Ideal------------------------------------------
2639 // If the store is from an AND mask that leaves the low bits untouched, then
2640 // we can skip the AND operation
2641 Node *StoreCNode::Ideal(PhaseGVN *phase, bool can_reshape){
2642   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFFFF);
2643   if( progress != NULL ) return progress;
2644 
2645   progress = StoreNode::Ideal_sign_extended_input(phase, 16);
2646   if( progress != NULL ) return progress;
2647 
2648   // Finally check the default case
2649   return StoreNode::Ideal(phase, can_reshape);
2650 }
2651 
2652 //=============================================================================
2653 //------------------------------Identity---------------------------------------
2654 Node *StoreCMNode::Identity( PhaseTransform *phase ) {
2655   // No need to card mark when storing a null ptr
2656   Node* my_store = in(MemNode::OopStore);
2657   if (my_store-&gt;is_Store()) {
2658     const Type *t1 = phase-&gt;type( my_store-&gt;in(MemNode::ValueIn) );
2659     if( t1 == TypePtr::NULL_PTR ) {
2660       return in(MemNode::Memory);
2661     }
2662   }
2663   return this;
2664 }
2665 
2666 //=============================================================================
2667 //------------------------------Ideal---------------------------------------
2668 Node *StoreCMNode::Ideal(PhaseGVN *phase, bool can_reshape){
2669   Node* progress = StoreNode::Ideal(phase, can_reshape);
2670   if (progress != NULL) return progress;
2671 
2672   Node* my_store = in(MemNode::OopStore);
2673   if (my_store-&gt;is_MergeMem()) {
2674     Node* mem = my_store-&gt;as_MergeMem()-&gt;memory_at(oop_alias_idx());
2675     set_req(MemNode::OopStore, mem);
2676     return this;
2677   }
2678 
2679   return NULL;
2680 }
2681 
2682 //------------------------------Value-----------------------------------------
2683 const Type *StoreCMNode::Value( PhaseTransform *phase ) const {
2684   // Either input is TOP ==&gt; the result is TOP
2685   const Type *t = phase-&gt;type( in(MemNode::Memory) );
2686   if( t == Type::TOP ) return Type::TOP;
2687   t = phase-&gt;type( in(MemNode::Address) );
2688   if( t == Type::TOP ) return Type::TOP;
2689   t = phase-&gt;type( in(MemNode::ValueIn) );
2690   if( t == Type::TOP ) return Type::TOP;
2691   // If extra input is TOP ==&gt; the result is TOP
2692   t = phase-&gt;type( in(MemNode::OopStore) );
2693   if( t == Type::TOP ) return Type::TOP;
2694 
2695   return StoreNode::Value( phase );
2696 }
2697 
2698 
2699 //=============================================================================
2700 //----------------------------------SCMemProjNode------------------------------
2701 const Type * SCMemProjNode::Value( PhaseTransform *phase ) const
2702 {
2703   return bottom_type();
2704 }
2705 
2706 //=============================================================================
2707 //----------------------------------LoadStoreNode------------------------------
2708 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2709   : Node(required),
2710     _type(rt),
2711     _adr_type(at)
2712 {
2713   init_req(MemNode::Control, c  );
2714   init_req(MemNode::Memory , mem);
2715   init_req(MemNode::Address, adr);
2716   init_req(MemNode::ValueIn, val);
2717   init_class_id(Class_LoadStore);
2718 }
2719 
2720 uint LoadStoreNode::ideal_reg() const {
2721   return _type-&gt;ideal_reg();
2722 }
2723 
2724 bool LoadStoreNode::result_not_used() const {
2725   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2726     Node *x = fast_out(i);
2727     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2728     return false;
2729   }
2730   return true;
2731 }
2732 
2733 uint LoadStoreNode::size_of() const { return sizeof(*this); }
2734 
2735 //=============================================================================
2736 //----------------------------------LoadStoreConditionalNode--------------------
2737 LoadStoreConditionalNode::LoadStoreConditionalNode( Node *c, Node *mem, Node *adr, Node *val, Node *ex ) : LoadStoreNode(c, mem, adr, val, NULL, TypeInt::BOOL, 5) {
2738   init_req(ExpectedIn, ex );
2739 }
2740 
2741 //=============================================================================
2742 //-------------------------------adr_type--------------------------------------
2743 const TypePtr* ClearArrayNode::adr_type() const {
2744   Node *adr = in(3);
2745   if (adr == NULL)  return NULL; // node is dead
2746   return MemNode::calculate_adr_type(adr-&gt;bottom_type());
2747 }
2748 
2749 //------------------------------match_edge-------------------------------------
2750 // Do we Match on this edge index or not?  Do not match memory
2751 uint ClearArrayNode::match_edge(uint idx) const {
2752   return idx &gt; 1;
2753 }
2754 
2755 //------------------------------Identity---------------------------------------
2756 // Clearing a zero length array does nothing
2757 Node *ClearArrayNode::Identity( PhaseTransform *phase ) {
2758   return phase-&gt;type(in(2))-&gt;higher_equal(TypeX::ZERO)  ? in(1) : this;
2759 }
2760 
2761 //------------------------------Idealize---------------------------------------
2762 // Clearing a short array is faster with stores
2763 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape){
2764   const int unit = BytesPerLong;
2765   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2766   if (!t)  return NULL;
2767   if (!t-&gt;is_con())  return NULL;
2768   intptr_t raw_count = t-&gt;get_con();
2769   intptr_t size = raw_count;
2770   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2771   // Clearing nothing uses the Identity call.
2772   // Negative clears are possible on dead ClearArrays
2773   // (see jck test stmt114.stmt11402.val).
2774   if (size &lt;= 0 || size % unit != 0)  return NULL;
2775   intptr_t count = size / unit;
2776   // Length too long; use fast hardware clear
2777   if (size &gt; Matcher::init_array_short_size)  return NULL;
2778   Node *mem = in(1);
2779   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2780   Node *adr = in(3);
2781   const Type* at = phase-&gt;type(adr);
2782   if( at==Type::TOP ) return NULL;
2783   const TypePtr* atp = at-&gt;isa_ptr();
2784   // adjust atp to be the correct array element address type
2785   if (atp == NULL)  atp = TypePtr::BOTTOM;
2786   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2787   // Get base for derived pointer purposes
2788   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2789   Node *base = adr-&gt;in(1);
2790 
2791   Node *zero = phase-&gt;makecon(TypeLong::ZERO);
2792   Node *off  = phase-&gt;MakeConX(BytesPerLong);
2793   mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2794   count--;
2795   while( count-- ) {
2796     mem = phase-&gt;transform(mem);
2797     adr = phase-&gt;transform(new AddPNode(base,adr,off));
2798     mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2799   }
2800   return mem;
2801 }
2802 
2803 //----------------------------step_through----------------------------------
2804 // Return allocation input memory edge if it is different instance
2805 // or itself if it is the one we are looking for.
2806 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2807   Node* n = *np;
2808   assert(n-&gt;is_ClearArray(), "sanity");
2809   intptr_t offset;
2810   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2811   // This method is called only before Allocate nodes are expanded
2812   // during macro nodes expansion. Before that ClearArray nodes are
2813   // only generated in PhaseMacroExpand::generate_arraycopy() (before
2814   // Allocate nodes are expanded) which follows allocations.
2815   assert(alloc != NULL, "should have allocation");
2816   if (alloc-&gt;_idx == instance_id) {
2817     // Can not bypass initialization of the instance we are looking for.
2818     return false;
2819   }
2820   // Otherwise skip it.
2821   InitializeNode* init = alloc-&gt;initialization();
2822   if (init != NULL)
2823     *np = init-&gt;in(TypeFunc::Memory);
2824   else
2825     *np = alloc-&gt;in(TypeFunc::Memory);
2826   return true;
2827 }
2828 
2829 //----------------------------clear_memory-------------------------------------
2830 // Generate code to initialize object storage to zero.
2831 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2832                                    intptr_t start_offset,
2833                                    Node* end_offset,
2834                                    PhaseGVN* phase) {
2835   intptr_t offset = start_offset;
2836 
2837   int unit = BytesPerLong;
2838   if ((offset % unit) != 0) {
2839     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(offset));
2840     adr = phase-&gt;transform(adr);
2841     const TypePtr* atp = TypeRawPtr::BOTTOM;
2842     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2843     mem = phase-&gt;transform(mem);
2844     offset += BytesPerInt;
2845   }
2846   assert((offset % unit) == 0, "");
2847 
2848   // Initialize the remaining stuff, if any, with a ClearArray.
2849   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);
2850 }
2851 
2852 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2853                                    Node* start_offset,
2854                                    Node* end_offset,
2855                                    PhaseGVN* phase) {
2856   if (start_offset == end_offset) {
2857     // nothing to do
2858     return mem;
2859   }
2860 
2861   int unit = BytesPerLong;
2862   Node* zbase = start_offset;
2863   Node* zend  = end_offset;
2864 
2865   // Scale to the unit required by the CPU:
2866   if (!Matcher::init_array_count_is_in_bytes) {
2867     Node* shift = phase-&gt;intcon(exact_log2(unit));
2868     zbase = phase-&gt;transform(new URShiftXNode(zbase, shift) );
2869     zend  = phase-&gt;transform(new URShiftXNode(zend,  shift) );
2870   }
2871 
2872   // Bulk clear double-words
2873   Node* zsize = phase-&gt;transform(new SubXNode(zend, zbase) );
2874   Node* adr = phase-&gt;transform(new AddPNode(dest, dest, start_offset) );
2875   mem = new ClearArrayNode(ctl, mem, zsize, adr);
2876   return phase-&gt;transform(mem);
2877 }
2878 
2879 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2880                                    intptr_t start_offset,
2881                                    intptr_t end_offset,
2882                                    PhaseGVN* phase) {
2883   if (start_offset == end_offset) {
2884     // nothing to do
2885     return mem;
2886   }
2887 
2888   assert((end_offset % BytesPerInt) == 0, "odd end offset");
2889   intptr_t done_offset = end_offset;
2890   if ((done_offset % BytesPerLong) != 0) {
2891     done_offset -= BytesPerInt;
2892   }
2893   if (done_offset &gt; start_offset) {
2894     mem = clear_memory(ctl, mem, dest,
2895                        start_offset, phase-&gt;MakeConX(done_offset), phase);
2896   }
2897   if (done_offset &lt; end_offset) { // emit the final 32-bit store
2898     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
2899     adr = phase-&gt;transform(adr);
2900     const TypePtr* atp = TypeRawPtr::BOTTOM;
2901     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2902     mem = phase-&gt;transform(mem);
2903     done_offset += BytesPerInt;
2904   }
2905   assert(done_offset == end_offset, "");
2906   return mem;
2907 }
2908 
2909 //=============================================================================
2910 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
2911   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
2912     _adr_type(C-&gt;get_adr_type(alias_idx))
2913 {
2914   init_class_id(Class_MemBar);
2915   Node* top = C-&gt;top();
2916   init_req(TypeFunc::I_O,top);
2917   init_req(TypeFunc::FramePtr,top);
2918   init_req(TypeFunc::ReturnAdr,top);
2919   if (precedent != NULL)
2920     init_req(TypeFunc::Parms, precedent);
2921 }
2922 
2923 //------------------------------cmp--------------------------------------------
2924 uint MemBarNode::hash() const { return NO_HASH; }
2925 uint MemBarNode::cmp( const Node &amp;n ) const {
2926   return (&amp;n == this);          // Always fail except on self
2927 }
2928 
2929 //------------------------------make-------------------------------------------
2930 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
2931   switch (opcode) {
2932   case Op_MemBarAcquire:     return new MemBarAcquireNode(C, atp, pn);
2933   case Op_LoadFence:         return new LoadFenceNode(C, atp, pn);
2934   case Op_MemBarRelease:     return new MemBarReleaseNode(C, atp, pn);
2935   case Op_StoreFence:        return new StoreFenceNode(C, atp, pn);
2936   case Op_MemBarAcquireLock: return new MemBarAcquireLockNode(C, atp, pn);
2937   case Op_MemBarReleaseLock: return new MemBarReleaseLockNode(C, atp, pn);
2938   case Op_MemBarVolatile:    return new MemBarVolatileNode(C, atp, pn);
2939   case Op_MemBarCPUOrder:    return new MemBarCPUOrderNode(C, atp, pn);
2940   case Op_Initialize:        return new InitializeNode(C, atp, pn);
2941   case Op_MemBarStoreStore:  return new MemBarStoreStoreNode(C, atp, pn);
2942   default: ShouldNotReachHere(); return NULL;
2943   }
2944 }
2945 
2946 //------------------------------Ideal------------------------------------------
2947 // Return a node which is more "ideal" than the current node.  Strip out
2948 // control copies
2949 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2950   if (remove_dead_region(phase, can_reshape)) return this;
2951   // Don't bother trying to transform a dead node
2952   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
2953     return NULL;
2954   }
2955 
2956   bool progress = false;
2957   // Eliminate volatile MemBars for scalar replaced objects.
2958   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
2959     bool eliminate = false;
2960     int opc = Opcode();
2961     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
2962       // Volatile field loads and stores.
2963       Node* my_mem = in(MemBarNode::Precedent);
2964       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
2965       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
2966         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
2967         // replace this Precedent (decodeN) with the Load instead.
2968         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
2969           Node* load_node = my_mem-&gt;in(1);
2970           set_req(MemBarNode::Precedent, load_node);
2971           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
2972           my_mem = load_node;
2973         } else {
2974           assert(my_mem-&gt;unique_out() == this, "sanity");
2975           del_req(Precedent);
2976           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem); // remove dead node later
2977           my_mem = NULL;
2978         }
2979         progress = true;
2980       }
2981       if (my_mem != NULL &amp;&amp; my_mem-&gt;is_Mem()) {
2982         const TypeOopPtr* t_oop = my_mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_oopptr();
2983         // Check for scalar replaced object reference.
2984         if( t_oop != NULL &amp;&amp; t_oop-&gt;is_known_instance_field() &amp;&amp;
2985             t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
2986             t_oop-&gt;offset() != Type::OffsetTop) {
2987           eliminate = true;
2988         }
2989       }
2990     } else if (opc == Op_MemBarRelease) {
2991       // Final field stores.
2992       Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent), phase);
2993       if ((alloc != NULL) &amp;&amp; alloc-&gt;is_Allocate() &amp;&amp;
2994           alloc-&gt;as_Allocate()-&gt;_is_non_escaping) {
2995         // The allocated object does not escape.
2996         eliminate = true;
2997       }
2998     }
2999     if (eliminate) {
3000       // Replace MemBar projections by its inputs.
3001       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3002       igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
3003       igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
3004       // Must return either the original node (now dead) or a new node
3005       // (Do not return a top here, since that would break the uniqueness of top.)
3006       return new ConINode(TypeInt::ZERO);
3007     }
3008   }
3009   return progress ? this : NULL;
3010 }
3011 
3012 //------------------------------Value------------------------------------------
3013 const Type *MemBarNode::Value( PhaseTransform *phase ) const {
3014   if( !in(0) ) return Type::TOP;
3015   if( phase-&gt;type(in(0)) == Type::TOP )
3016     return Type::TOP;
3017   return TypeTuple::MEMBAR;
3018 }
3019 
3020 //------------------------------match------------------------------------------
3021 // Construct projections for memory.
3022 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {
3023   switch (proj-&gt;_con) {
3024   case TypeFunc::Control:
3025   case TypeFunc::Memory:
3026     return new MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3027   }
3028   ShouldNotReachHere();
3029   return NULL;
3030 }
3031 
3032 //===========================InitializeNode====================================
3033 // SUMMARY:
3034 // This node acts as a memory barrier on raw memory, after some raw stores.
3035 // The 'cooked' oop value feeds from the Initialize, not the Allocation.
3036 // The Initialize can 'capture' suitably constrained stores as raw inits.
3037 // It can coalesce related raw stores into larger units (called 'tiles').
3038 // It can avoid zeroing new storage for memory units which have raw inits.
3039 // At macro-expansion, it is marked 'complete', and does not optimize further.
3040 //
3041 // EXAMPLE:
3042 // The object 'new short[2]' occupies 16 bytes in a 32-bit machine.
3043 //   ctl = incoming control; mem* = incoming memory
3044 // (Note:  A star * on a memory edge denotes I/O and other standard edges.)
3045 // First allocate uninitialized memory and fill in the header:
3046 //   alloc = (Allocate ctl mem* 16 #short[].klass ...)
3047 //   ctl := alloc.Control; mem* := alloc.Memory*
3048 //   rawmem = alloc.Memory; rawoop = alloc.RawAddress
3049 // Then initialize to zero the non-header parts of the raw memory block:
3050 //   init = (Initialize alloc.Control alloc.Memory* alloc.RawAddress)
3051 //   ctl := init.Control; mem.SLICE(#short[*]) := init.Memory
3052 // After the initialize node executes, the object is ready for service:
3053 //   oop := (CheckCastPP init.Control alloc.RawAddress #short[])
3054 // Suppose its body is immediately initialized as {1,2}:
3055 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3056 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3057 //   mem.SLICE(#short[*]) := store2
3058 //
3059 // DETAILS:
3060 // An InitializeNode collects and isolates object initialization after
3061 // an AllocateNode and before the next possible safepoint.  As a
3062 // memory barrier (MemBarNode), it keeps critical stores from drifting
3063 // down past any safepoint or any publication of the allocation.
3064 // Before this barrier, a newly-allocated object may have uninitialized bits.
3065 // After this barrier, it may be treated as a real oop, and GC is allowed.
3066 //
3067 // The semantics of the InitializeNode include an implicit zeroing of
3068 // the new object from object header to the end of the object.
3069 // (The object header and end are determined by the AllocateNode.)
3070 //
3071 // Certain stores may be added as direct inputs to the InitializeNode.
3072 // These stores must update raw memory, and they must be to addresses
3073 // derived from the raw address produced by AllocateNode, and with
3074 // a constant offset.  They must be ordered by increasing offset.
3075 // The first one is at in(RawStores), the last at in(req()-1).
3076 // Unlike most memory operations, they are not linked in a chain,
3077 // but are displayed in parallel as users of the rawmem output of
3078 // the allocation.
3079 //
3080 // (See comments in InitializeNode::capture_store, which continue
3081 // the example given above.)
3082 //
3083 // When the associated Allocate is macro-expanded, the InitializeNode
3084 // may be rewritten to optimize collected stores.  A ClearArrayNode
3085 // may also be created at that point to represent any required zeroing.
3086 // The InitializeNode is then marked 'complete', prohibiting further
3087 // capturing of nearby memory operations.
3088 //
3089 // During macro-expansion, all captured initializations which store
3090 // constant values of 32 bits or smaller are coalesced (if advantageous)
3091 // into larger 'tiles' 32 or 64 bits.  This allows an object to be
3092 // initialized in fewer memory operations.  Memory words which are
3093 // covered by neither tiles nor non-constant stores are pre-zeroed
3094 // by explicit stores of zero.  (The code shape happens to do all
3095 // zeroing first, then all other stores, with both sequences occurring
3096 // in order of ascending offsets.)
3097 //
3098 // Alternatively, code may be inserted between an AllocateNode and its
3099 // InitializeNode, to perform arbitrary initialization of the new object.
3100 // E.g., the object copying intrinsics insert complex data transfers here.
3101 // The initialization must then be marked as 'complete' disable the
3102 // built-in zeroing semantics and the collection of initializing stores.
3103 //
3104 // While an InitializeNode is incomplete, reads from the memory state
3105 // produced by it are optimizable if they match the control edge and
3106 // new oop address associated with the allocation/initialization.
3107 // They return a stored value (if the offset matches) or else zero.
3108 // A write to the memory state, if it matches control and address,
3109 // and if it is to a constant offset, may be 'captured' by the
3110 // InitializeNode.  It is cloned as a raw memory operation and rewired
3111 // inside the initialization, to the raw oop produced by the allocation.
3112 // Operations on addresses which are provably distinct (e.g., to
3113 // other AllocateNodes) are allowed to bypass the initialization.
3114 //
3115 // The effect of all this is to consolidate object initialization
3116 // (both arrays and non-arrays, both piecewise and bulk) into a
3117 // single location, where it can be optimized as a unit.
3118 //
3119 // Only stores with an offset less than TrackedInitializationLimit words
3120 // will be considered for capture by an InitializeNode.  This puts a
3121 // reasonable limit on the complexity of optimized initializations.
3122 
3123 //---------------------------InitializeNode------------------------------------
3124 InitializeNode::InitializeNode(Compile* C, int adr_type, Node* rawoop)
3125   : _is_complete(Incomplete), _does_not_escape(false),
3126     MemBarNode(C, adr_type, rawoop)
3127 {
3128   init_class_id(Class_Initialize);
3129 
3130   assert(adr_type == Compile::AliasIdxRaw, "only valid atp");
3131   assert(in(RawAddress) == rawoop, "proper init");
3132   // Note:  allocation() can be NULL, for secondary initialization barriers
3133 }
3134 
3135 // Since this node is not matched, it will be processed by the
3136 // register allocator.  Declare that there are no constraints
3137 // on the allocation of the RawAddress edge.
3138 const RegMask &amp;InitializeNode::in_RegMask(uint idx) const {
3139   // This edge should be set to top, by the set_complete.  But be conservative.
3140   if (idx == InitializeNode::RawAddress)
3141     return *(Compile::current()-&gt;matcher()-&gt;idealreg2spillmask[in(idx)-&gt;ideal_reg()]);
3142   return RegMask::Empty;
3143 }
3144 
3145 Node* InitializeNode::memory(uint alias_idx) {
3146   Node* mem = in(Memory);
3147   if (mem-&gt;is_MergeMem()) {
3148     return mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
3149   } else {
3150     // incoming raw memory is not split
3151     return mem;
3152   }
3153 }
3154 
3155 bool InitializeNode::is_non_zero() {
3156   if (is_complete())  return false;
3157   remove_extra_zeroes();
3158   return (req() &gt; RawStores);
3159 }
3160 
3161 void InitializeNode::set_complete(PhaseGVN* phase) {
3162   assert(!is_complete(), "caller responsibility");
3163   _is_complete = Complete;
3164 
3165   // After this node is complete, it contains a bunch of
3166   // raw-memory initializations.  There is no need for
3167   // it to have anything to do with non-raw memory effects.
3168   // Therefore, tell all non-raw users to re-optimize themselves,
3169   // after skipping the memory effects of this initialization.
3170   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3171   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3172 }
3173 
3174 // convenience function
3175 // return false if the init contains any stores already
3176 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3177   InitializeNode* init = initialization();
3178   if (init == NULL || init-&gt;is_complete())  return false;
3179   init-&gt;remove_extra_zeroes();
3180   // for now, if this allocation has already collected any inits, bail:
3181   if (init-&gt;is_non_zero())  return false;
3182   init-&gt;set_complete(phase);
3183   return true;
3184 }
3185 
3186 void InitializeNode::remove_extra_zeroes() {
3187   if (req() == RawStores)  return;
3188   Node* zmem = zero_memory();
3189   uint fill = RawStores;
3190   for (uint i = fill; i &lt; req(); i++) {
3191     Node* n = in(i);
3192     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3193     if (fill &lt; i)  set_req(fill, n);          // compact
3194     ++fill;
3195   }
3196   // delete any empty spaces created:
3197   while (fill &lt; req()) {
3198     del_req(fill);
3199   }
3200 }
3201 
3202 // Helper for remembering which stores go with which offsets.
3203 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3204   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3205   intptr_t offset = -1;
3206   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3207                                                phase, offset);
3208   if (base == NULL)     return -1;  // something is dead,
3209   if (offset &lt; 0)       return -1;  //        dead, dead
3210   return offset;
3211 }
3212 
3213 // Helper for proving that an initialization expression is
3214 // "simple enough" to be folded into an object initialization.
3215 // Attempts to prove that a store's initial value 'n' can be captured
3216 // within the initialization without creating a vicious cycle, such as:
3217 //     { Foo p = new Foo(); p.next = p; }
3218 // True for constants and parameters and small combinations thereof.
3219 bool InitializeNode::detect_init_independence(Node* n, int&amp; count) {
3220   if (n == NULL)      return true;   // (can this really happen?)
3221   if (n-&gt;is_Proj())   n = n-&gt;in(0);
3222   if (n == this)      return false;  // found a cycle
3223   if (n-&gt;is_Con())    return true;
3224   if (n-&gt;is_Start())  return true;   // params, etc., are OK
3225   if (n-&gt;is_Root())   return true;   // even better
3226 
3227   Node* ctl = n-&gt;in(0);
3228   if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {
3229     if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);
3230     if (ctl == this)  return false;
3231 
3232     // If we already know that the enclosing memory op is pinned right after
3233     // the init, then any control flow that the store has picked up
3234     // must have preceded the init, or else be equal to the init.
3235     // Even after loop optimizations (which might change control edges)
3236     // a store is never pinned *before* the availability of its inputs.
3237     if (!MemNode::all_controls_dominate(n, this))
3238       return false;                  // failed to prove a good control
3239   }
3240 
3241   // Check data edges for possible dependencies on 'this'.
3242   if ((count += 1) &gt; 20)  return false;  // complexity limit
3243   for (uint i = 1; i &lt; n-&gt;req(); i++) {
3244     Node* m = n-&gt;in(i);
3245     if (m == NULL || m == n || m-&gt;is_top())  continue;
3246     uint first_i = n-&gt;find_edge(m);
3247     if (i != first_i)  continue;  // process duplicate edge just once
3248     if (!detect_init_independence(m, count)) {
3249       return false;
3250     }
3251   }
3252 
3253   return true;
3254 }
3255 
3256 // Here are all the checks a Store must pass before it can be moved into
3257 // an initialization.  Returns zero if a check fails.
3258 // On success, returns the (constant) offset to which the store applies,
3259 // within the initialized memory.
3260 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape) {
3261   const int FAIL = 0;
3262   if (st-&gt;req() != MemNode::ValueIn + 1)
3263     return FAIL;                // an inscrutable StoreNode (card mark?)
3264   Node* ctl = st-&gt;in(MemNode::Control);
3265   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3266     return FAIL;                // must be unconditional after the initialization
3267   Node* mem = st-&gt;in(MemNode::Memory);
3268   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3269     return FAIL;                // must not be preceded by other stores
3270   Node* adr = st-&gt;in(MemNode::Address);
3271   intptr_t offset;
3272   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3273   if (alloc == NULL)
3274     return FAIL;                // inscrutable address
3275   if (alloc != allocation())
3276     return FAIL;                // wrong allocation!  (store needs to float up)
3277   Node* val = st-&gt;in(MemNode::ValueIn);
3278   int complexity_count = 0;
3279   if (!detect_init_independence(val, complexity_count))
3280     return FAIL;                // stored value must be 'simple enough'
3281 
3282   // The Store can be captured only if nothing after the allocation
3283   // and before the Store is using the memory location that the store
3284   // overwrites.
3285   bool failed = false;
3286   // If is_complete_with_arraycopy() is true the shape of the graph is
3287   // well defined and is safe so no need for extra checks.
3288   if (!is_complete_with_arraycopy()) {
3289     // We are going to look at each use of the memory state following
3290     // the allocation to make sure nothing reads the memory that the
3291     // Store writes.
3292     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3293     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3294     ResourceMark rm;
3295     Unique_Node_List mems;
3296     mems.push(mem);
3297     Node* unique_merge = NULL;
3298     for (uint next = 0; next &lt; mems.size(); ++next) {
3299       Node *m  = mems.at(next);
3300       for (DUIterator_Fast jmax, j = m-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3301         Node *n = m-&gt;fast_out(j);
3302         if (n-&gt;outcnt() == 0) {
3303           continue;
3304         }
3305         if (n == st) {
3306           continue;
3307         } else if (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0) != ctl) {
3308           // If the control of this use is different from the control
3309           // of the Store which is right after the InitializeNode then
3310           // this node cannot be between the InitializeNode and the
3311           // Store.
3312           continue;
3313         } else if (n-&gt;is_MergeMem()) {
3314           if (n-&gt;as_MergeMem()-&gt;memory_at(alias_idx) == m) {
3315             // We can hit a MergeMemNode (that will likely go away
3316             // later) that is a direct use of the memory state
3317             // following the InitializeNode on the same slice as the
3318             // store node that we'd like to capture. We need to check
3319             // the uses of the MergeMemNode.
3320             mems.push(n);
3321           }
3322         } else if (n-&gt;is_Mem()) {
3323           Node* other_adr = n-&gt;in(MemNode::Address);
3324           if (other_adr == adr) {
3325             failed = true;
3326             break;
3327           } else {
3328             const TypePtr* other_t_adr = phase-&gt;type(other_adr)-&gt;isa_ptr();
3329             if (other_t_adr != NULL) {
3330               int other_alias_idx = phase-&gt;C-&gt;get_alias_index(other_t_adr);
3331               if (other_alias_idx == alias_idx) {
3332                 // A load from the same memory slice as the store right
3333                 // after the InitializeNode. We check the control of the
3334                 // object/array that is loaded from. If it's the same as
3335                 // the store control then we cannot capture the store.
3336                 assert(!n-&gt;is_Store(), "2 stores to same slice on same control?");
3337                 Node* base = other_adr;
3338                 assert(base-&gt;is_AddP(), err_msg_res("should be addp but is %s", base-&gt;Name()));
3339                 base = base-&gt;in(AddPNode::Base);
3340                 if (base != NULL) {
3341                   base = base-&gt;uncast();
3342                   if (base-&gt;is_Proj() &amp;&amp; base-&gt;in(0) == alloc) {
3343                     failed = true;
3344                     break;
3345                   }
3346                 }
3347               }
3348             }
3349           }
3350         } else {
3351           failed = true;
3352           break;
3353         }
3354       }
3355     }
3356   }
3357   if (failed) {
3358     if (!can_reshape) {
3359       // We decided we couldn't capture the store during parsing. We
3360       // should try again during the next IGVN once the graph is
3361       // cleaner.
3362       phase-&gt;C-&gt;record_for_igvn(st);
3363     }
3364     return FAIL;
3365   }
3366 
3367   return offset;                // success
3368 }
3369 
3370 // Find the captured store in(i) which corresponds to the range
3371 // [start..start+size) in the initialized object.
3372 // If there is one, return its index i.  If there isn't, return the
3373 // negative of the index where it should be inserted.
3374 // Return 0 if the queried range overlaps an initialization boundary
3375 // or if dead code is encountered.
3376 // If size_in_bytes is zero, do not bother with overlap checks.
3377 int InitializeNode::captured_store_insertion_point(intptr_t start,
3378                                                    int size_in_bytes,
3379                                                    PhaseTransform* phase) {
3380   const int FAIL = 0, MAX_STORE = BytesPerLong;
3381 
3382   if (is_complete())
3383     return FAIL;                // arraycopy got here first; punt
3384 
3385   assert(allocation() != NULL, "must be present");
3386 
3387   // no negatives, no header fields:
3388   if (start &lt; (intptr_t) allocation()-&gt;minimum_header_size())  return FAIL;
3389 
3390   // after a certain size, we bail out on tracking all the stores:
3391   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3392   if (start &gt;= ti_limit)  return FAIL;
3393 
3394   for (uint i = InitializeNode::RawStores, limit = req(); ; ) {
3395     if (i &gt;= limit)  return -(int)i; // not found; here is where to put it
3396 
3397     Node*    st     = in(i);
3398     intptr_t st_off = get_store_offset(st, phase);
3399     if (st_off &lt; 0) {
3400       if (st != zero_memory()) {
3401         return FAIL;            // bail out if there is dead garbage
3402       }
3403     } else if (st_off &gt; start) {
3404       // ...we are done, since stores are ordered
3405       if (st_off &lt; start + size_in_bytes) {
3406         return FAIL;            // the next store overlaps
3407       }
3408       return -(int)i;           // not found; here is where to put it
3409     } else if (st_off &lt; start) {
3410       if (size_in_bytes != 0 &amp;&amp;
3411           start &lt; st_off + MAX_STORE &amp;&amp;
3412           start &lt; st_off + st-&gt;as_Store()-&gt;memory_size()) {
3413         return FAIL;            // the previous store overlaps
3414       }
3415     } else {
3416       if (size_in_bytes != 0 &amp;&amp;
3417           st-&gt;as_Store()-&gt;memory_size() != size_in_bytes) {
3418         return FAIL;            // mismatched store size
3419       }
3420       return i;
3421     }
3422 
3423     ++i;
3424   }
3425 }
3426 
3427 // Look for a captured store which initializes at the offset 'start'
3428 // with the given size.  If there is no such store, and no other
3429 // initialization interferes, then return zero_memory (the memory
3430 // projection of the AllocateNode).
3431 Node* InitializeNode::find_captured_store(intptr_t start, int size_in_bytes,
3432                                           PhaseTransform* phase) {
3433   assert(stores_are_sane(phase), "");
3434   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3435   if (i == 0) {
3436     return NULL;                // something is dead
3437   } else if (i &lt; 0) {
3438     return zero_memory();       // just primordial zero bits here
3439   } else {
3440     Node* st = in(i);           // here is the store at this position
3441     assert(get_store_offset(st-&gt;as_Store(), phase) == start, "sanity");
3442     return st;
3443   }
3444 }
3445 
3446 // Create, as a raw pointer, an address within my new object at 'offset'.
3447 Node* InitializeNode::make_raw_address(intptr_t offset,
3448                                        PhaseTransform* phase) {
3449   Node* addr = in(RawAddress);
3450   if (offset != 0) {
3451     Compile* C = phase-&gt;C;
3452     addr = phase-&gt;transform( new AddPNode(C-&gt;top(), addr,
3453                                                  phase-&gt;MakeConX(offset)) );
3454   }
3455   return addr;
3456 }
3457 
3458 // Clone the given store, converting it into a raw store
3459 // initializing a field or element of my new object.
3460 // Caller is responsible for retiring the original store,
3461 // with subsume_node or the like.
3462 //
3463 // From the example above InitializeNode::InitializeNode,
3464 // here are the old stores to be captured:
3465 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3466 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3467 //
3468 // Here is the changed code; note the extra edges on init:
3469 //   alloc = (Allocate ...)
3470 //   rawoop = alloc.RawAddress
3471 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3472 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3473 //   init = (Initialize alloc.Control alloc.Memory rawoop
3474 //                      rawstore1 rawstore2)
3475 //
3476 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
3477                                     PhaseTransform* phase, bool can_reshape) {
3478   assert(stores_are_sane(phase), "");
3479 
3480   if (start &lt; 0)  return NULL;
3481   assert(can_capture_store(st, phase, can_reshape) == start, "sanity");
3482 
3483   Compile* C = phase-&gt;C;
3484   int size_in_bytes = st-&gt;memory_size();
3485   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3486   if (i == 0)  return NULL;     // bail out
3487   Node* prev_mem = NULL;        // raw memory for the captured store
3488   if (i &gt; 0) {
3489     prev_mem = in(i);           // there is a pre-existing store under this one
3490     set_req(i, C-&gt;top());       // temporarily disconnect it
3491     // See StoreNode::Ideal 'st-&gt;outcnt() == 1' for the reason to disconnect.
3492   } else {
3493     i = -i;                     // no pre-existing store
3494     prev_mem = zero_memory();   // a slice of the newly allocated object
3495     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3496       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3497     else
3498       ins_req(i, C-&gt;top());     // build a new edge
3499   }
3500   Node* new_st = st-&gt;clone();
3501   new_st-&gt;set_req(MemNode::Control, in(Control));
3502   new_st-&gt;set_req(MemNode::Memory,  prev_mem);
3503   new_st-&gt;set_req(MemNode::Address, make_raw_address(start, phase));
3504   new_st = phase-&gt;transform(new_st);
3505 
3506   // At this point, new_st might have swallowed a pre-existing store
3507   // at the same offset, or perhaps new_st might have disappeared,
3508   // if it redundantly stored the same value (or zero to fresh memory).
3509 
3510   // In any case, wire it in:
3511   phase-&gt;igvn_rehash_node_delayed(this);
3512   set_req(i, new_st);
3513 
3514   // The caller may now kill the old guy.
3515   DEBUG_ONLY(Node* check_st = find_captured_store(start, size_in_bytes, phase));
3516   assert(check_st == new_st || check_st == NULL, "must be findable");
3517   assert(!is_complete(), "");
3518   return new_st;
3519 }
3520 
3521 static bool store_constant(jlong* tiles, int num_tiles,
3522                            intptr_t st_off, int st_size,
3523                            jlong con) {
3524   if ((st_off &amp; (st_size-1)) != 0)
3525     return false;               // strange store offset (assume size==2**N)
3526   address addr = (address)tiles + st_off;
3527   assert(st_off &gt;= 0 &amp;&amp; addr+st_size &lt;= (address)&amp;tiles[num_tiles], "oob");
3528   switch (st_size) {
3529   case sizeof(jbyte):  *(jbyte*) addr = (jbyte) con; break;
3530   case sizeof(jchar):  *(jchar*) addr = (jchar) con; break;
3531   case sizeof(jint):   *(jint*)  addr = (jint)  con; break;
3532   case sizeof(jlong):  *(jlong*) addr = (jlong) con; break;
3533   default: return false;        // strange store size (detect size!=2**N here)
3534   }
3535   return true;                  // return success to caller
3536 }
3537 
3538 // Coalesce subword constants into int constants and possibly
3539 // into long constants.  The goal, if the CPU permits,
3540 // is to initialize the object with a small number of 64-bit tiles.
3541 // Also, convert floating-point constants to bit patterns.
3542 // Non-constants are not relevant to this pass.
3543 //
3544 // In terms of the running example on InitializeNode::InitializeNode
3545 // and InitializeNode::capture_store, here is the transformation
3546 // of rawstore1 and rawstore2 into rawstore12:
3547 //   alloc = (Allocate ...)
3548 //   rawoop = alloc.RawAddress
3549 //   tile12 = 0x00010002
3550 //   rawstore12 = (StoreI alloc.Control alloc.Memory (+ rawoop 12) tile12)
3551 //   init = (Initialize alloc.Control alloc.Memory rawoop rawstore12)
3552 //
3553 void
3554 InitializeNode::coalesce_subword_stores(intptr_t header_size,
3555                                         Node* size_in_bytes,
3556                                         PhaseGVN* phase) {
3557   Compile* C = phase-&gt;C;
3558 
3559   assert(stores_are_sane(phase), "");
3560   // Note:  After this pass, they are not completely sane,
3561   // since there may be some overlaps.
3562 
3563   int old_subword = 0, old_long = 0, new_int = 0, new_long = 0;
3564 
3565   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3566   intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, ti_limit);
3567   size_limit = MIN2(size_limit, ti_limit);
3568   size_limit = align_size_up(size_limit, BytesPerLong);
3569   int num_tiles = size_limit / BytesPerLong;
3570 
3571   // allocate space for the tile map:
3572   const int small_len = DEBUG_ONLY(true ? 3 :) 30; // keep stack frames small
3573   jlong  tiles_buf[small_len];
3574   Node*  nodes_buf[small_len];
3575   jlong  inits_buf[small_len];
3576   jlong* tiles = ((num_tiles &lt;= small_len) ? &amp;tiles_buf[0]
3577                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3578   Node** nodes = ((num_tiles &lt;= small_len) ? &amp;nodes_buf[0]
3579                   : NEW_RESOURCE_ARRAY(Node*, num_tiles));
3580   jlong* inits = ((num_tiles &lt;= small_len) ? &amp;inits_buf[0]
3581                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3582   // tiles: exact bitwise model of all primitive constants
3583   // nodes: last constant-storing node subsumed into the tiles model
3584   // inits: which bytes (in each tile) are touched by any initializations
3585 
3586   //// Pass A: Fill in the tile model with any relevant stores.
3587 
3588   Copy::zero_to_bytes(tiles, sizeof(tiles[0]) * num_tiles);
3589   Copy::zero_to_bytes(nodes, sizeof(nodes[0]) * num_tiles);
3590   Copy::zero_to_bytes(inits, sizeof(inits[0]) * num_tiles);
3591   Node* zmem = zero_memory(); // initially zero memory state
3592   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3593     Node* st = in(i);
3594     intptr_t st_off = get_store_offset(st, phase);
3595 
3596     // Figure out the store's offset and constant value:
3597     if (st_off &lt; header_size)             continue; //skip (ignore header)
3598     if (st-&gt;in(MemNode::Memory) != zmem)  continue; //skip (odd store chain)
3599     int st_size = st-&gt;as_Store()-&gt;memory_size();
3600     if (st_off + st_size &gt; size_limit)    break;
3601 
3602     // Record which bytes are touched, whether by constant or not.
3603     if (!store_constant(inits, num_tiles, st_off, st_size, (jlong) -1))
3604       continue;                 // skip (strange store size)
3605 
3606     const Type* val = phase-&gt;type(st-&gt;in(MemNode::ValueIn));
3607     if (!val-&gt;singleton())                continue; //skip (non-con store)
3608     BasicType type = val-&gt;basic_type();
3609 
3610     jlong con = 0;
3611     switch (type) {
3612     case T_INT:    con = val-&gt;is_int()-&gt;get_con();  break;
3613     case T_LONG:   con = val-&gt;is_long()-&gt;get_con(); break;
3614     case T_FLOAT:  con = jint_cast(val-&gt;getf());    break;
3615     case T_DOUBLE: con = jlong_cast(val-&gt;getd());   break;
3616     default:                              continue; //skip (odd store type)
3617     }
3618 
3619     if (type == T_LONG &amp;&amp; Matcher::isSimpleConstant64(con) &amp;&amp;
3620         st-&gt;Opcode() == Op_StoreL) {
3621       continue;                 // This StoreL is already optimal.
3622     }
3623 
3624     // Store down the constant.
3625     store_constant(tiles, num_tiles, st_off, st_size, con);
3626 
3627     intptr_t j = st_off &gt;&gt; LogBytesPerLong;
3628 
3629     if (type == T_INT &amp;&amp; st_size == BytesPerInt
3630         &amp;&amp; (st_off &amp; BytesPerInt) == BytesPerInt) {
3631       jlong lcon = tiles[j];
3632       if (!Matcher::isSimpleConstant64(lcon) &amp;&amp;
3633           st-&gt;Opcode() == Op_StoreI) {
3634         // This StoreI is already optimal by itself.
3635         jint* intcon = (jint*) &amp;tiles[j];
3636         intcon[1] = 0;  // undo the store_constant()
3637 
3638         // If the previous store is also optimal by itself, back up and
3639         // undo the action of the previous loop iteration... if we can.
3640         // But if we can't, just let the previous half take care of itself.
3641         st = nodes[j];
3642         st_off -= BytesPerInt;
3643         con = intcon[0];
3644         if (con != 0 &amp;&amp; st != NULL &amp;&amp; st-&gt;Opcode() == Op_StoreI) {
3645           assert(st_off &gt;= header_size, "still ignoring header");
3646           assert(get_store_offset(st, phase) == st_off, "must be");
3647           assert(in(i-1) == zmem, "must be");
3648           DEBUG_ONLY(const Type* tcon = phase-&gt;type(st-&gt;in(MemNode::ValueIn)));
3649           assert(con == tcon-&gt;is_int()-&gt;get_con(), "must be");
3650           // Undo the effects of the previous loop trip, which swallowed st:
3651           intcon[0] = 0;        // undo store_constant()
3652           set_req(i-1, st);     // undo set_req(i, zmem)
3653           nodes[j] = NULL;      // undo nodes[j] = st
3654           --old_subword;        // undo ++old_subword
3655         }
3656         continue;               // This StoreI is already optimal.
3657       }
3658     }
3659 
3660     // This store is not needed.
3661     set_req(i, zmem);
3662     nodes[j] = st;              // record for the moment
3663     if (st_size &lt; BytesPerLong) // something has changed
3664           ++old_subword;        // includes int/float, but who's counting...
3665     else  ++old_long;
3666   }
3667 
3668   if ((old_subword + old_long) == 0)
3669     return;                     // nothing more to do
3670 
3671   //// Pass B: Convert any non-zero tiles into optimal constant stores.
3672   // Be sure to insert them before overlapping non-constant stores.
3673   // (E.g., byte[] x = { 1,2,y,4 }  =&gt;  x[int 0] = 0x01020004, x[2]=y.)
3674   for (int j = 0; j &lt; num_tiles; j++) {
3675     jlong con  = tiles[j];
3676     jlong init = inits[j];
3677     if (con == 0)  continue;
3678     jint con0,  con1;           // split the constant, address-wise
3679     jint init0, init1;          // split the init map, address-wise
3680     { union { jlong con; jint intcon[2]; } u;
3681       u.con = con;
3682       con0  = u.intcon[0];
3683       con1  = u.intcon[1];
3684       u.con = init;
3685       init0 = u.intcon[0];
3686       init1 = u.intcon[1];
3687     }
3688 
3689     Node* old = nodes[j];
3690     assert(old != NULL, "need the prior store");
3691     intptr_t offset = (j * BytesPerLong);
3692 
3693     bool split = !Matcher::isSimpleConstant64(con);
3694 
3695     if (offset &lt; header_size) {
3696       assert(offset + BytesPerInt &gt;= header_size, "second int counts");
3697       assert(*(jint*)&amp;tiles[j] == 0, "junk in header");
3698       split = true;             // only the second word counts
3699       // Example:  int a[] = { 42 ... }
3700     } else if (con0 == 0 &amp;&amp; init0 == -1) {
3701       split = true;             // first word is covered by full inits
3702       // Example:  int a[] = { ... foo(), 42 ... }
3703     } else if (con1 == 0 &amp;&amp; init1 == -1) {
3704       split = true;             // second word is covered by full inits
3705       // Example:  int a[] = { ... 42, foo() ... }
3706     }
3707 
3708     // Here's a case where init0 is neither 0 nor -1:
3709     //   byte a[] = { ... 0,0,foo(),0,  0,0,0,42 ... }
3710     // Assuming big-endian memory, init0, init1 are 0x0000FF00, 0x000000FF.
3711     // In this case the tile is not split; it is (jlong)42.
3712     // The big tile is stored down, and then the foo() value is inserted.
3713     // (If there were foo(),foo() instead of foo(),0, init0 would be -1.)
3714 
3715     Node* ctl = old-&gt;in(MemNode::Control);
3716     Node* adr = make_raw_address(offset, phase);
3717     const TypePtr* atp = TypeRawPtr::BOTTOM;
3718 
3719     // One or two coalesced stores to plop down.
3720     Node*    st[2];
3721     intptr_t off[2];
3722     int  nst = 0;
3723     if (!split) {
3724       ++new_long;
3725       off[nst] = offset;
3726       st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3727                                   phase-&gt;longcon(con), T_LONG, MemNode::unordered);
3728     } else {
3729       // Omit either if it is a zero.
3730       if (con0 != 0) {
3731         ++new_int;
3732         off[nst]  = offset;
3733         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3734                                     phase-&gt;intcon(con0), T_INT, MemNode::unordered);
3735       }
3736       if (con1 != 0) {
3737         ++new_int;
3738         offset += BytesPerInt;
3739         adr = make_raw_address(offset, phase);
3740         off[nst]  = offset;
3741         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3742                                     phase-&gt;intcon(con1), T_INT, MemNode::unordered);
3743       }
3744     }
3745 
3746     // Insert second store first, then the first before the second.
3747     // Insert each one just before any overlapping non-constant stores.
3748     while (nst &gt; 0) {
3749       Node* st1 = st[--nst];
3750       C-&gt;copy_node_notes_to(st1, old);
3751       st1 = phase-&gt;transform(st1);
3752       offset = off[nst];
3753       assert(offset &gt;= header_size, "do not smash header");
3754       int ins_idx = captured_store_insertion_point(offset, /*size:*/0, phase);
3755       guarantee(ins_idx != 0, "must re-insert constant store");
3756       if (ins_idx &lt; 0)  ins_idx = -ins_idx;  // never overlap
3757       if (ins_idx &gt; InitializeNode::RawStores &amp;&amp; in(ins_idx-1) == zmem)
3758         set_req(--ins_idx, st1);
3759       else
3760         ins_req(ins_idx, st1);
3761     }
3762   }
3763 
3764   if (PrintCompilation &amp;&amp; WizardMode)
3765     tty-&gt;print_cr("Changed %d/%d subword/long constants into %d/%d int/long",
3766                   old_subword, old_long, new_int, new_long);
3767   if (C-&gt;log() != NULL)
3768     C-&gt;log()-&gt;elem("comment that='%d/%d subword/long to %d/%d int/long'",
3769                    old_subword, old_long, new_int, new_long);
3770 
3771   // Clean up any remaining occurrences of zmem:
3772   remove_extra_zeroes();
3773 }
3774 
3775 // Explore forward from in(start) to find the first fully initialized
3776 // word, and return its offset.  Skip groups of subword stores which
3777 // together initialize full words.  If in(start) is itself part of a
3778 // fully initialized word, return the offset of in(start).  If there
3779 // are no following full-word stores, or if something is fishy, return
3780 // a negative value.
3781 intptr_t InitializeNode::find_next_fullword_store(uint start, PhaseGVN* phase) {
3782   int       int_map = 0;
3783   intptr_t  int_map_off = 0;
3784   const int FULL_MAP = right_n_bits(BytesPerInt);  // the int_map we hope for
3785 
3786   for (uint i = start, limit = req(); i &lt; limit; i++) {
3787     Node* st = in(i);
3788 
3789     intptr_t st_off = get_store_offset(st, phase);
3790     if (st_off &lt; 0)  break;  // return conservative answer
3791 
3792     int st_size = st-&gt;as_Store()-&gt;memory_size();
3793     if (st_size &gt;= BytesPerInt &amp;&amp; (st_off % BytesPerInt) == 0) {
3794       return st_off;            // we found a complete word init
3795     }
3796 
3797     // update the map:
3798 
3799     intptr_t this_int_off = align_size_down(st_off, BytesPerInt);
3800     if (this_int_off != int_map_off) {
3801       // reset the map:
3802       int_map = 0;
3803       int_map_off = this_int_off;
3804     }
3805 
3806     int subword_off = st_off - this_int_off;
3807     int_map |= right_n_bits(st_size) &lt;&lt; subword_off;
3808     if ((int_map &amp; FULL_MAP) == FULL_MAP) {
3809       return this_int_off;      // we found a complete word init
3810     }
3811 
3812     // Did this store hit or cross the word boundary?
3813     intptr_t next_int_off = align_size_down(st_off + st_size, BytesPerInt);
3814     if (next_int_off == this_int_off + BytesPerInt) {
3815       // We passed the current int, without fully initializing it.
3816       int_map_off = next_int_off;
3817       int_map &gt;&gt;= BytesPerInt;
3818     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
3819       // We passed the current and next int.
3820       return this_int_off + BytesPerInt;
3821     }
3822   }
3823 
3824   return -1;
3825 }
3826 
3827 
3828 // Called when the associated AllocateNode is expanded into CFG.
3829 // At this point, we may perform additional optimizations.
3830 // Linearize the stores by ascending offset, to make memory
3831 // activity as coherent as possible.
3832 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
3833                                       intptr_t header_size,
3834                                       Node* size_in_bytes,
3835                                       PhaseGVN* phase) {
3836   assert(!is_complete(), "not already complete");
3837   assert(stores_are_sane(phase), "");
3838   assert(allocation() != NULL, "must be present");
3839 
3840   remove_extra_zeroes();
3841 
3842   if (ReduceFieldZeroing || ReduceBulkZeroing)
3843     // reduce instruction count for common initialization patterns
3844     coalesce_subword_stores(header_size, size_in_bytes, phase);
3845 
3846   Node* zmem = zero_memory();   // initially zero memory state
3847   Node* inits = zmem;           // accumulating a linearized chain of inits
3848   #ifdef ASSERT
3849   intptr_t first_offset = allocation()-&gt;minimum_header_size();
3850   intptr_t last_init_off = first_offset;  // previous init offset
3851   intptr_t last_init_end = first_offset;  // previous init offset+size
3852   intptr_t last_tile_end = first_offset;  // previous tile offset+size
3853   #endif
3854   intptr_t zeroes_done = header_size;
3855 
3856   bool do_zeroing = true;       // we might give up if inits are very sparse
3857   int  big_init_gaps = 0;       // how many large gaps have we seen?
3858 
3859   if (ZeroTLAB)  do_zeroing = false;
3860   if (!ReduceFieldZeroing &amp;&amp; !ReduceBulkZeroing)  do_zeroing = false;
3861 
3862   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3863     Node* st = in(i);
3864     intptr_t st_off = get_store_offset(st, phase);
3865     if (st_off &lt; 0)
3866       break;                    // unknown junk in the inits
3867     if (st-&gt;in(MemNode::Memory) != zmem)
3868       break;                    // complicated store chains somehow in list
3869 
3870     int st_size = st-&gt;as_Store()-&gt;memory_size();
3871     intptr_t next_init_off = st_off + st_size;
3872 
3873     if (do_zeroing &amp;&amp; zeroes_done &lt; next_init_off) {
3874       // See if this store needs a zero before it or under it.
3875       intptr_t zeroes_needed = st_off;
3876 
3877       if (st_size &lt; BytesPerInt) {
3878         // Look for subword stores which only partially initialize words.
3879         // If we find some, we must lay down some word-level zeroes first,
3880         // underneath the subword stores.
3881         //
3882         // Examples:
3883         //   byte[] a = { p,q,r,s }  =&gt;  a[0]=p,a[1]=q,a[2]=r,a[3]=s
3884         //   byte[] a = { x,y,0,0 }  =&gt;  a[0..3] = 0, a[0]=x,a[1]=y
3885         //   byte[] a = { 0,0,z,0 }  =&gt;  a[0..3] = 0, a[2]=z
3886         //
3887         // Note:  coalesce_subword_stores may have already done this,
3888         // if it was prompted by constant non-zero subword initializers.
3889         // But this case can still arise with non-constant stores.
3890 
3891         intptr_t next_full_store = find_next_fullword_store(i, phase);
3892 
3893         // In the examples above:
3894         //   in(i)          p   q   r   s     x   y     z
3895         //   st_off        12  13  14  15    12  13    14
3896         //   st_size        1   1   1   1     1   1     1
3897         //   next_full_s.  12  16  16  16    16  16    16
3898         //   z's_done      12  16  16  16    12  16    12
3899         //   z's_needed    12  16  16  16    16  16    16
3900         //   zsize          0   0   0   0     4   0     4
3901         if (next_full_store &lt; 0) {
3902           // Conservative tack:  Zero to end of current word.
3903           zeroes_needed = align_size_up(zeroes_needed, BytesPerInt);
3904         } else {
3905           // Zero to beginning of next fully initialized word.
3906           // Or, don't zero at all, if we are already in that word.
3907           assert(next_full_store &gt;= zeroes_needed, "must go forward");
3908           assert((next_full_store &amp; (BytesPerInt-1)) == 0, "even boundary");
3909           zeroes_needed = next_full_store;
3910         }
3911       }
3912 
3913       if (zeroes_needed &gt; zeroes_done) {
3914         intptr_t zsize = zeroes_needed - zeroes_done;
3915         // Do some incremental zeroing on rawmem, in parallel with inits.
3916         zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3917         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3918                                               zeroes_done, zeroes_needed,
3919                                               phase);
3920         zeroes_done = zeroes_needed;
3921         if (zsize &gt; Matcher::init_array_short_size &amp;&amp; ++big_init_gaps &gt; 2)
3922           do_zeroing = false;   // leave the hole, next time
3923       }
3924     }
3925 
3926     // Collect the store and move on:
3927     st-&gt;set_req(MemNode::Memory, inits);
3928     inits = st;                 // put it on the linearized chain
3929     set_req(i, zmem);           // unhook from previous position
3930 
3931     if (zeroes_done == st_off)
3932       zeroes_done = next_init_off;
3933 
3934     assert(!do_zeroing || zeroes_done &gt;= next_init_off, "don't miss any");
3935 
3936     #ifdef ASSERT
3937     // Various order invariants.  Weaker than stores_are_sane because
3938     // a large constant tile can be filled in by smaller non-constant stores.
3939     assert(st_off &gt;= last_init_off, "inits do not reverse");
3940     last_init_off = st_off;
3941     const Type* val = NULL;
3942     if (st_size &gt;= BytesPerInt &amp;&amp;
3943         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
3944         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
3945       assert(st_off &gt;= last_tile_end, "tiles do not overlap");
3946       assert(st_off &gt;= last_init_end, "tiles do not overwrite inits");
3947       last_tile_end = MAX2(last_tile_end, next_init_off);
3948     } else {
3949       intptr_t st_tile_end = align_size_up(next_init_off, BytesPerLong);
3950       assert(st_tile_end &gt;= last_tile_end, "inits stay with tiles");
3951       assert(st_off      &gt;= last_init_end, "inits do not overlap");
3952       last_init_end = next_init_off;  // it's a non-tile
3953     }
3954     #endif //ASSERT
3955   }
3956 
3957   remove_extra_zeroes();        // clear out all the zmems left over
3958   add_req(inits);
3959 
3960   if (!ZeroTLAB) {
3961     // If anything remains to be zeroed, zero it all now.
3962     zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3963     // if it is the last unused 4 bytes of an instance, forget about it
3964     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
3965     if (zeroes_done + BytesPerLong &gt;= size_limit) {
3966       assert(allocation() != NULL, "");
3967       if (allocation()-&gt;Opcode() == Op_Allocate) {
3968         Node* klass_node = allocation()-&gt;in(AllocateNode::KlassNode);
3969         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
3970         if (zeroes_done == k-&gt;layout_helper())
3971           zeroes_done = size_limit;
3972       }
3973     }
3974     if (zeroes_done &lt; size_limit) {
3975       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3976                                             zeroes_done, size_in_bytes, phase);
3977     }
3978   }
3979 
3980   set_complete(phase);
3981   return rawmem;
3982 }
3983 
3984 
3985 #ifdef ASSERT
3986 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
3987   if (is_complete())
3988     return true;                // stores could be anything at this point
3989   assert(allocation() != NULL, "must be present");
3990   intptr_t last_off = allocation()-&gt;minimum_header_size();
3991   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
3992     Node* st = in(i);
3993     intptr_t st_off = get_store_offset(st, phase);
3994     if (st_off &lt; 0)  continue;  // ignore dead garbage
3995     if (last_off &gt; st_off) {
3996       tty-&gt;print_cr("*** bad store offset at %d: " INTX_FORMAT " &gt; " INTX_FORMAT, i, last_off, st_off);
3997       this-&gt;dump(2);
3998       assert(false, "ascending store offsets");
3999       return false;
4000     }
4001     last_off = st_off + st-&gt;as_Store()-&gt;memory_size();
4002   }
4003   return true;
4004 }
4005 #endif //ASSERT
4006 
4007 
4008 
4009 
4010 //============================MergeMemNode=====================================
4011 //
4012 // SEMANTICS OF MEMORY MERGES:  A MergeMem is a memory state assembled from several
4013 // contributing store or call operations.  Each contributor provides the memory
4014 // state for a particular "alias type" (see Compile::alias_type).  For example,
4015 // if a MergeMem has an input X for alias category #6, then any memory reference
4016 // to alias category #6 may use X as its memory state input, as an exact equivalent
4017 // to using the MergeMem as a whole.
4018 //   Load&lt;6&gt;( MergeMem(&lt;6&gt;: X, ...), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4019 //
4020 // (Here, the &lt;N&gt; notation gives the index of the relevant adr_type.)
4021 //
4022 // In one special case (and more cases in the future), alias categories overlap.
4023 // The special alias category "Bot" (Compile::AliasIdxBot) includes all memory
4024 // states.  Therefore, if a MergeMem has only one contributing input W for Bot,
4025 // it is exactly equivalent to that state W:
4026 //   MergeMem(&lt;Bot&gt;: W) &lt;==&gt; W
4027 //
4028 // Usually, the merge has more than one input.  In that case, where inputs
4029 // overlap (i.e., one is Bot), the narrower alias type determines the memory
4030 // state for that type, and the wider alias type (Bot) fills in everywhere else:
4031 //   Load&lt;5&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;5&gt;(W,p)
4032 //   Load&lt;6&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4033 //
4034 // A merge can take a "wide" memory state as one of its narrow inputs.
4035 // This simply means that the merge observes out only the relevant parts of
4036 // the wide input.  That is, wide memory states arriving at narrow merge inputs
4037 // are implicitly "filtered" or "sliced" as necessary.  (This is rare.)
4038 //
4039 // These rules imply that MergeMem nodes may cascade (via their &lt;Bot&gt; links),
4040 // and that memory slices "leak through":
4041 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)) &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)
4042 //
4043 // But, in such a cascade, repeated memory slices can "block the leak":
4044 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y), &lt;7&gt;: Y') &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y')
4045 //
4046 // In the last example, Y is not part of the combined memory state of the
4047 // outermost MergeMem.  The system must, of course, prevent unschedulable
4048 // memory states from arising, so you can be sure that the state Y is somehow
4049 // a precursor to state Y'.
4050 //
4051 //
4052 // REPRESENTATION OF MEMORY MERGES: The indexes used to address the Node::in array
4053 // of each MergeMemNode array are exactly the numerical alias indexes, including
4054 // but not limited to AliasIdxTop, AliasIdxBot, and AliasIdxRaw.  The functions
4055 // Compile::alias_type (and kin) produce and manage these indexes.
4056 //
4057 // By convention, the value of in(AliasIdxTop) (i.e., in(1)) is always the top node.
4058 // (Note that this provides quick access to the top node inside MergeMem methods,
4059 // without the need to reach out via TLS to Compile::current.)
4060 //
4061 // As a consequence of what was just described, a MergeMem that represents a full
4062 // memory state has an edge in(AliasIdxBot) which is a "wide" memory state,
4063 // containing all alias categories.
4064 //
4065 // MergeMem nodes never (?) have control inputs, so in(0) is NULL.
4066 //
4067 // All other edges in(N) (including in(AliasIdxRaw), which is in(3)) are either
4068 // a memory state for the alias type &lt;N&gt;, or else the top node, meaning that
4069 // there is no particular input for that alias type.  Note that the length of
4070 // a MergeMem is variable, and may be extended at any time to accommodate new
4071 // memory states at larger alias indexes.  When merges grow, they are of course
4072 // filled with "top" in the unused in() positions.
4073 //
4074 // This use of top is named "empty_memory()", or "empty_mem" (no-memory) as a variable.
4075 // (Top was chosen because it works smoothly with passes like GCM.)
4076 //
4077 // For convenience, we hardwire the alias index for TypeRawPtr::BOTTOM.  (It is
4078 // the type of random VM bits like TLS references.)  Since it is always the
4079 // first non-Bot memory slice, some low-level loops use it to initialize an
4080 // index variable:  for (i = AliasIdxRaw; i &lt; req(); i++).
4081 //
4082 //
4083 // ACCESSORS:  There is a special accessor MergeMemNode::base_memory which returns
4084 // the distinguished "wide" state.  The accessor MergeMemNode::memory_at(N) returns
4085 // the memory state for alias type &lt;N&gt;, or (if there is no particular slice at &lt;N&gt;,
4086 // it returns the base memory.  To prevent bugs, memory_at does not accept &lt;Top&gt;
4087 // or &lt;Bot&gt; indexes.  The iterator MergeMemStream provides robust iteration over
4088 // MergeMem nodes or pairs of such nodes, ensuring that the non-top edges are visited.
4089 //
4090 // %%%% We may get rid of base_memory as a separate accessor at some point; it isn't
4091 // really that different from the other memory inputs.  An abbreviation called
4092 // "bot_memory()" for "memory_at(AliasIdxBot)" would keep code tidy.
4093 //
4094 //
4095 // PARTIAL MEMORY STATES:  During optimization, MergeMem nodes may arise that represent
4096 // partial memory states.  When a Phi splits through a MergeMem, the copy of the Phi
4097 // that "emerges though" the base memory will be marked as excluding the alias types
4098 // of the other (narrow-memory) copies which "emerged through" the narrow edges:
4099 //
4100 //   Phi&lt;Bot&gt;(U, MergeMem(&lt;Bot&gt;: W, &lt;8&gt;: Y))
4101 //     ==Ideal=&gt;  MergeMem(&lt;Bot&gt;: Phi&lt;Bot-8&gt;(U, W), Phi&lt;8&gt;(U, Y))
4102 //
4103 // This strange "subtraction" effect is necessary to ensure IGVN convergence.
4104 // (It is currently unimplemented.)  As you can see, the resulting merge is
4105 // actually a disjoint union of memory states, rather than an overlay.
4106 //
4107 
4108 //------------------------------MergeMemNode-----------------------------------
4109 Node* MergeMemNode::make_empty_memory() {
4110   Node* empty_memory = (Node*) Compile::current()-&gt;top();
4111   assert(empty_memory-&gt;is_top(), "correct sentinel identity");
4112   return empty_memory;
4113 }
4114 
4115 MergeMemNode::MergeMemNode(Node *new_base) : Node(1+Compile::AliasIdxRaw) {
4116   init_class_id(Class_MergeMem);
4117   // all inputs are nullified in Node::Node(int)
4118   // set_input(0, NULL);  // no control input
4119 
4120   // Initialize the edges uniformly to top, for starters.
4121   Node* empty_mem = make_empty_memory();
4122   for (uint i = Compile::AliasIdxTop; i &lt; req(); i++) {
4123     init_req(i,empty_mem);
4124   }
4125   assert(empty_memory() == empty_mem, "");
4126 
4127   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4128     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4129     assert(mdef-&gt;empty_memory() == empty_mem, "consistent sentinels");
4130     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4131       mms.set_memory(mms.memory2());
4132     }
4133     assert(base_memory() == mdef-&gt;base_memory(), "");
4134   } else {
4135     set_base_memory(new_base);
4136   }
4137 }
4138 
4139 // Make a new, untransformed MergeMem with the same base as 'mem'.
4140 // If mem is itself a MergeMem, populate the result with the same edges.
4141 MergeMemNode* MergeMemNode::make(Node* mem) {
4142   return new MergeMemNode(mem);
4143 }
4144 
4145 //------------------------------cmp--------------------------------------------
4146 uint MergeMemNode::hash() const { return NO_HASH; }
4147 uint MergeMemNode::cmp( const Node &amp;n ) const {
4148   return (&amp;n == this);          // Always fail except on self
4149 }
4150 
4151 //------------------------------Identity---------------------------------------
4152 Node* MergeMemNode::Identity(PhaseTransform *phase) {
4153   // Identity if this merge point does not record any interesting memory
4154   // disambiguations.
4155   Node* base_mem = base_memory();
4156   Node* empty_mem = empty_memory();
4157   if (base_mem != empty_mem) {  // Memory path is not dead?
4158     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4159       Node* mem = in(i);
4160       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4161         return this;            // Many memory splits; no change
4162       }
4163     }
4164   }
4165   return base_mem;              // No memory splits; ID on the one true input
4166 }
4167 
4168 //------------------------------Ideal------------------------------------------
4169 // This method is invoked recursively on chains of MergeMem nodes
4170 Node *MergeMemNode::Ideal(PhaseGVN *phase, bool can_reshape) {
4171   // Remove chain'd MergeMems
4172   //
4173   // This is delicate, because the each "in(i)" (i &gt;= Raw) is interpreted
4174   // relative to the "in(Bot)".  Since we are patching both at the same time,
4175   // we have to be careful to read each "in(i)" relative to the old "in(Bot)",
4176   // but rewrite each "in(i)" relative to the new "in(Bot)".
4177   Node *progress = NULL;
4178 
4179 
4180   Node* old_base = base_memory();
4181   Node* empty_mem = empty_memory();
4182   if (old_base == empty_mem)
4183     return NULL; // Dead memory path.
4184 
4185   MergeMemNode* old_mbase;
4186   if (old_base != NULL &amp;&amp; old_base-&gt;is_MergeMem())
4187     old_mbase = old_base-&gt;as_MergeMem();
4188   else
4189     old_mbase = NULL;
4190   Node* new_base = old_base;
4191 
4192   // simplify stacked MergeMems in base memory
4193   if (old_mbase)  new_base = old_mbase-&gt;base_memory();
4194 
4195   // the base memory might contribute new slices beyond my req()
4196   if (old_mbase)  grow_to_match(old_mbase);
4197 
4198   // Look carefully at the base node if it is a phi.
4199   PhiNode* phi_base;
4200   if (new_base != NULL &amp;&amp; new_base-&gt;is_Phi())
4201     phi_base = new_base-&gt;as_Phi();
4202   else
4203     phi_base = NULL;
4204 
4205   Node*    phi_reg = NULL;
4206   uint     phi_len = (uint)-1;
4207   if (phi_base != NULL &amp;&amp; !phi_base-&gt;is_copy()) {
4208     // do not examine phi if degraded to a copy
4209     phi_reg = phi_base-&gt;region();
4210     phi_len = phi_base-&gt;req();
4211     // see if the phi is unfinished
4212     for (uint i = 1; i &lt; phi_len; i++) {
4213       if (phi_base-&gt;in(i) == NULL) {
4214         // incomplete phi; do not look at it yet!
4215         phi_reg = NULL;
4216         phi_len = (uint)-1;
4217         break;
4218       }
4219     }
4220   }
4221 
4222   // Note:  We do not call verify_sparse on entry, because inputs
4223   // can normalize to the base_memory via subsume_node or similar
4224   // mechanisms.  This method repairs that damage.
4225 
4226   assert(!old_mbase || old_mbase-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4227 
4228   // Look at each slice.
4229   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4230     Node* old_in = in(i);
4231     // calculate the old memory value
4232     Node* old_mem = old_in;
4233     if (old_mem == empty_mem)  old_mem = old_base;
4234     assert(old_mem == memory_at(i), "");
4235 
4236     // maybe update (reslice) the old memory value
4237 
4238     // simplify stacked MergeMems
4239     Node* new_mem = old_mem;
4240     MergeMemNode* old_mmem;
4241     if (old_mem != NULL &amp;&amp; old_mem-&gt;is_MergeMem())
4242       old_mmem = old_mem-&gt;as_MergeMem();
4243     else
4244       old_mmem = NULL;
4245     if (old_mmem == this) {
4246       // This can happen if loops break up and safepoints disappear.
4247       // A merge of BotPtr (default) with a RawPtr memory derived from a
4248       // safepoint can be rewritten to a merge of the same BotPtr with
4249       // the BotPtr phi coming into the loop.  If that phi disappears
4250       // also, we can end up with a self-loop of the mergemem.
4251       // In general, if loops degenerate and memory effects disappear,
4252       // a mergemem can be left looking at itself.  This simply means
4253       // that the mergemem's default should be used, since there is
4254       // no longer any apparent effect on this slice.
4255       // Note: If a memory slice is a MergeMem cycle, it is unreachable
4256       //       from start.  Update the input to TOP.
4257       new_mem = (new_base == this || new_base == empty_mem)? empty_mem : new_base;
4258     }
4259     else if (old_mmem != NULL) {
4260       new_mem = old_mmem-&gt;memory_at(i);
4261     }
4262     // else preceding memory was not a MergeMem
4263 
4264     // replace equivalent phis (unfortunately, they do not GVN together)
4265     if (new_mem != NULL &amp;&amp; new_mem != new_base &amp;&amp;
4266         new_mem-&gt;req() == phi_len &amp;&amp; new_mem-&gt;in(0) == phi_reg) {
4267       if (new_mem-&gt;is_Phi()) {
4268         PhiNode* phi_mem = new_mem-&gt;as_Phi();
4269         for (uint i = 1; i &lt; phi_len; i++) {
4270           if (phi_base-&gt;in(i) != phi_mem-&gt;in(i)) {
4271             phi_mem = NULL;
4272             break;
4273           }
4274         }
4275         if (phi_mem != NULL) {
4276           // equivalent phi nodes; revert to the def
4277           new_mem = new_base;
4278         }
4279       }
4280     }
4281 
4282     // maybe store down a new value
4283     Node* new_in = new_mem;
4284     if (new_in == new_base)  new_in = empty_mem;
4285 
4286     if (new_in != old_in) {
4287       // Warning:  Do not combine this "if" with the previous "if"
4288       // A memory slice might have be be rewritten even if it is semantically
4289       // unchanged, if the base_memory value has changed.
4290       set_req(i, new_in);
4291       progress = this;          // Report progress
4292     }
4293   }
4294 
4295   if (new_base != old_base) {
4296     set_req(Compile::AliasIdxBot, new_base);
4297     // Don't use set_base_memory(new_base), because we need to update du.
4298     assert(base_memory() == new_base, "");
4299     progress = this;
4300   }
4301 
4302   if( base_memory() == this ) {
4303     // a self cycle indicates this memory path is dead
4304     set_req(Compile::AliasIdxBot, empty_mem);
4305   }
4306 
4307   // Resolve external cycles by calling Ideal on a MergeMem base_memory
4308   // Recursion must occur after the self cycle check above
4309   if( base_memory()-&gt;is_MergeMem() ) {
4310     MergeMemNode *new_mbase = base_memory()-&gt;as_MergeMem();
4311     Node *m = phase-&gt;transform(new_mbase);  // Rollup any cycles
4312     if( m != NULL &amp;&amp; (m-&gt;is_top() ||
4313         m-&gt;is_MergeMem() &amp;&amp; m-&gt;as_MergeMem()-&gt;base_memory() == empty_mem) ) {
4314       // propagate rollup of dead cycle to self
4315       set_req(Compile::AliasIdxBot, empty_mem);
4316     }
4317   }
4318 
4319   if( base_memory() == empty_mem ) {
4320     progress = this;
4321     // Cut inputs during Parse phase only.
4322     // During Optimize phase a dead MergeMem node will be subsumed by Top.
4323     if( !can_reshape ) {
4324       for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4325         if( in(i) != empty_mem ) { set_req(i, empty_mem); }
4326       }
4327     }
4328   }
4329 
4330   if( !progress &amp;&amp; base_memory()-&gt;is_Phi() &amp;&amp; can_reshape ) {
4331     // Check if PhiNode::Ideal's "Split phis through memory merges"
4332     // transform should be attempted. Look for this-&gt;phi-&gt;this cycle.
4333     uint merge_width = req();
4334     if (merge_width &gt; Compile::AliasIdxRaw) {
4335       PhiNode* phi = base_memory()-&gt;as_Phi();
4336       for( uint i = 1; i &lt; phi-&gt;req(); ++i ) {// For all paths in
4337         if (phi-&gt;in(i) == this) {
4338           phase-&gt;is_IterGVN()-&gt;_worklist.push(phi);
4339           break;
4340         }
4341       }
4342     }
4343   }
4344 
4345   assert(progress || verify_sparse(), "please, no dups of base");
4346   return progress;
4347 }
4348 
4349 //-------------------------set_base_memory-------------------------------------
4350 void MergeMemNode::set_base_memory(Node *new_base) {
4351   Node* empty_mem = empty_memory();
4352   set_req(Compile::AliasIdxBot, new_base);
4353   assert(memory_at(req()) == new_base, "must set default memory");
4354   // Clear out other occurrences of new_base:
4355   if (new_base != empty_mem) {
4356     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4357       if (in(i) == new_base)  set_req(i, empty_mem);
4358     }
4359   }
4360 }
4361 
4362 //------------------------------out_RegMask------------------------------------
4363 const RegMask &amp;MergeMemNode::out_RegMask() const {
4364   return RegMask::Empty;
4365 }
4366 
4367 //------------------------------dump_spec--------------------------------------
4368 #ifndef PRODUCT
4369 void MergeMemNode::dump_spec(outputStream *st) const {
4370   st-&gt;print(" {");
4371   Node* base_mem = base_memory();
4372   for( uint i = Compile::AliasIdxRaw; i &lt; req(); i++ ) {
4373     Node* mem = (in(i) != NULL) ? memory_at(i) : base_mem;
4374     if (mem == base_mem) { st-&gt;print(" -"); continue; }
4375     st-&gt;print( " N%d:", mem-&gt;_idx );
4376     Compile::current()-&gt;get_adr_type(i)-&gt;dump_on(st);
4377   }
4378   st-&gt;print(" }");
4379 }
4380 #endif // !PRODUCT
4381 
4382 
4383 #ifdef ASSERT
4384 static bool might_be_same(Node* a, Node* b) {
4385   if (a == b)  return true;
4386   if (!(a-&gt;is_Phi() || b-&gt;is_Phi()))  return false;
4387   // phis shift around during optimization
4388   return true;  // pretty stupid...
4389 }
4390 
4391 // verify a narrow slice (either incoming or outgoing)
4392 static void verify_memory_slice(const MergeMemNode* m, int alias_idx, Node* n) {
4393   if (!VerifyAliases)       return;  // don't bother to verify unless requested
4394   if (is_error_reported())  return;  // muzzle asserts when debugging an error
4395   if (Node::in_dump())      return;  // muzzle asserts when printing
4396   assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not disturb base_memory or sentinel");
4397   assert(n != NULL, "");
4398   // Elide intervening MergeMem's
4399   while (n-&gt;is_MergeMem()) {
4400     n = n-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
4401   }
4402   Compile* C = Compile::current();
4403   const TypePtr* n_adr_type = n-&gt;adr_type();
4404   if (n == m-&gt;empty_memory()) {
4405     // Implicit copy of base_memory()
4406   } else if (n_adr_type != TypePtr::BOTTOM) {
4407     assert(n_adr_type != NULL, "new memory must have a well-defined adr_type");
4408     assert(C-&gt;must_alias(n_adr_type, alias_idx), "new memory must match selected slice");
4409   } else {
4410     // A few places like make_runtime_call "know" that VM calls are narrow,
4411     // and can be used to update only the VM bits stored as TypeRawPtr::BOTTOM.
4412     bool expected_wide_mem = false;
4413     if (n == m-&gt;base_memory()) {
4414       expected_wide_mem = true;
4415     } else if (alias_idx == Compile::AliasIdxRaw ||
4416                n == m-&gt;memory_at(Compile::AliasIdxRaw)) {
4417       expected_wide_mem = true;
4418     } else if (!C-&gt;alias_type(alias_idx)-&gt;is_rewritable()) {
4419       // memory can "leak through" calls on channels that
4420       // are write-once.  Allow this also.
4421       expected_wide_mem = true;
4422     }
4423     assert(expected_wide_mem, "expected narrow slice replacement");
4424   }
4425 }
4426 #else // !ASSERT
4427 #define verify_memory_slice(m,i,n) (void)(0)  // PRODUCT version is no-op
4428 #endif
4429 
4430 
4431 //-----------------------------memory_at---------------------------------------
4432 Node* MergeMemNode::memory_at(uint alias_idx) const {
4433   assert(alias_idx &gt;= Compile::AliasIdxRaw ||
4434          alias_idx == Compile::AliasIdxBot &amp;&amp; Compile::current()-&gt;AliasLevel() == 0,
4435          "must avoid base_memory and AliasIdxTop");
4436 
4437   // Otherwise, it is a narrow slice.
4438   Node* n = alias_idx &lt; req() ? in(alias_idx) : empty_memory();
4439   Compile *C = Compile::current();
4440   if (is_empty_memory(n)) {
4441     // the array is sparse; empty slots are the "top" node
4442     n = base_memory();
4443     assert(Node::in_dump()
4444            || n == NULL || n-&gt;bottom_type() == Type::TOP
4445            || n-&gt;adr_type() == NULL // address is TOP
4446            || n-&gt;adr_type() == TypePtr::BOTTOM
4447            || n-&gt;adr_type() == TypeRawPtr::BOTTOM
4448            || Compile::current()-&gt;AliasLevel() == 0,
4449            "must be a wide memory");
4450     // AliasLevel == 0 if we are organizing the memory states manually.
4451     // See verify_memory_slice for comments on TypeRawPtr::BOTTOM.
4452   } else {
4453     // make sure the stored slice is sane
4454     #ifdef ASSERT
4455     if (is_error_reported() || Node::in_dump()) {
4456     } else if (might_be_same(n, base_memory())) {
4457       // Give it a pass:  It is a mostly harmless repetition of the base.
4458       // This can arise normally from node subsumption during optimization.
4459     } else {
4460       verify_memory_slice(this, alias_idx, n);
4461     }
4462     #endif
4463   }
4464   return n;
4465 }
4466 
4467 //---------------------------set_memory_at-------------------------------------
4468 void MergeMemNode::set_memory_at(uint alias_idx, Node *n) {
4469   verify_memory_slice(this, alias_idx, n);
4470   Node* empty_mem = empty_memory();
4471   if (n == base_memory())  n = empty_mem;  // collapse default
4472   uint need_req = alias_idx+1;
4473   if (req() &lt; need_req) {
4474     if (n == empty_mem)  return;  // already the default, so do not grow me
4475     // grow the sparse array
4476     do {
4477       add_req(empty_mem);
4478     } while (req() &lt; need_req);
4479   }
4480   set_req( alias_idx, n );
4481 }
4482 
4483 
4484 
4485 //--------------------------iteration_setup------------------------------------
4486 void MergeMemNode::iteration_setup(const MergeMemNode* other) {
4487   if (other != NULL) {
4488     grow_to_match(other);
4489     // invariant:  the finite support of mm2 is within mm-&gt;req()
4490     #ifdef ASSERT
4491     for (uint i = req(); i &lt; other-&gt;req(); i++) {
4492       assert(other-&gt;is_empty_memory(other-&gt;in(i)), "slice left uncovered");
4493     }
4494     #endif
4495   }
4496   // Replace spurious copies of base_memory by top.
4497   Node* base_mem = base_memory();
4498   if (base_mem != NULL &amp;&amp; !base_mem-&gt;is_top()) {
4499     for (uint i = Compile::AliasIdxBot+1, imax = req(); i &lt; imax; i++) {
4500       if (in(i) == base_mem)
4501         set_req(i, empty_memory());
4502     }
4503   }
4504 }
4505 
4506 //---------------------------grow_to_match-------------------------------------
4507 void MergeMemNode::grow_to_match(const MergeMemNode* other) {
4508   Node* empty_mem = empty_memory();
4509   assert(other-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4510   // look for the finite support of the other memory
4511   for (uint i = other-&gt;req(); --i &gt;= req(); ) {
4512     if (other-&gt;in(i) != empty_mem) {
4513       uint new_len = i+1;
4514       while (req() &lt; new_len)  add_req(empty_mem);
4515       break;
4516     }
4517   }
4518 }
4519 
4520 //---------------------------verify_sparse-------------------------------------
4521 #ifndef PRODUCT
4522 bool MergeMemNode::verify_sparse() const {
4523   assert(is_empty_memory(make_empty_memory()), "sane sentinel");
4524   Node* base_mem = base_memory();
4525   // The following can happen in degenerate cases, since empty==top.
4526   if (is_empty_memory(base_mem))  return true;
4527   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4528     assert(in(i) != NULL, "sane slice");
4529     if (in(i) == base_mem)  return false;  // should have been the sentinel value!
4530   }
4531   return true;
4532 }
4533 
4534 bool MergeMemStream::match_memory(Node* mem, const MergeMemNode* mm, int idx) {
4535   Node* n;
4536   n = mm-&gt;in(idx);
4537   if (mem == n)  return true;  // might be empty_memory()
4538   n = (idx == Compile::AliasIdxBot)? mm-&gt;base_memory(): mm-&gt;memory_at(idx);
4539   if (mem == n)  return true;
4540   while (n-&gt;is_Phi() &amp;&amp; (n = n-&gt;as_Phi()-&gt;is_copy()) != NULL) {
4541     if (mem == n)  return true;
4542     if (n == NULL)  break;
4543   }
4544   return false;
4545 }
4546 #endif // !PRODUCT
</pre></body></html>
