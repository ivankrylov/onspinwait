<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/library_call.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "classfile/vmSymbols.hpp"
  29 #include "compiler/compileBroker.hpp"
  30 #include "compiler/compileLog.hpp"
  31 #include "oops/objArrayKlass.hpp"
  32 #include "opto/addnode.hpp"
  33 #include "opto/arraycopynode.hpp"
  34 #include "opto/c2compiler.hpp"
  35 #include "opto/callGenerator.hpp"
  36 #include "opto/castnode.hpp"
  37 #include "opto/cfgnode.hpp"
  38 #include "opto/convertnode.hpp"
  39 #include "opto/countbitsnode.hpp"
  40 #include "opto/intrinsicnode.hpp"
  41 #include "opto/idealKit.hpp"
  42 #include "opto/mathexactnode.hpp"
  43 #include "opto/movenode.hpp"
  44 #include "opto/mulnode.hpp"
  45 #include "opto/narrowptrnode.hpp"
  46 #include "opto/opaquenode.hpp"
  47 #include "opto/parse.hpp"
  48 #include "opto/runtime.hpp"
  49 #include "opto/subnode.hpp"
  50 #include "prims/nativeLookup.hpp"
  51 #include "runtime/sharedRuntime.hpp"
  52 #include "trace/traceMacros.hpp"
  53 
  54 class LibraryIntrinsic : public InlineCallGenerator {
  55   // Extend the set of intrinsics known to the runtime:
  56  public:
  57  private:
  58   bool             _is_virtual;
  59   bool             _does_virtual_dispatch;
  60   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  61   int8_t           _last_predicate; // Last generated predicate
  62   vmIntrinsics::ID _intrinsic_id;
  63 
  64  public:
  65   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  66     : InlineCallGenerator(m),
  67       _is_virtual(is_virtual),
  68       _does_virtual_dispatch(does_virtual_dispatch),
  69       _predicates_count((int8_t)predicates_count),
  70       _last_predicate((int8_t)-1),
  71       _intrinsic_id(id)
  72   {
  73   }
  74   virtual bool is_intrinsic() const { return true; }
  75   virtual bool is_virtual()   const { return _is_virtual; }
  76   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  77   virtual int  predicates_count() const { return _predicates_count; }
  78   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  79   virtual JVMState* generate(JVMState* jvms);
  80   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  81   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  82 };
  83 
  84 
  85 // Local helper class for LibraryIntrinsic:
  86 class LibraryCallKit : public GraphKit {
  87  private:
  88   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  89   Node*             _result;        // the result node, if any
  90   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  91 
  92   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  93 
  94  public:
  95   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  96     : GraphKit(jvms),
  97       _intrinsic(intrinsic),
  98       _result(NULL)
  99   {
 100     // Check if this is a root compile.  In that case we don't have a caller.
 101     if (!jvms-&gt;has_method()) {
 102       _reexecute_sp = sp();
 103     } else {
 104       // Find out how many arguments the interpreter needs when deoptimizing
 105       // and save the stack pointer value so it can used by uncommon_trap.
 106       // We find the argument count by looking at the declared signature.
 107       bool ignored_will_link;
 108       ciSignature* declared_signature = NULL;
 109       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 110       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 111       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 112     }
 113   }
 114 
 115   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 116 
 117   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 118   int               bci()       const    { return jvms()-&gt;bci(); }
 119   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 120   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 121   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 122 
 123   bool  try_to_inline(int predicate);
 124   Node* try_to_predicate(int predicate);
 125 
 126   void push_result() {
 127     // Push the result onto the stack.
 128     if (!stopped() &amp;&amp; result() != NULL) {
 129       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 130       push_node(bt, result());
 131     }
 132   }
 133 
 134  private:
 135   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 136     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 137   }
 138 
 139   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 140   void  set_result(RegionNode* region, PhiNode* value);
 141   Node*     result() { return _result; }
 142 
 143   virtual int reexecute_sp() { return _reexecute_sp; }
 144 
 145   // Helper functions to inline natives
 146   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 147   Node* generate_slow_guard(Node* test, RegionNode* region);
 148   Node* generate_fair_guard(Node* test, RegionNode* region);
 149   Node* generate_negative_guard(Node* index, RegionNode* region,
 150                                 // resulting CastII of index:
 151                                 Node* *pos_index = NULL);
 152   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 153                              Node* array_length,
 154                              RegionNode* region);
 155   Node* generate_current_thread(Node* &amp;tls_output);
 156   Node* load_mirror_from_klass(Node* klass);
 157   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 158                                       RegionNode* region, int null_path,
 159                                       int offset);
 160   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 161                                RegionNode* region, int null_path) {
 162     int offset = java_lang_Class::klass_offset_in_bytes();
 163     return load_klass_from_mirror_common(mirror, never_see_null,
 164                                          region, null_path,
 165                                          offset);
 166   }
 167   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 168                                      RegionNode* region, int null_path) {
 169     int offset = java_lang_Class::array_klass_offset_in_bytes();
 170     return load_klass_from_mirror_common(mirror, never_see_null,
 171                                          region, null_path,
 172                                          offset);
 173   }
 174   Node* generate_access_flags_guard(Node* kls,
 175                                     int modifier_mask, int modifier_bits,
 176                                     RegionNode* region);
 177   Node* generate_interface_guard(Node* kls, RegionNode* region);
 178   Node* generate_array_guard(Node* kls, RegionNode* region) {
 179     return generate_array_guard_common(kls, region, false, false);
 180   }
 181   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 182     return generate_array_guard_common(kls, region, false, true);
 183   }
 184   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 185     return generate_array_guard_common(kls, region, true, false);
 186   }
 187   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 188     return generate_array_guard_common(kls, region, true, true);
 189   }
 190   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 191                                     bool obj_array, bool not_array);
 192   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 193   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 194                                      bool is_virtual = false, bool is_static = false);
 195   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 196     return generate_method_call(method_id, false, true);
 197   }
 198   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 199     return generate_method_call(method_id, true, false);
 200   }
 201   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 202 
 203   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 204   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 205   bool inline_string_compareTo();
 206   bool inline_string_indexOf();
 207   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 208   bool inline_string_equals();
 209   Node* round_double_node(Node* n);
 210   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 211   bool inline_math_native(vmIntrinsics::ID id);
 212   bool inline_trig(vmIntrinsics::ID id);
 213   bool inline_math(vmIntrinsics::ID id);
 214   template &lt;typename OverflowOp&gt;
 215   bool inline_math_overflow(Node* arg1, Node* arg2);
 216   void inline_math_mathExact(Node* math, Node* test);
 217   bool inline_math_addExactI(bool is_increment);
 218   bool inline_math_addExactL(bool is_increment);
 219   bool inline_math_multiplyExactI();
 220   bool inline_math_multiplyExactL();
 221   bool inline_math_negateExactI();
 222   bool inline_math_negateExactL();
 223   bool inline_math_subtractExactI(bool is_decrement);
 224   bool inline_math_subtractExactL(bool is_decrement);
 225   bool inline_exp();
 226   bool inline_pow();
 227   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 228   bool inline_min_max(vmIntrinsics::ID id);
 229   bool inline_notify(vmIntrinsics::ID id);
 230   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 231   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 232   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 233   Node* make_unsafe_address(Node* base, Node* offset);
 234   // Helper for inline_unsafe_access.
 235   // Generates the guards that check whether the result of
 236   // Unsafe.getObject should be recorded in an SATB log buffer.
 237   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 238   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 239   static bool klass_needs_init_guard(Node* kls);
 240   bool inline_unsafe_allocate();
 241   bool inline_unsafe_copyMemory();
 242   bool inline_native_currentThread();
 243 #ifdef TRACE_HAVE_INTRINSICS
 244   bool inline_native_classID();
 245   bool inline_native_threadID();
 246 #endif
 247   bool inline_native_time_funcs(address method, const char* funcName);
 248   bool inline_native_isInterrupted();
 249   bool inline_native_Class_query(vmIntrinsics::ID id);
 250   bool inline_native_subtype_check();
 251 
 252   bool inline_native_newArray();
 253   bool inline_native_getLength();
 254   bool inline_array_copyOf(bool is_copyOfRange);
 255   bool inline_array_equals();
 256   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 257   bool inline_native_clone(bool is_virtual);
 258   bool inline_native_Reflection_getCallerClass();
 259   // Helper function for inlining native object hash method
 260   bool inline_native_hashcode(bool is_virtual, bool is_static);
 261   bool inline_native_getClass();
 262 
 263   // Helper functions for inlining arraycopy
 264   bool inline_arraycopy();
 265   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 266                                                 RegionNode* slow_region);
 267   JVMState* arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp);
 268   void arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp);
 269 
 270   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 271   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 272   bool inline_unsafe_ordered_store(BasicType type);
 273   bool inline_unsafe_fence(vmIntrinsics::ID id);
 274   bool inline_spinloophint();
 275   bool inline_fp_conversions(vmIntrinsics::ID id);
 276   bool inline_number_methods(vmIntrinsics::ID id);
 277   bool inline_reference_get();
 278   bool inline_Class_cast();
 279   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 280   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 281   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 282   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 283   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 284   bool inline_ghash_processBlocks();
 285   bool inline_sha_implCompress(vmIntrinsics::ID id);
 286   bool inline_digestBase_implCompressMB(int predicate);
 287   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 288                                  bool long_state, address stubAddr, const char *stubName,
 289                                  Node* src_start, Node* ofs, Node* limit);
 290   Node* get_state_from_sha_object(Node *sha_object);
 291   Node* get_state_from_sha5_object(Node *sha_object);
 292   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 293   bool inline_encodeISOArray();
 294   bool inline_updateCRC32();
 295   bool inline_updateBytesCRC32();
 296   bool inline_updateByteBufferCRC32();
 297   Node* get_table_from_crc32c_class(ciInstanceKlass *crc32c_class);
 298   bool inline_updateBytesCRC32C();
 299   bool inline_updateDirectByteBufferCRC32C();
 300   bool inline_updateBytesAdler32();
 301   bool inline_updateByteBufferAdler32();
 302   bool inline_multiplyToLen();
 303   bool inline_squareToLen();
 304   bool inline_mulAdd();
 305   bool inline_montgomeryMultiply();
 306   bool inline_montgomerySquare();
 307 
 308   bool inline_profileBoolean();
 309   bool inline_isCompileConstant();
 310 };
 311 
 312 //---------------------------make_vm_intrinsic----------------------------
 313 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 314   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 315   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 316 
 317   if (!m-&gt;is_loaded()) {
 318     // Do not attempt to inline unloaded methods.
 319     return NULL;
 320   }
 321 
 322   C2Compiler* compiler = (C2Compiler*)CompileBroker::compiler(CompLevel_full_optimization);
 323   bool is_available = false;
 324 
 325   {
 326     // For calling is_intrinsic_supported and is_intrinsic_disabled_by_flag
 327     // the compiler must transition to '_thread_in_vm' state because both
 328     // methods access VM-internal data.
 329     VM_ENTRY_MARK;
 330     methodHandle mh(THREAD, m-&gt;get_Method());
 331     methodHandle ct(THREAD, method()-&gt;get_Method());
 332     is_available = compiler-&gt;is_intrinsic_supported(mh, is_virtual) &amp;&amp;
 333                    !vmIntrinsics::is_disabled_by_flags(mh, ct);
 334   }
 335 
 336   if (is_available) {
 337     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 338     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 339     return new LibraryIntrinsic(m, is_virtual,
 340                                 vmIntrinsics::predicates_needed(id),
 341                                 vmIntrinsics::does_virtual_dispatch(id),
 342                                 (vmIntrinsics::ID) id);
 343   } else {
 344     return NULL;
 345   }
 346 }
 347 
 348 //----------------------register_library_intrinsics-----------------------
 349 // Initialize this file's data structures, for each Compile instance.
 350 void Compile::register_library_intrinsics() {
 351   // Nothing to do here.
 352 }
 353 
 354 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 355   LibraryCallKit kit(jvms, this);
 356   Compile* C = kit.C;
 357   int nodes = C-&gt;unique();
 358 #ifndef PRODUCT
 359   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 360     char buf[1000];
 361     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 362     tty-&gt;print_cr("Intrinsic %s", str);
 363   }
 364 #endif
 365   ciMethod* callee = kit.callee();
 366   const int bci    = kit.bci();
 367 
 368   // Try to inline the intrinsic.
 369   if ((CheckIntrinsics ? callee-&gt;intrinsic_candidate() : true) &amp;&amp;
 370       kit.try_to_inline(_last_predicate)) {
 371     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 372       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 373     }
 374     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 375     if (C-&gt;log()) {
 376       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 377                      vmIntrinsics::name_at(intrinsic_id()),
 378                      (is_virtual() ? " virtual='1'" : ""),
 379                      C-&gt;unique() - nodes);
 380     }
 381     // Push the result from the inlined method onto the stack.
 382     kit.push_result();
 383     C-&gt;print_inlining_update(this);
 384     return kit.transfer_exceptions_into_jvms();
 385   }
 386 
 387   // The intrinsic bailed out
 388   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 389     if (jvms-&gt;has_method()) {
 390       // Not a root compile.
 391       const char* msg;
 392       if (callee-&gt;intrinsic_candidate()) {
 393         msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 394       } else {
 395         msg = is_virtual() ? "failed to inline (intrinsic, virtual), method not annotated"
 396                            : "failed to inline (intrinsic), method not annotated";
 397       }
 398       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 399     } else {
 400       // Root compile
 401       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 402                vmIntrinsics::name_at(intrinsic_id()),
 403                (is_virtual() ? " (virtual)" : ""), bci);
 404     }
 405   }
 406   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 407   C-&gt;print_inlining_update(this);
 408   return NULL;
 409 }
 410 
 411 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 412   LibraryCallKit kit(jvms, this);
 413   Compile* C = kit.C;
 414   int nodes = C-&gt;unique();
 415   _last_predicate = predicate;
 416 #ifndef PRODUCT
 417   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 418   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 419     char buf[1000];
 420     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 421     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 422   }
 423 #endif
 424   ciMethod* callee = kit.callee();
 425   const int bci    = kit.bci();
 426 
 427   Node* slow_ctl = kit.try_to_predicate(predicate);
 428   if (!kit.failing()) {
 429     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 430       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 431     }
 432     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 433     if (C-&gt;log()) {
 434       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 435                      vmIntrinsics::name_at(intrinsic_id()),
 436                      (is_virtual() ? " virtual='1'" : ""),
 437                      C-&gt;unique() - nodes);
 438     }
 439     return slow_ctl; // Could be NULL if the check folds.
 440   }
 441 
 442   // The intrinsic bailed out
 443   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 444     if (jvms-&gt;has_method()) {
 445       // Not a root compile.
 446       const char* msg = "failed to generate predicate for intrinsic";
 447       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 448     } else {
 449       // Root compile
 450       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 451                                         vmIntrinsics::name_at(intrinsic_id()),
 452                                         (is_virtual() ? " (virtual)" : ""), bci);
 453     }
 454   }
 455   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 456   return NULL;
 457 }
 458 
 459 bool LibraryCallKit::try_to_inline(int predicate) {
 460   // Handle symbolic names for otherwise undistinguished boolean switches:
 461   const bool is_store       = true;
 462   const bool is_native_ptr  = true;
 463   const bool is_static      = true;
 464   const bool is_volatile    = true;
 465 
 466   if (!jvms()-&gt;has_method()) {
 467     // Root JVMState has a null method.
 468     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 469     // Insert the memory aliasing node
 470     set_all_memory(reset_memory());
 471   }
 472   assert(merged_memory(), "");
 473 
 474 
 475   switch (intrinsic_id()) {
 476   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 477   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 478   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 479 
 480   case vmIntrinsics::_dsin:
 481   case vmIntrinsics::_dcos:
 482   case vmIntrinsics::_dtan:
 483   case vmIntrinsics::_dabs:
 484   case vmIntrinsics::_datan2:
 485   case vmIntrinsics::_dsqrt:
 486   case vmIntrinsics::_dexp:
 487   case vmIntrinsics::_dlog:
 488   case vmIntrinsics::_dlog10:
 489   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 490 
 491   case vmIntrinsics::_min:
 492   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 493 
 494   case vmIntrinsics::_notify:
 495   case vmIntrinsics::_notifyAll:
 496     if (InlineNotify) {
 497       return inline_notify(intrinsic_id());
 498     }
 499     return false;
 500 
 501   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 502   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 503   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 504   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 505   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 506   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 507   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 508   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 509   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 510   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 511   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 512   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 513 
 514   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 515 
 516   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 517   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 518   case vmIntrinsics::_equals:                   return inline_string_equals();
 519 
 520   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 521   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 522   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 523   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 524   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 525   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 526   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 527   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 528   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 529   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 530   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 531   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 532   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 533   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 534   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 535   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 536   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 537   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 538 
 539   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 540   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 541   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 542   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 543   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 544   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 545   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 546   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 547 
 548   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 549   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 550   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 551   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 552   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 553   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 554   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 555   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 556 
 557   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 558   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 559   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 560   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 561   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 562   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 563   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 564   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 565   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 566 
 567   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 568   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 569   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 570   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 571   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 572   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 573   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 574   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 575   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 576 
 577   case vmIntrinsics::_getShortUnaligned:        return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 578   case vmIntrinsics::_getCharUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 579   case vmIntrinsics::_getIntUnaligned:          return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 580   case vmIntrinsics::_getLongUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 581 
 582   case vmIntrinsics::_putShortUnaligned:        return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 583   case vmIntrinsics::_putCharUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 584   case vmIntrinsics::_putIntUnaligned:          return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 585   case vmIntrinsics::_putLongUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 586 
 587   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 588   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 589   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 590 
 591   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 592   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 593   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 594 
 595   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 596   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 597   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 598   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 599   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 600 
 601   case vmIntrinsics::_loadFence:
 602   case vmIntrinsics::_storeFence:
 603   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 604 
 605   case vmIntrinsics::_spinLoopHint:             return inline_spinloophint();
 606 
 607   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 608   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 609 
 610 #ifdef TRACE_HAVE_INTRINSICS
 611   case vmIntrinsics::_classID:                  return inline_native_classID();
 612   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 613   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 614 #endif
 615   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 616   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 617   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 618   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 619   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 620   case vmIntrinsics::_getLength:                return inline_native_getLength();
 621   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 622   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 623   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 624   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 625 
 626   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 627 
 628   case vmIntrinsics::_isInstance:
 629   case vmIntrinsics::_getModifiers:
 630   case vmIntrinsics::_isInterface:
 631   case vmIntrinsics::_isArray:
 632   case vmIntrinsics::_isPrimitive:
 633   case vmIntrinsics::_getSuperclass:
 634   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 635 
 636   case vmIntrinsics::_floatToRawIntBits:
 637   case vmIntrinsics::_floatToIntBits:
 638   case vmIntrinsics::_intBitsToFloat:
 639   case vmIntrinsics::_doubleToRawLongBits:
 640   case vmIntrinsics::_doubleToLongBits:
 641   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 642 
 643   case vmIntrinsics::_numberOfLeadingZeros_i:
 644   case vmIntrinsics::_numberOfLeadingZeros_l:
 645   case vmIntrinsics::_numberOfTrailingZeros_i:
 646   case vmIntrinsics::_numberOfTrailingZeros_l:
 647   case vmIntrinsics::_bitCount_i:
 648   case vmIntrinsics::_bitCount_l:
 649   case vmIntrinsics::_reverseBytes_i:
 650   case vmIntrinsics::_reverseBytes_l:
 651   case vmIntrinsics::_reverseBytes_s:
 652   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 653 
 654   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 655 
 656   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 657 
 658   case vmIntrinsics::_Class_cast:               return inline_Class_cast();
 659 
 660   case vmIntrinsics::_aescrypt_encryptBlock:
 661   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 662 
 663   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 664   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 665     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 666 
 667   case vmIntrinsics::_sha_implCompress:
 668   case vmIntrinsics::_sha2_implCompress:
 669   case vmIntrinsics::_sha5_implCompress:
 670     return inline_sha_implCompress(intrinsic_id());
 671 
 672   case vmIntrinsics::_digestBase_implCompressMB:
 673     return inline_digestBase_implCompressMB(predicate);
 674 
 675   case vmIntrinsics::_multiplyToLen:
 676     return inline_multiplyToLen();
 677 
 678   case vmIntrinsics::_squareToLen:
 679     return inline_squareToLen();
 680 
 681   case vmIntrinsics::_mulAdd:
 682     return inline_mulAdd();
 683 
 684   case vmIntrinsics::_montgomeryMultiply:
 685     return inline_montgomeryMultiply();
 686   case vmIntrinsics::_montgomerySquare:
 687     return inline_montgomerySquare();
 688 
 689   case vmIntrinsics::_ghash_processBlocks:
 690     return inline_ghash_processBlocks();
 691 
 692   case vmIntrinsics::_encodeISOArray:
 693     return inline_encodeISOArray();
 694 
 695   case vmIntrinsics::_updateCRC32:
 696     return inline_updateCRC32();
 697   case vmIntrinsics::_updateBytesCRC32:
 698     return inline_updateBytesCRC32();
 699   case vmIntrinsics::_updateByteBufferCRC32:
 700     return inline_updateByteBufferCRC32();
 701 
 702   case vmIntrinsics::_updateBytesCRC32C:
 703     return inline_updateBytesCRC32C();
 704   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 705     return inline_updateDirectByteBufferCRC32C();
 706 
 707   case vmIntrinsics::_updateBytesAdler32:
 708     return inline_updateBytesAdler32();
 709   case vmIntrinsics::_updateByteBufferAdler32:
 710     return inline_updateByteBufferAdler32();
 711 
 712   case vmIntrinsics::_profileBoolean:
 713     return inline_profileBoolean();
 714   case vmIntrinsics::_isCompileConstant:
 715     return inline_isCompileConstant();
 716 
 717   default:
 718     // If you get here, it may be that someone has added a new intrinsic
 719     // to the list in vmSymbols.hpp without implementing it here.
 720 #ifndef PRODUCT
 721     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 722       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 723                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 724     }
 725 #endif
 726     return false;
 727   }
 728 }
 729 
 730 Node* LibraryCallKit::try_to_predicate(int predicate) {
 731   if (!jvms()-&gt;has_method()) {
 732     // Root JVMState has a null method.
 733     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 734     // Insert the memory aliasing node
 735     set_all_memory(reset_memory());
 736   }
 737   assert(merged_memory(), "");
 738 
 739   switch (intrinsic_id()) {
 740   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 741     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 742   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 743     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 744   case vmIntrinsics::_digestBase_implCompressMB:
 745     return inline_digestBase_implCompressMB_predicate(predicate);
 746 
 747   default:
 748     // If you get here, it may be that someone has added a new intrinsic
 749     // to the list in vmSymbols.hpp without implementing it here.
 750 #ifndef PRODUCT
 751     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 752       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 753                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 754     }
 755 #endif
 756     Node* slow_ctl = control();
 757     set_control(top()); // No fast path instrinsic
 758     return slow_ctl;
 759   }
 760 }
 761 
 762 //------------------------------set_result-------------------------------
 763 // Helper function for finishing intrinsics.
 764 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 765   record_for_igvn(region);
 766   set_control(_gvn.transform(region));
 767   set_result( _gvn.transform(value));
 768   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
 769 }
 770 
 771 //------------------------------generate_guard---------------------------
 772 // Helper function for generating guarded fast-slow graph structures.
 773 // The given 'test', if true, guards a slow path.  If the test fails
 774 // then a fast path can be taken.  (We generally hope it fails.)
 775 // In all cases, GraphKit::control() is updated to the fast path.
 776 // The returned value represents the control for the slow path.
 777 // The return value is never 'top'; it is either a valid control
 778 // or NULL if it is obvious that the slow path can never be taken.
 779 // Also, if region and the slow control are not NULL, the slow edge
 780 // is appended to the region.
 781 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
 782   if (stopped()) {
 783     // Already short circuited.
 784     return NULL;
 785   }
 786 
 787   // Build an if node and its projections.
 788   // If test is true we take the slow path, which we assume is uncommon.
 789   if (_gvn.type(test) == TypeInt::ZERO) {
 790     // The slow branch is never taken.  No need to build this guard.
 791     return NULL;
 792   }
 793 
 794   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
 795 
 796   Node* if_slow = _gvn.transform(new IfTrueNode(iff));
 797   if (if_slow == top()) {
 798     // The slow branch is never taken.  No need to build this guard.
 799     return NULL;
 800   }
 801 
 802   if (region != NULL)
 803     region-&gt;add_req(if_slow);
 804 
 805   Node* if_fast = _gvn.transform(new IfFalseNode(iff));
 806   set_control(if_fast);
 807 
 808   return if_slow;
 809 }
 810 
 811 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
 812   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
 813 }
 814 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
 815   return generate_guard(test, region, PROB_FAIR);
 816 }
 817 
 818 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
 819                                                      Node* *pos_index) {
 820   if (stopped())
 821     return NULL;                // already stopped
 822   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
 823     return NULL;                // index is already adequately typed
 824   Node* cmp_lt = _gvn.transform(new CmpINode(index, intcon(0)));
 825   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 826   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
 827   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
 828     // Emulate effect of Parse::adjust_map_after_if.
 829     Node* ccast = new CastIINode(index, TypeInt::POS);
 830     ccast-&gt;set_req(0, control());
 831     (*pos_index) = _gvn.transform(ccast);
 832   }
 833   return is_neg;
 834 }
 835 
 836 // Make sure that 'position' is a valid limit index, in [0..length].
 837 // There are two equivalent plans for checking this:
 838 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
 839 //   B. offset  &lt;=  (arrayLength - copyLength)
 840 // We require that all of the values above, except for the sum and
 841 // difference, are already known to be non-negative.
 842 // Plan A is robust in the face of overflow, if offset and copyLength
 843 // are both hugely positive.
 844 //
 845 // Plan B is less direct and intuitive, but it does not overflow at
 846 // all, since the difference of two non-negatives is always
 847 // representable.  Whenever Java methods must perform the equivalent
 848 // check they generally use Plan B instead of Plan A.
 849 // For the moment we use Plan A.
 850 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
 851                                                   Node* subseq_length,
 852                                                   Node* array_length,
 853                                                   RegionNode* region) {
 854   if (stopped())
 855     return NULL;                // already stopped
 856   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
 857   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
 858     return NULL;                // common case of whole-array copy
 859   Node* last = subseq_length;
 860   if (!zero_offset)             // last += offset
 861     last = _gvn.transform(new AddINode(last, offset));
 862   Node* cmp_lt = _gvn.transform(new CmpUNode(array_length, last));
 863   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 864   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
 865   return is_over;
 866 }
 867 
 868 
 869 //--------------------------generate_current_thread--------------------
 870 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
 871   ciKlass*    thread_klass = env()-&gt;Thread_klass();
 872   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
 873   Node* thread = _gvn.transform(new ThreadLocalNode());
 874   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
 875   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
 876   tls_output = thread;
 877   return threadObj;
 878 }
 879 
 880 
 881 //------------------------------make_string_method_node------------------------
 882 // Helper method for String intrinsic functions. This version is called
 883 // with str1 and str2 pointing to String object nodes.
 884 //
 885 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
 886   Node* no_ctrl = NULL;
 887 
 888   // Get start addr of string
 889   Node* str1_value   = load_String_value(no_ctrl, str1);
 890   Node* str1_offset  = load_String_offset(no_ctrl, str1);
 891   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
 892 
 893   // Get length of string 1
 894   Node* str1_len  = load_String_length(no_ctrl, str1);
 895 
 896   Node* str2_value   = load_String_value(no_ctrl, str2);
 897   Node* str2_offset  = load_String_offset(no_ctrl, str2);
 898   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
 899 
 900   Node* str2_len = NULL;
 901   Node* result = NULL;
 902 
 903   switch (opcode) {
 904   case Op_StrIndexOf:
 905     // Get length of string 2
 906     str2_len = load_String_length(no_ctrl, str2);
 907 
 908     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
 909                                 str1_start, str1_len, str2_start, str2_len);
 910     break;
 911   case Op_StrComp:
 912     // Get length of string 2
 913     str2_len = load_String_length(no_ctrl, str2);
 914 
 915     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
 916                              str1_start, str1_len, str2_start, str2_len);
 917     break;
 918   case Op_StrEquals:
 919     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
 920                                str1_start, str2_start, str1_len);
 921     break;
 922   default:
 923     ShouldNotReachHere();
 924     return NULL;
 925   }
 926 
 927   // All these intrinsics have checks.
 928   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
 929 
 930   return _gvn.transform(result);
 931 }
 932 
 933 // Helper method for String intrinsic functions. This version is called
 934 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
 935 // to Int nodes containing the lenghts of str1 and str2.
 936 //
 937 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
 938   Node* result = NULL;
 939   switch (opcode) {
 940   case Op_StrIndexOf:
 941     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
 942                                 str1_start, cnt1, str2_start, cnt2);
 943     break;
 944   case Op_StrComp:
 945     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
 946                              str1_start, cnt1, str2_start, cnt2);
 947     break;
 948   case Op_StrEquals:
 949     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
 950                                str1_start, str2_start, cnt1);
 951     break;
 952   default:
 953     ShouldNotReachHere();
 954     return NULL;
 955   }
 956 
 957   // All these intrinsics have checks.
 958   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
 959 
 960   return _gvn.transform(result);
 961 }
 962 
 963 //------------------------------inline_string_compareTo------------------------
 964 // public int java.lang.String.compareTo(String anotherString);
 965 bool LibraryCallKit::inline_string_compareTo() {
 966   Node* receiver = null_check(argument(0));
 967   Node* arg      = null_check(argument(1));
 968   if (stopped()) {
 969     return true;
 970   }
 971   set_result(make_string_method_node(Op_StrComp, receiver, arg));
 972   return true;
 973 }
 974 
 975 //------------------------------inline_string_equals------------------------
 976 bool LibraryCallKit::inline_string_equals() {
 977   Node* receiver = null_check_receiver();
 978   // NOTE: Do not null check argument for String.equals() because spec
 979   // allows to specify NULL as argument.
 980   Node* argument = this-&gt;argument(1);
 981   if (stopped()) {
 982     return true;
 983   }
 984 
 985   // paths (plus control) merge
 986   RegionNode* region = new RegionNode(5);
 987   Node* phi = new PhiNode(region, TypeInt::BOOL);
 988 
 989   // does source == target string?
 990   Node* cmp = _gvn.transform(new CmpPNode(receiver, argument));
 991   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
 992 
 993   Node* if_eq = generate_slow_guard(bol, NULL);
 994   if (if_eq != NULL) {
 995     // receiver == argument
 996     phi-&gt;init_req(2, intcon(1));
 997     region-&gt;init_req(2, if_eq);
 998   }
 999 
1000   // get String klass for instanceOf
1001   ciInstanceKlass* klass = env()-&gt;String_klass();
1002 
1003   if (!stopped()) {
1004     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1005     Node* cmp  = _gvn.transform(new CmpINode(inst, intcon(1)));
1006     Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1007 
1008     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1009     //instanceOf == true, fallthrough
1010 
1011     if (inst_false != NULL) {
1012       phi-&gt;init_req(3, intcon(0));
1013       region-&gt;init_req(3, inst_false);
1014     }
1015   }
1016 
1017   if (!stopped()) {
1018     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1019 
1020     // Properly cast the argument to String
1021     argument = _gvn.transform(new CheckCastPPNode(control(), argument, string_type));
1022     // This path is taken only when argument's type is String:NotNull.
1023     argument = cast_not_null(argument, false);
1024 
1025     Node* no_ctrl = NULL;
1026 
1027     // Get start addr of receiver
1028     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1029     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1030     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1031 
1032     // Get length of receiver
1033     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1034 
1035     // Get start addr of argument
1036     Node* argument_val    = load_String_value(no_ctrl, argument);
1037     Node* argument_offset = load_String_offset(no_ctrl, argument);
1038     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1039 
1040     // Get length of argument
1041     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1042 
1043     // Check for receiver count != argument count
1044     Node* cmp = _gvn.transform(new CmpINode(receiver_cnt, argument_cnt));
1045     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1046     Node* if_ne = generate_slow_guard(bol, NULL);
1047     if (if_ne != NULL) {
1048       phi-&gt;init_req(4, intcon(0));
1049       region-&gt;init_req(4, if_ne);
1050     }
1051 
1052     // Check for count == 0 is done by assembler code for StrEquals.
1053 
1054     if (!stopped()) {
1055       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1056       phi-&gt;init_req(1, equals);
1057       region-&gt;init_req(1, control());
1058     }
1059   }
1060 
1061   // post merge
1062   set_control(_gvn.transform(region));
1063   record_for_igvn(region);
1064 
1065   set_result(_gvn.transform(phi));
1066   return true;
1067 }
1068 
1069 //------------------------------inline_array_equals----------------------------
1070 bool LibraryCallKit::inline_array_equals() {
1071   Node* arg1 = argument(0);
1072   Node* arg2 = argument(1);
1073   set_result(_gvn.transform(new AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1074   return true;
1075 }
1076 
1077 // Java version of String.indexOf(constant string)
1078 // class StringDecl {
1079 //   StringDecl(char[] ca) {
1080 //     offset = 0;
1081 //     count = ca.length;
1082 //     value = ca;
1083 //   }
1084 //   int offset;
1085 //   int count;
1086 //   char[] value;
1087 // }
1088 //
1089 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1090 //                             int targetOffset, int cache_i, int md2) {
1091 //   int cache = cache_i;
1092 //   int sourceOffset = string_object.offset;
1093 //   int sourceCount = string_object.count;
1094 //   int targetCount = target_object.length;
1095 //
1096 //   int targetCountLess1 = targetCount - 1;
1097 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1098 //
1099 //   char[] source = string_object.value;
1100 //   char[] target = target_object;
1101 //   int lastChar = target[targetCountLess1];
1102 //
1103 //  outer_loop:
1104 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1105 //     int src = source[i + targetCountLess1];
1106 //     if (src == lastChar) {
1107 //       // With random strings and a 4-character alphabet,
1108 //       // reverse matching at this point sets up 0.8% fewer
1109 //       // frames, but (paradoxically) makes 0.3% more probes.
1110 //       // Since those probes are nearer the lastChar probe,
1111 //       // there is may be a net D$ win with reverse matching.
1112 //       // But, reversing loop inhibits unroll of inner loop
1113 //       // for unknown reason.  So, does running outer loop from
1114 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1115 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1116 //         if (target[targetOffset + j] != source[i+j]) {
1117 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1118 //             if (md2 &lt; j+1) {
1119 //               i += j+1;
1120 //               continue outer_loop;
1121 //             }
1122 //           }
1123 //           i += md2;
1124 //           continue outer_loop;
1125 //         }
1126 //       }
1127 //       return i - sourceOffset;
1128 //     }
1129 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1130 //       i += targetCountLess1;
1131 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1132 //     i++;
1133 //   }
1134 //   return -1;
1135 // }
1136 
1137 //------------------------------string_indexOf------------------------
1138 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1139                                      jint cache_i, jint md2_i) {
1140 
1141   Node* no_ctrl  = NULL;
1142   float likely   = PROB_LIKELY(0.9);
1143   float unlikely = PROB_UNLIKELY(0.9);
1144 
1145   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1146 
1147   Node* source        = load_String_value(no_ctrl, string_object);
1148   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1149   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1150 
1151   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1152   jint target_length = target_array-&gt;length();
1153   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1154   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1155 
1156   // String.value field is known to be @Stable.
1157   if (UseImplicitStableValues) {
1158     target = cast_array_to_stable(target, target_type);
1159   }
1160 
1161   IdealKit kit(this, false, true);
1162 #define __ kit.
1163   Node* zero             = __ ConI(0);
1164   Node* one              = __ ConI(1);
1165   Node* cache            = __ ConI(cache_i);
1166   Node* md2              = __ ConI(md2_i);
1167   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1168   Node* targetCountLess1 = __ ConI(target_length - 1);
1169   Node* targetOffset     = __ ConI(targetOffset_i);
1170   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1171 
1172   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1173   Node* outer_loop = __ make_label(2 /* goto */);
1174   Node* return_    = __ make_label(1);
1175 
1176   __ set(rtn,__ ConI(-1));
1177   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1178        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1179        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1180        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1181        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1182          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1183               Node* tpj = __ AddI(targetOffset, __ value(j));
1184               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1185               Node* ipj  = __ AddI(__ value(i), __ value(j));
1186               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1187               __ if_then(targ, BoolTest::ne, src2); {
1188                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1189                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1190                     __ increment(i, __ AddI(__ value(j), one));
1191                     __ goto_(outer_loop);
1192                   } __ end_if(); __ dead(j);
1193                 }__ end_if(); __ dead(j);
1194                 __ increment(i, md2);
1195                 __ goto_(outer_loop);
1196               }__ end_if();
1197               __ increment(j, one);
1198          }__ end_loop(); __ dead(j);
1199          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1200          __ goto_(return_);
1201        }__ end_if();
1202        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1203          __ increment(i, targetCountLess1);
1204        }__ end_if();
1205        __ increment(i, one);
1206        __ bind(outer_loop);
1207   }__ end_loop(); __ dead(i);
1208   __ bind(return_);
1209 
1210   // Final sync IdealKit and GraphKit.
1211   final_sync(kit);
1212   Node* result = __ value(rtn);
1213 #undef __
1214   C-&gt;set_has_loops(true);
1215   return result;
1216 }
1217 
1218 //------------------------------inline_string_indexOf------------------------
1219 bool LibraryCallKit::inline_string_indexOf() {
1220   Node* receiver = argument(0);
1221   Node* arg      = argument(1);
1222 
1223   Node* result;
1224   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1225       UseSSE42Intrinsics) {
1226     // Generate SSE4.2 version of indexOf
1227     // We currently only have match rules that use SSE4.2
1228 
1229     receiver = null_check(receiver);
1230     arg      = null_check(arg);
1231     if (stopped()) {
1232       return true;
1233     }
1234 
1235     // Make the merge point
1236     RegionNode* result_rgn = new RegionNode(4);
1237     Node*       result_phi = new PhiNode(result_rgn, TypeInt::INT);
1238     Node* no_ctrl  = NULL;
1239 
1240     // Get start addr of source string
1241     Node* source = load_String_value(no_ctrl, receiver);
1242     Node* source_offset = load_String_offset(no_ctrl, receiver);
1243     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1244 
1245     // Get length of source string
1246     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1247 
1248     // Get start addr of substring
1249     Node* substr = load_String_value(no_ctrl, arg);
1250     Node* substr_offset = load_String_offset(no_ctrl, arg);
1251     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1252 
1253     // Get length of source string
1254     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1255 
1256     // Check for substr count &gt; string count
1257     Node* cmp = _gvn.transform(new CmpINode(substr_cnt, source_cnt));
1258     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::gt));
1259     Node* if_gt = generate_slow_guard(bol, NULL);
1260     if (if_gt != NULL) {
1261       result_phi-&gt;init_req(2, intcon(-1));
1262       result_rgn-&gt;init_req(2, if_gt);
1263     }
1264 
1265     if (!stopped()) {
1266       // Check for substr count == 0
1267       cmp = _gvn.transform(new CmpINode(substr_cnt, intcon(0)));
1268       bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1269       Node* if_zero = generate_slow_guard(bol, NULL);
1270       if (if_zero != NULL) {
1271         result_phi-&gt;init_req(3, intcon(0));
1272         result_rgn-&gt;init_req(3, if_zero);
1273       }
1274     }
1275 
1276     if (!stopped()) {
1277       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1278       result_phi-&gt;init_req(1, result);
1279       result_rgn-&gt;init_req(1, control());
1280     }
1281     set_control(_gvn.transform(result_rgn));
1282     record_for_igvn(result_rgn);
1283     result = _gvn.transform(result_phi);
1284 
1285   } else { // Use LibraryCallKit::string_indexOf
1286     // don't intrinsify if argument isn't a constant string.
1287     if (!arg-&gt;is_Con()) {
1288      return false;
1289     }
1290     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1291     if (str_type == NULL) {
1292       return false;
1293     }
1294     ciInstanceKlass* klass = env()-&gt;String_klass();
1295     ciObject* str_const = str_type-&gt;const_oop();
1296     if (str_const == NULL || str_const-&gt;klass() != klass) {
1297       return false;
1298     }
1299     ciInstance* str = str_const-&gt;as_instance();
1300     assert(str != NULL, "must be instance");
1301 
1302     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1303     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1304 
1305     int o;
1306     int c;
1307     if (java_lang_String::has_offset_field()) {
1308       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1309       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1310     } else {
1311       o = 0;
1312       c = pat-&gt;length();
1313     }
1314 
1315     // constant strings have no offset and count == length which
1316     // simplifies the resulting code somewhat so lets optimize for that.
1317     if (o != 0 || c != pat-&gt;length()) {
1318      return false;
1319     }
1320 
1321     receiver = null_check(receiver, T_OBJECT);
1322     // NOTE: No null check on the argument is needed since it's a constant String oop.
1323     if (stopped()) {
1324       return true;
1325     }
1326 
1327     // The null string as a pattern always returns 0 (match at beginning of string)
1328     if (c == 0) {
1329       set_result(intcon(0));
1330       return true;
1331     }
1332 
1333     // Generate default indexOf
1334     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1335     int cache = 0;
1336     int i;
1337     for (i = 0; i &lt; c - 1; i++) {
1338       assert(i &lt; pat-&gt;length(), "out of range");
1339       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1340     }
1341 
1342     int md2 = c;
1343     for (i = 0; i &lt; c - 1; i++) {
1344       assert(i &lt; pat-&gt;length(), "out of range");
1345       if (pat-&gt;char_at(o + i) == lastChar) {
1346         md2 = (c - 1) - i;
1347       }
1348     }
1349 
1350     result = string_indexOf(receiver, pat, o, cache, md2);
1351   }
1352   set_result(result);
1353   return true;
1354 }
1355 
1356 //--------------------------round_double_node--------------------------------
1357 // Round a double node if necessary.
1358 Node* LibraryCallKit::round_double_node(Node* n) {
1359   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1360     n = _gvn.transform(new RoundDoubleNode(0, n));
1361   return n;
1362 }
1363 
1364 //------------------------------inline_math-----------------------------------
1365 // public static double Math.abs(double)
1366 // public static double Math.sqrt(double)
1367 // public static double Math.log(double)
1368 // public static double Math.log10(double)
1369 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1370   Node* arg = round_double_node(argument(0));
1371   Node* n;
1372   switch (id) {
1373   case vmIntrinsics::_dabs:   n = new AbsDNode(                arg);  break;
1374   case vmIntrinsics::_dsqrt:  n = new SqrtDNode(C, control(),  arg);  break;
1375   case vmIntrinsics::_dlog:   n = new LogDNode(C, control(),   arg);  break;
1376   case vmIntrinsics::_dlog10: n = new Log10DNode(C, control(), arg);  break;
1377   default:  fatal_unexpected_iid(id);  break;
1378   }
1379   set_result(_gvn.transform(n));
1380   return true;
1381 }
1382 
1383 //------------------------------inline_trig----------------------------------
1384 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1385 // argument reduction which will turn into a fast/slow diamond.
1386 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1387   Node* arg = round_double_node(argument(0));
1388   Node* n = NULL;
1389 
1390   switch (id) {
1391   case vmIntrinsics::_dsin:  n = new SinDNode(C, control(), arg);  break;
1392   case vmIntrinsics::_dcos:  n = new CosDNode(C, control(), arg);  break;
1393   case vmIntrinsics::_dtan:  n = new TanDNode(C, control(), arg);  break;
1394   default:  fatal_unexpected_iid(id);  break;
1395   }
1396   n = _gvn.transform(n);
1397 
1398   // Rounding required?  Check for argument reduction!
1399   if (Matcher::strict_fp_requires_explicit_rounding) {
1400     static const double     pi_4 =  0.7853981633974483;
1401     static const double neg_pi_4 = -0.7853981633974483;
1402     // pi/2 in 80-bit extended precision
1403     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1404     // -pi/2 in 80-bit extended precision
1405     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1406     // Cutoff value for using this argument reduction technique
1407     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1408     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1409 
1410     // Pseudocode for sin:
1411     // if (x &lt;= Math.PI / 4.0) {
1412     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1413     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1414     // } else {
1415     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1416     // }
1417     // return StrictMath.sin(x);
1418 
1419     // Pseudocode for cos:
1420     // if (x &lt;= Math.PI / 4.0) {
1421     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1422     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1423     // } else {
1424     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1425     // }
1426     // return StrictMath.cos(x);
1427 
1428     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1429     // requires a special machine instruction to load it.  Instead we'll try
1430     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1431     // probably do the math inside the SIN encoding.
1432 
1433     // Make the merge point
1434     RegionNode* r = new RegionNode(3);
1435     Node* phi = new PhiNode(r, Type::DOUBLE);
1436 
1437     // Flatten arg so we need only 1 test
1438     Node *abs = _gvn.transform(new AbsDNode(arg));
1439     // Node for PI/4 constant
1440     Node *pi4 = makecon(TypeD::make(pi_4));
1441     // Check PI/4 : abs(arg)
1442     Node *cmp = _gvn.transform(new CmpDNode(pi4,abs));
1443     // Check: If PI/4 &lt; abs(arg) then go slow
1444     Node *bol = _gvn.transform(new BoolNode( cmp, BoolTest::lt ));
1445     // Branch either way
1446     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1447     set_control(opt_iff(r,iff));
1448 
1449     // Set fast path result
1450     phi-&gt;init_req(2, n);
1451 
1452     // Slow path - non-blocking leaf call
1453     Node* call = NULL;
1454     switch (id) {
1455     case vmIntrinsics::_dsin:
1456       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1457                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1458                                "Sin", NULL, arg, top());
1459       break;
1460     case vmIntrinsics::_dcos:
1461       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1462                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1463                                "Cos", NULL, arg, top());
1464       break;
1465     case vmIntrinsics::_dtan:
1466       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1467                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1468                                "Tan", NULL, arg, top());
1469       break;
1470     }
1471     assert(control()-&gt;in(0) == call, "");
1472     Node* slow_result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1473     r-&gt;init_req(1, control());
1474     phi-&gt;init_req(1, slow_result);
1475 
1476     // Post-merge
1477     set_control(_gvn.transform(r));
1478     record_for_igvn(r);
1479     n = _gvn.transform(phi);
1480 
1481     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1482   }
1483   set_result(n);
1484   return true;
1485 }
1486 
1487 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1488   //-------------------
1489   //result=(result.isNaN())? funcAddr():result;
1490   // Check: If isNaN() by checking result!=result? then either trap
1491   // or go to runtime
1492   Node* cmpisnan = _gvn.transform(new CmpDNode(result, result));
1493   // Build the boolean node
1494   Node* bolisnum = _gvn.transform(new BoolNode(cmpisnan, BoolTest::eq));
1495 
1496   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1497     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1498       // The pow or exp intrinsic returned a NaN, which requires a call
1499       // to the runtime.  Recompile with the runtime call.
1500       uncommon_trap(Deoptimization::Reason_intrinsic,
1501                     Deoptimization::Action_make_not_entrant);
1502     }
1503     return result;
1504   } else {
1505     // If this inlining ever returned NaN in the past, we compile a call
1506     // to the runtime to properly handle corner cases
1507 
1508     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1509     Node* if_slow = _gvn.transform(new IfFalseNode(iff));
1510     Node* if_fast = _gvn.transform(new IfTrueNode(iff));
1511 
1512     if (!if_slow-&gt;is_top()) {
1513       RegionNode* result_region = new RegionNode(3);
1514       PhiNode*    result_val = new PhiNode(result_region, Type::DOUBLE);
1515 
1516       result_region-&gt;init_req(1, if_fast);
1517       result_val-&gt;init_req(1, result);
1518 
1519       set_control(if_slow);
1520 
1521       const TypePtr* no_memory_effects = NULL;
1522       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1523                                    no_memory_effects,
1524                                    x, top(), y, y ? top() : NULL);
1525       Node* value = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+0));
1526 #ifdef ASSERT
1527       Node* value_top = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+1));
1528       assert(value_top == top(), "second value must be top");
1529 #endif
1530 
1531       result_region-&gt;init_req(2, control());
1532       result_val-&gt;init_req(2, value);
1533       set_control(_gvn.transform(result_region));
1534       return _gvn.transform(result_val);
1535     } else {
1536       return result;
1537     }
1538   }
1539 }
1540 
1541 //------------------------------inline_exp-------------------------------------
1542 // Inline exp instructions, if possible.  The Intel hardware only misses
1543 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1544 bool LibraryCallKit::inline_exp() {
1545   Node* arg = round_double_node(argument(0));
1546   Node* n   = _gvn.transform(new ExpDNode(C, control(), arg));
1547 
1548   n = finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1549   set_result(n);
1550 
1551   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1552   return true;
1553 }
1554 
1555 //------------------------------inline_pow-------------------------------------
1556 // Inline power instructions, if possible.
1557 bool LibraryCallKit::inline_pow() {
1558   // Pseudocode for pow
1559   // if (y == 2) {
1560   //   return x * x;
1561   // } else {
1562   //   if (x &lt;= 0.0) {
1563   //     long longy = (long)y;
1564   //     if ((double)longy == y) { // if y is long
1565   //       if (y + 1 == y) longy = 0; // huge number: even
1566   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1567   //     } else {
1568   //       result = NaN;
1569   //     }
1570   //   } else {
1571   //     result = DPow(x,y);
1572   //   }
1573   //   if (result != result)?  {
1574   //     result = uncommon_trap() or runtime_call();
1575   //   }
1576   //   return result;
1577   // }
1578 
1579   Node* x = round_double_node(argument(0));
1580   Node* y = round_double_node(argument(2));
1581 
1582   Node* result = NULL;
1583 
1584   Node*   const_two_node = makecon(TypeD::make(2.0));
1585   Node*   cmp_node       = _gvn.transform(new CmpDNode(y, const_two_node));
1586   Node*   bool_node      = _gvn.transform(new BoolNode(cmp_node, BoolTest::eq));
1587   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1588   Node*   if_true        = _gvn.transform(new IfTrueNode(if_node));
1589   Node*   if_false       = _gvn.transform(new IfFalseNode(if_node));
1590 
1591   RegionNode* region_node = new RegionNode(3);
1592   region_node-&gt;init_req(1, if_true);
1593 
1594   Node* phi_node = new PhiNode(region_node, Type::DOUBLE);
1595   // special case for x^y where y == 2, we can convert it to x * x
1596   phi_node-&gt;init_req(1, _gvn.transform(new MulDNode(x, x)));
1597 
1598   // set control to if_false since we will now process the false branch
1599   set_control(if_false);
1600 
1601   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1602     // Short form: skip the fancy tests and just check for NaN result.
1603     result = _gvn.transform(new PowDNode(C, control(), x, y));
1604   } else {
1605     // If this inlining ever returned NaN in the past, include all
1606     // checks + call to the runtime.
1607 
1608     // Set the merge point for If node with condition of (x &lt;= 0.0)
1609     // There are four possible paths to region node and phi node
1610     RegionNode *r = new RegionNode(4);
1611     Node *phi = new PhiNode(r, Type::DOUBLE);
1612 
1613     // Build the first if node: if (x &lt;= 0.0)
1614     // Node for 0 constant
1615     Node *zeronode = makecon(TypeD::ZERO);
1616     // Check x:0
1617     Node *cmp = _gvn.transform(new CmpDNode(x, zeronode));
1618     // Check: If (x&lt;=0) then go complex path
1619     Node *bol1 = _gvn.transform(new BoolNode( cmp, BoolTest::le ));
1620     // Branch either way
1621     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1622     // Fast path taken; set region slot 3
1623     Node *fast_taken = _gvn.transform(new IfFalseNode(if1));
1624     r-&gt;init_req(3,fast_taken); // Capture fast-control
1625 
1626     // Fast path not-taken, i.e. slow path
1627     Node *complex_path = _gvn.transform(new IfTrueNode(if1));
1628 
1629     // Set fast path result
1630     Node *fast_result = _gvn.transform(new PowDNode(C, control(), x, y));
1631     phi-&gt;init_req(3, fast_result);
1632 
1633     // Complex path
1634     // Build the second if node (if y is long)
1635     // Node for (long)y
1636     Node *longy = _gvn.transform(new ConvD2LNode(y));
1637     // Node for (double)((long) y)
1638     Node *doublelongy= _gvn.transform(new ConvL2DNode(longy));
1639     // Check (double)((long) y) : y
1640     Node *cmplongy= _gvn.transform(new CmpDNode(doublelongy, y));
1641     // Check if (y isn't long) then go to slow path
1642 
1643     Node *bol2 = _gvn.transform(new BoolNode( cmplongy, BoolTest::ne ));
1644     // Branch either way
1645     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1646     Node* ylong_path = _gvn.transform(new IfFalseNode(if2));
1647 
1648     Node *slow_path = _gvn.transform(new IfTrueNode(if2));
1649 
1650     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1651     // Node for constant 1
1652     Node *conone = longcon(1);
1653     // 1&amp; (long)y
1654     Node *signnode= _gvn.transform(new AndLNode(conone, longy));
1655 
1656     // A huge number is always even. Detect a huge number by checking
1657     // if y + 1 == y and set integer to be tested for parity to 0.
1658     // Required for corner case:
1659     // (long)9.223372036854776E18 = max_jlong
1660     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1661     // max_jlong is odd but 9.223372036854776E18 is even
1662     Node* yplus1 = _gvn.transform(new AddDNode(y, makecon(TypeD::make(1))));
1663     Node *cmpyplus1= _gvn.transform(new CmpDNode(yplus1, y));
1664     Node *bolyplus1 = _gvn.transform(new BoolNode( cmpyplus1, BoolTest::eq ));
1665     Node* correctedsign = NULL;
1666     if (ConditionalMoveLimit != 0) {
1667       correctedsign = _gvn.transform(CMoveNode::make(NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1668     } else {
1669       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1670       RegionNode *r = new RegionNode(3);
1671       Node *phi = new PhiNode(r, TypeLong::LONG);
1672       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyplus1)));
1673       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyplus1)));
1674       phi-&gt;init_req(1, signnode);
1675       phi-&gt;init_req(2, longcon(0));
1676       correctedsign = _gvn.transform(phi);
1677       ylong_path = _gvn.transform(r);
1678       record_for_igvn(r);
1679     }
1680 
1681     // zero node
1682     Node *conzero = longcon(0);
1683     // Check (1&amp;(long)y)==0?
1684     Node *cmpeq1 = _gvn.transform(new CmpLNode(correctedsign, conzero));
1685     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1686     Node *bol3 = _gvn.transform(new BoolNode( cmpeq1, BoolTest::ne ));
1687     // abs(x)
1688     Node *absx=_gvn.transform(new AbsDNode(x));
1689     // abs(x)^y
1690     Node *absxpowy = _gvn.transform(new PowDNode(C, control(), absx, y));
1691     // -abs(x)^y
1692     Node *negabsxpowy = _gvn.transform(new NegDNode (absxpowy));
1693     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1694     Node *signresult = NULL;
1695     if (ConditionalMoveLimit != 0) {
1696       signresult = _gvn.transform(CMoveNode::make(NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1697     } else {
1698       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1699       RegionNode *r = new RegionNode(3);
1700       Node *phi = new PhiNode(r, Type::DOUBLE);
1701       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyeven)));
1702       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyeven)));
1703       phi-&gt;init_req(1, absxpowy);
1704       phi-&gt;init_req(2, negabsxpowy);
1705       signresult = _gvn.transform(phi);
1706       ylong_path = _gvn.transform(r);
1707       record_for_igvn(r);
1708     }
1709     // Set complex path fast result
1710     r-&gt;init_req(2, ylong_path);
1711     phi-&gt;init_req(2, signresult);
1712 
1713     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1714     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1715     r-&gt;init_req(1,slow_path);
1716     phi-&gt;init_req(1,slow_result);
1717 
1718     // Post merge
1719     set_control(_gvn.transform(r));
1720     record_for_igvn(r);
1721     result = _gvn.transform(phi);
1722   }
1723 
1724   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1725 
1726   // control from finish_pow_exp is now input to the region node
1727   region_node-&gt;set_req(2, control());
1728   // the result from finish_pow_exp is now input to the phi node
1729   phi_node-&gt;init_req(2, result);
1730   set_control(_gvn.transform(region_node));
1731   record_for_igvn(region_node);
1732   set_result(_gvn.transform(phi_node));
1733 
1734   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1735   return true;
1736 }
1737 
1738 //------------------------------runtime_math-----------------------------
1739 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1740   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1741          "must be (DD)D or (D)D type");
1742 
1743   // Inputs
1744   Node* a = round_double_node(argument(0));
1745   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1746 
1747   const TypePtr* no_memory_effects = NULL;
1748   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1749                                  no_memory_effects,
1750                                  a, top(), b, b ? top() : NULL);
1751   Node* value = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+0));
1752 #ifdef ASSERT
1753   Node* value_top = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+1));
1754   assert(value_top == top(), "second value must be top");
1755 #endif
1756 
1757   set_result(value);
1758   return true;
1759 }
1760 
1761 //------------------------------inline_math_native-----------------------------
1762 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
1763 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
1764   switch (id) {
1765     // These intrinsics are not properly supported on all hardware
1766   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
1767     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
1768   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
1769     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
1770   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
1771     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
1772 
1773   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
1774     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
1775   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
1776     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
1777 
1778     // These intrinsics are supported on all hardware
1779   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
1780   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
1781 
1782   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
1783     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
1784   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
1785     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
1786 #undef FN_PTR
1787 
1788    // These intrinsics are not yet correctly implemented
1789   case vmIntrinsics::_datan2:
1790     return false;
1791 
1792   default:
1793     fatal_unexpected_iid(id);
1794     return false;
1795   }
1796 }
1797 
1798 static bool is_simple_name(Node* n) {
1799   return (n-&gt;req() == 1         // constant
1800           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
1801           || n-&gt;is_Proj()       // parameter or return value
1802           || n-&gt;is_Phi()        // local of some sort
1803           );
1804 }
1805 
1806 //----------------------------inline_notify-----------------------------------*
1807 bool LibraryCallKit::inline_notify(vmIntrinsics::ID id) {
1808   const TypeFunc* ftype = OptoRuntime::monitor_notify_Type();
1809   address func;
1810   if (id == vmIntrinsics::_notify) {
1811     func = OptoRuntime::monitor_notify_Java();
1812   } else {
1813     func = OptoRuntime::monitor_notifyAll_Java();
1814   }
1815   Node* call = make_runtime_call(RC_NO_LEAF, ftype, func, NULL, TypeRawPtr::BOTTOM, argument(0));
1816   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
1817   return true;
1818 }
1819 
1820 
1821 //----------------------------inline_min_max-----------------------------------
1822 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
1823   set_result(generate_min_max(id, argument(0), argument(1)));
1824   return true;
1825 }
1826 
1827 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
1828   Node* bol = _gvn.transform( new BoolNode(test, BoolTest::overflow) );
1829   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
1830   Node* fast_path = _gvn.transform( new IfFalseNode(check));
1831   Node* slow_path = _gvn.transform( new IfTrueNode(check) );
1832 
1833   {
1834     PreserveJVMState pjvms(this);
1835     PreserveReexecuteState preexecs(this);
1836     jvms()-&gt;set_should_reexecute(true);
1837 
1838     set_control(slow_path);
1839     set_i_o(i_o());
1840 
1841     uncommon_trap(Deoptimization::Reason_intrinsic,
1842                   Deoptimization::Action_none);
1843   }
1844 
1845   set_control(fast_path);
1846   set_result(math);
1847 }
1848 
1849 template &lt;typename OverflowOp&gt;
1850 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
1851   typedef typename OverflowOp::MathOp MathOp;
1852 
1853   MathOp* mathOp = new MathOp(arg1, arg2);
1854   Node* operation = _gvn.transform( mathOp );
1855   Node* ofcheck = _gvn.transform( new OverflowOp(arg1, arg2) );
1856   inline_math_mathExact(operation, ofcheck);
1857   return true;
1858 }
1859 
1860 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
1861   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
1862 }
1863 
1864 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
1865   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
1866 }
1867 
1868 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
1869   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
1870 }
1871 
1872 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
1873   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
1874 }
1875 
1876 bool LibraryCallKit::inline_math_negateExactI() {
1877   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
1878 }
1879 
1880 bool LibraryCallKit::inline_math_negateExactL() {
1881   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
1882 }
1883 
1884 bool LibraryCallKit::inline_math_multiplyExactI() {
1885   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
1886 }
1887 
1888 bool LibraryCallKit::inline_math_multiplyExactL() {
1889   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
1890 }
1891 
1892 Node*
1893 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
1894   // These are the candidate return value:
1895   Node* xvalue = x0;
1896   Node* yvalue = y0;
1897 
1898   if (xvalue == yvalue) {
1899     return xvalue;
1900   }
1901 
1902   bool want_max = (id == vmIntrinsics::_max);
1903 
1904   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
1905   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
1906   if (txvalue == NULL || tyvalue == NULL)  return top();
1907   // This is not really necessary, but it is consistent with a
1908   // hypothetical MaxINode::Value method:
1909   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
1910 
1911   // %%% This folding logic should (ideally) be in a different place.
1912   // Some should be inside IfNode, and there to be a more reliable
1913   // transformation of ?: style patterns into cmoves.  We also want
1914   // more powerful optimizations around cmove and min/max.
1915 
1916   // Try to find a dominating comparison of these guys.
1917   // It can simplify the index computation for Arrays.copyOf
1918   // and similar uses of System.arraycopy.
1919   // First, compute the normalized version of CmpI(x, y).
1920   int   cmp_op = Op_CmpI;
1921   Node* xkey = xvalue;
1922   Node* ykey = yvalue;
1923   Node* ideal_cmpxy = _gvn.transform(new CmpINode(xkey, ykey));
1924   if (ideal_cmpxy-&gt;is_Cmp()) {
1925     // E.g., if we have CmpI(length - offset, count),
1926     // it might idealize to CmpI(length, count + offset)
1927     cmp_op = ideal_cmpxy-&gt;Opcode();
1928     xkey = ideal_cmpxy-&gt;in(1);
1929     ykey = ideal_cmpxy-&gt;in(2);
1930   }
1931 
1932   // Start by locating any relevant comparisons.
1933   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
1934   Node* cmpxy = NULL;
1935   Node* cmpyx = NULL;
1936   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
1937     Node* cmp = start_from-&gt;fast_out(k);
1938     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
1939         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
1940         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
1941       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
1942       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
1943     }
1944   }
1945 
1946   const int NCMPS = 2;
1947   Node* cmps[NCMPS] = { cmpxy, cmpyx };
1948   int cmpn;
1949   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
1950     if (cmps[cmpn] != NULL)  break;     // find a result
1951   }
1952   if (cmpn &lt; NCMPS) {
1953     // Look for a dominating test that tells us the min and max.
1954     int depth = 0;                // Limit search depth for speed
1955     Node* dom = control();
1956     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
1957       if (++depth &gt;= 100)  break;
1958       Node* ifproj = dom;
1959       if (!ifproj-&gt;is_Proj())  continue;
1960       Node* iff = ifproj-&gt;in(0);
1961       if (!iff-&gt;is_If())  continue;
1962       Node* bol = iff-&gt;in(1);
1963       if (!bol-&gt;is_Bool())  continue;
1964       Node* cmp = bol-&gt;in(1);
1965       if (cmp == NULL)  continue;
1966       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
1967         if (cmps[cmpn] == cmp)  break;
1968       if (cmpn == NCMPS)  continue;
1969       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
1970       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
1971       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
1972       // At this point, we know that 'x btest y' is true.
1973       switch (btest) {
1974       case BoolTest::eq:
1975         // They are proven equal, so we can collapse the min/max.
1976         // Either value is the answer.  Choose the simpler.
1977         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
1978           return yvalue;
1979         return xvalue;
1980       case BoolTest::lt:          // x &lt; y
1981       case BoolTest::le:          // x &lt;= y
1982         return (want_max ? yvalue : xvalue);
1983       case BoolTest::gt:          // x &gt; y
1984       case BoolTest::ge:          // x &gt;= y
1985         return (want_max ? xvalue : yvalue);
1986       }
1987     }
1988   }
1989 
1990   // We failed to find a dominating test.
1991   // Let's pick a test that might GVN with prior tests.
1992   Node*          best_bol   = NULL;
1993   BoolTest::mask best_btest = BoolTest::illegal;
1994   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
1995     Node* cmp = cmps[cmpn];
1996     if (cmp == NULL)  continue;
1997     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
1998       Node* bol = cmp-&gt;fast_out(j);
1999       if (!bol-&gt;is_Bool())  continue;
2000       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2001       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2002       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2003       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2004         best_bol   = bol-&gt;as_Bool();
2005         best_btest = btest;
2006       }
2007     }
2008   }
2009 
2010   Node* answer_if_true  = NULL;
2011   Node* answer_if_false = NULL;
2012   switch (best_btest) {
2013   default:
2014     if (cmpxy == NULL)
2015       cmpxy = ideal_cmpxy;
2016     best_bol = _gvn.transform(new BoolNode(cmpxy, BoolTest::lt));
2017     // and fall through:
2018   case BoolTest::lt:          // x &lt; y
2019   case BoolTest::le:          // x &lt;= y
2020     answer_if_true  = (want_max ? yvalue : xvalue);
2021     answer_if_false = (want_max ? xvalue : yvalue);
2022     break;
2023   case BoolTest::gt:          // x &gt; y
2024   case BoolTest::ge:          // x &gt;= y
2025     answer_if_true  = (want_max ? xvalue : yvalue);
2026     answer_if_false = (want_max ? yvalue : xvalue);
2027     break;
2028   }
2029 
2030   jint hi, lo;
2031   if (want_max) {
2032     // We can sharpen the minimum.
2033     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2034     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2035   } else {
2036     // We can sharpen the maximum.
2037     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2038     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2039   }
2040 
2041   // Use a flow-free graph structure, to avoid creating excess control edges
2042   // which could hinder other optimizations.
2043   // Since Math.min/max is often used with arraycopy, we want
2044   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2045   Node* cmov = CMoveNode::make(NULL, best_bol,
2046                                answer_if_false, answer_if_true,
2047                                TypeInt::make(lo, hi, widen));
2048 
2049   return _gvn.transform(cmov);
2050 
2051   /*
2052   // This is not as desirable as it may seem, since Min and Max
2053   // nodes do not have a full set of optimizations.
2054   // And they would interfere, anyway, with 'if' optimizations
2055   // and with CMoveI canonical forms.
2056   switch (id) {
2057   case vmIntrinsics::_min:
2058     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2059   case vmIntrinsics::_max:
2060     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2061   default:
2062     ShouldNotReachHere();
2063   }
2064   */
2065 }
2066 
2067 inline int
2068 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2069   const TypePtr* base_type = TypePtr::NULL_PTR;
2070   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2071   if (base_type == NULL) {
2072     // Unknown type.
2073     return Type::AnyPtr;
2074   } else if (base_type == TypePtr::NULL_PTR) {
2075     // Since this is a NULL+long form, we have to switch to a rawptr.
2076     base   = _gvn.transform(new CastX2PNode(offset));
2077     offset = MakeConX(0);
2078     return Type::RawPtr;
2079   } else if (base_type-&gt;base() == Type::RawPtr) {
2080     return Type::RawPtr;
2081   } else if (base_type-&gt;isa_oopptr()) {
2082     // Base is never null =&gt; always a heap address.
2083     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2084       return Type::OopPtr;
2085     }
2086     // Offset is small =&gt; always a heap address.
2087     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2088     if (offset_type != NULL &amp;&amp;
2089         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2090         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2091         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2092       return Type::OopPtr;
2093     }
2094     // Otherwise, it might either be oop+off or NULL+addr.
2095     return Type::AnyPtr;
2096   } else {
2097     // No information:
2098     return Type::AnyPtr;
2099   }
2100 }
2101 
2102 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2103   int kind = classify_unsafe_addr(base, offset);
2104   if (kind == Type::RawPtr) {
2105     return basic_plus_adr(top(), base, offset);
2106   } else {
2107     return basic_plus_adr(base, offset);
2108   }
2109 }
2110 
2111 //--------------------------inline_number_methods-----------------------------
2112 // inline int     Integer.numberOfLeadingZeros(int)
2113 // inline int        Long.numberOfLeadingZeros(long)
2114 //
2115 // inline int     Integer.numberOfTrailingZeros(int)
2116 // inline int        Long.numberOfTrailingZeros(long)
2117 //
2118 // inline int     Integer.bitCount(int)
2119 // inline int        Long.bitCount(long)
2120 //
2121 // inline char  Character.reverseBytes(char)
2122 // inline short     Short.reverseBytes(short)
2123 // inline int     Integer.reverseBytes(int)
2124 // inline long       Long.reverseBytes(long)
2125 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2126   Node* arg = argument(0);
2127   Node* n;
2128   switch (id) {
2129   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new CountLeadingZerosINode( arg);  break;
2130   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new CountLeadingZerosLNode( arg);  break;
2131   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new CountTrailingZerosINode(arg);  break;
2132   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new CountTrailingZerosLNode(arg);  break;
2133   case vmIntrinsics::_bitCount_i:               n = new PopCountINode(          arg);  break;
2134   case vmIntrinsics::_bitCount_l:               n = new PopCountLNode(          arg);  break;
2135   case vmIntrinsics::_reverseBytes_c:           n = new ReverseBytesUSNode(0,   arg);  break;
2136   case vmIntrinsics::_reverseBytes_s:           n = new ReverseBytesSNode( 0,   arg);  break;
2137   case vmIntrinsics::_reverseBytes_i:           n = new ReverseBytesINode( 0,   arg);  break;
2138   case vmIntrinsics::_reverseBytes_l:           n = new ReverseBytesLNode( 0,   arg);  break;
2139   default:  fatal_unexpected_iid(id);  break;
2140   }
2141   set_result(_gvn.transform(n));
2142   return true;
2143 }
2144 
2145 //----------------------------inline_unsafe_access----------------------------
2146 
2147 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2148 
2149 // Helper that guards and inserts a pre-barrier.
2150 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2151                                         Node* pre_val, bool need_mem_bar) {
2152   // We could be accessing the referent field of a reference object. If so, when G1
2153   // is enabled, we need to log the value in the referent field in an SATB buffer.
2154   // This routine performs some compile time filters and generates suitable
2155   // runtime filters that guard the pre-barrier code.
2156   // Also add memory barrier for non volatile load from the referent field
2157   // to prevent commoning of loads across safepoint.
2158   if (!UseG1GC &amp;&amp; !need_mem_bar)
2159     return;
2160 
2161   // Some compile time checks.
2162 
2163   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2164   const TypeX* otype = offset-&gt;find_intptr_t_type();
2165   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2166       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2167     // Constant offset but not the reference_offset so just return
2168     return;
2169   }
2170 
2171   // We only need to generate the runtime guards for instances.
2172   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2173   if (btype != NULL) {
2174     if (btype-&gt;isa_aryptr()) {
2175       // Array type so nothing to do
2176       return;
2177     }
2178 
2179     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2180     if (itype != NULL) {
2181       // Can the klass of base_oop be statically determined to be
2182       // _not_ a sub-class of Reference and _not_ Object?
2183       ciKlass* klass = itype-&gt;klass();
2184       if ( klass-&gt;is_loaded() &amp;&amp;
2185           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2186           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2187         return;
2188       }
2189     }
2190   }
2191 
2192   // The compile time filters did not reject base_oop/offset so
2193   // we need to generate the following runtime filters
2194   //
2195   // if (offset == java_lang_ref_Reference::_reference_offset) {
2196   //   if (instance_of(base, java.lang.ref.Reference)) {
2197   //     pre_barrier(_, pre_val, ...);
2198   //   }
2199   // }
2200 
2201   float likely   = PROB_LIKELY(  0.999);
2202   float unlikely = PROB_UNLIKELY(0.999);
2203 
2204   IdealKit ideal(this);
2205 #define __ ideal.
2206 
2207   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2208 
2209   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2210       // Update graphKit memory and control from IdealKit.
2211       sync_kit(ideal);
2212 
2213       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2214       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2215 
2216       // Update IdealKit memory and control from graphKit.
2217       __ sync_kit(this);
2218 
2219       Node* one = __ ConI(1);
2220       // is_instof == 0 if base_oop == NULL
2221       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2222 
2223         // Update graphKit from IdeakKit.
2224         sync_kit(ideal);
2225 
2226         // Use the pre-barrier to record the value in the referent field
2227         pre_barrier(false /* do_load */,
2228                     __ ctrl(),
2229                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2230                     pre_val /* pre_val */,
2231                     T_OBJECT);
2232         if (need_mem_bar) {
2233           // Add memory barrier to prevent commoning reads from this field
2234           // across safepoint since GC can change its value.
2235           insert_mem_bar(Op_MemBarCPUOrder);
2236         }
2237         // Update IdealKit from graphKit.
2238         __ sync_kit(this);
2239 
2240       } __ end_if(); // _ref_type != ref_none
2241   } __ end_if(); // offset == referent_offset
2242 
2243   // Final sync IdealKit and GraphKit.
2244   final_sync(ideal);
2245 #undef __
2246 }
2247 
2248 
2249 // Interpret Unsafe.fieldOffset cookies correctly:
2250 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2251 
2252 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2253   // Attempt to infer a sharper value type from the offset and base type.
2254   ciKlass* sharpened_klass = NULL;
2255 
2256   // See if it is an instance field, with an object type.
2257   if (alias_type-&gt;field() != NULL) {
2258     assert(!is_native_ptr, "native pointer op cannot use a java address");
2259     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2260       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2261     }
2262   }
2263 
2264   // See if it is a narrow oop array.
2265   if (adr_type-&gt;isa_aryptr()) {
2266     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2267       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2268       if (elem_type != NULL) {
2269         sharpened_klass = elem_type-&gt;klass();
2270       }
2271     }
2272   }
2273 
2274   // The sharpened class might be unloaded if there is no class loader
2275   // contraint in place.
2276   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2277     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2278 
2279 #ifndef PRODUCT
2280     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2281       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2282       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2283     }
2284 #endif
2285     // Sharpen the value type.
2286     return tjp;
2287   }
2288   return NULL;
2289 }
2290 
2291 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2292   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2293 
2294 #ifndef PRODUCT
2295   {
2296     ResourceMark rm;
2297     // Check the signatures.
2298     ciSignature* sig = callee()-&gt;signature();
2299 #ifdef ASSERT
2300     if (!is_store) {
2301       // Object getObject(Object base, int/long offset), etc.
2302       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2303       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2304           rtype = T_ADDRESS;  // it is really a C void*
2305       assert(rtype == type, "getter must return the expected value");
2306       if (!is_native_ptr) {
2307         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2308         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2309         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2310       } else {
2311         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2312         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2313       }
2314     } else {
2315       // void putObject(Object base, int/long offset, Object x), etc.
2316       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2317       if (!is_native_ptr) {
2318         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2319         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2320         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2321       } else {
2322         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2323         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2324       }
2325       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2326       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2327         vtype = T_ADDRESS;  // it is really a C void*
2328       assert(vtype == type, "putter must accept the expected value");
2329     }
2330 #endif // ASSERT
2331  }
2332 #endif //PRODUCT
2333 
2334   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2335 
2336   Node* receiver = argument(0);  // type: oop
2337 
2338   // Build address expression.
2339   Node* adr;
2340   Node* heap_base_oop = top();
2341   Node* offset = top();
2342   Node* val;
2343 
2344   if (!is_native_ptr) {
2345     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2346     Node* base = argument(1);  // type: oop
2347     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2348     offset = argument(2);  // type: long
2349     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2350     // to be plain byte offsets, which are also the same as those accepted
2351     // by oopDesc::field_base.
2352     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2353            "fieldOffset must be byte-scaled");
2354     // 32-bit machines ignore the high half!
2355     offset = ConvL2X(offset);
2356     adr = make_unsafe_address(base, offset);
2357     heap_base_oop = base;
2358     val = is_store ? argument(4) : NULL;
2359   } else {
2360     Node* ptr = argument(1);  // type: long
2361     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2362     adr = make_unsafe_address(NULL, ptr);
2363     val = is_store ? argument(3) : NULL;
2364   }
2365 
2366   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2367 
2368   // First guess at the value type.
2369   const Type *value_type = Type::get_const_basic_type(type);
2370 
2371   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2372   // there was not enough information to nail it down.
2373   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2374   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2375 
2376   // We will need memory barriers unless we can determine a unique
2377   // alias category for this reference.  (Note:  If for some reason
2378   // the barriers get omitted and the unsafe reference begins to "pollute"
2379   // the alias analysis of the rest of the graph, either Compile::can_alias
2380   // or Compile::must_alias will throw a diagnostic assert.)
2381   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2382 
2383   // If we are reading the value of the referent field of a Reference
2384   // object (either by using Unsafe directly or through reflection)
2385   // then, if G1 is enabled, we need to record the referent in an
2386   // SATB log buffer using the pre-barrier mechanism.
2387   // Also we need to add memory barrier to prevent commoning reads
2388   // from this field across safepoint since GC can change its value.
2389   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2390                            offset != top() &amp;&amp; heap_base_oop != top();
2391 
2392   if (!is_store &amp;&amp; type == T_OBJECT) {
2393     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2394     if (tjp != NULL) {
2395       value_type = tjp;
2396     }
2397   }
2398 
2399   receiver = null_check(receiver);
2400   if (stopped()) {
2401     return true;
2402   }
2403   // Heap pointers get a null-check from the interpreter,
2404   // as a courtesy.  However, this is not guaranteed by Unsafe,
2405   // and it is not possible to fully distinguish unintended nulls
2406   // from intended ones in this API.
2407 
2408   if (is_volatile) {
2409     // We need to emit leading and trailing CPU membars (see below) in
2410     // addition to memory membars when is_volatile. This is a little
2411     // too strong, but avoids the need to insert per-alias-type
2412     // volatile membars (for stores; compare Parse::do_put_xxx), which
2413     // we cannot do effectively here because we probably only have a
2414     // rough approximation of type.
2415     need_mem_bar = true;
2416     // For Stores, place a memory ordering barrier now.
2417     if (is_store) {
2418       insert_mem_bar(Op_MemBarRelease);
2419     } else {
2420       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2421         insert_mem_bar(Op_MemBarVolatile);
2422       }
2423     }
2424   }
2425 
2426   // Memory barrier to prevent normal and 'unsafe' accesses from
2427   // bypassing each other.  Happens after null checks, so the
2428   // exception paths do not take memory state from the memory barrier,
2429   // so there's no problems making a strong assert about mixing users
2430   // of safe &amp; unsafe memory.
2431   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2432 
2433    if (!is_store) {
2434     Node* p = NULL;
2435     // Try to constant fold a load from a constant field
2436     ciField* field = alias_type-&gt;field();
2437     if (heap_base_oop != top() &amp;&amp;
2438         field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; field-&gt;layout_type() == type) {
2439       // final or stable field
2440       const Type* con_type = Type::make_constant(alias_type-&gt;field(), heap_base_oop);
2441       if (con_type != NULL) {
2442         p = makecon(con_type);
2443       }
2444     }
2445     if (p == NULL) {
2446       MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2447       // To be valid, unsafe loads may depend on other conditions than
2448       // the one that guards them: pin the Load node
2449       p = make_load(control(), adr, value_type, type, adr_type, mo, LoadNode::Pinned, is_volatile);
2450       // load value
2451       switch (type) {
2452       case T_BOOLEAN:
2453       case T_CHAR:
2454       case T_BYTE:
2455       case T_SHORT:
2456       case T_INT:
2457       case T_LONG:
2458       case T_FLOAT:
2459       case T_DOUBLE:
2460         break;
2461       case T_OBJECT:
2462         if (need_read_barrier) {
2463           insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2464         }
2465         break;
2466       case T_ADDRESS:
2467         // Cast to an int type.
2468         p = _gvn.transform(new CastP2XNode(NULL, p));
2469         p = ConvX2UL(p);
2470         break;
2471       default:
2472         fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2473         break;
2474       }
2475     }
2476     // The load node has the control of the preceding MemBarCPUOrder.  All
2477     // following nodes will have the control of the MemBarCPUOrder inserted at
2478     // the end of this method.  So, pushing the load onto the stack at a later
2479     // point is fine.
2480     set_result(p);
2481   } else {
2482     // place effect of store into memory
2483     switch (type) {
2484     case T_DOUBLE:
2485       val = dstore_rounding(val);
2486       break;
2487     case T_ADDRESS:
2488       // Repackage the long as a pointer.
2489       val = ConvL2X(val);
2490       val = _gvn.transform(new CastX2PNode(val));
2491       break;
2492     }
2493 
2494     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2495     if (type != T_OBJECT ) {
2496       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2497     } else {
2498       // Possibly an oop being stored to Java heap or native memory
2499       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2500         // oop to Java heap.
2501         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2502       } else {
2503         // We can't tell at compile time if we are storing in the Java heap or outside
2504         // of it. So we need to emit code to conditionally do the proper type of
2505         // store.
2506 
2507         IdealKit ideal(this);
2508 #define __ ideal.
2509         // QQQ who knows what probability is here??
2510         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2511           // Sync IdealKit and graphKit.
2512           sync_kit(ideal);
2513           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2514           // Update IdealKit memory.
2515           __ sync_kit(this);
2516         } __ else_(); {
2517           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2518         } __ end_if();
2519         // Final sync IdealKit and GraphKit.
2520         final_sync(ideal);
2521 #undef __
2522       }
2523     }
2524   }
2525 
2526   if (is_volatile) {
2527     if (!is_store) {
2528       insert_mem_bar(Op_MemBarAcquire);
2529     } else {
2530       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2531         insert_mem_bar(Op_MemBarVolatile);
2532       }
2533     }
2534   }
2535 
2536   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2537 
2538   return true;
2539 }
2540 
2541 //----------------------------inline_unsafe_load_store----------------------------
2542 // This method serves a couple of different customers (depending on LoadStoreKind):
2543 //
2544 // LS_cmpxchg:
2545 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2546 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2547 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2548 //
2549 // LS_xadd:
2550 //   public int  getAndAddInt( Object o, long offset, int  delta)
2551 //   public long getAndAddLong(Object o, long offset, long delta)
2552 //
2553 // LS_xchg:
2554 //   int    getAndSet(Object o, long offset, int    newValue)
2555 //   long   getAndSet(Object o, long offset, long   newValue)
2556 //   Object getAndSet(Object o, long offset, Object newValue)
2557 //
2558 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2559   // This basic scheme here is the same as inline_unsafe_access, but
2560   // differs in enough details that combining them would make the code
2561   // overly confusing.  (This is a true fact! I originally combined
2562   // them, but even I was confused by it!) As much code/comments as
2563   // possible are retained from inline_unsafe_access though to make
2564   // the correspondences clearer. - dl
2565 
2566   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2567 
2568 #ifndef PRODUCT
2569   BasicType rtype;
2570   {
2571     ResourceMark rm;
2572     // Check the signatures.
2573     ciSignature* sig = callee()-&gt;signature();
2574     rtype = sig-&gt;return_type()-&gt;basic_type();
2575     if (kind == LS_xadd || kind == LS_xchg) {
2576       // Check the signatures.
2577 #ifdef ASSERT
2578       assert(rtype == type, "get and set must return the expected type");
2579       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2580       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2581       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2582       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2583 #endif // ASSERT
2584     } else if (kind == LS_cmpxchg) {
2585       // Check the signatures.
2586 #ifdef ASSERT
2587       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2588       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2589       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2590       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2591 #endif // ASSERT
2592     } else {
2593       ShouldNotReachHere();
2594     }
2595   }
2596 #endif //PRODUCT
2597 
2598   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2599 
2600   // Get arguments:
2601   Node* receiver = NULL;
2602   Node* base     = NULL;
2603   Node* offset   = NULL;
2604   Node* oldval   = NULL;
2605   Node* newval   = NULL;
2606   if (kind == LS_cmpxchg) {
2607     const bool two_slot_type = type2size[type] == 2;
2608     receiver = argument(0);  // type: oop
2609     base     = argument(1);  // type: oop
2610     offset   = argument(2);  // type: long
2611     oldval   = argument(4);  // type: oop, int, or long
2612     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2613   } else if (kind == LS_xadd || kind == LS_xchg){
2614     receiver = argument(0);  // type: oop
2615     base     = argument(1);  // type: oop
2616     offset   = argument(2);  // type: long
2617     oldval   = NULL;
2618     newval   = argument(4);  // type: oop, int, or long
2619   }
2620 
2621   // Null check receiver.
2622   receiver = null_check(receiver);
2623   if (stopped()) {
2624     return true;
2625   }
2626 
2627   // Build field offset expression.
2628   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2629   // to be plain byte offsets, which are also the same as those accepted
2630   // by oopDesc::field_base.
2631   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2632   // 32-bit machines ignore the high half of long offsets
2633   offset = ConvL2X(offset);
2634   Node* adr = make_unsafe_address(base, offset);
2635   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2636 
2637   // For CAS, unlike inline_unsafe_access, there seems no point in
2638   // trying to refine types. Just use the coarse types here.
2639   const Type *value_type = Type::get_const_basic_type(type);
2640   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2641   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2642 
2643   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2644     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2645     if (tjp != NULL) {
2646       value_type = tjp;
2647     }
2648   }
2649 
2650   int alias_idx = C-&gt;get_alias_index(adr_type);
2651 
2652   // Memory-model-wise, a LoadStore acts like a little synchronized
2653   // block, so needs barriers on each side.  These don't translate
2654   // into actual barriers on most machines, but we still need rest of
2655   // compiler to respect ordering.
2656 
2657   insert_mem_bar(Op_MemBarRelease);
2658   insert_mem_bar(Op_MemBarCPUOrder);
2659 
2660   // 4984716: MemBars must be inserted before this
2661   //          memory node in order to avoid a false
2662   //          dependency which will confuse the scheduler.
2663   Node *mem = memory(alias_idx);
2664 
2665   // For now, we handle only those cases that actually exist: ints,
2666   // longs, and Object. Adding others should be straightforward.
2667   Node* load_store;
2668   switch(type) {
2669   case T_INT:
2670     if (kind == LS_xadd) {
2671       load_store = _gvn.transform(new GetAndAddINode(control(), mem, adr, newval, adr_type));
2672     } else if (kind == LS_xchg) {
2673       load_store = _gvn.transform(new GetAndSetINode(control(), mem, adr, newval, adr_type));
2674     } else if (kind == LS_cmpxchg) {
2675       load_store = _gvn.transform(new CompareAndSwapINode(control(), mem, adr, newval, oldval));
2676     } else {
2677       ShouldNotReachHere();
2678     }
2679     break;
2680   case T_LONG:
2681     if (kind == LS_xadd) {
2682       load_store = _gvn.transform(new GetAndAddLNode(control(), mem, adr, newval, adr_type));
2683     } else if (kind == LS_xchg) {
2684       load_store = _gvn.transform(new GetAndSetLNode(control(), mem, adr, newval, adr_type));
2685     } else if (kind == LS_cmpxchg) {
2686       load_store = _gvn.transform(new CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2687     } else {
2688       ShouldNotReachHere();
2689     }
2690     break;
2691   case T_OBJECT:
2692     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2693     // could be delayed during Parse (for example, in adjust_map_after_if()).
2694     // Execute transformation here to avoid barrier generation in such case.
2695     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2696       newval = _gvn.makecon(TypePtr::NULL_PTR);
2697 
2698     // Reference stores need a store barrier.
2699     if (kind == LS_xchg) {
2700       // If pre-barrier must execute before the oop store, old value will require do_load here.
2701       if (!can_move_pre_barrier()) {
2702         pre_barrier(true /* do_load*/,
2703                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2704                     NULL /* pre_val*/,
2705                     T_OBJECT);
2706       } // Else move pre_barrier to use load_store value, see below.
2707     } else if (kind == LS_cmpxchg) {
2708       // Same as for newval above:
2709       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2710         oldval = _gvn.makecon(TypePtr::NULL_PTR);
2711       }
2712       // The only known value which might get overwritten is oldval.
2713       pre_barrier(false /* do_load */,
2714                   control(), NULL, NULL, max_juint, NULL, NULL,
2715                   oldval /* pre_val */,
2716                   T_OBJECT);
2717     } else {
2718       ShouldNotReachHere();
2719     }
2720 
2721 #ifdef _LP64
2722     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2723       Node *newval_enc = _gvn.transform(new EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2724       if (kind == LS_xchg) {
2725         load_store = _gvn.transform(new GetAndSetNNode(control(), mem, adr,
2726                                                        newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2727       } else {
2728         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2729         Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2730         load_store = _gvn.transform(new CompareAndSwapNNode(control(), mem, adr,
2731                                                                 newval_enc, oldval_enc));
2732       }
2733     } else
2734 #endif
2735     {
2736       if (kind == LS_xchg) {
2737         load_store = _gvn.transform(new GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2738       } else {
2739         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2740         load_store = _gvn.transform(new CompareAndSwapPNode(control(), mem, adr, newval, oldval));
2741       }
2742     }
2743     if (kind == LS_cmpxchg) {
2744       // Emit the post barrier only when the actual store happened.
2745       // This makes sense to check only for compareAndSet that can fail to set the value.
2746       // CAS success path is marked more likely since we anticipate this is a performance
2747       // critical path, while CAS failure path can use the penalty for going through unlikely
2748       // path as backoff. Which is still better than doing a store barrier there.
2749       IdealKit ideal(this);
2750       ideal.if_then(load_store, BoolTest::ne, ideal.ConI(0), PROB_STATIC_FREQUENT); {
2751         sync_kit(ideal);
2752         post_barrier(ideal.ctrl(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2753         ideal.sync_kit(this);
2754       } ideal.end_if();
2755       final_sync(ideal);
2756     } else {
2757       post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2758     }
2759     break;
2760   default:
2761     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2762     break;
2763   }
2764 
2765   // SCMemProjNodes represent the memory state of a LoadStore. Their
2766   // main role is to prevent LoadStore nodes from being optimized away
2767   // when their results aren't used.
2768   Node* proj = _gvn.transform(new SCMemProjNode(load_store));
2769   set_memory(proj, alias_idx);
2770 
2771   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
2772 #ifdef _LP64
2773     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2774       load_store = _gvn.transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
2775     }
2776 #endif
2777     if (can_move_pre_barrier()) {
2778       // Don't need to load pre_val. The old value is returned by load_store.
2779       // The pre_barrier can execute after the xchg as long as no safepoint
2780       // gets inserted between them.
2781       pre_barrier(false /* do_load */,
2782                   control(), NULL, NULL, max_juint, NULL, NULL,
2783                   load_store /* pre_val */,
2784                   T_OBJECT);
2785     }
2786   }
2787 
2788   // Add the trailing membar surrounding the access
2789   insert_mem_bar(Op_MemBarCPUOrder);
2790   insert_mem_bar(Op_MemBarAcquire);
2791 
2792   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
2793   set_result(load_store);
2794   return true;
2795 }
2796 
2797 //----------------------------inline_unsafe_ordered_store----------------------
2798 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
2799 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
2800 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
2801 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
2802   // This is another variant of inline_unsafe_access, differing in
2803   // that it always issues store-store ("release") barrier and ensures
2804   // store-atomicity (which only matters for "long").
2805 
2806   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2807 
2808 #ifndef PRODUCT
2809   {
2810     ResourceMark rm;
2811     // Check the signatures.
2812     ciSignature* sig = callee()-&gt;signature();
2813 #ifdef ASSERT
2814     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2815     assert(rtype == T_VOID, "must return void");
2816     assert(sig-&gt;count() == 3, "has 3 arguments");
2817     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
2818     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
2819 #endif // ASSERT
2820   }
2821 #endif //PRODUCT
2822 
2823   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2824 
2825   // Get arguments:
2826   Node* receiver = argument(0);  // type: oop
2827   Node* base     = argument(1);  // type: oop
2828   Node* offset   = argument(2);  // type: long
2829   Node* val      = argument(4);  // type: oop, int, or long
2830 
2831   // Null check receiver.
2832   receiver = null_check(receiver);
2833   if (stopped()) {
2834     return true;
2835   }
2836 
2837   // Build field offset expression.
2838   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2839   // 32-bit machines ignore the high half of long offsets
2840   offset = ConvL2X(offset);
2841   Node* adr = make_unsafe_address(base, offset);
2842   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2843   const Type *value_type = Type::get_const_basic_type(type);
2844   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2845 
2846   insert_mem_bar(Op_MemBarRelease);
2847   insert_mem_bar(Op_MemBarCPUOrder);
2848   // Ensure that the store is atomic for longs:
2849   const bool require_atomic_access = true;
2850   Node* store;
2851   if (type == T_OBJECT) // reference stores need a store barrier.
2852     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
2853   else {
2854     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
2855   }
2856   insert_mem_bar(Op_MemBarCPUOrder);
2857   return true;
2858 }
2859 
2860 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
2861   // Regardless of form, don't allow previous ld/st to move down,
2862   // then issue acquire, release, or volatile mem_bar.
2863   insert_mem_bar(Op_MemBarCPUOrder);
2864   switch(id) {
2865     case vmIntrinsics::_loadFence:
2866       insert_mem_bar(Op_LoadFence);
2867       return true;
2868     case vmIntrinsics::_storeFence:
2869       insert_mem_bar(Op_StoreFence);
2870       return true;
2871     case vmIntrinsics::_fullFence:
2872       insert_mem_bar(Op_MemBarVolatile);
2873       return true;
2874     default:
2875       fatal_unexpected_iid(id);
2876       return false;
2877   }
2878 }
2879 
2880 bool LibraryCallKit::inline_spinloophint() {
2881   insert_mem_bar(Op_SpinLoopHint);
2882   return true;
2883 }
2884 
2885 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
2886   if (!kls-&gt;is_Con()) {
2887     return true;
2888   }
2889   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
2890   if (klsptr == NULL) {
2891     return true;
2892   }
2893   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
2894   // don't need a guard for a klass that is already initialized
2895   return !ik-&gt;is_initialized();
2896 }
2897 
2898 //----------------------------inline_unsafe_allocate---------------------------
2899 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
2900 bool LibraryCallKit::inline_unsafe_allocate() {
2901   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2902 
2903   null_check_receiver();  // null-check, then ignore
2904   Node* cls = null_check(argument(1));
2905   if (stopped())  return true;
2906 
2907   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
2908   kls = null_check(kls);
2909   if (stopped())  return true;  // argument was like int.class
2910 
2911   Node* test = NULL;
2912   if (LibraryCallKit::klass_needs_init_guard(kls)) {
2913     // Note:  The argument might still be an illegal value like
2914     // Serializable.class or Object[].class.   The runtime will handle it.
2915     // But we must make an explicit check for initialization.
2916     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
2917     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
2918     // can generate code to load it as unsigned byte.
2919     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
2920     Node* bits = intcon(InstanceKlass::fully_initialized);
2921     test = _gvn.transform(new SubINode(inst, bits));
2922     // The 'test' is non-zero if we need to take a slow path.
2923   }
2924 
2925   Node* obj = new_instance(kls, test);
2926   set_result(obj);
2927   return true;
2928 }
2929 
2930 #ifdef TRACE_HAVE_INTRINSICS
2931 /*
2932  * oop -&gt; myklass
2933  * myklass-&gt;trace_id |= USED
2934  * return myklass-&gt;trace_id &amp; ~0x3
2935  */
2936 bool LibraryCallKit::inline_native_classID() {
2937   null_check_receiver();  // null-check, then ignore
2938   Node* cls = null_check(argument(1), T_OBJECT);
2939   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
2940   kls = null_check(kls, T_OBJECT);
2941   ByteSize offset = TRACE_ID_OFFSET;
2942   Node* insp = basic_plus_adr(kls, in_bytes(offset));
2943   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
2944   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
2945   Node* andl = _gvn.transform(new AndLNode(tvalue, bits));
2946   Node* clsused = longcon(0x01l); // set the class bit
2947   Node* orl = _gvn.transform(new OrLNode(tvalue, clsused));
2948 
2949   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
2950   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
2951   set_result(andl);
2952   return true;
2953 }
2954 
2955 bool LibraryCallKit::inline_native_threadID() {
2956   Node* tls_ptr = NULL;
2957   Node* cur_thr = generate_current_thread(tls_ptr);
2958   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
2959   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
2960   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
2961 
2962   Node* threadid = NULL;
2963   size_t thread_id_size = OSThread::thread_id_size();
2964   if (thread_id_size == (size_t) BytesPerLong) {
2965     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
2966   } else if (thread_id_size == (size_t) BytesPerInt) {
2967     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
2968   } else {
2969     ShouldNotReachHere();
2970   }
2971   set_result(threadid);
2972   return true;
2973 }
2974 #endif
2975 
2976 //------------------------inline_native_time_funcs--------------
2977 // inline code for System.currentTimeMillis() and System.nanoTime()
2978 // these have the same type and signature
2979 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
2980   const TypeFunc* tf = OptoRuntime::void_long_Type();
2981   const TypePtr* no_memory_effects = NULL;
2982   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
2983   Node* value = _gvn.transform(new ProjNode(time, TypeFunc::Parms+0));
2984 #ifdef ASSERT
2985   Node* value_top = _gvn.transform(new ProjNode(time, TypeFunc::Parms+1));
2986   assert(value_top == top(), "second value must be top");
2987 #endif
2988   set_result(value);
2989   return true;
2990 }
2991 
2992 //------------------------inline_native_currentThread------------------
2993 bool LibraryCallKit::inline_native_currentThread() {
2994   Node* junk = NULL;
2995   set_result(generate_current_thread(junk));
2996   return true;
2997 }
2998 
2999 //------------------------inline_native_isInterrupted------------------
3000 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3001 bool LibraryCallKit::inline_native_isInterrupted() {
3002   // Add a fast path to t.isInterrupted(clear_int):
3003   //   (t == Thread.current() &amp;&amp;
3004   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3005   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3006   // So, in the common case that the interrupt bit is false,
3007   // we avoid making a call into the VM.  Even if the interrupt bit
3008   // is true, if the clear_int argument is false, we avoid the VM call.
3009   // However, if the receiver is not currentThread, we must call the VM,
3010   // because there must be some locking done around the operation.
3011 
3012   // We only go to the fast case code if we pass two guards.
3013   // Paths which do not pass are accumulated in the slow_region.
3014 
3015   enum {
3016     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3017     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3018     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3019     PATH_LIMIT
3020   };
3021 
3022   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3023   // out of the function.
3024   insert_mem_bar(Op_MemBarCPUOrder);
3025 
3026   RegionNode* result_rgn = new RegionNode(PATH_LIMIT);
3027   PhiNode*    result_val = new PhiNode(result_rgn, TypeInt::BOOL);
3028 
3029   RegionNode* slow_region = new RegionNode(1);
3030   record_for_igvn(slow_region);
3031 
3032   // (a) Receiving thread must be the current thread.
3033   Node* rec_thr = argument(0);
3034   Node* tls_ptr = NULL;
3035   Node* cur_thr = generate_current_thread(tls_ptr);
3036   Node* cmp_thr = _gvn.transform(new CmpPNode(cur_thr, rec_thr));
3037   Node* bol_thr = _gvn.transform(new BoolNode(cmp_thr, BoolTest::ne));
3038 
3039   generate_slow_guard(bol_thr, slow_region);
3040 
3041   // (b) Interrupt bit on TLS must be false.
3042   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3043   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3044   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3045 
3046   // Set the control input on the field _interrupted read to prevent it floating up.
3047   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3048   Node* cmp_bit = _gvn.transform(new CmpINode(int_bit, intcon(0)));
3049   Node* bol_bit = _gvn.transform(new BoolNode(cmp_bit, BoolTest::ne));
3050 
3051   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3052 
3053   // First fast path:  if (!TLS._interrupted) return false;
3054   Node* false_bit = _gvn.transform(new IfFalseNode(iff_bit));
3055   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3056   result_val-&gt;init_req(no_int_result_path, intcon(0));
3057 
3058   // drop through to next case
3059   set_control( _gvn.transform(new IfTrueNode(iff_bit)));
3060 
3061 #ifndef TARGET_OS_FAMILY_windows
3062   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3063   Node* clr_arg = argument(1);
3064   Node* cmp_arg = _gvn.transform(new CmpINode(clr_arg, intcon(0)));
3065   Node* bol_arg = _gvn.transform(new BoolNode(cmp_arg, BoolTest::ne));
3066   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3067 
3068   // Second fast path:  ... else if (!clear_int) return true;
3069   Node* false_arg = _gvn.transform(new IfFalseNode(iff_arg));
3070   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3071   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3072 
3073   // drop through to next case
3074   set_control( _gvn.transform(new IfTrueNode(iff_arg)));
3075 #else
3076   // To return true on Windows you must read the _interrupted field
3077   // and check the the event state i.e. take the slow path.
3078 #endif // TARGET_OS_FAMILY_windows
3079 
3080   // (d) Otherwise, go to the slow path.
3081   slow_region-&gt;add_req(control());
3082   set_control( _gvn.transform(slow_region));
3083 
3084   if (stopped()) {
3085     // There is no slow path.
3086     result_rgn-&gt;init_req(slow_result_path, top());
3087     result_val-&gt;init_req(slow_result_path, top());
3088   } else {
3089     // non-virtual because it is a private non-static
3090     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3091 
3092     Node* slow_val = set_results_for_java_call(slow_call);
3093     // this-&gt;control() comes from set_results_for_java_call
3094 
3095     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3096     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3097 
3098     // These two phis are pre-filled with copies of of the fast IO and Memory
3099     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3100     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3101 
3102     result_rgn-&gt;init_req(slow_result_path, control());
3103     result_io -&gt;init_req(slow_result_path, i_o());
3104     result_mem-&gt;init_req(slow_result_path, reset_memory());
3105     result_val-&gt;init_req(slow_result_path, slow_val);
3106 
3107     set_all_memory(_gvn.transform(result_mem));
3108     set_i_o(       _gvn.transform(result_io));
3109   }
3110 
3111   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3112   set_result(result_rgn, result_val);
3113   return true;
3114 }
3115 
3116 //---------------------------load_mirror_from_klass----------------------------
3117 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3118 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3119   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3120   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3121 }
3122 
3123 //-----------------------load_klass_from_mirror_common-------------------------
3124 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3125 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3126 // and branch to the given path on the region.
3127 // If never_see_null, take an uncommon trap on null, so we can optimistically
3128 // compile for the non-null case.
3129 // If the region is NULL, force never_see_null = true.
3130 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3131                                                     bool never_see_null,
3132                                                     RegionNode* region,
3133                                                     int null_path,
3134                                                     int offset) {
3135   if (region == NULL)  never_see_null = true;
3136   Node* p = basic_plus_adr(mirror, offset);
3137   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3138   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3139   Node* null_ctl = top();
3140   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3141   if (region != NULL) {
3142     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3143     region-&gt;init_req(null_path, null_ctl);
3144   } else {
3145     assert(null_ctl == top(), "no loose ends");
3146   }
3147   return kls;
3148 }
3149 
3150 //--------------------(inline_native_Class_query helpers)---------------------
3151 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3152 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3153 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3154   // Branch around if the given klass has the given modifier bit set.
3155   // Like generate_guard, adds a new path onto the region.
3156   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3157   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3158   Node* mask = intcon(modifier_mask);
3159   Node* bits = intcon(modifier_bits);
3160   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3161   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3162   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3163   return generate_fair_guard(bol, region);
3164 }
3165 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3166   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3167 }
3168 
3169 //-------------------------inline_native_Class_query-------------------
3170 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3171   const Type* return_type = TypeInt::BOOL;
3172   Node* prim_return_value = top();  // what happens if it's a primitive class?
3173   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3174   bool expect_prim = false;     // most of these guys expect to work on refs
3175 
3176   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3177 
3178   Node* mirror = argument(0);
3179   Node* obj    = top();
3180 
3181   switch (id) {
3182   case vmIntrinsics::_isInstance:
3183     // nothing is an instance of a primitive type
3184     prim_return_value = intcon(0);
3185     obj = argument(1);
3186     break;
3187   case vmIntrinsics::_getModifiers:
3188     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3189     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3190     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3191     break;
3192   case vmIntrinsics::_isInterface:
3193     prim_return_value = intcon(0);
3194     break;
3195   case vmIntrinsics::_isArray:
3196     prim_return_value = intcon(0);
3197     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3198     break;
3199   case vmIntrinsics::_isPrimitive:
3200     prim_return_value = intcon(1);
3201     expect_prim = true;  // obviously
3202     break;
3203   case vmIntrinsics::_getSuperclass:
3204     prim_return_value = null();
3205     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3206     break;
3207   case vmIntrinsics::_getClassAccessFlags:
3208     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3209     return_type = TypeInt::INT;  // not bool!  6297094
3210     break;
3211   default:
3212     fatal_unexpected_iid(id);
3213     break;
3214   }
3215 
3216   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3217   if (mirror_con == NULL)  return false;  // cannot happen?
3218 
3219 #ifndef PRODUCT
3220   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3221     ciType* k = mirror_con-&gt;java_mirror_type();
3222     if (k) {
3223       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3224       k-&gt;print_name();
3225       tty-&gt;cr();
3226     }
3227   }
3228 #endif
3229 
3230   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3231   RegionNode* region = new RegionNode(PATH_LIMIT);
3232   record_for_igvn(region);
3233   PhiNode* phi = new PhiNode(region, return_type);
3234 
3235   // The mirror will never be null of Reflection.getClassAccessFlags, however
3236   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3237   // if it is. See bug 4774291.
3238 
3239   // For Reflection.getClassAccessFlags(), the null check occurs in
3240   // the wrong place; see inline_unsafe_access(), above, for a similar
3241   // situation.
3242   mirror = null_check(mirror);
3243   // If mirror or obj is dead, only null-path is taken.
3244   if (stopped())  return true;
3245 
3246   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3247 
3248   // Now load the mirror's klass metaobject, and null-check it.
3249   // Side-effects region with the control path if the klass is null.
3250   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3251   // If kls is null, we have a primitive mirror.
3252   phi-&gt;init_req(_prim_path, prim_return_value);
3253   if (stopped()) { set_result(region, phi); return true; }
3254   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3255 
3256   Node* p;  // handy temp
3257   Node* null_ctl;
3258 
3259   // Now that we have the non-null klass, we can perform the real query.
3260   // For constant classes, the query will constant-fold in LoadNode::Value.
3261   Node* query_value = top();
3262   switch (id) {
3263   case vmIntrinsics::_isInstance:
3264     // nothing is an instance of a primitive type
3265     query_value = gen_instanceof(obj, kls, safe_for_replace);
3266     break;
3267   
3268   case vmIntrinsics::_spinLoopHint:
3269     break;
3270 
3271   case vmIntrinsics::_getModifiers:
3272     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3273     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3274     break;
3275 
3276   case vmIntrinsics::_isInterface:
3277     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3278     if (generate_interface_guard(kls, region) != NULL)
3279       // A guard was added.  If the guard is taken, it was an interface.
3280       phi-&gt;add_req(intcon(1));
3281     // If we fall through, it's a plain class.
3282     query_value = intcon(0);
3283     break;
3284 
3285   case vmIntrinsics::_isArray:
3286     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3287     if (generate_array_guard(kls, region) != NULL)
3288       // A guard was added.  If the guard is taken, it was an array.
3289       phi-&gt;add_req(intcon(1));
3290     // If we fall through, it's a plain class.
3291     query_value = intcon(0);
3292     break;
3293 
3294   case vmIntrinsics::_isPrimitive:
3295     query_value = intcon(0); // "normal" path produces false
3296     break;
3297 
3298   case vmIntrinsics::_getSuperclass:
3299     // The rules here are somewhat unfortunate, but we can still do better
3300     // with random logic than with a JNI call.
3301     // Interfaces store null or Object as _super, but must report null.
3302     // Arrays store an intermediate super as _super, but must report Object.
3303     // Other types can report the actual _super.
3304     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3305     if (generate_interface_guard(kls, region) != NULL)
3306       // A guard was added.  If the guard is taken, it was an interface.
3307       phi-&gt;add_req(null());
3308     if (generate_array_guard(kls, region) != NULL)
3309       // A guard was added.  If the guard is taken, it was an array.
3310       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3311     // If we fall through, it's a plain class.  Get its _super.
3312     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3313     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3314     null_ctl = top();
3315     kls = null_check_oop(kls, &amp;null_ctl);
3316     if (null_ctl != top()) {
3317       // If the guard is taken, Object.superClass is null (both klass and mirror).
3318       region-&gt;add_req(null_ctl);
3319       phi   -&gt;add_req(null());
3320     }
3321     if (!stopped()) {
3322       query_value = load_mirror_from_klass(kls);
3323     }
3324     break;
3325 
3326   case vmIntrinsics::_getClassAccessFlags:
3327     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3328     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3329     break;
3330 
3331   default:
3332     fatal_unexpected_iid(id);
3333     break;
3334   }
3335 
3336   // Fall-through is the normal case of a query to a real class.
3337   phi-&gt;init_req(1, query_value);
3338   region-&gt;init_req(1, control());
3339 
3340   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3341   set_result(region, phi);
3342   return true;
3343 }
3344 
3345 //-------------------------inline_Class_cast-------------------
3346 bool LibraryCallKit::inline_Class_cast() {
3347   Node* mirror = argument(0); // Class
3348   Node* obj    = argument(1);
3349   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3350   if (mirror_con == NULL) {
3351     return false;  // dead path (mirror-&gt;is_top()).
3352   }
3353   if (obj == NULL || obj-&gt;is_top()) {
3354     return false;  // dead path
3355   }
3356   const TypeOopPtr* tp = _gvn.type(obj)-&gt;isa_oopptr();
3357 
3358   // First, see if Class.cast() can be folded statically.
3359   // java_mirror_type() returns non-null for compile-time Class constants.
3360   ciType* tm = mirror_con-&gt;java_mirror_type();
3361   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp;
3362       tp != NULL &amp;&amp; tp-&gt;klass() != NULL) {
3363     if (!tp-&gt;klass()-&gt;is_loaded()) {
3364       // Don't use intrinsic when class is not loaded.
3365       return false;
3366     } else {
3367       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), tp-&gt;klass());
3368       if (static_res == Compile::SSC_always_true) {
3369         // isInstance() is true - fold the code.
3370         set_result(obj);
3371         return true;
3372       } else if (static_res == Compile::SSC_always_false) {
3373         // Don't use intrinsic, have to throw ClassCastException.
3374         // If the reference is null, the non-intrinsic bytecode will
3375         // be optimized appropriately.
3376         return false;
3377       }
3378     }
3379   }
3380 
3381   // Bailout intrinsic and do normal inlining if exception path is frequent.
3382   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3383     return false;
3384   }
3385 
3386   // Generate dynamic checks.
3387   // Class.cast() is java implementation of _checkcast bytecode.
3388   // Do checkcast (Parse::do_checkcast()) optimizations here.
3389 
3390   mirror = null_check(mirror);
3391   // If mirror is dead, only null-path is taken.
3392   if (stopped()) {
3393     return true;
3394   }
3395 
3396   // Not-subtype or the mirror's klass ptr is NULL (in case it is a primitive).
3397   enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };
3398   RegionNode* region = new RegionNode(PATH_LIMIT);
3399   record_for_igvn(region);
3400 
3401   // Now load the mirror's klass metaobject, and null-check it.
3402   // If kls is null, we have a primitive mirror and
3403   // nothing is an instance of a primitive type.
3404   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3405 
3406   Node* res = top();
3407   if (!stopped()) {
3408     Node* bad_type_ctrl = top();
3409     // Do checkcast optimizations.
3410     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3411     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3412   }
3413   if (region-&gt;in(_prim_path) != top() ||
3414       region-&gt;in(_bad_type_path) != top()) {
3415     // Let Interpreter throw ClassCastException.
3416     PreserveJVMState pjvms(this);
3417     set_control(_gvn.transform(region));
3418     uncommon_trap(Deoptimization::Reason_intrinsic,
3419                   Deoptimization::Action_maybe_recompile);
3420   }
3421   if (!stopped()) {
3422     set_result(res);
3423   }
3424   return true;
3425 }
3426 
3427 
3428 //--------------------------inline_native_subtype_check------------------------
3429 // This intrinsic takes the JNI calls out of the heart of
3430 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3431 bool LibraryCallKit::inline_native_subtype_check() {
3432   // Pull both arguments off the stack.
3433   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3434   args[0] = argument(0);
3435   args[1] = argument(1);
3436   Node* klasses[2];             // corresponding Klasses: superk, subk
3437   klasses[0] = klasses[1] = top();
3438 
3439   enum {
3440     // A full decision tree on {superc is prim, subc is prim}:
3441     _prim_0_path = 1,           // {P,N} =&gt; false
3442                                 // {P,P} &amp; superc!=subc =&gt; false
3443     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3444     _prim_1_path,               // {N,P} =&gt; false
3445     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3446     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3447     PATH_LIMIT
3448   };
3449 
3450   RegionNode* region = new RegionNode(PATH_LIMIT);
3451   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3452   record_for_igvn(region);
3453 
3454   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3455   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3456   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3457 
3458   // First null-check both mirrors and load each mirror's klass metaobject.
3459   int which_arg;
3460   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3461     Node* arg = args[which_arg];
3462     arg = null_check(arg);
3463     if (stopped())  break;
3464     args[which_arg] = arg;
3465 
3466     Node* p = basic_plus_adr(arg, class_klass_offset);
3467     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3468     klasses[which_arg] = _gvn.transform(kls);
3469   }
3470 
3471   // Having loaded both klasses, test each for null.
3472   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3473   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3474     Node* kls = klasses[which_arg];
3475     Node* null_ctl = top();
3476     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3477     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3478     region-&gt;init_req(prim_path, null_ctl);
3479     if (stopped())  break;
3480     klasses[which_arg] = kls;
3481   }
3482 
3483   if (!stopped()) {
3484     // now we have two reference types, in klasses[0..1]
3485     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3486     Node* superk = klasses[0];  // the receiver
3487     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3488     // now we have a successful reference subtype check
3489     region-&gt;set_req(_ref_subtype_path, control());
3490   }
3491 
3492   // If both operands are primitive (both klasses null), then
3493   // we must return true when they are identical primitives.
3494   // It is convenient to test this after the first null klass check.
3495   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3496   if (!stopped()) {
3497     // Since superc is primitive, make a guard for the superc==subc case.
3498     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3499     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
3500     generate_guard(bol_eq, region, PROB_FAIR);
3501     if (region-&gt;req() == PATH_LIMIT+1) {
3502       // A guard was added.  If the added guard is taken, superc==subc.
3503       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3504       region-&gt;del_req(PATH_LIMIT);
3505     }
3506     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3507   }
3508 
3509   // these are the only paths that produce 'true':
3510   phi-&gt;set_req(_prim_same_path,   intcon(1));
3511   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3512 
3513   // pull together the cases:
3514   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3515   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3516     Node* ctl = region-&gt;in(i);
3517     if (ctl == NULL || ctl == top()) {
3518       region-&gt;set_req(i, top());
3519       phi   -&gt;set_req(i, top());
3520     } else if (phi-&gt;in(i) == NULL) {
3521       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3522     }
3523   }
3524 
3525   set_control(_gvn.transform(region));
3526   set_result(_gvn.transform(phi));
3527   return true;
3528 }
3529 
3530 //---------------------generate_array_guard_common------------------------
3531 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3532                                                   bool obj_array, bool not_array) {
3533 
3534   if (stopped()) {
3535     return NULL;
3536   }
3537 
3538   // If obj_array/non_array==false/false:
3539   // Branch around if the given klass is in fact an array (either obj or prim).
3540   // If obj_array/non_array==false/true:
3541   // Branch around if the given klass is not an array klass of any kind.
3542   // If obj_array/non_array==true/true:
3543   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3544   // If obj_array/non_array==true/false:
3545   // Branch around if the kls is an oop array (Object[] or subtype)
3546   //
3547   // Like generate_guard, adds a new path onto the region.
3548   jint  layout_con = 0;
3549   Node* layout_val = get_layout_helper(kls, layout_con);
3550   if (layout_val == NULL) {
3551     bool query = (obj_array
3552                   ? Klass::layout_helper_is_objArray(layout_con)
3553                   : Klass::layout_helper_is_array(layout_con));
3554     if (query == not_array) {
3555       return NULL;                       // never a branch
3556     } else {                             // always a branch
3557       Node* always_branch = control();
3558       if (region != NULL)
3559         region-&gt;add_req(always_branch);
3560       set_control(top());
3561       return always_branch;
3562     }
3563   }
3564   // Now test the correct condition.
3565   jint  nval = (obj_array
3566                 ? ((jint)Klass::_lh_array_tag_type_value
3567                    &lt;&lt;    Klass::_lh_array_tag_shift)
3568                 : Klass::_lh_neutral_value);
3569   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3570   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3571   // invert the test if we are looking for a non-array
3572   if (not_array)  btest = BoolTest(btest).negate();
3573   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3574   return generate_fair_guard(bol, region);
3575 }
3576 
3577 
3578 //-----------------------inline_native_newArray--------------------------
3579 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3580 bool LibraryCallKit::inline_native_newArray() {
3581   Node* mirror    = argument(0);
3582   Node* count_val = argument(1);
3583 
3584   mirror = null_check(mirror);
3585   // If mirror or obj is dead, only null-path is taken.
3586   if (stopped())  return true;
3587 
3588   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3589   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3590   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3591   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3592   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3593 
3594   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3595   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3596                                                   result_reg, _slow_path);
3597   Node* normal_ctl   = control();
3598   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3599 
3600   // Generate code for the slow case.  We make a call to newArray().
3601   set_control(no_array_ctl);
3602   if (!stopped()) {
3603     // Either the input type is void.class, or else the
3604     // array klass has not yet been cached.  Either the
3605     // ensuing call will throw an exception, or else it
3606     // will cache the array klass for next time.
3607     PreserveJVMState pjvms(this);
3608     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3609     Node* slow_result = set_results_for_java_call(slow_call);
3610     // this-&gt;control() comes from set_results_for_java_call
3611     result_reg-&gt;set_req(_slow_path, control());
3612     result_val-&gt;set_req(_slow_path, slow_result);
3613     result_io -&gt;set_req(_slow_path, i_o());
3614     result_mem-&gt;set_req(_slow_path, reset_memory());
3615   }
3616 
3617   set_control(normal_ctl);
3618   if (!stopped()) {
3619     // Normal case:  The array type has been cached in the java.lang.Class.
3620     // The following call works fine even if the array type is polymorphic.
3621     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3622     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3623     result_reg-&gt;init_req(_normal_path, control());
3624     result_val-&gt;init_req(_normal_path, obj);
3625     result_io -&gt;init_req(_normal_path, i_o());
3626     result_mem-&gt;init_req(_normal_path, reset_memory());
3627   }
3628 
3629   // Return the combined state.
3630   set_i_o(        _gvn.transform(result_io)  );
3631   set_all_memory( _gvn.transform(result_mem));
3632 
3633   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3634   set_result(result_reg, result_val);
3635   return true;
3636 }
3637 
3638 //----------------------inline_native_getLength--------------------------
3639 // public static native int java.lang.reflect.Array.getLength(Object array);
3640 bool LibraryCallKit::inline_native_getLength() {
3641   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3642 
3643   Node* array = null_check(argument(0));
3644   // If array is dead, only null-path is taken.
3645   if (stopped())  return true;
3646 
3647   // Deoptimize if it is a non-array.
3648   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3649 
3650   if (non_array != NULL) {
3651     PreserveJVMState pjvms(this);
3652     set_control(non_array);
3653     uncommon_trap(Deoptimization::Reason_intrinsic,
3654                   Deoptimization::Action_maybe_recompile);
3655   }
3656 
3657   // If control is dead, only non-array-path is taken.
3658   if (stopped())  return true;
3659 
3660   // The works fine even if the array type is polymorphic.
3661   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3662   Node* result = load_array_length(array);
3663 
3664   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3665   set_result(result);
3666   return true;
3667 }
3668 
3669 //------------------------inline_array_copyOf----------------------------
3670 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3671 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3672 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3673   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3674 
3675   // Get the arguments.
3676   Node* original          = argument(0);
3677   Node* start             = is_copyOfRange? argument(1): intcon(0);
3678   Node* end               = is_copyOfRange? argument(2): argument(1);
3679   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3680 
3681   Node* newcopy;
3682 
3683   // Set the original stack and the reexecute bit for the interpreter to reexecute
3684   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3685   { PreserveReexecuteState preexecs(this);
3686     jvms()-&gt;set_should_reexecute(true);
3687 
3688     array_type_mirror = null_check(array_type_mirror);
3689     original          = null_check(original);
3690 
3691     // Check if a null path was taken unconditionally.
3692     if (stopped())  return true;
3693 
3694     Node* orig_length = load_array_length(original);
3695 
3696     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3697     klass_node = null_check(klass_node);
3698 
3699     RegionNode* bailout = new RegionNode(1);
3700     record_for_igvn(bailout);
3701 
3702     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3703     // Bail out if that is so.
3704     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3705     if (not_objArray != NULL) {
3706       // Improve the klass node's type from the new optimistic assumption:
3707       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3708       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3709       Node* cast = new CastPPNode(klass_node, akls);
3710       cast-&gt;init_req(0, control());
3711       klass_node = _gvn.transform(cast);
3712     }
3713 
3714     // Bail out if either start or end is negative.
3715     generate_negative_guard(start, bailout, &amp;start);
3716     generate_negative_guard(end,   bailout, &amp;end);
3717 
3718     Node* length = end;
3719     if (_gvn.type(start) != TypeInt::ZERO) {
3720       length = _gvn.transform(new SubINode(end, start));
3721     }
3722 
3723     // Bail out if length is negative.
3724     // Without this the new_array would throw
3725     // NegativeArraySizeException but IllegalArgumentException is what
3726     // should be thrown
3727     generate_negative_guard(length, bailout, &amp;length);
3728 
3729     if (bailout-&gt;req() &gt; 1) {
3730       PreserveJVMState pjvms(this);
3731       set_control(_gvn.transform(bailout));
3732       uncommon_trap(Deoptimization::Reason_intrinsic,
3733                     Deoptimization::Action_maybe_recompile);
3734     }
3735 
3736     if (!stopped()) {
3737       // How many elements will we copy from the original?
3738       // The answer is MinI(orig_length - start, length).
3739       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
3740       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3741 
3742       // Generate a direct call to the right arraycopy function(s).
3743       // We know the copy is disjoint but we might not know if the
3744       // oop stores need checking.
3745       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3746       // This will fail a store-check if x contains any non-nulls.
3747 
3748       // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3749       // loads/stores but it is legal only if we're sure the
3750       // Arrays.copyOf would succeed. So we need all input arguments
3751       // to the copyOf to be validated, including that the copy to the
3752       // new array won't trigger an ArrayStoreException. That subtype
3753       // check can be optimized if we know something on the type of
3754       // the input array from type speculation.
3755       if (_gvn.type(klass_node)-&gt;singleton()) {
3756         ciKlass* subk   = _gvn.type(load_object_klass(original))-&gt;is_klassptr()-&gt;klass();
3757         ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3758 
3759         int test = C-&gt;static_subtype_check(superk, subk);
3760         if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3761           const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3762           if (t_original-&gt;speculative_type() != NULL) {
3763             original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3764           }
3765         }
3766       }
3767 
3768       bool validated = false;
3769       // Reason_class_check rather than Reason_intrinsic because we
3770       // want to intrinsify even if this traps.
3771       if (!too_many_traps(Deoptimization::Reason_class_check)) {
3772         Node* not_subtype_ctrl = gen_subtype_check(load_object_klass(original),
3773                                                    klass_node);
3774 
3775         if (not_subtype_ctrl != top()) {
3776           PreserveJVMState pjvms(this);
3777           set_control(not_subtype_ctrl);
3778           uncommon_trap(Deoptimization::Reason_class_check,
3779                         Deoptimization::Action_make_not_entrant);
3780           assert(stopped(), "Should be stopped");
3781         }
3782         validated = true;
3783       }
3784 
3785       if (!stopped()) {
3786         newcopy = new_array(klass_node, length, 0);  // no arguments to push
3787 
3788         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true,
3789                                                 load_object_klass(original), klass_node);
3790         if (!is_copyOfRange) {
3791           ac-&gt;set_copyof(validated);
3792         } else {
3793           ac-&gt;set_copyofrange(validated);
3794         }
3795         Node* n = _gvn.transform(ac);
3796         if (n == ac) {
3797           ac-&gt;connect_outputs(this);
3798         } else {
3799           assert(validated, "shouldn't transform if all arguments not validated");
3800           set_all_memory(n);
3801         }
3802       }
3803     }
3804   } // original reexecute is set back here
3805 
3806   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3807   if (!stopped()) {
3808     set_result(newcopy);
3809   }
3810   return true;
3811 }
3812 
3813 
3814 //----------------------generate_virtual_guard---------------------------
3815 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
3816 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
3817                                              RegionNode* slow_region) {
3818   ciMethod* method = callee();
3819   int vtable_index = method-&gt;vtable_index();
3820   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3821          err_msg_res("bad index %d", vtable_index));
3822   // Get the Method* out of the appropriate vtable entry.
3823   int entry_offset  = (InstanceKlass::vtable_start_offset() +
3824                      vtable_index*vtableEntry::size()) * wordSize +
3825                      vtableEntry::method_offset_in_bytes();
3826   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
3827   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3828 
3829   // Compare the target method with the expected method (e.g., Object.hashCode).
3830   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
3831 
3832   Node* native_call = makecon(native_call_addr);
3833   Node* chk_native  = _gvn.transform(new CmpPNode(target_call, native_call));
3834   Node* test_native = _gvn.transform(new BoolNode(chk_native, BoolTest::ne));
3835 
3836   return generate_slow_guard(test_native, slow_region);
3837 }
3838 
3839 //-----------------------generate_method_call----------------------------
3840 // Use generate_method_call to make a slow-call to the real
3841 // method if the fast path fails.  An alternative would be to
3842 // use a stub like OptoRuntime::slow_arraycopy_Java.
3843 // This only works for expanding the current library call,
3844 // not another intrinsic.  (E.g., don't use this for making an
3845 // arraycopy call inside of the copyOf intrinsic.)
3846 CallJavaNode*
3847 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
3848   // When compiling the intrinsic method itself, do not use this technique.
3849   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
3850 
3851   ciMethod* method = callee();
3852   // ensure the JVMS we have will be correct for this call
3853   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
3854 
3855   const TypeFunc* tf = TypeFunc::make(method);
3856   CallJavaNode* slow_call;
3857   if (is_static) {
3858     assert(!is_virtual, "");
3859     slow_call = new CallStaticJavaNode(C, tf,
3860                            SharedRuntime::get_resolve_static_call_stub(),
3861                            method, bci());
3862   } else if (is_virtual) {
3863     null_check_receiver();
3864     int vtable_index = Method::invalid_vtable_index;
3865     if (UseInlineCaches) {
3866       // Suppress the vtable call
3867     } else {
3868       // hashCode and clone are not a miranda methods,
3869       // so the vtable index is fixed.
3870       // No need to use the linkResolver to get it.
3871        vtable_index = method-&gt;vtable_index();
3872        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3873               err_msg_res("bad index %d", vtable_index));
3874     }
3875     slow_call = new CallDynamicJavaNode(tf,
3876                           SharedRuntime::get_resolve_virtual_call_stub(),
3877                           method, vtable_index, bci());
3878   } else {  // neither virtual nor static:  opt_virtual
3879     null_check_receiver();
3880     slow_call = new CallStaticJavaNode(C, tf,
3881                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
3882                                 method, bci());
3883     slow_call-&gt;set_optimized_virtual(true);
3884   }
3885   set_arguments_for_java_call(slow_call);
3886   set_edges_for_java_call(slow_call);
3887   return slow_call;
3888 }
3889 
3890 
3891 /**
3892  * Build special case code for calls to hashCode on an object. This call may
3893  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
3894  * slightly different code.
3895  */
3896 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
3897   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
3898   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
3899 
3900   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
3901 
3902   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3903   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
3904   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3905   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3906   Node* obj = NULL;
3907   if (!is_static) {
3908     // Check for hashing null object
3909     obj = null_check_receiver();
3910     if (stopped())  return true;        // unconditionally null
3911     result_reg-&gt;init_req(_null_path, top());
3912     result_val-&gt;init_req(_null_path, top());
3913   } else {
3914     // Do a null check, and return zero if null.
3915     // System.identityHashCode(null) == 0
3916     obj = argument(0);
3917     Node* null_ctl = top();
3918     obj = null_check_oop(obj, &amp;null_ctl);
3919     result_reg-&gt;init_req(_null_path, null_ctl);
3920     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
3921   }
3922 
3923   // Unconditionally null?  Then return right away.
3924   if (stopped()) {
3925     set_control( result_reg-&gt;in(_null_path));
3926     if (!stopped())
3927       set_result(result_val-&gt;in(_null_path));
3928     return true;
3929   }
3930 
3931   // We only go to the fast case code if we pass a number of guards.  The
3932   // paths which do not pass are accumulated in the slow_region.
3933   RegionNode* slow_region = new RegionNode(1);
3934   record_for_igvn(slow_region);
3935 
3936   // If this is a virtual call, we generate a funny guard.  We pull out
3937   // the vtable entry corresponding to hashCode() from the target object.
3938   // If the target method which we are calling happens to be the native
3939   // Object hashCode() method, we pass the guard.  We do not need this
3940   // guard for non-virtual calls -- the caller is known to be the native
3941   // Object hashCode().
3942   if (is_virtual) {
3943     // After null check, get the object's klass.
3944     Node* obj_klass = load_object_klass(obj);
3945     generate_virtual_guard(obj_klass, slow_region);
3946   }
3947 
3948   // Get the header out of the object, use LoadMarkNode when available
3949   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
3950   // The control of the load must be NULL. Otherwise, the load can move before
3951   // the null check after castPP removal.
3952   Node* no_ctrl = NULL;
3953   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
3954 
3955   // Test the header to see if it is unlocked.
3956   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
3957   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
3958   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
3959   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
3960   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
3961 
3962   generate_slow_guard(test_unlocked, slow_region);
3963 
3964   // Get the hash value and check to see that it has been properly assigned.
3965   // We depend on hash_mask being at most 32 bits and avoid the use of
3966   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
3967   // vm: see markOop.hpp.
3968   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
3969   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
3970   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
3971   // This hack lets the hash bits live anywhere in the mark object now, as long
3972   // as the shift drops the relevant bits into the low 32 bits.  Note that
3973   // Java spec says that HashCode is an int so there's no point in capturing
3974   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
3975   hshifted_header      = ConvX2I(hshifted_header);
3976   Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));
3977 
3978   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
3979   Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));
3980   Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));
3981 
3982   generate_slow_guard(test_assigned, slow_region);
3983 
3984   Node* init_mem = reset_memory();
3985   // fill in the rest of the null path:
3986   result_io -&gt;init_req(_null_path, i_o());
3987   result_mem-&gt;init_req(_null_path, init_mem);
3988 
3989   result_val-&gt;init_req(_fast_path, hash_val);
3990   result_reg-&gt;init_req(_fast_path, control());
3991   result_io -&gt;init_req(_fast_path, i_o());
3992   result_mem-&gt;init_req(_fast_path, init_mem);
3993 
3994   // Generate code for the slow case.  We make a call to hashCode().
3995   set_control(_gvn.transform(slow_region));
3996   if (!stopped()) {
3997     // No need for PreserveJVMState, because we're using up the present state.
3998     set_all_memory(init_mem);
3999     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4000     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4001     Node* slow_result = set_results_for_java_call(slow_call);
4002     // this-&gt;control() comes from set_results_for_java_call
4003     result_reg-&gt;init_req(_slow_path, control());
4004     result_val-&gt;init_req(_slow_path, slow_result);
4005     result_io  -&gt;set_req(_slow_path, i_o());
4006     result_mem -&gt;set_req(_slow_path, reset_memory());
4007   }
4008 
4009   // Return the combined state.
4010   set_i_o(        _gvn.transform(result_io)  );
4011   set_all_memory( _gvn.transform(result_mem));
4012 
4013   set_result(result_reg, result_val);
4014   return true;
4015 }
4016 
4017 //---------------------------inline_native_getClass----------------------------
4018 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4019 //
4020 // Build special case code for calls to getClass on an object.
4021 bool LibraryCallKit::inline_native_getClass() {
4022   Node* obj = null_check_receiver();
4023   if (stopped())  return true;
4024   set_result(load_mirror_from_klass(load_object_klass(obj)));
4025   return true;
4026 }
4027 
4028 //-----------------inline_native_Reflection_getCallerClass---------------------
4029 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4030 //
4031 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4032 //
4033 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4034 // in that it must skip particular security frames and checks for
4035 // caller sensitive methods.
4036 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4037 #ifndef PRODUCT
4038   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4039     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4040   }
4041 #endif
4042 
4043   if (!jvms()-&gt;has_method()) {
4044 #ifndef PRODUCT
4045     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4046       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4047     }
4048 #endif
4049     return false;
4050   }
4051 
4052   // Walk back up the JVM state to find the caller at the required
4053   // depth.
4054   JVMState* caller_jvms = jvms();
4055 
4056   // Cf. JVM_GetCallerClass
4057   // NOTE: Start the loop at depth 1 because the current JVM state does
4058   // not include the Reflection.getCallerClass() frame.
4059   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4060     ciMethod* m = caller_jvms-&gt;method();
4061     switch (n) {
4062     case 0:
4063       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4064       break;
4065     case 1:
4066       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4067       if (!m-&gt;caller_sensitive()) {
4068 #ifndef PRODUCT
4069         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4070           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4071         }
4072 #endif
4073         return false;  // bail-out; let JVM_GetCallerClass do the work
4074       }
4075       break;
4076     default:
4077       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4078         // We have reached the desired frame; return the holder class.
4079         // Acquire method holder as java.lang.Class and push as constant.
4080         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4081         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4082         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4083 
4084 #ifndef PRODUCT
4085         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4086           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4087           tty-&gt;print_cr("  JVM state at this point:");
4088           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4089             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4090             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4091           }
4092         }
4093 #endif
4094         return true;
4095       }
4096       break;
4097     }
4098   }
4099 
4100 #ifndef PRODUCT
4101   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4102     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4103     tty-&gt;print_cr("  JVM state at this point:");
4104     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4105       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4106       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4107     }
4108   }
4109 #endif
4110 
4111   return false;  // bail-out; let JVM_GetCallerClass do the work
4112 }
4113 
4114 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4115   Node* arg = argument(0);
4116   Node* result;
4117 
4118   switch (id) {
4119   case vmIntrinsics::_floatToRawIntBits:    result = new MoveF2INode(arg);  break;
4120   case vmIntrinsics::_intBitsToFloat:       result = new MoveI2FNode(arg);  break;
4121   case vmIntrinsics::_doubleToRawLongBits:  result = new MoveD2LNode(arg);  break;
4122   case vmIntrinsics::_longBitsToDouble:     result = new MoveL2DNode(arg);  break;
4123 
4124   case vmIntrinsics::_doubleToLongBits: {
4125     // two paths (plus control) merge in a wood
4126     RegionNode *r = new RegionNode(3);
4127     Node *phi = new PhiNode(r, TypeLong::LONG);
4128 
4129     Node *cmpisnan = _gvn.transform(new CmpDNode(arg, arg));
4130     // Build the boolean node
4131     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4132 
4133     // Branch either way.
4134     // NaN case is less traveled, which makes all the difference.
4135     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4136     Node *opt_isnan = _gvn.transform(ifisnan);
4137     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4138     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4139     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4140 
4141     set_control(iftrue);
4142 
4143     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4144     Node *slow_result = longcon(nan_bits); // return NaN
4145     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4146     r-&gt;init_req(1, iftrue);
4147 
4148     // Else fall through
4149     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4150     set_control(iffalse);
4151 
4152     phi-&gt;init_req(2, _gvn.transform(new MoveD2LNode(arg)));
4153     r-&gt;init_req(2, iffalse);
4154 
4155     // Post merge
4156     set_control(_gvn.transform(r));
4157     record_for_igvn(r);
4158 
4159     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4160     result = phi;
4161     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4162     break;
4163   }
4164 
4165   case vmIntrinsics::_floatToIntBits: {
4166     // two paths (plus control) merge in a wood
4167     RegionNode *r = new RegionNode(3);
4168     Node *phi = new PhiNode(r, TypeInt::INT);
4169 
4170     Node *cmpisnan = _gvn.transform(new CmpFNode(arg, arg));
4171     // Build the boolean node
4172     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4173 
4174     // Branch either way.
4175     // NaN case is less traveled, which makes all the difference.
4176     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4177     Node *opt_isnan = _gvn.transform(ifisnan);
4178     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4179     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4180     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4181 
4182     set_control(iftrue);
4183 
4184     static const jint nan_bits = 0x7fc00000;
4185     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4186     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4187     r-&gt;init_req(1, iftrue);
4188 
4189     // Else fall through
4190     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4191     set_control(iffalse);
4192 
4193     phi-&gt;init_req(2, _gvn.transform(new MoveF2INode(arg)));
4194     r-&gt;init_req(2, iffalse);
4195 
4196     // Post merge
4197     set_control(_gvn.transform(r));
4198     record_for_igvn(r);
4199 
4200     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4201     result = phi;
4202     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4203     break;
4204   }
4205 
4206   default:
4207     fatal_unexpected_iid(id);
4208     break;
4209   }
4210   set_result(_gvn.transform(result));
4211   return true;
4212 }
4213 
4214 #ifdef _LP64
4215 #define XTOP ,top() /*additional argument*/
4216 #else  //_LP64
4217 #define XTOP        /*no additional argument*/
4218 #endif //_LP64
4219 
4220 //----------------------inline_unsafe_copyMemory-------------------------
4221 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4222 bool LibraryCallKit::inline_unsafe_copyMemory() {
4223   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4224   null_check_receiver();  // null-check receiver
4225   if (stopped())  return true;
4226 
4227   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4228 
4229   Node* src_ptr =         argument(1);   // type: oop
4230   Node* src_off = ConvL2X(argument(2));  // type: long
4231   Node* dst_ptr =         argument(4);   // type: oop
4232   Node* dst_off = ConvL2X(argument(5));  // type: long
4233   Node* size    = ConvL2X(argument(7));  // type: long
4234 
4235   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4236          "fieldOffset must be byte-scaled");
4237 
4238   Node* src = make_unsafe_address(src_ptr, src_off);
4239   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4240 
4241   // Conservatively insert a memory barrier on all memory slices.
4242   // Do not let writes of the copy source or destination float below the copy.
4243   insert_mem_bar(Op_MemBarCPUOrder);
4244 
4245   // Call it.  Note that the length argument is not scaled.
4246   make_runtime_call(RC_LEAF|RC_NO_FP,
4247                     OptoRuntime::fast_arraycopy_Type(),
4248                     StubRoutines::unsafe_arraycopy(),
4249                     "unsafe_arraycopy",
4250                     TypeRawPtr::BOTTOM,
4251                     src, dst, size XTOP);
4252 
4253   // Do not let reads of the copy destination float above the copy.
4254   insert_mem_bar(Op_MemBarCPUOrder);
4255 
4256   return true;
4257 }
4258 
4259 //------------------------clone_coping-----------------------------------
4260 // Helper function for inline_native_clone.
4261 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4262   assert(obj_size != NULL, "");
4263   Node* raw_obj = alloc_obj-&gt;in(1);
4264   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4265 
4266   AllocateNode* alloc = NULL;
4267   if (ReduceBulkZeroing) {
4268     // We will be completely responsible for initializing this object -
4269     // mark Initialize node as complete.
4270     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4271     // The object was just allocated - there should be no any stores!
4272     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4273     // Mark as complete_with_arraycopy so that on AllocateNode
4274     // expansion, we know this AllocateNode is initialized by an array
4275     // copy and a StoreStore barrier exists after the array copy.
4276     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4277   }
4278 
4279   // Copy the fastest available way.
4280   // TODO: generate fields copies for small objects instead.
4281   Node* src  = obj;
4282   Node* dest = alloc_obj;
4283   Node* size = _gvn.transform(obj_size);
4284 
4285   // Exclude the header but include array length to copy by 8 bytes words.
4286   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4287   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4288                             instanceOopDesc::base_offset_in_bytes();
4289   // base_off:
4290   // 8  - 32-bit VM
4291   // 12 - 64-bit VM, compressed klass
4292   // 16 - 64-bit VM, normal klass
4293   if (base_off % BytesPerLong != 0) {
4294     assert(UseCompressedClassPointers, "");
4295     if (is_array) {
4296       // Exclude length to copy by 8 bytes words.
4297       base_off += sizeof(int);
4298     } else {
4299       // Include klass to copy by 8 bytes words.
4300       base_off = instanceOopDesc::klass_offset_in_bytes();
4301     }
4302     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4303   }
4304   src  = basic_plus_adr(src,  base_off);
4305   dest = basic_plus_adr(dest, base_off);
4306 
4307   // Compute the length also, if needed:
4308   Node* countx = size;
4309   countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
4310   countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong) ));
4311 
4312   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4313 
4314   ArrayCopyNode* ac = ArrayCopyNode::make(this, false, src, NULL, dest, NULL, countx, false);
4315   ac-&gt;set_clonebasic();
4316   Node* n = _gvn.transform(ac);
4317   if (n == ac) {
4318     set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
4319   } else {
4320     set_all_memory(n);
4321   }
4322 
4323   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4324   if (card_mark) {
4325     assert(!is_array, "");
4326     // Put in store barrier for any and all oops we are sticking
4327     // into this object.  (We could avoid this if we could prove
4328     // that the object type contains no oop fields at all.)
4329     Node* no_particular_value = NULL;
4330     Node* no_particular_field = NULL;
4331     int raw_adr_idx = Compile::AliasIdxRaw;
4332     post_barrier(control(),
4333                  memory(raw_adr_type),
4334                  alloc_obj,
4335                  no_particular_field,
4336                  raw_adr_idx,
4337                  no_particular_value,
4338                  T_OBJECT,
4339                  false);
4340   }
4341 
4342   // Do not let reads from the cloned object float above the arraycopy.
4343   if (alloc != NULL) {
4344     // Do not let stores that initialize this object be reordered with
4345     // a subsequent store that would make this object accessible by
4346     // other threads.
4347     // Record what AllocateNode this StoreStore protects so that
4348     // escape analysis can go from the MemBarStoreStoreNode to the
4349     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4350     // based on the escape status of the AllocateNode.
4351     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4352   } else {
4353     insert_mem_bar(Op_MemBarCPUOrder);
4354   }
4355 }
4356 
4357 //------------------------inline_native_clone----------------------------
4358 // protected native Object java.lang.Object.clone();
4359 //
4360 // Here are the simple edge cases:
4361 //  null receiver =&gt; normal trap
4362 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4363 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4364 //
4365 // The general case has two steps, allocation and copying.
4366 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4367 //
4368 // Copying also has two cases, oop arrays and everything else.
4369 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4370 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4371 //
4372 // These steps fold up nicely if and when the cloned object's klass
4373 // can be sharply typed as an object array, a type array, or an instance.
4374 //
4375 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4376   PhiNode* result_val;
4377 
4378   // Set the reexecute bit for the interpreter to reexecute
4379   // the bytecode that invokes Object.clone if deoptimization happens.
4380   { PreserveReexecuteState preexecs(this);
4381     jvms()-&gt;set_should_reexecute(true);
4382 
4383     Node* obj = null_check_receiver();
4384     if (stopped())  return true;
4385 
4386     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4387 
4388     // If we are going to clone an instance, we need its exact type to
4389     // know the number and types of fields to convert the clone to
4390     // loads/stores. Maybe a speculative type can help us.
4391     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4392         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4393         obj_type-&gt;speculative_type()-&gt;is_instance_klass()) {
4394       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4395       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4396           !spec_ik-&gt;has_injected_fields()) {
4397         ciKlass* k = obj_type-&gt;klass();
4398         if (!k-&gt;is_instance_klass() ||
4399             k-&gt;as_instance_klass()-&gt;is_interface() ||
4400             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4401           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4402         }
4403       }
4404     }
4405 
4406     Node* obj_klass = load_object_klass(obj);
4407     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4408     const TypeOopPtr*   toop   = ((tklass != NULL)
4409                                 ? tklass-&gt;as_instance_type()
4410                                 : TypeInstPtr::NOTNULL);
4411 
4412     // Conservatively insert a memory barrier on all memory slices.
4413     // Do not let writes into the original float below the clone.
4414     insert_mem_bar(Op_MemBarCPUOrder);
4415 
4416     // paths into result_reg:
4417     enum {
4418       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4419       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4420       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4421       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4422       PATH_LIMIT
4423     };
4424     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4425     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4426     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4427     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4428     record_for_igvn(result_reg);
4429 
4430     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4431     int raw_adr_idx = Compile::AliasIdxRaw;
4432 
4433     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4434     if (array_ctl != NULL) {
4435       // It's an array.
4436       PreserveJVMState pjvms(this);
4437       set_control(array_ctl);
4438       Node* obj_length = load_array_length(obj);
4439       Node* obj_size  = NULL;
4440       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4441 
4442       if (!use_ReduceInitialCardMarks()) {
4443         // If it is an oop array, it requires very special treatment,
4444         // because card marking is required on each card of the array.
4445         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4446         if (is_obja != NULL) {
4447           PreserveJVMState pjvms2(this);
4448           set_control(is_obja);
4449           // Generate a direct call to the right arraycopy function(s).
4450           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4451           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL);
4452           ac-&gt;set_cloneoop();
4453           Node* n = _gvn.transform(ac);
4454           assert(n == ac, "cannot disappear");
4455           ac-&gt;connect_outputs(this);
4456 
4457           result_reg-&gt;init_req(_objArray_path, control());
4458           result_val-&gt;init_req(_objArray_path, alloc_obj);
4459           result_i_o -&gt;set_req(_objArray_path, i_o());
4460           result_mem -&gt;set_req(_objArray_path, reset_memory());
4461         }
4462       }
4463       // Otherwise, there are no card marks to worry about.
4464       // (We can dispense with card marks if we know the allocation
4465       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4466       //  causes the non-eden paths to take compensating steps to
4467       //  simulate a fresh allocation, so that no further
4468       //  card marks are required in compiled code to initialize
4469       //  the object.)
4470 
4471       if (!stopped()) {
4472         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4473 
4474         // Present the results of the copy.
4475         result_reg-&gt;init_req(_array_path, control());
4476         result_val-&gt;init_req(_array_path, alloc_obj);
4477         result_i_o -&gt;set_req(_array_path, i_o());
4478         result_mem -&gt;set_req(_array_path, reset_memory());
4479       }
4480     }
4481 
4482     // We only go to the instance fast case code if we pass a number of guards.
4483     // The paths which do not pass are accumulated in the slow_region.
4484     RegionNode* slow_region = new RegionNode(1);
4485     record_for_igvn(slow_region);
4486     if (!stopped()) {
4487       // It's an instance (we did array above).  Make the slow-path tests.
4488       // If this is a virtual call, we generate a funny guard.  We grab
4489       // the vtable entry corresponding to clone() from the target object.
4490       // If the target method which we are calling happens to be the
4491       // Object clone() method, we pass the guard.  We do not need this
4492       // guard for non-virtual calls; the caller is known to be the native
4493       // Object clone().
4494       if (is_virtual) {
4495         generate_virtual_guard(obj_klass, slow_region);
4496       }
4497 
4498       // The object must be cloneable and must not have a finalizer.
4499       // Both of these conditions may be checked in a single test.
4500       // We could optimize the cloneable test further, but we don't care.
4501       generate_access_flags_guard(obj_klass,
4502                                   // Test both conditions:
4503                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4504                                   // Must be cloneable but not finalizer:
4505                                   JVM_ACC_IS_CLONEABLE,
4506                                   slow_region);
4507     }
4508 
4509     if (!stopped()) {
4510       // It's an instance, and it passed the slow-path tests.
4511       PreserveJVMState pjvms(this);
4512       Node* obj_size  = NULL;
4513       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4514       // is reexecuted if deoptimization occurs and there could be problems when merging
4515       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4516       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4517 
4518       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4519 
4520       // Present the results of the slow call.
4521       result_reg-&gt;init_req(_instance_path, control());
4522       result_val-&gt;init_req(_instance_path, alloc_obj);
4523       result_i_o -&gt;set_req(_instance_path, i_o());
4524       result_mem -&gt;set_req(_instance_path, reset_memory());
4525     }
4526 
4527     // Generate code for the slow case.  We make a call to clone().
4528     set_control(_gvn.transform(slow_region));
4529     if (!stopped()) {
4530       PreserveJVMState pjvms(this);
4531       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4532       Node* slow_result = set_results_for_java_call(slow_call);
4533       // this-&gt;control() comes from set_results_for_java_call
4534       result_reg-&gt;init_req(_slow_path, control());
4535       result_val-&gt;init_req(_slow_path, slow_result);
4536       result_i_o -&gt;set_req(_slow_path, i_o());
4537       result_mem -&gt;set_req(_slow_path, reset_memory());
4538     }
4539 
4540     // Return the combined state.
4541     set_control(    _gvn.transform(result_reg));
4542     set_i_o(        _gvn.transform(result_i_o));
4543     set_all_memory( _gvn.transform(result_mem));
4544   } // original reexecute is set back here
4545 
4546   set_result(_gvn.transform(result_val));
4547   return true;
4548 }
4549 
4550 // If we have a tighly coupled allocation, the arraycopy may take care
4551 // of the array initialization. If one of the guards we insert between
4552 // the allocation and the arraycopy causes a deoptimization, an
4553 // unitialized array will escape the compiled method. To prevent that
4554 // we set the JVM state for uncommon traps between the allocation and
4555 // the arraycopy to the state before the allocation so, in case of
4556 // deoptimization, we'll reexecute the allocation and the
4557 // initialization.
4558 JVMState* LibraryCallKit::arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp) {
4559   if (alloc != NULL) {
4560     ciMethod* trap_method = alloc-&gt;jvms()-&gt;method();
4561     int trap_bci = alloc-&gt;jvms()-&gt;bci();
4562 
4563     if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;
4564           !C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
4565       // Make sure there's no store between the allocation and the
4566       // arraycopy otherwise visible side effects could be rexecuted
4567       // in case of deoptimization and cause incorrect execution.
4568       bool no_interfering_store = true;
4569       Node* mem = alloc-&gt;in(TypeFunc::Memory);
4570       if (mem-&gt;is_MergeMem()) {
4571         for (MergeMemStream mms(merged_memory(), mem-&gt;as_MergeMem()); mms.next_non_empty2(); ) {
4572           Node* n = mms.memory();
4573           if (n != mms.memory2() &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4574             assert(n-&gt;is_Store(), "what else?");
4575             no_interfering_store = false;
4576             break;
4577           }
4578         }
4579       } else {
4580         for (MergeMemStream mms(merged_memory()); mms.next_non_empty(); ) {
4581           Node* n = mms.memory();
4582           if (n != mem &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4583             assert(n-&gt;is_Store(), "what else?");
4584             no_interfering_store = false;
4585             break;
4586           }
4587         }
4588       }
4589 
4590       if (no_interfering_store) {
4591         JVMState* old_jvms = alloc-&gt;jvms()-&gt;clone_shallow(C);
4592         uint size = alloc-&gt;req();
4593         SafePointNode* sfpt = new SafePointNode(size, old_jvms);
4594         old_jvms-&gt;set_map(sfpt);
4595         for (uint i = 0; i &lt; size; i++) {
4596           sfpt-&gt;init_req(i, alloc-&gt;in(i));
4597         }
4598         // re-push array length for deoptimization
4599         sfpt-&gt;ins_req(old_jvms-&gt;stkoff() + old_jvms-&gt;sp(), alloc-&gt;in(AllocateNode::ALength));
4600         old_jvms-&gt;set_sp(old_jvms-&gt;sp()+1);
4601         old_jvms-&gt;set_monoff(old_jvms-&gt;monoff()+1);
4602         old_jvms-&gt;set_scloff(old_jvms-&gt;scloff()+1);
4603         old_jvms-&gt;set_endoff(old_jvms-&gt;endoff()+1);
4604         old_jvms-&gt;set_should_reexecute(true);
4605 
4606         sfpt-&gt;set_i_o(map()-&gt;i_o());
4607         sfpt-&gt;set_memory(map()-&gt;memory());
4608         sfpt-&gt;set_control(map()-&gt;control());
4609 
4610         JVMState* saved_jvms = jvms();
4611         saved_reexecute_sp = _reexecute_sp;
4612 
4613         set_jvms(sfpt-&gt;jvms());
4614         _reexecute_sp = jvms()-&gt;sp();
4615 
4616         return saved_jvms;
4617       }
4618     }
4619   }
4620   return NULL;
4621 }
4622 
4623 // In case of a deoptimization, we restart execution at the
4624 // allocation, allocating a new array. We would leave an uninitialized
4625 // array in the heap that GCs wouldn't expect. Move the allocation
4626 // after the traps so we don't allocate the array if we
4627 // deoptimize. This is possible because tightly_coupled_allocation()
4628 // guarantees there's no observer of the allocated array at this point
4629 // and the control flow is simple enough.
4630 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp) {
4631   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4632     assert(alloc != NULL, "only with a tightly coupled allocation");
4633     // restore JVM state to the state at the arraycopy
4634     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4635     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), "memory state changed?");
4636     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), "IO state changed?");
4637     // If we've improved the types of some nodes (null check) while
4638     // emitting the guards, propagate them to the current state
4639     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map());
4640     set_jvms(saved_jvms);
4641     _reexecute_sp = saved_reexecute_sp;
4642 
4643     // Remove the allocation from above the guards
4644     CallProjections callprojs;
4645     alloc-&gt;extract_projections(&amp;callprojs, true);
4646     InitializeNode* init = alloc-&gt;initialization();
4647     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
4648     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));
4649     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4650     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4651 
4652     // move the allocation here (after the guards)
4653     _gvn.hash_delete(alloc);
4654     alloc-&gt;set_req(TypeFunc::Control, control());
4655     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4656     Node *mem = reset_memory();
4657     set_all_memory(mem);
4658     alloc-&gt;set_req(TypeFunc::Memory, mem);
4659     set_control(init-&gt;proj_out(TypeFunc::Control));
4660     set_i_o(callprojs.fallthrough_ioproj);
4661 
4662     // Update memory as done in GraphKit::set_output_for_allocation()
4663     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4664     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4665     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4666       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4667     }
4668     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4669     int            elemidx  = C-&gt;get_alias_index(telemref);
4670     set_memory(init-&gt;proj_out(TypeFunc::Memory), Compile::AliasIdxRaw);
4671     set_memory(init-&gt;proj_out(TypeFunc::Memory), elemidx);
4672 
4673     Node* allocx = _gvn.transform(alloc);
4674     assert(allocx == alloc, "where has the allocation gone?");
4675     assert(dest-&gt;is_CheckCastPP(), "not an allocation result?");
4676 
4677     _gvn.hash_delete(dest);
4678     dest-&gt;set_req(0, control());
4679     Node* destx = _gvn.transform(dest);
4680     assert(destx == dest, "where has the allocation result gone?");
4681   }
4682 }
4683 
4684 
4685 //------------------------------inline_arraycopy-----------------------
4686 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4687 //                                                      Object dest, int destPos,
4688 //                                                      int length);
4689 bool LibraryCallKit::inline_arraycopy() {
4690   // Get the arguments.
4691   Node* src         = argument(0);  // type: oop
4692   Node* src_offset  = argument(1);  // type: int
4693   Node* dest        = argument(2);  // type: oop
4694   Node* dest_offset = argument(3);  // type: int
4695   Node* length      = argument(4);  // type: int
4696 
4697 
4698   // Check for allocation before we add nodes that would confuse
4699   // tightly_coupled_allocation()
4700   AllocateArrayNode* alloc = tightly_coupled_allocation(dest, NULL);
4701 
4702   int saved_reexecute_sp = -1;
4703   JVMState* saved_jvms = arraycopy_restore_alloc_state(alloc, saved_reexecute_sp);
4704   // See arraycopy_restore_alloc_state() comment
4705   // if alloc == NULL we don't have to worry about a tightly coupled allocation so we can emit all needed guards
4706   // if saved_jvms != NULL (then alloc != NULL) then we can handle guards and a tightly coupled allocation
4707   // if saved_jvms == NULL and alloc != NULL, we can’t emit any guards
4708   bool can_emit_guards = (alloc == NULL || saved_jvms != NULL);
4709 
4710   // The following tests must be performed
4711   // (1) src and dest are arrays.
4712   // (2) src and dest arrays must have elements of the same BasicType
4713   // (3) src and dest must not be null.
4714   // (4) src_offset must not be negative.
4715   // (5) dest_offset must not be negative.
4716   // (6) length must not be negative.
4717   // (7) src_offset + length must not exceed length of src.
4718   // (8) dest_offset + length must not exceed length of dest.
4719   // (9) each element of an oop array must be assignable
4720 
4721   // (3) src and dest must not be null.
4722   // always do this here because we need the JVM state for uncommon traps
4723   Node* null_ctl = top();
4724   src  = saved_jvms != NULL ? null_check_oop(src, &amp;null_ctl, true, true) : null_check(src,  T_ARRAY);
4725   assert(null_ctl-&gt;is_top(), "no null control here");
4726   dest = null_check(dest, T_ARRAY);
4727 
4728   if (!can_emit_guards) {
4729     // if saved_jvms == NULL and alloc != NULL, we don't emit any
4730     // guards but the arraycopy node could still take advantage of a
4731     // tightly allocated allocation. tightly_coupled_allocation() is
4732     // called again to make sure it takes the null check above into
4733     // account: the null check is mandatory and if it caused an
4734     // uncommon trap to be emitted then the allocation can't be
4735     // considered tightly coupled in this context.
4736     alloc = tightly_coupled_allocation(dest, NULL);
4737   }
4738 
4739   bool validated = false;
4740 
4741   const Type* src_type  = _gvn.type(src);
4742   const Type* dest_type = _gvn.type(dest);
4743   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4744   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4745 
4746   // Do we have the type of src?
4747   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4748   // Do we have the type of dest?
4749   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4750   // Is the type for src from speculation?
4751   bool src_spec = false;
4752   // Is the type for dest from speculation?
4753   bool dest_spec = false;
4754 
4755   if ((!has_src || !has_dest) &amp;&amp; can_emit_guards) {
4756     // We don't have sufficient type information, let's see if
4757     // speculative types can help. We need to have types for both src
4758     // and dest so that it pays off.
4759 
4760     // Do we already have or could we have type information for src
4761     bool could_have_src = has_src;
4762     // Do we already have or could we have type information for dest
4763     bool could_have_dest = has_dest;
4764 
4765     ciKlass* src_k = NULL;
4766     if (!has_src) {
4767       src_k = src_type-&gt;speculative_type_not_null();
4768       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4769         could_have_src = true;
4770       }
4771     }
4772 
4773     ciKlass* dest_k = NULL;
4774     if (!has_dest) {
4775       dest_k = dest_type-&gt;speculative_type_not_null();
4776       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4777         could_have_dest = true;
4778       }
4779     }
4780 
4781     if (could_have_src &amp;&amp; could_have_dest) {
4782       // This is going to pay off so emit the required guards
4783       if (!has_src) {
4784         src = maybe_cast_profiled_obj(src, src_k, true);
4785         src_type  = _gvn.type(src);
4786         top_src  = src_type-&gt;isa_aryptr();
4787         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4788         src_spec = true;
4789       }
4790       if (!has_dest) {
4791         dest = maybe_cast_profiled_obj(dest, dest_k, true);
4792         dest_type  = _gvn.type(dest);
4793         top_dest  = dest_type-&gt;isa_aryptr();
4794         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4795         dest_spec = true;
4796       }
4797     }
4798   }
4799 
4800   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
4801     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4802     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4803     if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
4804     if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
4805 
4806     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
4807       // If both arrays are object arrays then having the exact types
4808       // for both will remove the need for a subtype check at runtime
4809       // before the call and may make it possible to pick a faster copy
4810       // routine (without a subtype check on every element)
4811       // Do we have the exact type of src?
4812       bool could_have_src = src_spec;
4813       // Do we have the exact type of dest?
4814       bool could_have_dest = dest_spec;
4815       ciKlass* src_k = top_src-&gt;klass();
4816       ciKlass* dest_k = top_dest-&gt;klass();
4817       if (!src_spec) {
4818         src_k = src_type-&gt;speculative_type_not_null();
4819         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4820           could_have_src = true;
4821         }
4822       }
4823       if (!dest_spec) {
4824         dest_k = dest_type-&gt;speculative_type_not_null();
4825         if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4826           could_have_dest = true;
4827         }
4828       }
4829       if (could_have_src &amp;&amp; could_have_dest) {
4830         // If we can have both exact types, emit the missing guards
4831         if (could_have_src &amp;&amp; !src_spec) {
4832           src = maybe_cast_profiled_obj(src, src_k, true);
4833         }
4834         if (could_have_dest &amp;&amp; !dest_spec) {
4835           dest = maybe_cast_profiled_obj(dest, dest_k, true);
4836         }
4837       }
4838     }
4839   }
4840 
4841   ciMethod* trap_method = method();
4842   int trap_bci = bci();
4843   if (saved_jvms != NULL) {
4844     trap_method = alloc-&gt;jvms()-&gt;method();
4845     trap_bci = alloc-&gt;jvms()-&gt;bci();
4846   }
4847 
4848   if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;&amp;
4849       can_emit_guards &amp;&amp;
4850       !src-&gt;is_top() &amp;&amp; !dest-&gt;is_top()) {
4851     // validate arguments: enables transformation the ArrayCopyNode
4852     validated = true;
4853 
4854     RegionNode* slow_region = new RegionNode(1);
4855     record_for_igvn(slow_region);
4856 
4857     // (1) src and dest are arrays.
4858     generate_non_array_guard(load_object_klass(src), slow_region);
4859     generate_non_array_guard(load_object_klass(dest), slow_region);
4860 
4861     // (2) src and dest arrays must have elements of the same BasicType
4862     // done at macro expansion or at Ideal transformation time
4863 
4864     // (4) src_offset must not be negative.
4865     generate_negative_guard(src_offset, slow_region);
4866 
4867     // (5) dest_offset must not be negative.
4868     generate_negative_guard(dest_offset, slow_region);
4869 
4870     // (7) src_offset + length must not exceed length of src.
4871     generate_limit_guard(src_offset, length,
4872                          load_array_length(src),
4873                          slow_region);
4874 
4875     // (8) dest_offset + length must not exceed length of dest.
4876     generate_limit_guard(dest_offset, length,
4877                          load_array_length(dest),
4878                          slow_region);
4879 
4880     // (9) each element of an oop array must be assignable
4881     Node* src_klass  = load_object_klass(src);
4882     Node* dest_klass = load_object_klass(dest);
4883     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
4884 
4885     if (not_subtype_ctrl != top()) {
4886       PreserveJVMState pjvms(this);
4887       set_control(not_subtype_ctrl);
4888       uncommon_trap(Deoptimization::Reason_intrinsic,
4889                     Deoptimization::Action_make_not_entrant);
4890       assert(stopped(), "Should be stopped");
4891     }
4892     {
4893       PreserveJVMState pjvms(this);
4894       set_control(_gvn.transform(slow_region));
4895       uncommon_trap(Deoptimization::Reason_intrinsic,
4896                     Deoptimization::Action_make_not_entrant);
4897       assert(stopped(), "Should be stopped");
4898     }
4899   }
4900 
4901   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp);
4902 
4903   if (stopped()) {
4904     return true;
4905   }
4906 
4907   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL,
4908                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
4909                                           // so the compiler has a chance to eliminate them: during macro expansion,
4910                                           // we have to set their control (CastPP nodes are eliminated).
4911                                           load_object_klass(src), load_object_klass(dest),
4912                                           load_array_length(src), load_array_length(dest));
4913 
4914   ac-&gt;set_arraycopy(validated);
4915 
4916   Node* n = _gvn.transform(ac);
4917   if (n == ac) {
4918     ac-&gt;connect_outputs(this);
4919   } else {
4920     assert(validated, "shouldn't transform if all arguments not validated");
4921     set_all_memory(n);
4922   }
4923 
4924   return true;
4925 }
4926 
4927 
4928 // Helper function which determines if an arraycopy immediately follows
4929 // an allocation, with no intervening tests or other escapes for the object.
4930 AllocateArrayNode*
4931 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
4932                                            RegionNode* slow_region) {
4933   if (stopped())             return NULL;  // no fast path
4934   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
4935 
4936   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
4937   if (alloc == NULL)  return NULL;
4938 
4939   Node* rawmem = memory(Compile::AliasIdxRaw);
4940   // Is the allocation's memory state untouched?
4941   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
4942     // Bail out if there have been raw-memory effects since the allocation.
4943     // (Example:  There might have been a call or safepoint.)
4944     return NULL;
4945   }
4946   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
4947   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
4948     return NULL;
4949   }
4950 
4951   // There must be no unexpected observers of this allocation.
4952   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
4953     Node* obs = ptr-&gt;fast_out(i);
4954     if (obs != this-&gt;map()) {
4955       return NULL;
4956     }
4957   }
4958 
4959   // This arraycopy must unconditionally follow the allocation of the ptr.
4960   Node* alloc_ctl = ptr-&gt;in(0);
4961   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
4962 
4963   Node* ctl = control();
4964   while (ctl != alloc_ctl) {
4965     // There may be guards which feed into the slow_region.
4966     // Any other control flow means that we might not get a chance
4967     // to finish initializing the allocated object.
4968     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
4969       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
4970       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
4971       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
4972       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
4973         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
4974         continue;
4975       }
4976       // One more try:  Various low-level checks bottom out in
4977       // uncommon traps.  If the debug-info of the trap omits
4978       // any reference to the allocation, as we've already
4979       // observed, then there can be no objection to the trap.
4980       bool found_trap = false;
4981       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
4982         Node* obs = not_ctl-&gt;fast_out(j);
4983         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
4984             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
4985           found_trap = true; break;
4986         }
4987       }
4988       if (found_trap) {
4989         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
4990         continue;
4991       }
4992     }
4993     return NULL;
4994   }
4995 
4996   // If we get this far, we have an allocation which immediately
4997   // precedes the arraycopy, and we can take over zeroing the new object.
4998   // The arraycopy will finish the initialization, and provide
4999   // a new control state to which we will anchor the destination pointer.
5000 
5001   return alloc;
5002 }
5003 
5004 //-------------inline_encodeISOArray-----------------------------------
5005 // encode char[] to byte[] in ISO_8859_1
5006 bool LibraryCallKit::inline_encodeISOArray() {
5007   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5008   // no receiver since it is static method
5009   Node *src         = argument(0);
5010   Node *src_offset  = argument(1);
5011   Node *dst         = argument(2);
5012   Node *dst_offset  = argument(3);
5013   Node *length      = argument(4);
5014 
5015   const Type* src_type = src-&gt;Value(&amp;_gvn);
5016   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5017   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5018   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5019   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5020       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5021     // failed array check
5022     return false;
5023   }
5024 
5025   // Figure out the size and type of the elements we will be copying.
5026   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5027   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5028   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5029     return false;
5030   }
5031   Node* src_start = array_element_address(src, src_offset, src_elem);
5032   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5033   // 'src_start' points to src array + scaled offset
5034   // 'dst_start' points to dst array + scaled offset
5035 
5036   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5037   Node* enc = new EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5038   enc = _gvn.transform(enc);
5039   Node* res_mem = _gvn.transform(new SCMemProjNode(enc));
5040   set_memory(res_mem, mtype);
5041   set_result(enc);
5042   return true;
5043 }
5044 
5045 //-------------inline_multiplyToLen-----------------------------------
5046 bool LibraryCallKit::inline_multiplyToLen() {
5047   assert(UseMultiplyToLenIntrinsic, "not implemented on this platform");
5048 
5049   address stubAddr = StubRoutines::multiplyToLen();
5050   if (stubAddr == NULL) {
5051     return false; // Intrinsic's stub is not implemented on this platform
5052   }
5053   const char* stubName = "multiplyToLen";
5054 
5055   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5056 
5057   // no receiver because it is a static method
5058   Node* x    = argument(0);
5059   Node* xlen = argument(1);
5060   Node* y    = argument(2);
5061   Node* ylen = argument(3);
5062   Node* z    = argument(4);
5063 
5064   const Type* x_type = x-&gt;Value(&amp;_gvn);
5065   const Type* y_type = y-&gt;Value(&amp;_gvn);
5066   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5067   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5068   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5069       top_y == NULL || top_y-&gt;klass() == NULL) {
5070     // failed array check
5071     return false;
5072   }
5073 
5074   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5075   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5076   if (x_elem != T_INT || y_elem != T_INT) {
5077     return false;
5078   }
5079 
5080   // Set the original stack and the reexecute bit for the interpreter to reexecute
5081   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5082   // on the return from z array allocation in runtime.
5083   { PreserveReexecuteState preexecs(this);
5084     jvms()-&gt;set_should_reexecute(true);
5085 
5086     Node* x_start = array_element_address(x, intcon(0), x_elem);
5087     Node* y_start = array_element_address(y, intcon(0), y_elem);
5088     // 'x_start' points to x array + scaled xlen
5089     // 'y_start' points to y array + scaled ylen
5090 
5091     // Allocate the result array
5092     Node* zlen = _gvn.transform(new AddINode(xlen, ylen));
5093     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5094     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5095 
5096     IdealKit ideal(this);
5097 
5098 #define __ ideal.
5099      Node* one = __ ConI(1);
5100      Node* zero = __ ConI(0);
5101      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5102      __ set(need_alloc, zero);
5103      __ set(z_alloc, z);
5104      __ if_then(z, BoolTest::eq, null()); {
5105        __ increment (need_alloc, one);
5106      } __ else_(); {
5107        // Update graphKit memory and control from IdealKit.
5108        sync_kit(ideal);
5109        Node* zlen_arg = load_array_length(z);
5110        // Update IdealKit memory and control from graphKit.
5111        __ sync_kit(this);
5112        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5113          __ increment (need_alloc, one);
5114        } __ end_if();
5115      } __ end_if();
5116 
5117      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5118        // Update graphKit memory and control from IdealKit.
5119        sync_kit(ideal);
5120        Node * narr = new_array(klass_node, zlen, 1);
5121        // Update IdealKit memory and control from graphKit.
5122        __ sync_kit(this);
5123        __ set(z_alloc, narr);
5124      } __ end_if();
5125 
5126      sync_kit(ideal);
5127      z = __ value(z_alloc);
5128      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5129      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5130      // Final sync IdealKit and GraphKit.
5131      final_sync(ideal);
5132 #undef __
5133 
5134     Node* z_start = array_element_address(z, intcon(0), T_INT);
5135 
5136     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5137                                    OptoRuntime::multiplyToLen_Type(),
5138                                    stubAddr, stubName, TypePtr::BOTTOM,
5139                                    x_start, xlen, y_start, ylen, z_start, zlen);
5140   } // original reexecute is set back here
5141 
5142   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5143   set_result(z);
5144   return true;
5145 }
5146 
5147 //-------------inline_squareToLen------------------------------------
5148 bool LibraryCallKit::inline_squareToLen() {
5149   assert(UseSquareToLenIntrinsic, "not implementated on this platform");
5150 
5151   address stubAddr = StubRoutines::squareToLen();
5152   if (stubAddr == NULL) {
5153     return false; // Intrinsic's stub is not implemented on this platform
5154   }
5155   const char* stubName = "squareToLen";
5156 
5157   assert(callee()-&gt;signature()-&gt;size() == 4, "implSquareToLen has 4 parameters");
5158 
5159   Node* x    = argument(0);
5160   Node* len  = argument(1);
5161   Node* z    = argument(2);
5162   Node* zlen = argument(3);
5163 
5164   const Type* x_type = x-&gt;Value(&amp;_gvn);
5165   const Type* z_type = z-&gt;Value(&amp;_gvn);
5166   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5167   const TypeAryPtr* top_z = z_type-&gt;isa_aryptr();
5168   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5169       top_z  == NULL || top_z-&gt;klass()  == NULL) {
5170     // failed array check
5171     return false;
5172   }
5173 
5174   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5175   BasicType z_elem = z_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5176   if (x_elem != T_INT || z_elem != T_INT) {
5177     return false;
5178   }
5179 
5180 
5181   Node* x_start = array_element_address(x, intcon(0), x_elem);
5182   Node* z_start = array_element_address(z, intcon(0), z_elem);
5183 
5184   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5185                                   OptoRuntime::squareToLen_Type(),
5186                                   stubAddr, stubName, TypePtr::BOTTOM,
5187                                   x_start, len, z_start, zlen);
5188 
5189   set_result(z);
5190   return true;
5191 }
5192 
5193 //-------------inline_mulAdd------------------------------------------
5194 bool LibraryCallKit::inline_mulAdd() {
5195   assert(UseMulAddIntrinsic, "not implementated on this platform");
5196 
5197   address stubAddr = StubRoutines::mulAdd();
5198   if (stubAddr == NULL) {
5199     return false; // Intrinsic's stub is not implemented on this platform
5200   }
5201   const char* stubName = "mulAdd";
5202 
5203   assert(callee()-&gt;signature()-&gt;size() == 5, "mulAdd has 5 parameters");
5204 
5205   Node* out      = argument(0);
5206   Node* in       = argument(1);
5207   Node* offset   = argument(2);
5208   Node* len      = argument(3);
5209   Node* k        = argument(4);
5210 
5211   const Type* out_type = out-&gt;Value(&amp;_gvn);
5212   const Type* in_type = in-&gt;Value(&amp;_gvn);
5213   const TypeAryPtr* top_out = out_type-&gt;isa_aryptr();
5214   const TypeAryPtr* top_in = in_type-&gt;isa_aryptr();
5215   if (top_out  == NULL || top_out-&gt;klass()  == NULL ||
5216       top_in == NULL || top_in-&gt;klass() == NULL) {
5217     // failed array check
5218     return false;
5219   }
5220 
5221   BasicType out_elem = out_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5222   BasicType in_elem = in_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5223   if (out_elem != T_INT || in_elem != T_INT) {
5224     return false;
5225   }
5226 
5227   Node* outlen = load_array_length(out);
5228   Node* new_offset = _gvn.transform(new SubINode(outlen, offset));
5229   Node* out_start = array_element_address(out, intcon(0), out_elem);
5230   Node* in_start = array_element_address(in, intcon(0), in_elem);
5231 
5232   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5233                                   OptoRuntime::mulAdd_Type(),
5234                                   stubAddr, stubName, TypePtr::BOTTOM,
5235                                   out_start,in_start, new_offset, len, k);
5236   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5237   set_result(result);
5238   return true;
5239 }
5240 
5241 //-------------inline_montgomeryMultiply-----------------------------------
5242 bool LibraryCallKit::inline_montgomeryMultiply() {
5243   address stubAddr = StubRoutines::montgomeryMultiply();
5244   if (stubAddr == NULL) {
5245     return false; // Intrinsic's stub is not implemented on this platform
5246   }
5247 
5248   assert(UseMontgomeryMultiplyIntrinsic, "not implemented on this platform");
5249   const char* stubName = "montgomery_square";
5250 
5251   assert(callee()-&gt;signature()-&gt;size() == 7, "montgomeryMultiply has 7 parameters");
5252 
5253   Node* a    = argument(0);
5254   Node* b    = argument(1);
5255   Node* n    = argument(2);
5256   Node* len  = argument(3);
5257   Node* inv  = argument(4);
5258   Node* m    = argument(6);
5259 
5260   const Type* a_type = a-&gt;Value(&amp;_gvn);
5261   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5262   const Type* b_type = b-&gt;Value(&amp;_gvn);
5263   const TypeAryPtr* top_b = b_type-&gt;isa_aryptr();
5264   const Type* n_type = a-&gt;Value(&amp;_gvn);
5265   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5266   const Type* m_type = a-&gt;Value(&amp;_gvn);
5267   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5268   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5269       top_b == NULL || top_b-&gt;klass()  == NULL ||
5270       top_n == NULL || top_n-&gt;klass()  == NULL ||
5271       top_m == NULL || top_m-&gt;klass()  == NULL) {
5272     // failed array check
5273     return false;
5274   }
5275 
5276   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5277   BasicType b_elem = b_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5278   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5279   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5280   if (a_elem != T_INT || b_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5281     return false;
5282   }
5283 
5284   // Make the call
5285   {
5286     Node* a_start = array_element_address(a, intcon(0), a_elem);
5287     Node* b_start = array_element_address(b, intcon(0), b_elem);
5288     Node* n_start = array_element_address(n, intcon(0), n_elem);
5289     Node* m_start = array_element_address(m, intcon(0), m_elem);
5290 
5291     Node* call = make_runtime_call(RC_LEAF,
5292                                    OptoRuntime::montgomeryMultiply_Type(),
5293                                    stubAddr, stubName, TypePtr::BOTTOM,
5294                                    a_start, b_start, n_start, len, inv, top(),
5295                                    m_start);
5296     set_result(m);
5297   }
5298 
5299   return true;
5300 }
5301 
5302 bool LibraryCallKit::inline_montgomerySquare() {
5303   address stubAddr = StubRoutines::montgomerySquare();
5304   if (stubAddr == NULL) {
5305     return false; // Intrinsic's stub is not implemented on this platform
5306   }
5307 
5308   assert(UseMontgomerySquareIntrinsic, "not implemented on this platform");
5309   const char* stubName = "montgomery_square";
5310 
5311   assert(callee()-&gt;signature()-&gt;size() == 6, "montgomerySquare has 6 parameters");
5312 
5313   Node* a    = argument(0);
5314   Node* n    = argument(1);
5315   Node* len  = argument(2);
5316   Node* inv  = argument(3);
5317   Node* m    = argument(5);
5318 
5319   const Type* a_type = a-&gt;Value(&amp;_gvn);
5320   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5321   const Type* n_type = a-&gt;Value(&amp;_gvn);
5322   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5323   const Type* m_type = a-&gt;Value(&amp;_gvn);
5324   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5325   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5326       top_n == NULL || top_n-&gt;klass()  == NULL ||
5327       top_m == NULL || top_m-&gt;klass()  == NULL) {
5328     // failed array check
5329     return false;
5330   }
5331 
5332   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5333   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5334   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5335   if (a_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5336     return false;
5337   }
5338 
5339   // Make the call
5340   {
5341     Node* a_start = array_element_address(a, intcon(0), a_elem);
5342     Node* n_start = array_element_address(n, intcon(0), n_elem);
5343     Node* m_start = array_element_address(m, intcon(0), m_elem);
5344 
5345     Node* call = make_runtime_call(RC_LEAF,
5346                                    OptoRuntime::montgomerySquare_Type(),
5347                                    stubAddr, stubName, TypePtr::BOTTOM,
5348                                    a_start, n_start, len, inv, top(),
5349                                    m_start);
5350     set_result(m);
5351   }
5352 
5353   return true;
5354 }
5355 
5356 
5357 /**
5358  * Calculate CRC32 for byte.
5359  * int java.util.zip.CRC32.update(int crc, int b)
5360  */
5361 bool LibraryCallKit::inline_updateCRC32() {
5362   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5363   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5364   // no receiver since it is static method
5365   Node* crc  = argument(0); // type: int
5366   Node* b    = argument(1); // type: int
5367 
5368   /*
5369    *    int c = ~ crc;
5370    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5371    *    b = b ^ (c &gt;&gt;&gt; 8);
5372    *    crc = ~b;
5373    */
5374 
5375   Node* M1 = intcon(-1);
5376   crc = _gvn.transform(new XorINode(crc, M1));
5377   Node* result = _gvn.transform(new XorINode(crc, b));
5378   result = _gvn.transform(new AndINode(result, intcon(0xFF)));
5379 
5380   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5381   Node* offset = _gvn.transform(new LShiftINode(result, intcon(0x2)));
5382   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5383   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5384 
5385   crc = _gvn.transform(new URShiftINode(crc, intcon(8)));
5386   result = _gvn.transform(new XorINode(crc, result));
5387   result = _gvn.transform(new XorINode(result, M1));
5388   set_result(result);
5389   return true;
5390 }
5391 
5392 /**
5393  * Calculate CRC32 for byte[] array.
5394  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5395  */
5396 bool LibraryCallKit::inline_updateBytesCRC32() {
5397   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5398   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5399   // no receiver since it is static method
5400   Node* crc     = argument(0); // type: int
5401   Node* src     = argument(1); // type: oop
5402   Node* offset  = argument(2); // type: int
5403   Node* length  = argument(3); // type: int
5404 
5405   const Type* src_type = src-&gt;Value(&amp;_gvn);
5406   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5407   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5408     // failed array check
5409     return false;
5410   }
5411 
5412   // Figure out the size and type of the elements we will be copying.
5413   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5414   if (src_elem != T_BYTE) {
5415     return false;
5416   }
5417 
5418   // 'src_start' points to src array + scaled offset
5419   Node* src_start = array_element_address(src, offset, src_elem);
5420 
5421   // We assume that range check is done by caller.
5422   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5423 
5424   // Call the stub.
5425   address stubAddr = StubRoutines::updateBytesCRC32();
5426   const char *stubName = "updateBytesCRC32";
5427 
5428   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5429                                  stubAddr, stubName, TypePtr::BOTTOM,
5430                                  crc, src_start, length);
5431   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5432   set_result(result);
5433   return true;
5434 }
5435 
5436 /**
5437  * Calculate CRC32 for ByteBuffer.
5438  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5439  */
5440 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5441   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5442   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5443   // no receiver since it is static method
5444   Node* crc     = argument(0); // type: int
5445   Node* src     = argument(1); // type: long
5446   Node* offset  = argument(3); // type: int
5447   Node* length  = argument(4); // type: int
5448 
5449   src = ConvL2X(src);  // adjust Java long to machine word
5450   Node* base = _gvn.transform(new CastX2PNode(src));
5451   offset = ConvI2X(offset);
5452 
5453   // 'src_start' points to src array + scaled offset
5454   Node* src_start = basic_plus_adr(top(), base, offset);
5455 
5456   // Call the stub.
5457   address stubAddr = StubRoutines::updateBytesCRC32();
5458   const char *stubName = "updateBytesCRC32";
5459 
5460   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5461                                  stubAddr, stubName, TypePtr::BOTTOM,
5462                                  crc, src_start, length);
5463   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5464   set_result(result);
5465   return true;
5466 }
5467 
5468 //------------------------------get_table_from_crc32c_class-----------------------
5469 Node * LibraryCallKit::get_table_from_crc32c_class(ciInstanceKlass *crc32c_class) {
5470   Node* table = load_field_from_object(NULL, "byteTable", "[I", /*is_exact*/ false, /*is_static*/ true, crc32c_class);
5471   assert (table != NULL, "wrong version of java.util.zip.CRC32C");
5472 
5473   return table;
5474 }
5475 
5476 //------------------------------inline_updateBytesCRC32C-----------------------
5477 //
5478 // Calculate CRC32C for byte[] array.
5479 // int java.util.zip.CRC32C.updateBytes(int crc, byte[] buf, int off, int end)
5480 //
5481 bool LibraryCallKit::inline_updateBytesCRC32C() {
5482   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5483   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5484   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5485   // no receiver since it is a static method
5486   Node* crc     = argument(0); // type: int
5487   Node* src     = argument(1); // type: oop
5488   Node* offset  = argument(2); // type: int
5489   Node* end     = argument(3); // type: int
5490 
5491   Node* length = _gvn.transform(new SubINode(end, offset));
5492 
5493   const Type* src_type = src-&gt;Value(&amp;_gvn);
5494   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5495   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5496     // failed array check
5497     return false;
5498   }
5499 
5500   // Figure out the size and type of the elements we will be copying.
5501   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5502   if (src_elem != T_BYTE) {
5503     return false;
5504   }
5505 
5506   // 'src_start' points to src array + scaled offset
5507   Node* src_start = array_element_address(src, offset, src_elem);
5508 
5509   // static final int[] byteTable in class CRC32C
5510   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5511   Node* table_start = array_element_address(table, intcon(0), T_INT);
5512 
5513   // We assume that range check is done by caller.
5514   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5515 
5516   // Call the stub.
5517   address stubAddr = StubRoutines::updateBytesCRC32C();
5518   const char *stubName = "updateBytesCRC32C";
5519 
5520   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5521                                  stubAddr, stubName, TypePtr::BOTTOM,
5522                                  crc, src_start, length, table_start);
5523   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5524   set_result(result);
5525   return true;
5526 }
5527 
5528 //------------------------------inline_updateDirectByteBufferCRC32C-----------------------
5529 //
5530 // Calculate CRC32C for DirectByteBuffer.
5531 // int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)
5532 //
5533 bool LibraryCallKit::inline_updateDirectByteBufferCRC32C() {
5534   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5535   assert(callee()-&gt;signature()-&gt;size() == 5, "updateDirectByteBuffer has 4 parameters and one is long");
5536   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5537   // no receiver since it is a static method
5538   Node* crc     = argument(0); // type: int
5539   Node* src     = argument(1); // type: long
5540   Node* offset  = argument(3); // type: int
5541   Node* end     = argument(4); // type: int
5542 
5543   Node* length = _gvn.transform(new SubINode(end, offset));
5544 
5545   src = ConvL2X(src);  // adjust Java long to machine word
5546   Node* base = _gvn.transform(new CastX2PNode(src));
5547   offset = ConvI2X(offset);
5548 
5549   // 'src_start' points to src array + scaled offset
5550   Node* src_start = basic_plus_adr(top(), base, offset);
5551 
5552   // static final int[] byteTable in class CRC32C
5553   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5554   Node* table_start = array_element_address(table, intcon(0), T_INT);
5555 
5556   // Call the stub.
5557   address stubAddr = StubRoutines::updateBytesCRC32C();
5558   const char *stubName = "updateBytesCRC32C";
5559 
5560   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5561                                  stubAddr, stubName, TypePtr::BOTTOM,
5562                                  crc, src_start, length, table_start);
5563   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5564   set_result(result);
5565   return true;
5566 }
5567 
5568 //------------------------------inline_updateBytesAdler32----------------------
5569 //
5570 // Calculate Adler32 checksum for byte[] array.
5571 // int java.util.zip.Adler32.updateBytes(int crc, byte[] buf, int off, int len)
5572 //
5573 bool LibraryCallKit::inline_updateBytesAdler32() {
5574   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5575   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5576   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5577   // no receiver since it is static method
5578   Node* crc     = argument(0); // type: int
5579   Node* src     = argument(1); // type: oop
5580   Node* offset  = argument(2); // type: int
5581   Node* length  = argument(3); // type: int
5582 
5583   const Type* src_type = src-&gt;Value(&amp;_gvn);
5584   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5585   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5586     // failed array check
5587     return false;
5588   }
5589 
5590   // Figure out the size and type of the elements we will be copying.
5591   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5592   if (src_elem != T_BYTE) {
5593     return false;
5594   }
5595 
5596   // 'src_start' points to src array + scaled offset
5597   Node* src_start = array_element_address(src, offset, src_elem);
5598 
5599   // We assume that range check is done by caller.
5600   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5601 
5602   // Call the stub.
5603   address stubAddr = StubRoutines::updateBytesAdler32();
5604   const char *stubName = "updateBytesAdler32";
5605 
5606   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5607                                  stubAddr, stubName, TypePtr::BOTTOM,
5608                                  crc, src_start, length);
5609   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5610   set_result(result);
5611   return true;
5612 }
5613 
5614 //------------------------------inline_updateByteBufferAdler32---------------
5615 //
5616 // Calculate Adler32 checksum for DirectByteBuffer.
5617 // int java.util.zip.Adler32.updateByteBuffer(int crc, long buf, int off, int len)
5618 //
5619 bool LibraryCallKit::inline_updateByteBufferAdler32() {
5620   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5621   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5622   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5623   // no receiver since it is static method
5624   Node* crc     = argument(0); // type: int
5625   Node* src     = argument(1); // type: long
5626   Node* offset  = argument(3); // type: int
5627   Node* length  = argument(4); // type: int
5628 
5629   src = ConvL2X(src);  // adjust Java long to machine word
5630   Node* base = _gvn.transform(new CastX2PNode(src));
5631   offset = ConvI2X(offset);
5632 
5633   // 'src_start' points to src array + scaled offset
5634   Node* src_start = basic_plus_adr(top(), base, offset);
5635 
5636   // Call the stub.
5637   address stubAddr = StubRoutines::updateBytesAdler32();
5638   const char *stubName = "updateBytesAdler32";
5639 
5640   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5641                                  stubAddr, stubName, TypePtr::BOTTOM,
5642                                  crc, src_start, length);
5643 
5644   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5645   set_result(result);
5646   return true;
5647 }
5648 
5649 //----------------------------inline_reference_get----------------------------
5650 // public T java.lang.ref.Reference.get();
5651 bool LibraryCallKit::inline_reference_get() {
5652   const int referent_offset = java_lang_ref_Reference::referent_offset;
5653   guarantee(referent_offset &gt; 0, "should have already been set");
5654 
5655   // Get the argument:
5656   Node* reference_obj = null_check_receiver();
5657   if (stopped()) return true;
5658 
5659   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5660 
5661   ciInstanceKlass* klass = env()-&gt;Object_klass();
5662   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5663 
5664   Node* no_ctrl = NULL;
5665   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5666 
5667   // Use the pre-barrier to record the value in the referent field
5668   pre_barrier(false /* do_load */,
5669               control(),
5670               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5671               result /* pre_val */,
5672               T_OBJECT);
5673 
5674   // Add memory barrier to prevent commoning reads from this field
5675   // across safepoint since GC can change its value.
5676   insert_mem_bar(Op_MemBarCPUOrder);
5677 
5678   set_result(result);
5679   return true;
5680 }
5681 
5682 
5683 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5684                                               bool is_exact=true, bool is_static=false,
5685                                               ciInstanceKlass * fromKls=NULL) {
5686   if (fromKls == NULL) {
5687     const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5688     assert(tinst != NULL, "obj is null");
5689     assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5690     assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5691     fromKls = tinst-&gt;klass()-&gt;as_instance_klass();
5692   } else {
5693     assert(is_static, "only for static field access");
5694   }
5695   ciField* field = fromKls-&gt;get_field_by_name(ciSymbol::make(fieldName),
5696                                               ciSymbol::make(fieldTypeString),
5697                                               is_static);
5698 
5699   assert (field != NULL, "undefined field");
5700   if (field == NULL) return (Node *) NULL;
5701 
5702   if (is_static) {
5703     const TypeInstPtr* tip = TypeInstPtr::make(fromKls-&gt;java_mirror());
5704     fromObj = makecon(tip);
5705   }
5706 
5707   // Next code  copied from Parse::do_get_xxx():
5708 
5709   // Compute address and memory type.
5710   int offset  = field-&gt;offset_in_bytes();
5711   bool is_vol = field-&gt;is_volatile();
5712   ciType* field_klass = field-&gt;type();
5713   assert(field_klass-&gt;is_loaded(), "should be loaded");
5714   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5715   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5716   BasicType bt = field-&gt;layout_type();
5717 
5718   // Build the resultant type of the load
5719   const Type *type;
5720   if (bt == T_OBJECT) {
5721     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5722   } else {
5723     type = Type::get_const_basic_type(bt);
5724   }
5725 
5726   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
5727     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
5728   }
5729   // Build the load.
5730   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
5731   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, LoadNode::DependsOnlyOnTest, is_vol);
5732   // If reference is volatile, prevent following memory ops from
5733   // floating up past the volatile read.  Also prevents commoning
5734   // another volatile read.
5735   if (is_vol) {
5736     // Memory barrier includes bogus read of value to force load BEFORE membar
5737     insert_mem_bar(Op_MemBarAcquire, loadedField);
5738   }
5739   return loadedField;
5740 }
5741 
5742 
5743 //------------------------------inline_aescrypt_Block-----------------------
5744 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
5745   address stubAddr;
5746   const char *stubName;
5747   assert(UseAES, "need AES instruction support");
5748 
5749   switch(id) {
5750   case vmIntrinsics::_aescrypt_encryptBlock:
5751     stubAddr = StubRoutines::aescrypt_encryptBlock();
5752     stubName = "aescrypt_encryptBlock";
5753     break;
5754   case vmIntrinsics::_aescrypt_decryptBlock:
5755     stubAddr = StubRoutines::aescrypt_decryptBlock();
5756     stubName = "aescrypt_decryptBlock";
5757     break;
5758   }
5759   if (stubAddr == NULL) return false;
5760 
5761   Node* aescrypt_object = argument(0);
5762   Node* src             = argument(1);
5763   Node* src_offset      = argument(2);
5764   Node* dest            = argument(3);
5765   Node* dest_offset     = argument(4);
5766 
5767   // (1) src and dest are arrays.
5768   const Type* src_type = src-&gt;Value(&amp;_gvn);
5769   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5770   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5771   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5772   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5773 
5774   // for the quick and dirty code we will skip all the checks.
5775   // we are just trying to get the call to be generated.
5776   Node* src_start  = src;
5777   Node* dest_start = dest;
5778   if (src_offset != NULL || dest_offset != NULL) {
5779     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5780     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5781     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5782   }
5783 
5784   // now need to get the start of its expanded key array
5785   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5786   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5787   if (k_start == NULL) return false;
5788 
5789   if (Matcher::pass_original_key_for_aes()) {
5790     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5791     // compatibility issues between Java key expansion and SPARC crypto instructions
5792     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5793     if (original_k_start == NULL) return false;
5794 
5795     // Call the stub.
5796     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5797                       stubAddr, stubName, TypePtr::BOTTOM,
5798                       src_start, dest_start, k_start, original_k_start);
5799   } else {
5800     // Call the stub.
5801     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5802                       stubAddr, stubName, TypePtr::BOTTOM,
5803                       src_start, dest_start, k_start);
5804   }
5805 
5806   return true;
5807 }
5808 
5809 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
5810 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
5811   address stubAddr;
5812   const char *stubName;
5813 
5814   assert(UseAES, "need AES instruction support");
5815 
5816   switch(id) {
5817   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
5818     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
5819     stubName = "cipherBlockChaining_encryptAESCrypt";
5820     break;
5821   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
5822     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
5823     stubName = "cipherBlockChaining_decryptAESCrypt";
5824     break;
5825   }
5826   if (stubAddr == NULL) return false;
5827 
5828   Node* cipherBlockChaining_object = argument(0);
5829   Node* src                        = argument(1);
5830   Node* src_offset                 = argument(2);
5831   Node* len                        = argument(3);
5832   Node* dest                       = argument(4);
5833   Node* dest_offset                = argument(5);
5834 
5835   // (1) src and dest are arrays.
5836   const Type* src_type = src-&gt;Value(&amp;_gvn);
5837   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5838   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5839   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5840   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
5841           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5842 
5843   // checks are the responsibility of the caller
5844   Node* src_start  = src;
5845   Node* dest_start = dest;
5846   if (src_offset != NULL || dest_offset != NULL) {
5847     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5848     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5849     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5850   }
5851 
5852   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
5853   // (because of the predicated logic executed earlier).
5854   // so we cast it here safely.
5855   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5856 
5857   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5858   if (embeddedCipherObj == NULL) return false;
5859 
5860   // cast it to what we know it will be at runtime
5861   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
5862   assert(tinst != NULL, "CBC obj is null");
5863   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
5864   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5865   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
5866 
5867   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5868   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
5869   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
5870   Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
5871   aescrypt_object = _gvn.transform(aescrypt_object);
5872 
5873   // we need to get the start of the aescrypt_object's expanded key array
5874   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5875   if (k_start == NULL) return false;
5876 
5877   // similarly, get the start address of the r vector
5878   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
5879   if (objRvec == NULL) return false;
5880   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
5881 
5882   Node* cbcCrypt;
5883   if (Matcher::pass_original_key_for_aes()) {
5884     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5885     // compatibility issues between Java key expansion and SPARC crypto instructions
5886     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5887     if (original_k_start == NULL) return false;
5888 
5889     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
5890     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5891                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5892                                  stubAddr, stubName, TypePtr::BOTTOM,
5893                                  src_start, dest_start, k_start, r_start, len, original_k_start);
5894   } else {
5895     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
5896     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5897                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5898                                  stubAddr, stubName, TypePtr::BOTTOM,
5899                                  src_start, dest_start, k_start, r_start, len);
5900   }
5901 
5902   // return cipher length (int)
5903   Node* retvalue = _gvn.transform(new ProjNode(cbcCrypt, TypeFunc::Parms));
5904   set_result(retvalue);
5905   return true;
5906 }
5907 
5908 //------------------------------get_key_start_from_aescrypt_object-----------------------
5909 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
5910   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
5911   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5912   if (objAESCryptKey == NULL) return (Node *) NULL;
5913 
5914   // now have the array, need to get the start address of the K array
5915   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
5916   return k_start;
5917 }
5918 
5919 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
5920 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
5921   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
5922   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5923   if (objAESCryptKey == NULL) return (Node *) NULL;
5924 
5925   // now have the array, need to get the start address of the lastKey array
5926   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
5927   return original_k_start;
5928 }
5929 
5930 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
5931 // Return node representing slow path of predicate check.
5932 // the pseudo code we want to emulate with this predicate is:
5933 // for encryption:
5934 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
5935 // for decryption:
5936 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
5937 //    note cipher==plain is more conservative than the original java code but that's OK
5938 //
5939 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
5940   // The receiver was checked for NULL already.
5941   Node* objCBC = argument(0);
5942 
5943   // Load embeddedCipher field of CipherBlockChaining object.
5944   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5945 
5946   // get AESCrypt klass for instanceOf check
5947   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
5948   // will have same classloader as CipherBlockChaining object
5949   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
5950   assert(tinst != NULL, "CBCobj is null");
5951   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
5952 
5953   // we want to do an instanceof comparison against the AESCrypt class
5954   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5955   if (!klass_AESCrypt-&gt;is_loaded()) {
5956     // if AESCrypt is not even loaded, we never take the intrinsic fast path
5957     Node* ctrl = control();
5958     set_control(top()); // no regular fast path
5959     return ctrl;
5960   }
5961   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5962 
5963   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
5964   Node* cmp_instof  = _gvn.transform(new CmpINode(instof, intcon(1)));
5965   Node* bool_instof  = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
5966 
5967   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
5968 
5969   // for encryption, we are done
5970   if (!decrypting)
5971     return instof_false;  // even if it is NULL
5972 
5973   // for decryption, we need to add a further check to avoid
5974   // taking the intrinsic path when cipher and plain are the same
5975   // see the original java code for why.
5976   RegionNode* region = new RegionNode(3);
5977   region-&gt;init_req(1, instof_false);
5978   Node* src = argument(1);
5979   Node* dest = argument(4);
5980   Node* cmp_src_dest = _gvn.transform(new CmpPNode(src, dest));
5981   Node* bool_src_dest = _gvn.transform(new BoolNode(cmp_src_dest, BoolTest::eq));
5982   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
5983   region-&gt;init_req(2, src_dest_conjoint);
5984 
5985   record_for_igvn(region);
5986   return _gvn.transform(region);
5987 }
5988 
5989 //------------------------------inline_ghash_processBlocks
5990 bool LibraryCallKit::inline_ghash_processBlocks() {
5991   address stubAddr;
5992   const char *stubName;
5993   assert(UseGHASHIntrinsics, "need GHASH intrinsics support");
5994 
5995   stubAddr = StubRoutines::ghash_processBlocks();
5996   stubName = "ghash_processBlocks";
5997 
5998   Node* data           = argument(0);
5999   Node* offset         = argument(1);
6000   Node* len            = argument(2);
6001   Node* state          = argument(3);
6002   Node* subkeyH        = argument(4);
6003 
6004   Node* state_start  = array_element_address(state, intcon(0), T_LONG);
6005   assert(state_start, "state is NULL");
6006   Node* subkeyH_start  = array_element_address(subkeyH, intcon(0), T_LONG);
6007   assert(subkeyH_start, "subkeyH is NULL");
6008   Node* data_start  = array_element_address(data, offset, T_BYTE);
6009   assert(data_start, "data is NULL");
6010 
6011   Node* ghash = make_runtime_call(RC_LEAF|RC_NO_FP,
6012                                   OptoRuntime::ghash_processBlocks_Type(),
6013                                   stubAddr, stubName, TypePtr::BOTTOM,
6014                                   state_start, subkeyH_start, data_start, len);
6015   return true;
6016 }
6017 
6018 //------------------------------inline_sha_implCompress-----------------------
6019 //
6020 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6021 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6022 //
6023 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6024 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6025 //
6026 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6027 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6028 //
6029 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6030   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6031 
6032   Node* sha_obj = argument(0);
6033   Node* src     = argument(1); // type oop
6034   Node* ofs     = argument(2); // type int
6035 
6036   const Type* src_type = src-&gt;Value(&amp;_gvn);
6037   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6038   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6039     // failed array check
6040     return false;
6041   }
6042   // Figure out the size and type of the elements we will be copying.
6043   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6044   if (src_elem != T_BYTE) {
6045     return false;
6046   }
6047   // 'src_start' points to src array + offset
6048   Node* src_start = array_element_address(src, ofs, src_elem);
6049   Node* state = NULL;
6050   address stubAddr;
6051   const char *stubName;
6052 
6053   switch(id) {
6054   case vmIntrinsics::_sha_implCompress:
6055     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6056     state = get_state_from_sha_object(sha_obj);
6057     stubAddr = StubRoutines::sha1_implCompress();
6058     stubName = "sha1_implCompress";
6059     break;
6060   case vmIntrinsics::_sha2_implCompress:
6061     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6062     state = get_state_from_sha_object(sha_obj);
6063     stubAddr = StubRoutines::sha256_implCompress();
6064     stubName = "sha256_implCompress";
6065     break;
6066   case vmIntrinsics::_sha5_implCompress:
6067     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6068     state = get_state_from_sha5_object(sha_obj);
6069     stubAddr = StubRoutines::sha512_implCompress();
6070     stubName = "sha512_implCompress";
6071     break;
6072   default:
6073     fatal_unexpected_iid(id);
6074     return false;
6075   }
6076   if (state == NULL) return false;
6077 
6078   // Call the stub.
6079   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6080                                  stubAddr, stubName, TypePtr::BOTTOM,
6081                                  src_start, state);
6082 
6083   return true;
6084 }
6085 
6086 //------------------------------inline_digestBase_implCompressMB-----------------------
6087 //
6088 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6089 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6090 //
6091 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6092   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6093          "need SHA1/SHA256/SHA512 instruction support");
6094   assert((uint)predicate &lt; 3, "sanity");
6095   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6096 
6097   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6098   Node* src            = argument(1); // byte[] array
6099   Node* ofs            = argument(2); // type int
6100   Node* limit          = argument(3); // type int
6101 
6102   const Type* src_type = src-&gt;Value(&amp;_gvn);
6103   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6104   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6105     // failed array check
6106     return false;
6107   }
6108   // Figure out the size and type of the elements we will be copying.
6109   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6110   if (src_elem != T_BYTE) {
6111     return false;
6112   }
6113   // 'src_start' points to src array + offset
6114   Node* src_start = array_element_address(src, ofs, src_elem);
6115 
6116   const char* klass_SHA_name = NULL;
6117   const char* stub_name = NULL;
6118   address     stub_addr = NULL;
6119   bool        long_state = false;
6120 
6121   switch (predicate) {
6122   case 0:
6123     if (UseSHA1Intrinsics) {
6124       klass_SHA_name = "sun/security/provider/SHA";
6125       stub_name = "sha1_implCompressMB";
6126       stub_addr = StubRoutines::sha1_implCompressMB();
6127     }
6128     break;
6129   case 1:
6130     if (UseSHA256Intrinsics) {
6131       klass_SHA_name = "sun/security/provider/SHA2";
6132       stub_name = "sha256_implCompressMB";
6133       stub_addr = StubRoutines::sha256_implCompressMB();
6134     }
6135     break;
6136   case 2:
6137     if (UseSHA512Intrinsics) {
6138       klass_SHA_name = "sun/security/provider/SHA5";
6139       stub_name = "sha512_implCompressMB";
6140       stub_addr = StubRoutines::sha512_implCompressMB();
6141       long_state = true;
6142     }
6143     break;
6144   default:
6145     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6146   }
6147   if (klass_SHA_name != NULL) {
6148     // get DigestBase klass to lookup for SHA klass
6149     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6150     assert(tinst != NULL, "digestBase_obj is not instance???");
6151     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6152 
6153     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6154     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6155     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6156     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6157   }
6158   return false;
6159 }
6160 //------------------------------inline_sha_implCompressMB-----------------------
6161 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6162                                                bool long_state, address stubAddr, const char *stubName,
6163                                                Node* src_start, Node* ofs, Node* limit) {
6164   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6165   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6166   Node* sha_obj = new CheckCastPPNode(control(), digestBase_obj, xtype);
6167   sha_obj = _gvn.transform(sha_obj);
6168 
6169   Node* state;
6170   if (long_state) {
6171     state = get_state_from_sha5_object(sha_obj);
6172   } else {
6173     state = get_state_from_sha_object(sha_obj);
6174   }
6175   if (state == NULL) return false;
6176 
6177   // Call the stub.
6178   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6179                                  OptoRuntime::digestBase_implCompressMB_Type(),
6180                                  stubAddr, stubName, TypePtr::BOTTOM,
6181                                  src_start, state, ofs, limit);
6182   // return ofs (int)
6183   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
6184   set_result(result);
6185 
6186   return true;
6187 }
6188 
6189 //------------------------------get_state_from_sha_object-----------------------
6190 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6191   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6192   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6193   if (sha_state == NULL) return (Node *) NULL;
6194 
6195   // now have the array, need to get the start address of the state array
6196   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6197   return state;
6198 }
6199 
6200 //------------------------------get_state_from_sha5_object-----------------------
6201 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6202   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6203   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6204   if (sha_state == NULL) return (Node *) NULL;
6205 
6206   // now have the array, need to get the start address of the state array
6207   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6208   return state;
6209 }
6210 
6211 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6212 // Return node representing slow path of predicate check.
6213 // the pseudo code we want to emulate with this predicate is:
6214 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6215 //
6216 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6217   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6218          "need SHA1/SHA256/SHA512 instruction support");
6219   assert((uint)predicate &lt; 3, "sanity");
6220 
6221   // The receiver was checked for NULL already.
6222   Node* digestBaseObj = argument(0);
6223 
6224   // get DigestBase klass for instanceOf check
6225   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6226   assert(tinst != NULL, "digestBaseObj is null");
6227   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6228 
6229   const char* klass_SHA_name = NULL;
6230   switch (predicate) {
6231   case 0:
6232     if (UseSHA1Intrinsics) {
6233       // we want to do an instanceof comparison against the SHA class
6234       klass_SHA_name = "sun/security/provider/SHA";
6235     }
6236     break;
6237   case 1:
6238     if (UseSHA256Intrinsics) {
6239       // we want to do an instanceof comparison against the SHA2 class
6240       klass_SHA_name = "sun/security/provider/SHA2";
6241     }
6242     break;
6243   case 2:
6244     if (UseSHA512Intrinsics) {
6245       // we want to do an instanceof comparison against the SHA5 class
6246       klass_SHA_name = "sun/security/provider/SHA5";
6247     }
6248     break;
6249   default:
6250     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6251   }
6252 
6253   ciKlass* klass_SHA = NULL;
6254   if (klass_SHA_name != NULL) {
6255     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6256   }
6257   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6258     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6259     Node* ctrl = control();
6260     set_control(top()); // no intrinsic path
6261     return ctrl;
6262   }
6263   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6264 
6265   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6266   Node* cmp_instof = _gvn.transform(new CmpINode(instofSHA, intcon(1)));
6267   Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6268   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6269 
6270   return instof_false;  // even if it is NULL
6271 }
6272 
6273 bool LibraryCallKit::inline_profileBoolean() {
6274   Node* counts = argument(1);
6275   const TypeAryPtr* ary = NULL;
6276   ciArray* aobj = NULL;
6277   if (counts-&gt;is_Con()
6278       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL
6279       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL
6280       &amp;&amp; (aobj-&gt;length() == 2)) {
6281     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
6282     jint false_cnt = aobj-&gt;element_value(0).as_int();
6283     jint  true_cnt = aobj-&gt;element_value(1).as_int();
6284 
6285     if (C-&gt;log() != NULL) {
6286       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",
6287                      false_cnt, true_cnt);
6288     }
6289 
6290     if (false_cnt + true_cnt == 0) {
6291       // According to profile, never executed.
6292       uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6293                           Deoptimization::Action_reinterpret);
6294       return true;
6295     }
6296 
6297     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)
6298     // is a number of each value occurrences.
6299     Node* result = argument(0);
6300     if (false_cnt == 0 || true_cnt == 0) {
6301       // According to profile, one value has been never seen.
6302       int expected_val = (false_cnt == 0) ? 1 : 0;
6303 
6304       Node* cmp  = _gvn.transform(new CmpINode(result, intcon(expected_val)));
6305       Node* test = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
6306 
6307       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
6308       Node* fast_path = _gvn.transform(new IfTrueNode(check));
6309       Node* slow_path = _gvn.transform(new IfFalseNode(check));
6310 
6311       { // Slow path: uncommon trap for never seen value and then reexecute
6312         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
6313         // the value has been seen at least once.
6314         PreserveJVMState pjvms(this);
6315         PreserveReexecuteState preexecs(this);
6316         jvms()-&gt;set_should_reexecute(true);
6317 
6318         set_control(slow_path);
6319         set_i_o(i_o());
6320 
6321         uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6322                             Deoptimization::Action_reinterpret);
6323       }
6324       // The guard for never seen value enables sharpening of the result and
6325       // returning a constant. It allows to eliminate branches on the same value
6326       // later on.
6327       set_control(fast_path);
6328       result = intcon(expected_val);
6329     }
6330     // Stop profiling.
6331     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
6332     // By replacing method body with profile data (represented as ProfileBooleanNode
6333     // on IR level) we effectively disable profiling.
6334     // It enables full speed execution once optimized code is generated.
6335     Node* profile = _gvn.transform(new ProfileBooleanNode(result, false_cnt, true_cnt));
6336     C-&gt;record_for_igvn(profile);
6337     set_result(profile);
6338     return true;
6339   } else {
6340     // Continue profiling.
6341     // Profile data isn't available at the moment. So, execute method's bytecode version.
6342     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
6343     // is compiled and counters aren't available since corresponding MethodHandle
6344     // isn't a compile-time constant.
6345     return false;
6346   }
6347 }
6348 
6349 bool LibraryCallKit::inline_isCompileConstant() {
6350   Node* n = argument(0);
6351   set_result(n-&gt;is_Con() ? intcon(1) : intcon(0));
6352   return true;
6353 }
</pre></body></html>
