<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/library_call.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "classfile/vmSymbols.hpp"
  29 #include "compiler/compileBroker.hpp"
  30 #include "compiler/compileLog.hpp"
  31 #include "oops/objArrayKlass.hpp"
  32 #include "opto/addnode.hpp"
  33 #include "opto/arraycopynode.hpp"
  34 #include "opto/callGenerator.hpp"
  35 #include "opto/castnode.hpp"
  36 #include "opto/cfgnode.hpp"
  37 #include "opto/convertnode.hpp"
  38 #include "opto/countbitsnode.hpp"
  39 #include "opto/intrinsicnode.hpp"
  40 #include "opto/idealKit.hpp"
  41 #include "opto/mathexactnode.hpp"
  42 #include "opto/movenode.hpp"
  43 #include "opto/mulnode.hpp"
  44 #include "opto/narrowptrnode.hpp"
  45 #include "opto/opaquenode.hpp"
  46 #include "opto/parse.hpp"
  47 #include "opto/runtime.hpp"
  48 #include "opto/subnode.hpp"
  49 #include "prims/nativeLookup.hpp"
  50 #include "runtime/sharedRuntime.hpp"
  51 #include "trace/traceMacros.hpp"
  52 
  53 class LibraryIntrinsic : public InlineCallGenerator {
  54   // Extend the set of intrinsics known to the runtime:
  55  public:
  56  private:
  57   bool             _is_virtual;
  58   bool             _does_virtual_dispatch;
  59   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  60   int8_t           _last_predicate; // Last generated predicate
  61   vmIntrinsics::ID _intrinsic_id;
  62 
  63  public:
  64   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  65     : InlineCallGenerator(m),
  66       _is_virtual(is_virtual),
  67       _does_virtual_dispatch(does_virtual_dispatch),
  68       _predicates_count((int8_t)predicates_count),
  69       _last_predicate((int8_t)-1),
  70       _intrinsic_id(id)
  71   {
  72   }
  73   virtual bool is_intrinsic() const { return true; }
  74   virtual bool is_virtual()   const { return _is_virtual; }
  75   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  76   virtual int  predicates_count() const { return _predicates_count; }
  77   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  78   virtual JVMState* generate(JVMState* jvms);
  79   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  80   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  81 };
  82 
  83 
  84 // Local helper class for LibraryIntrinsic:
  85 class LibraryCallKit : public GraphKit {
  86  private:
  87   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  88   Node*             _result;        // the result node, if any
  89   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  90 
  91   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  92 
  93  public:
  94   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  95     : GraphKit(jvms),
  96       _intrinsic(intrinsic),
  97       _result(NULL)
  98   {
  99     // Check if this is a root compile.  In that case we don't have a caller.
 100     if (!jvms-&gt;has_method()) {
 101       _reexecute_sp = sp();
 102     } else {
 103       // Find out how many arguments the interpreter needs when deoptimizing
 104       // and save the stack pointer value so it can used by uncommon_trap.
 105       // We find the argument count by looking at the declared signature.
 106       bool ignored_will_link;
 107       ciSignature* declared_signature = NULL;
 108       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 109       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 110       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 111     }
 112   }
 113 
 114   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 115 
 116   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 117   int               bci()       const    { return jvms()-&gt;bci(); }
 118   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 119   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 120   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 121 
 122   bool  try_to_inline(int predicate);
 123   Node* try_to_predicate(int predicate);
 124 
 125   void push_result() {
 126     // Push the result onto the stack.
 127     if (!stopped() &amp;&amp; result() != NULL) {
 128       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 129       push_node(bt, result());
 130     }
 131   }
 132 
 133  private:
 134   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 135     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 136   }
 137 
 138   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 139   void  set_result(RegionNode* region, PhiNode* value);
 140   Node*     result() { return _result; }
 141 
 142   virtual int reexecute_sp() { return _reexecute_sp; }
 143 
 144   // Helper functions to inline natives
 145   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 146   Node* generate_slow_guard(Node* test, RegionNode* region);
 147   Node* generate_fair_guard(Node* test, RegionNode* region);
 148   Node* generate_negative_guard(Node* index, RegionNode* region,
 149                                 // resulting CastII of index:
 150                                 Node* *pos_index = NULL);
 151   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 152                              Node* array_length,
 153                              RegionNode* region);
 154   Node* generate_current_thread(Node* &amp;tls_output);
 155   Node* load_mirror_from_klass(Node* klass);
 156   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 157                                       RegionNode* region, int null_path,
 158                                       int offset);
 159   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 160                                RegionNode* region, int null_path) {
 161     int offset = java_lang_Class::klass_offset_in_bytes();
 162     return load_klass_from_mirror_common(mirror, never_see_null,
 163                                          region, null_path,
 164                                          offset);
 165   }
 166   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 167                                      RegionNode* region, int null_path) {
 168     int offset = java_lang_Class::array_klass_offset_in_bytes();
 169     return load_klass_from_mirror_common(mirror, never_see_null,
 170                                          region, null_path,
 171                                          offset);
 172   }
 173   Node* generate_access_flags_guard(Node* kls,
 174                                     int modifier_mask, int modifier_bits,
 175                                     RegionNode* region);
 176   Node* generate_interface_guard(Node* kls, RegionNode* region);
 177   Node* generate_array_guard(Node* kls, RegionNode* region) {
 178     return generate_array_guard_common(kls, region, false, false);
 179   }
 180   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 181     return generate_array_guard_common(kls, region, false, true);
 182   }
 183   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 184     return generate_array_guard_common(kls, region, true, false);
 185   }
 186   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 187     return generate_array_guard_common(kls, region, true, true);
 188   }
 189   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 190                                     bool obj_array, bool not_array);
 191   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 192   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 193                                      bool is_virtual = false, bool is_static = false);
 194   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 195     return generate_method_call(method_id, false, true);
 196   }
 197   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 198     return generate_method_call(method_id, true, false);
 199   }
 200   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 201 
 202   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 203   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 204   bool inline_string_compareTo();
 205   bool inline_string_indexOf();
 206   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 207   bool inline_string_equals();
 208   Node* round_double_node(Node* n);
 209   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 210   bool inline_math_native(vmIntrinsics::ID id);
 211   bool inline_trig(vmIntrinsics::ID id);
 212   bool inline_math(vmIntrinsics::ID id);
 213   template &lt;typename OverflowOp&gt;
 214   bool inline_math_overflow(Node* arg1, Node* arg2);
 215   void inline_math_mathExact(Node* math, Node* test);
 216   bool inline_math_addExactI(bool is_increment);
 217   bool inline_math_addExactL(bool is_increment);
 218   bool inline_math_multiplyExactI();
 219   bool inline_math_multiplyExactL();
 220   bool inline_math_negateExactI();
 221   bool inline_math_negateExactL();
 222   bool inline_math_subtractExactI(bool is_decrement);
 223   bool inline_math_subtractExactL(bool is_decrement);
 224   bool inline_exp();
 225   bool inline_pow();
 226   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 227   bool inline_min_max(vmIntrinsics::ID id);
 228   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 229   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 230   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 231   Node* make_unsafe_address(Node* base, Node* offset);
 232   // Helper for inline_unsafe_access.
 233   // Generates the guards that check whether the result of
 234   // Unsafe.getObject should be recorded in an SATB log buffer.
 235   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 236   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 237   static bool klass_needs_init_guard(Node* kls);
 238   bool inline_unsafe_allocate();
 239   bool inline_unsafe_copyMemory();
 240   bool inline_native_currentThread();
 241 #ifdef TRACE_HAVE_INTRINSICS
 242   bool inline_native_classID();
 243   bool inline_native_threadID();
 244 #endif
 245   bool inline_native_time_funcs(address method, const char* funcName);
 246   bool inline_native_isInterrupted();
 247   bool inline_native_Class_query(vmIntrinsics::ID id);
 248   bool inline_native_subtype_check();
 249 
 250   bool inline_native_newArray();
 251   bool inline_native_getLength();
 252   bool inline_array_copyOf(bool is_copyOfRange);
 253   bool inline_array_equals();
 254   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 255   bool inline_native_clone(bool is_virtual);
 256   bool inline_native_Reflection_getCallerClass();
 257   // Helper function for inlining native object hash method
 258   bool inline_native_hashcode(bool is_virtual, bool is_static);
 259   bool inline_native_getClass();
 260 
 261   // Helper functions for inlining arraycopy
 262   bool inline_arraycopy();
 263   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 264                                                 RegionNode* slow_region);
 265   JVMState* arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp);
 266   void arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp);
 267 
 268   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 269   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 270   bool inline_unsafe_ordered_store(BasicType type);
 271   bool inline_unsafe_fence(vmIntrinsics::ID id);
 272   bool inline_spinloophint();
 273   bool inline_fp_conversions(vmIntrinsics::ID id);
 274   bool inline_number_methods(vmIntrinsics::ID id);
 275   bool inline_reference_get();
 276   bool inline_Class_cast();
 277   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 278   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 279   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 280   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 281   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 282   bool inline_ghash_processBlocks();
 283   bool inline_sha_implCompress(vmIntrinsics::ID id);
 284   bool inline_digestBase_implCompressMB(int predicate);
 285   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 286                                  bool long_state, address stubAddr, const char *stubName,
 287                                  Node* src_start, Node* ofs, Node* limit);
 288   Node* get_state_from_sha_object(Node *sha_object);
 289   Node* get_state_from_sha5_object(Node *sha_object);
 290   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 291   bool inline_encodeISOArray();
 292   bool inline_updateCRC32();
 293   bool inline_updateBytesCRC32();
 294   bool inline_updateByteBufferCRC32();
 295   Node* get_table_from_crc32c_class(ciInstanceKlass *crc32c_class);
 296   bool inline_updateBytesCRC32C();
 297   bool inline_updateDirectByteBufferCRC32C();
 298   bool inline_multiplyToLen();
 299   bool inline_squareToLen();
 300   bool inline_mulAdd();
 301 
 302   bool inline_profileBoolean();
 303   bool inline_isCompileConstant();
 304 };
 305 
 306 
 307 //---------------------------make_vm_intrinsic----------------------------
 308 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 309   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 310   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 311 
 312   ccstr disable_intr = NULL;
 313 
 314   if ((DisableIntrinsic[0] != '\0'
 315        &amp;&amp; strstr(DisableIntrinsic, vmIntrinsics::name_at(id)) != NULL) ||
 316       (method_has_option_value("DisableIntrinsic", disable_intr)
 317        &amp;&amp; strstr(disable_intr, vmIntrinsics::name_at(id)) != NULL)) {
 318     // disabled by a user request on the command line:
 319     // example: -XX:DisableIntrinsic=_hashCode,_getClass
 320     return NULL;
 321   }
 322 
 323   if (!m-&gt;is_loaded()) {
 324     // do not attempt to inline unloaded methods
 325     return NULL;
 326   }
 327 
 328   // Only a few intrinsics implement a virtual dispatch.
 329   // They are expensive calls which are also frequently overridden.
 330   if (is_virtual) {
 331     switch (id) {
 332     case vmIntrinsics::_hashCode:
 333     case vmIntrinsics::_clone:
 334       // OK, Object.hashCode and Object.clone intrinsics come in both flavors
 335       break;
 336     default:
 337       return NULL;
 338     }
 339   }
 340 
 341   // -XX:-InlineNatives disables nearly all intrinsics:
 342   if (!InlineNatives) {
 343     switch (id) {
 344     case vmIntrinsics::_indexOf:
 345     case vmIntrinsics::_compareTo:
 346     case vmIntrinsics::_equals:
 347     case vmIntrinsics::_equalsC:
 348     case vmIntrinsics::_getAndAddInt:
 349     case vmIntrinsics::_getAndAddLong:
 350     case vmIntrinsics::_getAndSetInt:
 351     case vmIntrinsics::_getAndSetLong:
 352     case vmIntrinsics::_getAndSetObject:
 353     case vmIntrinsics::_loadFence:
 354     case vmIntrinsics::_storeFence:
 355     case vmIntrinsics::_fullFence:
 356       break;  // InlineNatives does not control String.compareTo
 357     case vmIntrinsics::_Reference_get:
 358       break;  // InlineNatives does not control Reference.get
 359     default:
 360       return NULL;
 361     }
 362   }
 363 
 364   int predicates = 0;
 365   bool does_virtual_dispatch = false;
 366 
 367   switch (id) {
 368   case vmIntrinsics::_compareTo:
 369     if (!SpecialStringCompareTo)  return NULL;
 370     if (!Matcher::match_rule_supported(Op_StrComp))  return NULL;
 371     break;
 372   case vmIntrinsics::_indexOf:
 373     if (!SpecialStringIndexOf)  return NULL;
 374     break;
 375   case vmIntrinsics::_equals:
 376     if (!SpecialStringEquals)  return NULL;
 377     if (!Matcher::match_rule_supported(Op_StrEquals))  return NULL;
 378     break;
 379   case vmIntrinsics::_equalsC:
 380     if (!SpecialArraysEquals)  return NULL;
 381     if (!Matcher::match_rule_supported(Op_AryEq))  return NULL;
 382     break;
 383   case vmIntrinsics::_arraycopy:
 384     if (!InlineArrayCopy)  return NULL;
 385     break;
 386   case vmIntrinsics::_copyMemory:
 387     if (StubRoutines::unsafe_arraycopy() == NULL)  return NULL;
 388     if (!InlineArrayCopy)  return NULL;
 389     break;
 390   case vmIntrinsics::_hashCode:
 391     if (!InlineObjectHash)  return NULL;
 392     does_virtual_dispatch = true;
 393     break;
 394   case vmIntrinsics::_clone:
 395     does_virtual_dispatch = true;
 396   case vmIntrinsics::_copyOf:
 397   case vmIntrinsics::_copyOfRange:
 398     if (!InlineObjectCopy)  return NULL;
 399     // These also use the arraycopy intrinsic mechanism:
 400     if (!InlineArrayCopy)  return NULL;
 401     break;
 402   case vmIntrinsics::_encodeISOArray:
 403     if (!SpecialEncodeISOArray)  return NULL;
 404     if (!Matcher::match_rule_supported(Op_EncodeISOArray))  return NULL;
 405     break;
 406   case vmIntrinsics::_checkIndex:
 407     // We do not intrinsify this.  The optimizer does fine with it.
 408     return NULL;
 409 
 410   case vmIntrinsics::_getCallerClass:
 411     if (!InlineReflectionGetCallerClass)  return NULL;
 412     if (SystemDictionary::reflect_CallerSensitive_klass() == NULL)  return NULL;
 413     break;
 414 
 415   case vmIntrinsics::_bitCount_i:
 416     if (!Matcher::match_rule_supported(Op_PopCountI)) return NULL;
 417     break;
 418 
 419   case vmIntrinsics::_bitCount_l:
 420     if (!Matcher::match_rule_supported(Op_PopCountL)) return NULL;
 421     break;
 422 
 423   case vmIntrinsics::_numberOfLeadingZeros_i:
 424     if (!Matcher::match_rule_supported(Op_CountLeadingZerosI)) return NULL;
 425     break;
 426 
 427   case vmIntrinsics::_numberOfLeadingZeros_l:
 428     if (!Matcher::match_rule_supported(Op_CountLeadingZerosL)) return NULL;
 429     break;
 430 
 431   case vmIntrinsics::_numberOfTrailingZeros_i:
 432     if (!Matcher::match_rule_supported(Op_CountTrailingZerosI)) return NULL;
 433     break;
 434 
 435   case vmIntrinsics::_numberOfTrailingZeros_l:
 436     if (!Matcher::match_rule_supported(Op_CountTrailingZerosL)) return NULL;
 437     break;
 438 
 439   case vmIntrinsics::_reverseBytes_c:
 440     if (!Matcher::match_rule_supported(Op_ReverseBytesUS)) return NULL;
 441     break;
 442   case vmIntrinsics::_reverseBytes_s:
 443     if (!Matcher::match_rule_supported(Op_ReverseBytesS))  return NULL;
 444     break;
 445   case vmIntrinsics::_reverseBytes_i:
 446     if (!Matcher::match_rule_supported(Op_ReverseBytesI))  return NULL;
 447     break;
 448   case vmIntrinsics::_reverseBytes_l:
 449     if (!Matcher::match_rule_supported(Op_ReverseBytesL))  return NULL;
 450     break;
 451 
 452   case vmIntrinsics::_Reference_get:
 453     // Use the intrinsic version of Reference.get() so that the value in
 454     // the referent field can be registered by the G1 pre-barrier code.
 455     // Also add memory barrier to prevent commoning reads from this field
 456     // across safepoint since GC can change it value.
 457     break;
 458 
 459   case vmIntrinsics::_compareAndSwapObject:
 460 #ifdef _LP64
 461     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_CompareAndSwapP)) return NULL;
 462 #endif
 463     break;
 464 
 465   case vmIntrinsics::_compareAndSwapLong:
 466     if (!Matcher::match_rule_supported(Op_CompareAndSwapL)) return NULL;
 467     break;
 468 
 469   case vmIntrinsics::_getAndAddInt:
 470     if (!Matcher::match_rule_supported(Op_GetAndAddI)) return NULL;
 471     break;
 472 
 473   case vmIntrinsics::_getAndAddLong:
 474     if (!Matcher::match_rule_supported(Op_GetAndAddL)) return NULL;
 475     break;
 476 
 477   case vmIntrinsics::_getAndSetInt:
 478     if (!Matcher::match_rule_supported(Op_GetAndSetI)) return NULL;
 479     break;
 480 
 481   case vmIntrinsics::_getAndSetLong:
 482     if (!Matcher::match_rule_supported(Op_GetAndSetL)) return NULL;
 483     break;
 484 
 485   case vmIntrinsics::_getAndSetObject:
 486 #ifdef _LP64
 487     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 488     if (UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetN)) return NULL;
 489     break;
 490 #else
 491     if (!Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 492     break;
 493 #endif
 494 
 495   case vmIntrinsics::_aescrypt_encryptBlock:
 496   case vmIntrinsics::_aescrypt_decryptBlock:
 497     if (!UseAESIntrinsics) return NULL;
 498     break;
 499 
 500   case vmIntrinsics::_multiplyToLen:
 501     if (!UseMultiplyToLenIntrinsic) return NULL;
 502     break;
 503 
 504   case vmIntrinsics::_squareToLen:
 505     if (!UseSquareToLenIntrinsic) return NULL;
 506     break;
 507 
 508   case vmIntrinsics::_mulAdd:
 509     if (!UseMulAddIntrinsic) return NULL;
 510     break;
 511   
 512   case vmIntrinsics::_spinLoopHint:
 513     if (!UseSpinLoopHintIntrinsic) return NULL;
 514     break;
 515 
 516   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 517   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 518     if (!UseAESIntrinsics) return NULL;
 519     // these two require the predicated logic
 520     predicates = 1;
 521     break;
 522 
 523   case vmIntrinsics::_sha_implCompress:
 524     if (!UseSHA1Intrinsics) return NULL;
 525     break;
 526 
 527   case vmIntrinsics::_sha2_implCompress:
 528     if (!UseSHA256Intrinsics) return NULL;
 529     break;
 530 
 531   case vmIntrinsics::_sha5_implCompress:
 532     if (!UseSHA512Intrinsics) return NULL;
 533     break;
 534 
 535   case vmIntrinsics::_digestBase_implCompressMB:
 536     if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) return NULL;
 537     predicates = 3;
 538     break;
 539 
 540   case vmIntrinsics::_ghash_processBlocks:
 541     if (!UseGHASHIntrinsics) return NULL;
 542     break;
 543 
 544   case vmIntrinsics::_updateCRC32:
 545   case vmIntrinsics::_updateBytesCRC32:
 546   case vmIntrinsics::_updateByteBufferCRC32:
 547     if (!UseCRC32Intrinsics) return NULL;
 548     break;
 549 
 550   case vmIntrinsics::_updateBytesCRC32C:
 551   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 552     if (!UseCRC32CIntrinsics) return NULL;
 553     break;
 554 
 555   case vmIntrinsics::_incrementExactI:
 556   case vmIntrinsics::_addExactI:
 557     if (!Matcher::match_rule_supported(Op_OverflowAddI) || !UseMathExactIntrinsics) return NULL;
 558     break;
 559   case vmIntrinsics::_incrementExactL:
 560   case vmIntrinsics::_addExactL:
 561     if (!Matcher::match_rule_supported(Op_OverflowAddL) || !UseMathExactIntrinsics) return NULL;
 562     break;
 563   case vmIntrinsics::_decrementExactI:
 564   case vmIntrinsics::_subtractExactI:
 565     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 566     break;
 567   case vmIntrinsics::_decrementExactL:
 568   case vmIntrinsics::_subtractExactL:
 569     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 570     break;
 571   case vmIntrinsics::_negateExactI:
 572     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 573     break;
 574   case vmIntrinsics::_negateExactL:
 575     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 576     break;
 577   case vmIntrinsics::_multiplyExactI:
 578     if (!Matcher::match_rule_supported(Op_OverflowMulI) || !UseMathExactIntrinsics) return NULL;
 579     break;
 580   case vmIntrinsics::_multiplyExactL:
 581     if (!Matcher::match_rule_supported(Op_OverflowMulL) || !UseMathExactIntrinsics) return NULL;
 582     break;
 583 
 584   case vmIntrinsics::_getShortUnaligned:
 585   case vmIntrinsics::_getCharUnaligned:
 586   case vmIntrinsics::_getIntUnaligned:
 587   case vmIntrinsics::_getLongUnaligned:
 588   case vmIntrinsics::_putShortUnaligned:
 589   case vmIntrinsics::_putCharUnaligned:
 590   case vmIntrinsics::_putIntUnaligned:
 591   case vmIntrinsics::_putLongUnaligned:
 592     if (!UseUnalignedAccesses) return NULL;
 593     break;
 594 
 595  default:
 596     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 597     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 598     break;
 599   }
 600 
 601   // -XX:-InlineClassNatives disables natives from the Class class.
 602   // The flag applies to all reflective calls, notably Array.newArray
 603   // (visible to Java programmers as Array.newInstance).
 604   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Class() ||
 605       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_reflect_Array()) {
 606     if (!InlineClassNatives)  return NULL;
 607   }
 608 
 609   // -XX:-InlineThreadNatives disables natives from the Thread class.
 610   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Thread()) {
 611     if (!InlineThreadNatives)  return NULL;
 612   }
 613 
 614   // -XX:-InlineMathNatives disables natives from the Math,Float and Double classes.
 615   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Math() ||
 616       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Float() ||
 617       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Double()) {
 618     if (!InlineMathNatives)  return NULL;
 619   }
 620 
 621   // -XX:-InlineUnsafeOps disables natives from the Unsafe class.
 622   if (m-&gt;holder()-&gt;name() == ciSymbol::sun_misc_Unsafe()) {
 623     if (!InlineUnsafeOps)  return NULL;
 624   }
 625 
 626   return new LibraryIntrinsic(m, is_virtual, predicates, does_virtual_dispatch, (vmIntrinsics::ID) id);
 627 }
 628 
 629 //----------------------register_library_intrinsics-----------------------
 630 // Initialize this file's data structures, for each Compile instance.
 631 void Compile::register_library_intrinsics() {
 632   // Nothing to do here.
 633 }
 634 
 635 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 636   LibraryCallKit kit(jvms, this);
 637   Compile* C = kit.C;
 638   int nodes = C-&gt;unique();
 639 #ifndef PRODUCT
 640   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 641     char buf[1000];
 642     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 643     tty-&gt;print_cr("Intrinsic %s", str);
 644   }
 645 #endif
 646   ciMethod* callee = kit.callee();
 647   const int bci    = kit.bci();
 648 
 649   // Try to inline the intrinsic.
 650   if (kit.try_to_inline(_last_predicate)) {
 651     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 652       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 653     }
 654     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 655     if (C-&gt;log()) {
 656       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 657                      vmIntrinsics::name_at(intrinsic_id()),
 658                      (is_virtual() ? " virtual='1'" : ""),
 659                      C-&gt;unique() - nodes);
 660     }
 661     // Push the result from the inlined method onto the stack.
 662     kit.push_result();
 663     C-&gt;print_inlining_update(this);
 664     return kit.transfer_exceptions_into_jvms();
 665   }
 666 
 667   // The intrinsic bailed out
 668   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 669     if (jvms-&gt;has_method()) {
 670       // Not a root compile.
 671       const char* msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 672       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 673     } else {
 674       // Root compile
 675       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 676                vmIntrinsics::name_at(intrinsic_id()),
 677                (is_virtual() ? " (virtual)" : ""), bci);
 678     }
 679   }
 680   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 681   C-&gt;print_inlining_update(this);
 682   return NULL;
 683 }
 684 
 685 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 686   LibraryCallKit kit(jvms, this);
 687   Compile* C = kit.C;
 688   int nodes = C-&gt;unique();
 689   _last_predicate = predicate;
 690 #ifndef PRODUCT
 691   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 692   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 693     char buf[1000];
 694     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 695     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 696   }
 697 #endif
 698   ciMethod* callee = kit.callee();
 699   const int bci    = kit.bci();
 700 
 701   Node* slow_ctl = kit.try_to_predicate(predicate);
 702   if (!kit.failing()) {
 703     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 704       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 705     }
 706     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 707     if (C-&gt;log()) {
 708       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 709                      vmIntrinsics::name_at(intrinsic_id()),
 710                      (is_virtual() ? " virtual='1'" : ""),
 711                      C-&gt;unique() - nodes);
 712     }
 713     return slow_ctl; // Could be NULL if the check folds.
 714   }
 715 
 716   // The intrinsic bailed out
 717   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 718     if (jvms-&gt;has_method()) {
 719       // Not a root compile.
 720       const char* msg = "failed to generate predicate for intrinsic";
 721       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 722     } else {
 723       // Root compile
 724       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 725                                         vmIntrinsics::name_at(intrinsic_id()),
 726                                         (is_virtual() ? " (virtual)" : ""), bci);
 727     }
 728   }
 729   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 730   return NULL;
 731 }
 732 
 733 bool LibraryCallKit::try_to_inline(int predicate) {
 734   // Handle symbolic names for otherwise undistinguished boolean switches:
 735   const bool is_store       = true;
 736   const bool is_native_ptr  = true;
 737   const bool is_static      = true;
 738   const bool is_volatile    = true;
 739 
 740   if (!jvms()-&gt;has_method()) {
 741     // Root JVMState has a null method.
 742     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 743     // Insert the memory aliasing node
 744     set_all_memory(reset_memory());
 745   }
 746   assert(merged_memory(), "");
 747 
 748 
 749   switch (intrinsic_id()) {
 750   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 751   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 752   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 753 
 754   case vmIntrinsics::_dsin:
 755   case vmIntrinsics::_dcos:
 756   case vmIntrinsics::_dtan:
 757   case vmIntrinsics::_dabs:
 758   case vmIntrinsics::_datan2:
 759   case vmIntrinsics::_dsqrt:
 760   case vmIntrinsics::_dexp:
 761   case vmIntrinsics::_dlog:
 762   case vmIntrinsics::_dlog10:
 763   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 764 
 765   case vmIntrinsics::_min:
 766   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 767 
 768   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 769   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 770   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 771   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 772   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 773   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 774   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 775   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 776   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 777   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 778   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 779   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 780 
 781   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 782 
 783   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 784   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 785   case vmIntrinsics::_equals:                   return inline_string_equals();
 786 
 787   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 788   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 789   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 790   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 791   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 792   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 793   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 794   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 795   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 796 
 797   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 798   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 799   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 800   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 801   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 802   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 803   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 804   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 805   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 806 
 807   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 808   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 809   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 810   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 811   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 812   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 813   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 814   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 815 
 816   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 817   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 818   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 819   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 820   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 821   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 822   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 823   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 824 
 825   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 826   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 827   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 828   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 829   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 830   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 831   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 832   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 833   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 834 
 835   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 836   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 837   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 838   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 839   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 840   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 841   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 842   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 843   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 844 
 845   case vmIntrinsics::_getShortUnaligned:        return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 846   case vmIntrinsics::_getCharUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 847   case vmIntrinsics::_getIntUnaligned:          return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 848   case vmIntrinsics::_getLongUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 849 
 850   case vmIntrinsics::_putShortUnaligned:        return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 851   case vmIntrinsics::_putCharUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 852   case vmIntrinsics::_putIntUnaligned:          return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 853   case vmIntrinsics::_putLongUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 854 
 855   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 856   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 857   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 858 
 859   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 860   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 861   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 862 
 863   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 864   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 865   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 866   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 867   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 868 
 869   case vmIntrinsics::_loadFence:
 870   case vmIntrinsics::_storeFence:
 871   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 872 
 873   case vmIntrinsics::_spinLoopHint:             return inline_spinloophint();
 874 
 875   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 876   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 877 
 878 #ifdef TRACE_HAVE_INTRINSICS
 879   case vmIntrinsics::_classID:                  return inline_native_classID();
 880   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 881   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 882 #endif
 883   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 884   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 885   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 886   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 887   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 888   case vmIntrinsics::_getLength:                return inline_native_getLength();
 889   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 890   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 891   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 892   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 893 
 894   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 895 
 896   case vmIntrinsics::_isInstance:
 897   case vmIntrinsics::_getModifiers:
 898   case vmIntrinsics::_isInterface:
 899   case vmIntrinsics::_isArray:
 900   case vmIntrinsics::_isPrimitive:
 901   case vmIntrinsics::_getSuperclass:
 902   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 903 
 904   case vmIntrinsics::_floatToRawIntBits:
 905   case vmIntrinsics::_floatToIntBits:
 906   case vmIntrinsics::_intBitsToFloat:
 907   case vmIntrinsics::_doubleToRawLongBits:
 908   case vmIntrinsics::_doubleToLongBits:
 909   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 910 
 911   case vmIntrinsics::_numberOfLeadingZeros_i:
 912   case vmIntrinsics::_numberOfLeadingZeros_l:
 913   case vmIntrinsics::_numberOfTrailingZeros_i:
 914   case vmIntrinsics::_numberOfTrailingZeros_l:
 915   case vmIntrinsics::_bitCount_i:
 916   case vmIntrinsics::_bitCount_l:
 917   case vmIntrinsics::_reverseBytes_i:
 918   case vmIntrinsics::_reverseBytes_l:
 919   case vmIntrinsics::_reverseBytes_s:
 920   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 921 
 922   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 923 
 924   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 925 
 926   case vmIntrinsics::_Class_cast:               return inline_Class_cast();
 927 
 928   case vmIntrinsics::_aescrypt_encryptBlock:
 929   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 930 
 931   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 932   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 933     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 934 
 935   case vmIntrinsics::_sha_implCompress:
 936   case vmIntrinsics::_sha2_implCompress:
 937   case vmIntrinsics::_sha5_implCompress:
 938     return inline_sha_implCompress(intrinsic_id());
 939 
 940   case vmIntrinsics::_digestBase_implCompressMB:
 941     return inline_digestBase_implCompressMB(predicate);
 942 
 943   case vmIntrinsics::_multiplyToLen:
 944     return inline_multiplyToLen();
 945 
 946   case vmIntrinsics::_squareToLen:
 947     return inline_squareToLen();
 948 
 949   case vmIntrinsics::_mulAdd:
 950     return inline_mulAdd();
 951 
 952   case vmIntrinsics::_ghash_processBlocks:
 953     return inline_ghash_processBlocks();
 954 
 955   case vmIntrinsics::_encodeISOArray:
 956     return inline_encodeISOArray();
 957 
 958   case vmIntrinsics::_updateCRC32:
 959     return inline_updateCRC32();
 960   case vmIntrinsics::_updateBytesCRC32:
 961     return inline_updateBytesCRC32();
 962   case vmIntrinsics::_updateByteBufferCRC32:
 963     return inline_updateByteBufferCRC32();
 964 
 965   case vmIntrinsics::_updateBytesCRC32C:
 966     return inline_updateBytesCRC32C();
 967   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 968     return inline_updateDirectByteBufferCRC32C();
 969 
 970   case vmIntrinsics::_profileBoolean:
 971     return inline_profileBoolean();
 972   case vmIntrinsics::_isCompileConstant:
 973     return inline_isCompileConstant();
 974 
 975   default:
 976     // If you get here, it may be that someone has added a new intrinsic
 977     // to the list in vmSymbols.hpp without implementing it here.
 978 #ifndef PRODUCT
 979     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 980       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 981                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 982     }
 983 #endif
 984     return false;
 985   }
 986 }
 987 
 988 Node* LibraryCallKit::try_to_predicate(int predicate) {
 989   if (!jvms()-&gt;has_method()) {
 990     // Root JVMState has a null method.
 991     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 992     // Insert the memory aliasing node
 993     set_all_memory(reset_memory());
 994   }
 995   assert(merged_memory(), "");
 996 
 997   switch (intrinsic_id()) {
 998   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 999     return inline_cipherBlockChaining_AESCrypt_predicate(false);
1000   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
1001     return inline_cipherBlockChaining_AESCrypt_predicate(true);
1002   case vmIntrinsics::_digestBase_implCompressMB:
1003     return inline_digestBase_implCompressMB_predicate(predicate);
1004 
1005   default:
1006     // If you get here, it may be that someone has added a new intrinsic
1007     // to the list in vmSymbols.hpp without implementing it here.
1008 #ifndef PRODUCT
1009     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
1010       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
1011                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
1012     }
1013 #endif
1014     Node* slow_ctl = control();
1015     set_control(top()); // No fast path instrinsic
1016     return slow_ctl;
1017   }
1018 }
1019 
1020 //------------------------------set_result-------------------------------
1021 // Helper function for finishing intrinsics.
1022 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
1023   record_for_igvn(region);
1024   set_control(_gvn.transform(region));
1025   set_result( _gvn.transform(value));
1026   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
1027 }
1028 
1029 //------------------------------generate_guard---------------------------
1030 // Helper function for generating guarded fast-slow graph structures.
1031 // The given 'test', if true, guards a slow path.  If the test fails
1032 // then a fast path can be taken.  (We generally hope it fails.)
1033 // In all cases, GraphKit::control() is updated to the fast path.
1034 // The returned value represents the control for the slow path.
1035 // The return value is never 'top'; it is either a valid control
1036 // or NULL if it is obvious that the slow path can never be taken.
1037 // Also, if region and the slow control are not NULL, the slow edge
1038 // is appended to the region.
1039 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
1040   if (stopped()) {
1041     // Already short circuited.
1042     return NULL;
1043   }
1044 
1045   // Build an if node and its projections.
1046   // If test is true we take the slow path, which we assume is uncommon.
1047   if (_gvn.type(test) == TypeInt::ZERO) {
1048     // The slow branch is never taken.  No need to build this guard.
1049     return NULL;
1050   }
1051 
1052   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
1053 
1054   Node* if_slow = _gvn.transform(new IfTrueNode(iff));
1055   if (if_slow == top()) {
1056     // The slow branch is never taken.  No need to build this guard.
1057     return NULL;
1058   }
1059 
1060   if (region != NULL)
1061     region-&gt;add_req(if_slow);
1062 
1063   Node* if_fast = _gvn.transform(new IfFalseNode(iff));
1064   set_control(if_fast);
1065 
1066   return if_slow;
1067 }
1068 
1069 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
1070   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
1071 }
1072 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
1073   return generate_guard(test, region, PROB_FAIR);
1074 }
1075 
1076 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
1077                                                      Node* *pos_index) {
1078   if (stopped())
1079     return NULL;                // already stopped
1080   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
1081     return NULL;                // index is already adequately typed
1082   Node* cmp_lt = _gvn.transform(new CmpINode(index, intcon(0)));
1083   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
1084   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
1085   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
1086     // Emulate effect of Parse::adjust_map_after_if.
1087     Node* ccast = new CastIINode(index, TypeInt::POS);
1088     ccast-&gt;set_req(0, control());
1089     (*pos_index) = _gvn.transform(ccast);
1090   }
1091   return is_neg;
1092 }
1093 
1094 // Make sure that 'position' is a valid limit index, in [0..length].
1095 // There are two equivalent plans for checking this:
1096 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
1097 //   B. offset  &lt;=  (arrayLength - copyLength)
1098 // We require that all of the values above, except for the sum and
1099 // difference, are already known to be non-negative.
1100 // Plan A is robust in the face of overflow, if offset and copyLength
1101 // are both hugely positive.
1102 //
1103 // Plan B is less direct and intuitive, but it does not overflow at
1104 // all, since the difference of two non-negatives is always
1105 // representable.  Whenever Java methods must perform the equivalent
1106 // check they generally use Plan B instead of Plan A.
1107 // For the moment we use Plan A.
1108 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
1109                                                   Node* subseq_length,
1110                                                   Node* array_length,
1111                                                   RegionNode* region) {
1112   if (stopped())
1113     return NULL;                // already stopped
1114   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
1115   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
1116     return NULL;                // common case of whole-array copy
1117   Node* last = subseq_length;
1118   if (!zero_offset)             // last += offset
1119     last = _gvn.transform(new AddINode(last, offset));
1120   Node* cmp_lt = _gvn.transform(new CmpUNode(array_length, last));
1121   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
1122   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
1123   return is_over;
1124 }
1125 
1126 
1127 //--------------------------generate_current_thread--------------------
1128 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1129   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1130   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1131   Node* thread = _gvn.transform(new ThreadLocalNode());
1132   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1133   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1134   tls_output = thread;
1135   return threadObj;
1136 }
1137 
1138 
1139 //------------------------------make_string_method_node------------------------
1140 // Helper method for String intrinsic functions. This version is called
1141 // with str1 and str2 pointing to String object nodes.
1142 //
1143 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
1144   Node* no_ctrl = NULL;
1145 
1146   // Get start addr of string
1147   Node* str1_value   = load_String_value(no_ctrl, str1);
1148   Node* str1_offset  = load_String_offset(no_ctrl, str1);
1149   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
1150 
1151   // Get length of string 1
1152   Node* str1_len  = load_String_length(no_ctrl, str1);
1153 
1154   Node* str2_value   = load_String_value(no_ctrl, str2);
1155   Node* str2_offset  = load_String_offset(no_ctrl, str2);
1156   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
1157 
1158   Node* str2_len = NULL;
1159   Node* result = NULL;
1160 
1161   switch (opcode) {
1162   case Op_StrIndexOf:
1163     // Get length of string 2
1164     str2_len = load_String_length(no_ctrl, str2);
1165 
1166     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1167                                 str1_start, str1_len, str2_start, str2_len);
1168     break;
1169   case Op_StrComp:
1170     // Get length of string 2
1171     str2_len = load_String_length(no_ctrl, str2);
1172 
1173     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
1174                              str1_start, str1_len, str2_start, str2_len);
1175     break;
1176   case Op_StrEquals:
1177     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1178                                str1_start, str2_start, str1_len);
1179     break;
1180   default:
1181     ShouldNotReachHere();
1182     return NULL;
1183   }
1184 
1185   // All these intrinsics have checks.
1186   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1187 
1188   return _gvn.transform(result);
1189 }
1190 
1191 // Helper method for String intrinsic functions. This version is called
1192 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
1193 // to Int nodes containing the lenghts of str1 and str2.
1194 //
1195 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
1196   Node* result = NULL;
1197   switch (opcode) {
1198   case Op_StrIndexOf:
1199     result = new StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1200                                 str1_start, cnt1, str2_start, cnt2);
1201     break;
1202   case Op_StrComp:
1203     result = new StrCompNode(control(), memory(TypeAryPtr::CHARS),
1204                              str1_start, cnt1, str2_start, cnt2);
1205     break;
1206   case Op_StrEquals:
1207     result = new StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1208                                str1_start, str2_start, cnt1);
1209     break;
1210   default:
1211     ShouldNotReachHere();
1212     return NULL;
1213   }
1214 
1215   // All these intrinsics have checks.
1216   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1217 
1218   return _gvn.transform(result);
1219 }
1220 
1221 //------------------------------inline_string_compareTo------------------------
1222 // public int java.lang.String.compareTo(String anotherString);
1223 bool LibraryCallKit::inline_string_compareTo() {
1224   Node* receiver = null_check(argument(0));
1225   Node* arg      = null_check(argument(1));
1226   if (stopped()) {
1227     return true;
1228   }
1229   set_result(make_string_method_node(Op_StrComp, receiver, arg));
1230   return true;
1231 }
1232 
1233 //------------------------------inline_string_equals------------------------
1234 bool LibraryCallKit::inline_string_equals() {
1235   Node* receiver = null_check_receiver();
1236   // NOTE: Do not null check argument for String.equals() because spec
1237   // allows to specify NULL as argument.
1238   Node* argument = this-&gt;argument(1);
1239   if (stopped()) {
1240     return true;
1241   }
1242 
1243   // paths (plus control) merge
1244   RegionNode* region = new RegionNode(5);
1245   Node* phi = new PhiNode(region, TypeInt::BOOL);
1246 
1247   // does source == target string?
1248   Node* cmp = _gvn.transform(new CmpPNode(receiver, argument));
1249   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1250 
1251   Node* if_eq = generate_slow_guard(bol, NULL);
1252   if (if_eq != NULL) {
1253     // receiver == argument
1254     phi-&gt;init_req(2, intcon(1));
1255     region-&gt;init_req(2, if_eq);
1256   }
1257 
1258   // get String klass for instanceOf
1259   ciInstanceKlass* klass = env()-&gt;String_klass();
1260 
1261   if (!stopped()) {
1262     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1263     Node* cmp  = _gvn.transform(new CmpINode(inst, intcon(1)));
1264     Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1265 
1266     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1267     //instanceOf == true, fallthrough
1268 
1269     if (inst_false != NULL) {
1270       phi-&gt;init_req(3, intcon(0));
1271       region-&gt;init_req(3, inst_false);
1272     }
1273   }
1274 
1275   if (!stopped()) {
1276     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1277 
1278     // Properly cast the argument to String
1279     argument = _gvn.transform(new CheckCastPPNode(control(), argument, string_type));
1280     // This path is taken only when argument's type is String:NotNull.
1281     argument = cast_not_null(argument, false);
1282 
1283     Node* no_ctrl = NULL;
1284 
1285     // Get start addr of receiver
1286     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1287     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1288     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1289 
1290     // Get length of receiver
1291     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1292 
1293     // Get start addr of argument
1294     Node* argument_val    = load_String_value(no_ctrl, argument);
1295     Node* argument_offset = load_String_offset(no_ctrl, argument);
1296     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1297 
1298     // Get length of argument
1299     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1300 
1301     // Check for receiver count != argument count
1302     Node* cmp = _gvn.transform(new CmpINode(receiver_cnt, argument_cnt));
1303     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1304     Node* if_ne = generate_slow_guard(bol, NULL);
1305     if (if_ne != NULL) {
1306       phi-&gt;init_req(4, intcon(0));
1307       region-&gt;init_req(4, if_ne);
1308     }
1309 
1310     // Check for count == 0 is done by assembler code for StrEquals.
1311 
1312     if (!stopped()) {
1313       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1314       phi-&gt;init_req(1, equals);
1315       region-&gt;init_req(1, control());
1316     }
1317   }
1318 
1319   // post merge
1320   set_control(_gvn.transform(region));
1321   record_for_igvn(region);
1322 
1323   set_result(_gvn.transform(phi));
1324   return true;
1325 }
1326 
1327 //------------------------------inline_array_equals----------------------------
1328 bool LibraryCallKit::inline_array_equals() {
1329   Node* arg1 = argument(0);
1330   Node* arg2 = argument(1);
1331   set_result(_gvn.transform(new AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1332   return true;
1333 }
1334 
1335 // Java version of String.indexOf(constant string)
1336 // class StringDecl {
1337 //   StringDecl(char[] ca) {
1338 //     offset = 0;
1339 //     count = ca.length;
1340 //     value = ca;
1341 //   }
1342 //   int offset;
1343 //   int count;
1344 //   char[] value;
1345 // }
1346 //
1347 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1348 //                             int targetOffset, int cache_i, int md2) {
1349 //   int cache = cache_i;
1350 //   int sourceOffset = string_object.offset;
1351 //   int sourceCount = string_object.count;
1352 //   int targetCount = target_object.length;
1353 //
1354 //   int targetCountLess1 = targetCount - 1;
1355 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1356 //
1357 //   char[] source = string_object.value;
1358 //   char[] target = target_object;
1359 //   int lastChar = target[targetCountLess1];
1360 //
1361 //  outer_loop:
1362 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1363 //     int src = source[i + targetCountLess1];
1364 //     if (src == lastChar) {
1365 //       // With random strings and a 4-character alphabet,
1366 //       // reverse matching at this point sets up 0.8% fewer
1367 //       // frames, but (paradoxically) makes 0.3% more probes.
1368 //       // Since those probes are nearer the lastChar probe,
1369 //       // there is may be a net D$ win with reverse matching.
1370 //       // But, reversing loop inhibits unroll of inner loop
1371 //       // for unknown reason.  So, does running outer loop from
1372 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1373 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1374 //         if (target[targetOffset + j] != source[i+j]) {
1375 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1376 //             if (md2 &lt; j+1) {
1377 //               i += j+1;
1378 //               continue outer_loop;
1379 //             }
1380 //           }
1381 //           i += md2;
1382 //           continue outer_loop;
1383 //         }
1384 //       }
1385 //       return i - sourceOffset;
1386 //     }
1387 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1388 //       i += targetCountLess1;
1389 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1390 //     i++;
1391 //   }
1392 //   return -1;
1393 // }
1394 
1395 //------------------------------string_indexOf------------------------
1396 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1397                                      jint cache_i, jint md2_i) {
1398 
1399   Node* no_ctrl  = NULL;
1400   float likely   = PROB_LIKELY(0.9);
1401   float unlikely = PROB_UNLIKELY(0.9);
1402 
1403   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1404 
1405   Node* source        = load_String_value(no_ctrl, string_object);
1406   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1407   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1408 
1409   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1410   jint target_length = target_array-&gt;length();
1411   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1412   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1413 
1414   // String.value field is known to be @Stable.
1415   if (UseImplicitStableValues) {
1416     target = cast_array_to_stable(target, target_type);
1417   }
1418 
1419   IdealKit kit(this, false, true);
1420 #define __ kit.
1421   Node* zero             = __ ConI(0);
1422   Node* one              = __ ConI(1);
1423   Node* cache            = __ ConI(cache_i);
1424   Node* md2              = __ ConI(md2_i);
1425   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1426   Node* targetCountLess1 = __ ConI(target_length - 1);
1427   Node* targetOffset     = __ ConI(targetOffset_i);
1428   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1429 
1430   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1431   Node* outer_loop = __ make_label(2 /* goto */);
1432   Node* return_    = __ make_label(1);
1433 
1434   __ set(rtn,__ ConI(-1));
1435   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1436        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1437        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1438        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1439        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1440          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1441               Node* tpj = __ AddI(targetOffset, __ value(j));
1442               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1443               Node* ipj  = __ AddI(__ value(i), __ value(j));
1444               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1445               __ if_then(targ, BoolTest::ne, src2); {
1446                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1447                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1448                     __ increment(i, __ AddI(__ value(j), one));
1449                     __ goto_(outer_loop);
1450                   } __ end_if(); __ dead(j);
1451                 }__ end_if(); __ dead(j);
1452                 __ increment(i, md2);
1453                 __ goto_(outer_loop);
1454               }__ end_if();
1455               __ increment(j, one);
1456          }__ end_loop(); __ dead(j);
1457          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1458          __ goto_(return_);
1459        }__ end_if();
1460        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1461          __ increment(i, targetCountLess1);
1462        }__ end_if();
1463        __ increment(i, one);
1464        __ bind(outer_loop);
1465   }__ end_loop(); __ dead(i);
1466   __ bind(return_);
1467 
1468   // Final sync IdealKit and GraphKit.
1469   final_sync(kit);
1470   Node* result = __ value(rtn);
1471 #undef __
1472   C-&gt;set_has_loops(true);
1473   return result;
1474 }
1475 
1476 //------------------------------inline_string_indexOf------------------------
1477 bool LibraryCallKit::inline_string_indexOf() {
1478   Node* receiver = argument(0);
1479   Node* arg      = argument(1);
1480 
1481   Node* result;
1482   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1483       UseSSE42Intrinsics) {
1484     // Generate SSE4.2 version of indexOf
1485     // We currently only have match rules that use SSE4.2
1486 
1487     receiver = null_check(receiver);
1488     arg      = null_check(arg);
1489     if (stopped()) {
1490       return true;
1491     }
1492 
1493     // Make the merge point
1494     RegionNode* result_rgn = new RegionNode(4);
1495     Node*       result_phi = new PhiNode(result_rgn, TypeInt::INT);
1496     Node* no_ctrl  = NULL;
1497 
1498     // Get start addr of source string
1499     Node* source = load_String_value(no_ctrl, receiver);
1500     Node* source_offset = load_String_offset(no_ctrl, receiver);
1501     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1502 
1503     // Get length of source string
1504     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1505 
1506     // Get start addr of substring
1507     Node* substr = load_String_value(no_ctrl, arg);
1508     Node* substr_offset = load_String_offset(no_ctrl, arg);
1509     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1510 
1511     // Get length of source string
1512     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1513 
1514     // Check for substr count &gt; string count
1515     Node* cmp = _gvn.transform(new CmpINode(substr_cnt, source_cnt));
1516     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::gt));
1517     Node* if_gt = generate_slow_guard(bol, NULL);
1518     if (if_gt != NULL) {
1519       result_phi-&gt;init_req(2, intcon(-1));
1520       result_rgn-&gt;init_req(2, if_gt);
1521     }
1522 
1523     if (!stopped()) {
1524       // Check for substr count == 0
1525       cmp = _gvn.transform(new CmpINode(substr_cnt, intcon(0)));
1526       bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1527       Node* if_zero = generate_slow_guard(bol, NULL);
1528       if (if_zero != NULL) {
1529         result_phi-&gt;init_req(3, intcon(0));
1530         result_rgn-&gt;init_req(3, if_zero);
1531       }
1532     }
1533 
1534     if (!stopped()) {
1535       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1536       result_phi-&gt;init_req(1, result);
1537       result_rgn-&gt;init_req(1, control());
1538     }
1539     set_control(_gvn.transform(result_rgn));
1540     record_for_igvn(result_rgn);
1541     result = _gvn.transform(result_phi);
1542 
1543   } else { // Use LibraryCallKit::string_indexOf
1544     // don't intrinsify if argument isn't a constant string.
1545     if (!arg-&gt;is_Con()) {
1546      return false;
1547     }
1548     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1549     if (str_type == NULL) {
1550       return false;
1551     }
1552     ciInstanceKlass* klass = env()-&gt;String_klass();
1553     ciObject* str_const = str_type-&gt;const_oop();
1554     if (str_const == NULL || str_const-&gt;klass() != klass) {
1555       return false;
1556     }
1557     ciInstance* str = str_const-&gt;as_instance();
1558     assert(str != NULL, "must be instance");
1559 
1560     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1561     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1562 
1563     int o;
1564     int c;
1565     if (java_lang_String::has_offset_field()) {
1566       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1567       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1568     } else {
1569       o = 0;
1570       c = pat-&gt;length();
1571     }
1572 
1573     // constant strings have no offset and count == length which
1574     // simplifies the resulting code somewhat so lets optimize for that.
1575     if (o != 0 || c != pat-&gt;length()) {
1576      return false;
1577     }
1578 
1579     receiver = null_check(receiver, T_OBJECT);
1580     // NOTE: No null check on the argument is needed since it's a constant String oop.
1581     if (stopped()) {
1582       return true;
1583     }
1584 
1585     // The null string as a pattern always returns 0 (match at beginning of string)
1586     if (c == 0) {
1587       set_result(intcon(0));
1588       return true;
1589     }
1590 
1591     // Generate default indexOf
1592     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1593     int cache = 0;
1594     int i;
1595     for (i = 0; i &lt; c - 1; i++) {
1596       assert(i &lt; pat-&gt;length(), "out of range");
1597       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1598     }
1599 
1600     int md2 = c;
1601     for (i = 0; i &lt; c - 1; i++) {
1602       assert(i &lt; pat-&gt;length(), "out of range");
1603       if (pat-&gt;char_at(o + i) == lastChar) {
1604         md2 = (c - 1) - i;
1605       }
1606     }
1607 
1608     result = string_indexOf(receiver, pat, o, cache, md2);
1609   }
1610   set_result(result);
1611   return true;
1612 }
1613 
1614 //--------------------------round_double_node--------------------------------
1615 // Round a double node if necessary.
1616 Node* LibraryCallKit::round_double_node(Node* n) {
1617   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1618     n = _gvn.transform(new RoundDoubleNode(0, n));
1619   return n;
1620 }
1621 
1622 //------------------------------inline_math-----------------------------------
1623 // public static double Math.abs(double)
1624 // public static double Math.sqrt(double)
1625 // public static double Math.log(double)
1626 // public static double Math.log10(double)
1627 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1628   Node* arg = round_double_node(argument(0));
1629   Node* n;
1630   switch (id) {
1631   case vmIntrinsics::_dabs:   n = new AbsDNode(                arg);  break;
1632   case vmIntrinsics::_dsqrt:  n = new SqrtDNode(C, control(),  arg);  break;
1633   case vmIntrinsics::_dlog:   n = new LogDNode(C, control(),   arg);  break;
1634   case vmIntrinsics::_dlog10: n = new Log10DNode(C, control(), arg);  break;
1635   default:  fatal_unexpected_iid(id);  break;
1636   }
1637   set_result(_gvn.transform(n));
1638   return true;
1639 }
1640 
1641 //------------------------------inline_trig----------------------------------
1642 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1643 // argument reduction which will turn into a fast/slow diamond.
1644 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1645   Node* arg = round_double_node(argument(0));
1646   Node* n = NULL;
1647 
1648   switch (id) {
1649   case vmIntrinsics::_dsin:  n = new SinDNode(C, control(), arg);  break;
1650   case vmIntrinsics::_dcos:  n = new CosDNode(C, control(), arg);  break;
1651   case vmIntrinsics::_dtan:  n = new TanDNode(C, control(), arg);  break;
1652   default:  fatal_unexpected_iid(id);  break;
1653   }
1654   n = _gvn.transform(n);
1655 
1656   // Rounding required?  Check for argument reduction!
1657   if (Matcher::strict_fp_requires_explicit_rounding) {
1658     static const double     pi_4 =  0.7853981633974483;
1659     static const double neg_pi_4 = -0.7853981633974483;
1660     // pi/2 in 80-bit extended precision
1661     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1662     // -pi/2 in 80-bit extended precision
1663     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1664     // Cutoff value for using this argument reduction technique
1665     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1666     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1667 
1668     // Pseudocode for sin:
1669     // if (x &lt;= Math.PI / 4.0) {
1670     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1671     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1672     // } else {
1673     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1674     // }
1675     // return StrictMath.sin(x);
1676 
1677     // Pseudocode for cos:
1678     // if (x &lt;= Math.PI / 4.0) {
1679     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1680     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1681     // } else {
1682     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1683     // }
1684     // return StrictMath.cos(x);
1685 
1686     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1687     // requires a special machine instruction to load it.  Instead we'll try
1688     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1689     // probably do the math inside the SIN encoding.
1690 
1691     // Make the merge point
1692     RegionNode* r = new RegionNode(3);
1693     Node* phi = new PhiNode(r, Type::DOUBLE);
1694 
1695     // Flatten arg so we need only 1 test
1696     Node *abs = _gvn.transform(new AbsDNode(arg));
1697     // Node for PI/4 constant
1698     Node *pi4 = makecon(TypeD::make(pi_4));
1699     // Check PI/4 : abs(arg)
1700     Node *cmp = _gvn.transform(new CmpDNode(pi4,abs));
1701     // Check: If PI/4 &lt; abs(arg) then go slow
1702     Node *bol = _gvn.transform(new BoolNode( cmp, BoolTest::lt ));
1703     // Branch either way
1704     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1705     set_control(opt_iff(r,iff));
1706 
1707     // Set fast path result
1708     phi-&gt;init_req(2, n);
1709 
1710     // Slow path - non-blocking leaf call
1711     Node* call = NULL;
1712     switch (id) {
1713     case vmIntrinsics::_dsin:
1714       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1715                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1716                                "Sin", NULL, arg, top());
1717       break;
1718     case vmIntrinsics::_dcos:
1719       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1720                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1721                                "Cos", NULL, arg, top());
1722       break;
1723     case vmIntrinsics::_dtan:
1724       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1725                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1726                                "Tan", NULL, arg, top());
1727       break;
1728     }
1729     assert(control()-&gt;in(0) == call, "");
1730     Node* slow_result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1731     r-&gt;init_req(1, control());
1732     phi-&gt;init_req(1, slow_result);
1733 
1734     // Post-merge
1735     set_control(_gvn.transform(r));
1736     record_for_igvn(r);
1737     n = _gvn.transform(phi);
1738 
1739     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1740   }
1741   set_result(n);
1742   return true;
1743 }
1744 
1745 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1746   //-------------------
1747   //result=(result.isNaN())? funcAddr():result;
1748   // Check: If isNaN() by checking result!=result? then either trap
1749   // or go to runtime
1750   Node* cmpisnan = _gvn.transform(new CmpDNode(result, result));
1751   // Build the boolean node
1752   Node* bolisnum = _gvn.transform(new BoolNode(cmpisnan, BoolTest::eq));
1753 
1754   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1755     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1756       // The pow or exp intrinsic returned a NaN, which requires a call
1757       // to the runtime.  Recompile with the runtime call.
1758       uncommon_trap(Deoptimization::Reason_intrinsic,
1759                     Deoptimization::Action_make_not_entrant);
1760     }
1761     return result;
1762   } else {
1763     // If this inlining ever returned NaN in the past, we compile a call
1764     // to the runtime to properly handle corner cases
1765 
1766     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1767     Node* if_slow = _gvn.transform(new IfFalseNode(iff));
1768     Node* if_fast = _gvn.transform(new IfTrueNode(iff));
1769 
1770     if (!if_slow-&gt;is_top()) {
1771       RegionNode* result_region = new RegionNode(3);
1772       PhiNode*    result_val = new PhiNode(result_region, Type::DOUBLE);
1773 
1774       result_region-&gt;init_req(1, if_fast);
1775       result_val-&gt;init_req(1, result);
1776 
1777       set_control(if_slow);
1778 
1779       const TypePtr* no_memory_effects = NULL;
1780       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1781                                    no_memory_effects,
1782                                    x, top(), y, y ? top() : NULL);
1783       Node* value = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+0));
1784 #ifdef ASSERT
1785       Node* value_top = _gvn.transform(new ProjNode(rt, TypeFunc::Parms+1));
1786       assert(value_top == top(), "second value must be top");
1787 #endif
1788 
1789       result_region-&gt;init_req(2, control());
1790       result_val-&gt;init_req(2, value);
1791       set_control(_gvn.transform(result_region));
1792       return _gvn.transform(result_val);
1793     } else {
1794       return result;
1795     }
1796   }
1797 }
1798 
1799 //------------------------------inline_exp-------------------------------------
1800 // Inline exp instructions, if possible.  The Intel hardware only misses
1801 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1802 bool LibraryCallKit::inline_exp() {
1803   Node* arg = round_double_node(argument(0));
1804   Node* n   = _gvn.transform(new ExpDNode(C, control(), arg));
1805 
1806   n = finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1807   set_result(n);
1808 
1809   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1810   return true;
1811 }
1812 
1813 //------------------------------inline_pow-------------------------------------
1814 // Inline power instructions, if possible.
1815 bool LibraryCallKit::inline_pow() {
1816   // Pseudocode for pow
1817   // if (y == 2) {
1818   //   return x * x;
1819   // } else {
1820   //   if (x &lt;= 0.0) {
1821   //     long longy = (long)y;
1822   //     if ((double)longy == y) { // if y is long
1823   //       if (y + 1 == y) longy = 0; // huge number: even
1824   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1825   //     } else {
1826   //       result = NaN;
1827   //     }
1828   //   } else {
1829   //     result = DPow(x,y);
1830   //   }
1831   //   if (result != result)?  {
1832   //     result = uncommon_trap() or runtime_call();
1833   //   }
1834   //   return result;
1835   // }
1836 
1837   Node* x = round_double_node(argument(0));
1838   Node* y = round_double_node(argument(2));
1839 
1840   Node* result = NULL;
1841 
1842   Node*   const_two_node = makecon(TypeD::make(2.0));
1843   Node*   cmp_node       = _gvn.transform(new CmpDNode(y, const_two_node));
1844   Node*   bool_node      = _gvn.transform(new BoolNode(cmp_node, BoolTest::eq));
1845   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1846   Node*   if_true        = _gvn.transform(new IfTrueNode(if_node));
1847   Node*   if_false       = _gvn.transform(new IfFalseNode(if_node));
1848 
1849   RegionNode* region_node = new RegionNode(3);
1850   region_node-&gt;init_req(1, if_true);
1851 
1852   Node* phi_node = new PhiNode(region_node, Type::DOUBLE);
1853   // special case for x^y where y == 2, we can convert it to x * x
1854   phi_node-&gt;init_req(1, _gvn.transform(new MulDNode(x, x)));
1855 
1856   // set control to if_false since we will now process the false branch
1857   set_control(if_false);
1858 
1859   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1860     // Short form: skip the fancy tests and just check for NaN result.
1861     result = _gvn.transform(new PowDNode(C, control(), x, y));
1862   } else {
1863     // If this inlining ever returned NaN in the past, include all
1864     // checks + call to the runtime.
1865 
1866     // Set the merge point for If node with condition of (x &lt;= 0.0)
1867     // There are four possible paths to region node and phi node
1868     RegionNode *r = new RegionNode(4);
1869     Node *phi = new PhiNode(r, Type::DOUBLE);
1870 
1871     // Build the first if node: if (x &lt;= 0.0)
1872     // Node for 0 constant
1873     Node *zeronode = makecon(TypeD::ZERO);
1874     // Check x:0
1875     Node *cmp = _gvn.transform(new CmpDNode(x, zeronode));
1876     // Check: If (x&lt;=0) then go complex path
1877     Node *bol1 = _gvn.transform(new BoolNode( cmp, BoolTest::le ));
1878     // Branch either way
1879     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1880     // Fast path taken; set region slot 3
1881     Node *fast_taken = _gvn.transform(new IfFalseNode(if1));
1882     r-&gt;init_req(3,fast_taken); // Capture fast-control
1883 
1884     // Fast path not-taken, i.e. slow path
1885     Node *complex_path = _gvn.transform(new IfTrueNode(if1));
1886 
1887     // Set fast path result
1888     Node *fast_result = _gvn.transform(new PowDNode(C, control(), x, y));
1889     phi-&gt;init_req(3, fast_result);
1890 
1891     // Complex path
1892     // Build the second if node (if y is long)
1893     // Node for (long)y
1894     Node *longy = _gvn.transform(new ConvD2LNode(y));
1895     // Node for (double)((long) y)
1896     Node *doublelongy= _gvn.transform(new ConvL2DNode(longy));
1897     // Check (double)((long) y) : y
1898     Node *cmplongy= _gvn.transform(new CmpDNode(doublelongy, y));
1899     // Check if (y isn't long) then go to slow path
1900 
1901     Node *bol2 = _gvn.transform(new BoolNode( cmplongy, BoolTest::ne ));
1902     // Branch either way
1903     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1904     Node* ylong_path = _gvn.transform(new IfFalseNode(if2));
1905 
1906     Node *slow_path = _gvn.transform(new IfTrueNode(if2));
1907 
1908     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1909     // Node for constant 1
1910     Node *conone = longcon(1);
1911     // 1&amp; (long)y
1912     Node *signnode= _gvn.transform(new AndLNode(conone, longy));
1913 
1914     // A huge number is always even. Detect a huge number by checking
1915     // if y + 1 == y and set integer to be tested for parity to 0.
1916     // Required for corner case:
1917     // (long)9.223372036854776E18 = max_jlong
1918     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1919     // max_jlong is odd but 9.223372036854776E18 is even
1920     Node* yplus1 = _gvn.transform(new AddDNode(y, makecon(TypeD::make(1))));
1921     Node *cmpyplus1= _gvn.transform(new CmpDNode(yplus1, y));
1922     Node *bolyplus1 = _gvn.transform(new BoolNode( cmpyplus1, BoolTest::eq ));
1923     Node* correctedsign = NULL;
1924     if (ConditionalMoveLimit != 0) {
1925       correctedsign = _gvn.transform(CMoveNode::make(NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1926     } else {
1927       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1928       RegionNode *r = new RegionNode(3);
1929       Node *phi = new PhiNode(r, TypeLong::LONG);
1930       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyplus1)));
1931       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyplus1)));
1932       phi-&gt;init_req(1, signnode);
1933       phi-&gt;init_req(2, longcon(0));
1934       correctedsign = _gvn.transform(phi);
1935       ylong_path = _gvn.transform(r);
1936       record_for_igvn(r);
1937     }
1938 
1939     // zero node
1940     Node *conzero = longcon(0);
1941     // Check (1&amp;(long)y)==0?
1942     Node *cmpeq1 = _gvn.transform(new CmpLNode(correctedsign, conzero));
1943     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1944     Node *bol3 = _gvn.transform(new BoolNode( cmpeq1, BoolTest::ne ));
1945     // abs(x)
1946     Node *absx=_gvn.transform(new AbsDNode(x));
1947     // abs(x)^y
1948     Node *absxpowy = _gvn.transform(new PowDNode(C, control(), absx, y));
1949     // -abs(x)^y
1950     Node *negabsxpowy = _gvn.transform(new NegDNode (absxpowy));
1951     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1952     Node *signresult = NULL;
1953     if (ConditionalMoveLimit != 0) {
1954       signresult = _gvn.transform(CMoveNode::make(NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1955     } else {
1956       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1957       RegionNode *r = new RegionNode(3);
1958       Node *phi = new PhiNode(r, Type::DOUBLE);
1959       r-&gt;init_req(1, _gvn.transform(new IfFalseNode(ifyeven)));
1960       r-&gt;init_req(2, _gvn.transform(new IfTrueNode(ifyeven)));
1961       phi-&gt;init_req(1, absxpowy);
1962       phi-&gt;init_req(2, negabsxpowy);
1963       signresult = _gvn.transform(phi);
1964       ylong_path = _gvn.transform(r);
1965       record_for_igvn(r);
1966     }
1967     // Set complex path fast result
1968     r-&gt;init_req(2, ylong_path);
1969     phi-&gt;init_req(2, signresult);
1970 
1971     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1972     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1973     r-&gt;init_req(1,slow_path);
1974     phi-&gt;init_req(1,slow_result);
1975 
1976     // Post merge
1977     set_control(_gvn.transform(r));
1978     record_for_igvn(r);
1979     result = _gvn.transform(phi);
1980   }
1981 
1982   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1983 
1984   // control from finish_pow_exp is now input to the region node
1985   region_node-&gt;set_req(2, control());
1986   // the result from finish_pow_exp is now input to the phi node
1987   phi_node-&gt;init_req(2, result);
1988   set_control(_gvn.transform(region_node));
1989   record_for_igvn(region_node);
1990   set_result(_gvn.transform(phi_node));
1991 
1992   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1993   return true;
1994 }
1995 
1996 //------------------------------runtime_math-----------------------------
1997 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1998   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1999          "must be (DD)D or (D)D type");
2000 
2001   // Inputs
2002   Node* a = round_double_node(argument(0));
2003   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
2004 
2005   const TypePtr* no_memory_effects = NULL;
2006   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
2007                                  no_memory_effects,
2008                                  a, top(), b, b ? top() : NULL);
2009   Node* value = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+0));
2010 #ifdef ASSERT
2011   Node* value_top = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+1));
2012   assert(value_top == top(), "second value must be top");
2013 #endif
2014 
2015   set_result(value);
2016   return true;
2017 }
2018 
2019 //------------------------------inline_math_native-----------------------------
2020 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
2021 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
2022   switch (id) {
2023     // These intrinsics are not properly supported on all hardware
2024   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
2025     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
2026   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
2027     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
2028   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
2029     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
2030 
2031   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
2032     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
2033   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
2034     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
2035 
2036     // These intrinsics are supported on all hardware
2037   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
2038   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
2039 
2040   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
2041     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
2042   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
2043     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
2044 #undef FN_PTR
2045 
2046    // These intrinsics are not yet correctly implemented
2047   case vmIntrinsics::_datan2:
2048     return false;
2049 
2050   default:
2051     fatal_unexpected_iid(id);
2052     return false;
2053   }
2054 }
2055 
2056 static bool is_simple_name(Node* n) {
2057   return (n-&gt;req() == 1         // constant
2058           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
2059           || n-&gt;is_Proj()       // parameter or return value
2060           || n-&gt;is_Phi()        // local of some sort
2061           );
2062 }
2063 
2064 //----------------------------inline_min_max-----------------------------------
2065 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
2066   set_result(generate_min_max(id, argument(0), argument(1)));
2067   return true;
2068 }
2069 
2070 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
2071   Node* bol = _gvn.transform( new BoolNode(test, BoolTest::overflow) );
2072   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
2073   Node* fast_path = _gvn.transform( new IfFalseNode(check));
2074   Node* slow_path = _gvn.transform( new IfTrueNode(check) );
2075 
2076   {
2077     PreserveJVMState pjvms(this);
2078     PreserveReexecuteState preexecs(this);
2079     jvms()-&gt;set_should_reexecute(true);
2080 
2081     set_control(slow_path);
2082     set_i_o(i_o());
2083 
2084     uncommon_trap(Deoptimization::Reason_intrinsic,
2085                   Deoptimization::Action_none);
2086   }
2087 
2088   set_control(fast_path);
2089   set_result(math);
2090 }
2091 
2092 template &lt;typename OverflowOp&gt;
2093 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2094   typedef typename OverflowOp::MathOp MathOp;
2095 
2096   MathOp* mathOp = new MathOp(arg1, arg2);
2097   Node* operation = _gvn.transform( mathOp );
2098   Node* ofcheck = _gvn.transform( new OverflowOp(arg1, arg2) );
2099   inline_math_mathExact(operation, ofcheck);
2100   return true;
2101 }
2102 
2103 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2104   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2105 }
2106 
2107 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2108   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2109 }
2110 
2111 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2112   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2113 }
2114 
2115 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2116   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2117 }
2118 
2119 bool LibraryCallKit::inline_math_negateExactI() {
2120   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2121 }
2122 
2123 bool LibraryCallKit::inline_math_negateExactL() {
2124   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2125 }
2126 
2127 bool LibraryCallKit::inline_math_multiplyExactI() {
2128   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2129 }
2130 
2131 bool LibraryCallKit::inline_math_multiplyExactL() {
2132   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2133 }
2134 
2135 Node*
2136 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2137   // These are the candidate return value:
2138   Node* xvalue = x0;
2139   Node* yvalue = y0;
2140 
2141   if (xvalue == yvalue) {
2142     return xvalue;
2143   }
2144 
2145   bool want_max = (id == vmIntrinsics::_max);
2146 
2147   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2148   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2149   if (txvalue == NULL || tyvalue == NULL)  return top();
2150   // This is not really necessary, but it is consistent with a
2151   // hypothetical MaxINode::Value method:
2152   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2153 
2154   // %%% This folding logic should (ideally) be in a different place.
2155   // Some should be inside IfNode, and there to be a more reliable
2156   // transformation of ?: style patterns into cmoves.  We also want
2157   // more powerful optimizations around cmove and min/max.
2158 
2159   // Try to find a dominating comparison of these guys.
2160   // It can simplify the index computation for Arrays.copyOf
2161   // and similar uses of System.arraycopy.
2162   // First, compute the normalized version of CmpI(x, y).
2163   int   cmp_op = Op_CmpI;
2164   Node* xkey = xvalue;
2165   Node* ykey = yvalue;
2166   Node* ideal_cmpxy = _gvn.transform(new CmpINode(xkey, ykey));
2167   if (ideal_cmpxy-&gt;is_Cmp()) {
2168     // E.g., if we have CmpI(length - offset, count),
2169     // it might idealize to CmpI(length, count + offset)
2170     cmp_op = ideal_cmpxy-&gt;Opcode();
2171     xkey = ideal_cmpxy-&gt;in(1);
2172     ykey = ideal_cmpxy-&gt;in(2);
2173   }
2174 
2175   // Start by locating any relevant comparisons.
2176   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2177   Node* cmpxy = NULL;
2178   Node* cmpyx = NULL;
2179   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2180     Node* cmp = start_from-&gt;fast_out(k);
2181     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2182         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2183         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2184       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2185       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2186     }
2187   }
2188 
2189   const int NCMPS = 2;
2190   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2191   int cmpn;
2192   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2193     if (cmps[cmpn] != NULL)  break;     // find a result
2194   }
2195   if (cmpn &lt; NCMPS) {
2196     // Look for a dominating test that tells us the min and max.
2197     int depth = 0;                // Limit search depth for speed
2198     Node* dom = control();
2199     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2200       if (++depth &gt;= 100)  break;
2201       Node* ifproj = dom;
2202       if (!ifproj-&gt;is_Proj())  continue;
2203       Node* iff = ifproj-&gt;in(0);
2204       if (!iff-&gt;is_If())  continue;
2205       Node* bol = iff-&gt;in(1);
2206       if (!bol-&gt;is_Bool())  continue;
2207       Node* cmp = bol-&gt;in(1);
2208       if (cmp == NULL)  continue;
2209       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2210         if (cmps[cmpn] == cmp)  break;
2211       if (cmpn == NCMPS)  continue;
2212       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2213       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2214       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2215       // At this point, we know that 'x btest y' is true.
2216       switch (btest) {
2217       case BoolTest::eq:
2218         // They are proven equal, so we can collapse the min/max.
2219         // Either value is the answer.  Choose the simpler.
2220         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2221           return yvalue;
2222         return xvalue;
2223       case BoolTest::lt:          // x &lt; y
2224       case BoolTest::le:          // x &lt;= y
2225         return (want_max ? yvalue : xvalue);
2226       case BoolTest::gt:          // x &gt; y
2227       case BoolTest::ge:          // x &gt;= y
2228         return (want_max ? xvalue : yvalue);
2229       }
2230     }
2231   }
2232 
2233   // We failed to find a dominating test.
2234   // Let's pick a test that might GVN with prior tests.
2235   Node*          best_bol   = NULL;
2236   BoolTest::mask best_btest = BoolTest::illegal;
2237   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2238     Node* cmp = cmps[cmpn];
2239     if (cmp == NULL)  continue;
2240     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2241       Node* bol = cmp-&gt;fast_out(j);
2242       if (!bol-&gt;is_Bool())  continue;
2243       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2244       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2245       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2246       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2247         best_bol   = bol-&gt;as_Bool();
2248         best_btest = btest;
2249       }
2250     }
2251   }
2252 
2253   Node* answer_if_true  = NULL;
2254   Node* answer_if_false = NULL;
2255   switch (best_btest) {
2256   default:
2257     if (cmpxy == NULL)
2258       cmpxy = ideal_cmpxy;
2259     best_bol = _gvn.transform(new BoolNode(cmpxy, BoolTest::lt));
2260     // and fall through:
2261   case BoolTest::lt:          // x &lt; y
2262   case BoolTest::le:          // x &lt;= y
2263     answer_if_true  = (want_max ? yvalue : xvalue);
2264     answer_if_false = (want_max ? xvalue : yvalue);
2265     break;
2266   case BoolTest::gt:          // x &gt; y
2267   case BoolTest::ge:          // x &gt;= y
2268     answer_if_true  = (want_max ? xvalue : yvalue);
2269     answer_if_false = (want_max ? yvalue : xvalue);
2270     break;
2271   }
2272 
2273   jint hi, lo;
2274   if (want_max) {
2275     // We can sharpen the minimum.
2276     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2277     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2278   } else {
2279     // We can sharpen the maximum.
2280     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2281     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2282   }
2283 
2284   // Use a flow-free graph structure, to avoid creating excess control edges
2285   // which could hinder other optimizations.
2286   // Since Math.min/max is often used with arraycopy, we want
2287   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2288   Node* cmov = CMoveNode::make(NULL, best_bol,
2289                                answer_if_false, answer_if_true,
2290                                TypeInt::make(lo, hi, widen));
2291 
2292   return _gvn.transform(cmov);
2293 
2294   /*
2295   // This is not as desirable as it may seem, since Min and Max
2296   // nodes do not have a full set of optimizations.
2297   // And they would interfere, anyway, with 'if' optimizations
2298   // and with CMoveI canonical forms.
2299   switch (id) {
2300   case vmIntrinsics::_min:
2301     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2302   case vmIntrinsics::_max:
2303     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2304   default:
2305     ShouldNotReachHere();
2306   }
2307   */
2308 }
2309 
2310 inline int
2311 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2312   const TypePtr* base_type = TypePtr::NULL_PTR;
2313   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2314   if (base_type == NULL) {
2315     // Unknown type.
2316     return Type::AnyPtr;
2317   } else if (base_type == TypePtr::NULL_PTR) {
2318     // Since this is a NULL+long form, we have to switch to a rawptr.
2319     base   = _gvn.transform(new CastX2PNode(offset));
2320     offset = MakeConX(0);
2321     return Type::RawPtr;
2322   } else if (base_type-&gt;base() == Type::RawPtr) {
2323     return Type::RawPtr;
2324   } else if (base_type-&gt;isa_oopptr()) {
2325     // Base is never null =&gt; always a heap address.
2326     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2327       return Type::OopPtr;
2328     }
2329     // Offset is small =&gt; always a heap address.
2330     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2331     if (offset_type != NULL &amp;&amp;
2332         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2333         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2334         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2335       return Type::OopPtr;
2336     }
2337     // Otherwise, it might either be oop+off or NULL+addr.
2338     return Type::AnyPtr;
2339   } else {
2340     // No information:
2341     return Type::AnyPtr;
2342   }
2343 }
2344 
2345 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2346   int kind = classify_unsafe_addr(base, offset);
2347   if (kind == Type::RawPtr) {
2348     return basic_plus_adr(top(), base, offset);
2349   } else {
2350     return basic_plus_adr(base, offset);
2351   }
2352 }
2353 
2354 //--------------------------inline_number_methods-----------------------------
2355 // inline int     Integer.numberOfLeadingZeros(int)
2356 // inline int        Long.numberOfLeadingZeros(long)
2357 //
2358 // inline int     Integer.numberOfTrailingZeros(int)
2359 // inline int        Long.numberOfTrailingZeros(long)
2360 //
2361 // inline int     Integer.bitCount(int)
2362 // inline int        Long.bitCount(long)
2363 //
2364 // inline char  Character.reverseBytes(char)
2365 // inline short     Short.reverseBytes(short)
2366 // inline int     Integer.reverseBytes(int)
2367 // inline long       Long.reverseBytes(long)
2368 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2369   Node* arg = argument(0);
2370   Node* n;
2371   switch (id) {
2372   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new CountLeadingZerosINode( arg);  break;
2373   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new CountLeadingZerosLNode( arg);  break;
2374   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new CountTrailingZerosINode(arg);  break;
2375   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new CountTrailingZerosLNode(arg);  break;
2376   case vmIntrinsics::_bitCount_i:               n = new PopCountINode(          arg);  break;
2377   case vmIntrinsics::_bitCount_l:               n = new PopCountLNode(          arg);  break;
2378   case vmIntrinsics::_reverseBytes_c:           n = new ReverseBytesUSNode(0,   arg);  break;
2379   case vmIntrinsics::_reverseBytes_s:           n = new ReverseBytesSNode( 0,   arg);  break;
2380   case vmIntrinsics::_reverseBytes_i:           n = new ReverseBytesINode( 0,   arg);  break;
2381   case vmIntrinsics::_reverseBytes_l:           n = new ReverseBytesLNode( 0,   arg);  break;
2382   default:  fatal_unexpected_iid(id);  break;
2383   }
2384   set_result(_gvn.transform(n));
2385   return true;
2386 }
2387 
2388 //----------------------------inline_unsafe_access----------------------------
2389 
2390 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2391 
2392 // Helper that guards and inserts a pre-barrier.
2393 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2394                                         Node* pre_val, bool need_mem_bar) {
2395   // We could be accessing the referent field of a reference object. If so, when G1
2396   // is enabled, we need to log the value in the referent field in an SATB buffer.
2397   // This routine performs some compile time filters and generates suitable
2398   // runtime filters that guard the pre-barrier code.
2399   // Also add memory barrier for non volatile load from the referent field
2400   // to prevent commoning of loads across safepoint.
2401   if (!UseG1GC &amp;&amp; !need_mem_bar)
2402     return;
2403 
2404   // Some compile time checks.
2405 
2406   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2407   const TypeX* otype = offset-&gt;find_intptr_t_type();
2408   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2409       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2410     // Constant offset but not the reference_offset so just return
2411     return;
2412   }
2413 
2414   // We only need to generate the runtime guards for instances.
2415   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2416   if (btype != NULL) {
2417     if (btype-&gt;isa_aryptr()) {
2418       // Array type so nothing to do
2419       return;
2420     }
2421 
2422     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2423     if (itype != NULL) {
2424       // Can the klass of base_oop be statically determined to be
2425       // _not_ a sub-class of Reference and _not_ Object?
2426       ciKlass* klass = itype-&gt;klass();
2427       if ( klass-&gt;is_loaded() &amp;&amp;
2428           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2429           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2430         return;
2431       }
2432     }
2433   }
2434 
2435   // The compile time filters did not reject base_oop/offset so
2436   // we need to generate the following runtime filters
2437   //
2438   // if (offset == java_lang_ref_Reference::_reference_offset) {
2439   //   if (instance_of(base, java.lang.ref.Reference)) {
2440   //     pre_barrier(_, pre_val, ...);
2441   //   }
2442   // }
2443 
2444   float likely   = PROB_LIKELY(  0.999);
2445   float unlikely = PROB_UNLIKELY(0.999);
2446 
2447   IdealKit ideal(this);
2448 #define __ ideal.
2449 
2450   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2451 
2452   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2453       // Update graphKit memory and control from IdealKit.
2454       sync_kit(ideal);
2455 
2456       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2457       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2458 
2459       // Update IdealKit memory and control from graphKit.
2460       __ sync_kit(this);
2461 
2462       Node* one = __ ConI(1);
2463       // is_instof == 0 if base_oop == NULL
2464       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2465 
2466         // Update graphKit from IdeakKit.
2467         sync_kit(ideal);
2468 
2469         // Use the pre-barrier to record the value in the referent field
2470         pre_barrier(false /* do_load */,
2471                     __ ctrl(),
2472                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2473                     pre_val /* pre_val */,
2474                     T_OBJECT);
2475         if (need_mem_bar) {
2476           // Add memory barrier to prevent commoning reads from this field
2477           // across safepoint since GC can change its value.
2478           insert_mem_bar(Op_MemBarCPUOrder);
2479         }
2480         // Update IdealKit from graphKit.
2481         __ sync_kit(this);
2482 
2483       } __ end_if(); // _ref_type != ref_none
2484   } __ end_if(); // offset == referent_offset
2485 
2486   // Final sync IdealKit and GraphKit.
2487   final_sync(ideal);
2488 #undef __
2489 }
2490 
2491 
2492 // Interpret Unsafe.fieldOffset cookies correctly:
2493 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2494 
2495 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2496   // Attempt to infer a sharper value type from the offset and base type.
2497   ciKlass* sharpened_klass = NULL;
2498 
2499   // See if it is an instance field, with an object type.
2500   if (alias_type-&gt;field() != NULL) {
2501     assert(!is_native_ptr, "native pointer op cannot use a java address");
2502     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2503       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2504     }
2505   }
2506 
2507   // See if it is a narrow oop array.
2508   if (adr_type-&gt;isa_aryptr()) {
2509     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2510       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2511       if (elem_type != NULL) {
2512         sharpened_klass = elem_type-&gt;klass();
2513       }
2514     }
2515   }
2516 
2517   // The sharpened class might be unloaded if there is no class loader
2518   // contraint in place.
2519   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2520     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2521 
2522 #ifndef PRODUCT
2523     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2524       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2525       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2526     }
2527 #endif
2528     // Sharpen the value type.
2529     return tjp;
2530   }
2531   return NULL;
2532 }
2533 
2534 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2535   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2536 
2537 #ifndef PRODUCT
2538   {
2539     ResourceMark rm;
2540     // Check the signatures.
2541     ciSignature* sig = callee()-&gt;signature();
2542 #ifdef ASSERT
2543     if (!is_store) {
2544       // Object getObject(Object base, int/long offset), etc.
2545       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2546       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2547           rtype = T_ADDRESS;  // it is really a C void*
2548       assert(rtype == type, "getter must return the expected value");
2549       if (!is_native_ptr) {
2550         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2551         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2552         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2553       } else {
2554         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2555         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2556       }
2557     } else {
2558       // void putObject(Object base, int/long offset, Object x), etc.
2559       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2560       if (!is_native_ptr) {
2561         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2562         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2563         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2564       } else {
2565         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2566         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2567       }
2568       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2569       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2570         vtype = T_ADDRESS;  // it is really a C void*
2571       assert(vtype == type, "putter must accept the expected value");
2572     }
2573 #endif // ASSERT
2574  }
2575 #endif //PRODUCT
2576 
2577   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2578 
2579   Node* receiver = argument(0);  // type: oop
2580 
2581   // Build address expression.
2582   Node* adr;
2583   Node* heap_base_oop = top();
2584   Node* offset = top();
2585   Node* val;
2586 
2587   if (!is_native_ptr) {
2588     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2589     Node* base = argument(1);  // type: oop
2590     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2591     offset = argument(2);  // type: long
2592     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2593     // to be plain byte offsets, which are also the same as those accepted
2594     // by oopDesc::field_base.
2595     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2596            "fieldOffset must be byte-scaled");
2597     // 32-bit machines ignore the high half!
2598     offset = ConvL2X(offset);
2599     adr = make_unsafe_address(base, offset);
2600     heap_base_oop = base;
2601     val = is_store ? argument(4) : NULL;
2602   } else {
2603     Node* ptr = argument(1);  // type: long
2604     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2605     adr = make_unsafe_address(NULL, ptr);
2606     val = is_store ? argument(3) : NULL;
2607   }
2608 
2609   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2610 
2611   // First guess at the value type.
2612   const Type *value_type = Type::get_const_basic_type(type);
2613 
2614   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2615   // there was not enough information to nail it down.
2616   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2617   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2618 
2619   // We will need memory barriers unless we can determine a unique
2620   // alias category for this reference.  (Note:  If for some reason
2621   // the barriers get omitted and the unsafe reference begins to "pollute"
2622   // the alias analysis of the rest of the graph, either Compile::can_alias
2623   // or Compile::must_alias will throw a diagnostic assert.)
2624   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2625 
2626   // If we are reading the value of the referent field of a Reference
2627   // object (either by using Unsafe directly or through reflection)
2628   // then, if G1 is enabled, we need to record the referent in an
2629   // SATB log buffer using the pre-barrier mechanism.
2630   // Also we need to add memory barrier to prevent commoning reads
2631   // from this field across safepoint since GC can change its value.
2632   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2633                            offset != top() &amp;&amp; heap_base_oop != top();
2634 
2635   if (!is_store &amp;&amp; type == T_OBJECT) {
2636     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2637     if (tjp != NULL) {
2638       value_type = tjp;
2639     }
2640   }
2641 
2642   receiver = null_check(receiver);
2643   if (stopped()) {
2644     return true;
2645   }
2646   // Heap pointers get a null-check from the interpreter,
2647   // as a courtesy.  However, this is not guaranteed by Unsafe,
2648   // and it is not possible to fully distinguish unintended nulls
2649   // from intended ones in this API.
2650 
2651   if (is_volatile) {
2652     // We need to emit leading and trailing CPU membars (see below) in
2653     // addition to memory membars when is_volatile. This is a little
2654     // too strong, but avoids the need to insert per-alias-type
2655     // volatile membars (for stores; compare Parse::do_put_xxx), which
2656     // we cannot do effectively here because we probably only have a
2657     // rough approximation of type.
2658     need_mem_bar = true;
2659     // For Stores, place a memory ordering barrier now.
2660     if (is_store) {
2661       insert_mem_bar(Op_MemBarRelease);
2662     } else {
2663       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2664         insert_mem_bar(Op_MemBarVolatile);
2665       }
2666     }
2667   }
2668 
2669   // Memory barrier to prevent normal and 'unsafe' accesses from
2670   // bypassing each other.  Happens after null checks, so the
2671   // exception paths do not take memory state from the memory barrier,
2672   // so there's no problems making a strong assert about mixing users
2673   // of safe &amp; unsafe memory.
2674   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2675 
2676   if (!is_store) {
2677     MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2678     // To be valid, unsafe loads may depend on other conditions than
2679     // the one that guards them: pin the Load node
2680     Node* p = make_load(control(), adr, value_type, type, adr_type, mo, LoadNode::Pinned, is_volatile);
2681     // load value
2682     switch (type) {
2683     case T_BOOLEAN:
2684     case T_CHAR:
2685     case T_BYTE:
2686     case T_SHORT:
2687     case T_INT:
2688     case T_LONG:
2689     case T_FLOAT:
2690     case T_DOUBLE:
2691       break;
2692     case T_OBJECT:
2693       if (need_read_barrier) {
2694         insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2695       }
2696       break;
2697     case T_ADDRESS:
2698       // Cast to an int type.
2699       p = _gvn.transform(new CastP2XNode(NULL, p));
2700       p = ConvX2UL(p);
2701       break;
2702     default:
2703       fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2704       break;
2705     }
2706     // The load node has the control of the preceding MemBarCPUOrder.  All
2707     // following nodes will have the control of the MemBarCPUOrder inserted at
2708     // the end of this method.  So, pushing the load onto the stack at a later
2709     // point is fine.
2710     set_result(p);
2711   } else {
2712     // place effect of store into memory
2713     switch (type) {
2714     case T_DOUBLE:
2715       val = dstore_rounding(val);
2716       break;
2717     case T_ADDRESS:
2718       // Repackage the long as a pointer.
2719       val = ConvL2X(val);
2720       val = _gvn.transform(new CastX2PNode(val));
2721       break;
2722     }
2723 
2724     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2725     if (type != T_OBJECT ) {
2726       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2727     } else {
2728       // Possibly an oop being stored to Java heap or native memory
2729       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2730         // oop to Java heap.
2731         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2732       } else {
2733         // We can't tell at compile time if we are storing in the Java heap or outside
2734         // of it. So we need to emit code to conditionally do the proper type of
2735         // store.
2736 
2737         IdealKit ideal(this);
2738 #define __ ideal.
2739         // QQQ who knows what probability is here??
2740         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2741           // Sync IdealKit and graphKit.
2742           sync_kit(ideal);
2743           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2744           // Update IdealKit memory.
2745           __ sync_kit(this);
2746         } __ else_(); {
2747           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2748         } __ end_if();
2749         // Final sync IdealKit and GraphKit.
2750         final_sync(ideal);
2751 #undef __
2752       }
2753     }
2754   }
2755 
2756   if (is_volatile) {
2757     if (!is_store) {
2758       insert_mem_bar(Op_MemBarAcquire);
2759     } else {
2760       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2761         insert_mem_bar(Op_MemBarVolatile);
2762       }
2763     }
2764   }
2765 
2766   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2767 
2768   return true;
2769 }
2770 
2771 //----------------------------inline_unsafe_load_store----------------------------
2772 // This method serves a couple of different customers (depending on LoadStoreKind):
2773 //
2774 // LS_cmpxchg:
2775 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2776 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2777 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2778 //
2779 // LS_xadd:
2780 //   public int  getAndAddInt( Object o, long offset, int  delta)
2781 //   public long getAndAddLong(Object o, long offset, long delta)
2782 //
2783 // LS_xchg:
2784 //   int    getAndSet(Object o, long offset, int    newValue)
2785 //   long   getAndSet(Object o, long offset, long   newValue)
2786 //   Object getAndSet(Object o, long offset, Object newValue)
2787 //
2788 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2789   // This basic scheme here is the same as inline_unsafe_access, but
2790   // differs in enough details that combining them would make the code
2791   // overly confusing.  (This is a true fact! I originally combined
2792   // them, but even I was confused by it!) As much code/comments as
2793   // possible are retained from inline_unsafe_access though to make
2794   // the correspondences clearer. - dl
2795 
2796   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2797 
2798 #ifndef PRODUCT
2799   BasicType rtype;
2800   {
2801     ResourceMark rm;
2802     // Check the signatures.
2803     ciSignature* sig = callee()-&gt;signature();
2804     rtype = sig-&gt;return_type()-&gt;basic_type();
2805     if (kind == LS_xadd || kind == LS_xchg) {
2806       // Check the signatures.
2807 #ifdef ASSERT
2808       assert(rtype == type, "get and set must return the expected type");
2809       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2810       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2811       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2812       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2813 #endif // ASSERT
2814     } else if (kind == LS_cmpxchg) {
2815       // Check the signatures.
2816 #ifdef ASSERT
2817       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2818       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2819       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2820       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2821 #endif // ASSERT
2822     } else {
2823       ShouldNotReachHere();
2824     }
2825   }
2826 #endif //PRODUCT
2827 
2828   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2829 
2830   // Get arguments:
2831   Node* receiver = NULL;
2832   Node* base     = NULL;
2833   Node* offset   = NULL;
2834   Node* oldval   = NULL;
2835   Node* newval   = NULL;
2836   if (kind == LS_cmpxchg) {
2837     const bool two_slot_type = type2size[type] == 2;
2838     receiver = argument(0);  // type: oop
2839     base     = argument(1);  // type: oop
2840     offset   = argument(2);  // type: long
2841     oldval   = argument(4);  // type: oop, int, or long
2842     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2843   } else if (kind == LS_xadd || kind == LS_xchg){
2844     receiver = argument(0);  // type: oop
2845     base     = argument(1);  // type: oop
2846     offset   = argument(2);  // type: long
2847     oldval   = NULL;
2848     newval   = argument(4);  // type: oop, int, or long
2849   }
2850 
2851   // Null check receiver.
2852   receiver = null_check(receiver);
2853   if (stopped()) {
2854     return true;
2855   }
2856 
2857   // Build field offset expression.
2858   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2859   // to be plain byte offsets, which are also the same as those accepted
2860   // by oopDesc::field_base.
2861   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2862   // 32-bit machines ignore the high half of long offsets
2863   offset = ConvL2X(offset);
2864   Node* adr = make_unsafe_address(base, offset);
2865   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2866 
2867   // For CAS, unlike inline_unsafe_access, there seems no point in
2868   // trying to refine types. Just use the coarse types here.
2869   const Type *value_type = Type::get_const_basic_type(type);
2870   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2871   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2872 
2873   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2874     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2875     if (tjp != NULL) {
2876       value_type = tjp;
2877     }
2878   }
2879 
2880   int alias_idx = C-&gt;get_alias_index(adr_type);
2881 
2882   // Memory-model-wise, a LoadStore acts like a little synchronized
2883   // block, so needs barriers on each side.  These don't translate
2884   // into actual barriers on most machines, but we still need rest of
2885   // compiler to respect ordering.
2886 
2887   insert_mem_bar(Op_MemBarRelease);
2888   insert_mem_bar(Op_MemBarCPUOrder);
2889 
2890   // 4984716: MemBars must be inserted before this
2891   //          memory node in order to avoid a false
2892   //          dependency which will confuse the scheduler.
2893   Node *mem = memory(alias_idx);
2894 
2895   // For now, we handle only those cases that actually exist: ints,
2896   // longs, and Object. Adding others should be straightforward.
2897   Node* load_store;
2898   switch(type) {
2899   case T_INT:
2900     if (kind == LS_xadd) {
2901       load_store = _gvn.transform(new GetAndAddINode(control(), mem, adr, newval, adr_type));
2902     } else if (kind == LS_xchg) {
2903       load_store = _gvn.transform(new GetAndSetINode(control(), mem, adr, newval, adr_type));
2904     } else if (kind == LS_cmpxchg) {
2905       load_store = _gvn.transform(new CompareAndSwapINode(control(), mem, adr, newval, oldval));
2906     } else {
2907       ShouldNotReachHere();
2908     }
2909     break;
2910   case T_LONG:
2911     if (kind == LS_xadd) {
2912       load_store = _gvn.transform(new GetAndAddLNode(control(), mem, adr, newval, adr_type));
2913     } else if (kind == LS_xchg) {
2914       load_store = _gvn.transform(new GetAndSetLNode(control(), mem, adr, newval, adr_type));
2915     } else if (kind == LS_cmpxchg) {
2916       load_store = _gvn.transform(new CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2917     } else {
2918       ShouldNotReachHere();
2919     }
2920     break;
2921   case T_OBJECT:
2922     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2923     // could be delayed during Parse (for example, in adjust_map_after_if()).
2924     // Execute transformation here to avoid barrier generation in such case.
2925     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2926       newval = _gvn.makecon(TypePtr::NULL_PTR);
2927 
2928     // Reference stores need a store barrier.
2929     if (kind == LS_xchg) {
2930       // If pre-barrier must execute before the oop store, old value will require do_load here.
2931       if (!can_move_pre_barrier()) {
2932         pre_barrier(true /* do_load*/,
2933                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2934                     NULL /* pre_val*/,
2935                     T_OBJECT);
2936       } // Else move pre_barrier to use load_store value, see below.
2937     } else if (kind == LS_cmpxchg) {
2938       // Same as for newval above:
2939       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2940         oldval = _gvn.makecon(TypePtr::NULL_PTR);
2941       }
2942       // The only known value which might get overwritten is oldval.
2943       pre_barrier(false /* do_load */,
2944                   control(), NULL, NULL, max_juint, NULL, NULL,
2945                   oldval /* pre_val */,
2946                   T_OBJECT);
2947     } else {
2948       ShouldNotReachHere();
2949     }
2950 
2951 #ifdef _LP64
2952     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2953       Node *newval_enc = _gvn.transform(new EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2954       if (kind == LS_xchg) {
2955         load_store = _gvn.transform(new GetAndSetNNode(control(), mem, adr,
2956                                                        newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2957       } else {
2958         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2959         Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2960         load_store = _gvn.transform(new CompareAndSwapNNode(control(), mem, adr,
2961                                                                 newval_enc, oldval_enc));
2962       }
2963     } else
2964 #endif
2965     {
2966       if (kind == LS_xchg) {
2967         load_store = _gvn.transform(new GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2968       } else {
2969         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2970         load_store = _gvn.transform(new CompareAndSwapPNode(control(), mem, adr, newval, oldval));
2971       }
2972     }
2973     post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2974     break;
2975   default:
2976     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2977     break;
2978   }
2979 
2980   // SCMemProjNodes represent the memory state of a LoadStore. Their
2981   // main role is to prevent LoadStore nodes from being optimized away
2982   // when their results aren't used.
2983   Node* proj = _gvn.transform(new SCMemProjNode(load_store));
2984   set_memory(proj, alias_idx);
2985 
2986   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
2987 #ifdef _LP64
2988     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2989       load_store = _gvn.transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
2990     }
2991 #endif
2992     if (can_move_pre_barrier()) {
2993       // Don't need to load pre_val. The old value is returned by load_store.
2994       // The pre_barrier can execute after the xchg as long as no safepoint
2995       // gets inserted between them.
2996       pre_barrier(false /* do_load */,
2997                   control(), NULL, NULL, max_juint, NULL, NULL,
2998                   load_store /* pre_val */,
2999                   T_OBJECT);
3000     }
3001   }
3002 
3003   // Add the trailing membar surrounding the access
3004   insert_mem_bar(Op_MemBarCPUOrder);
3005   insert_mem_bar(Op_MemBarAcquire);
3006 
3007   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
3008   set_result(load_store);
3009   return true;
3010 }
3011 
3012 //----------------------------inline_unsafe_ordered_store----------------------
3013 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
3014 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
3015 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
3016 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
3017   // This is another variant of inline_unsafe_access, differing in
3018   // that it always issues store-store ("release") barrier and ensures
3019   // store-atomicity (which only matters for "long").
3020 
3021   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3022 
3023 #ifndef PRODUCT
3024   {
3025     ResourceMark rm;
3026     // Check the signatures.
3027     ciSignature* sig = callee()-&gt;signature();
3028 #ifdef ASSERT
3029     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3030     assert(rtype == T_VOID, "must return void");
3031     assert(sig-&gt;count() == 3, "has 3 arguments");
3032     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3033     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3034 #endif // ASSERT
3035   }
3036 #endif //PRODUCT
3037 
3038   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3039 
3040   // Get arguments:
3041   Node* receiver = argument(0);  // type: oop
3042   Node* base     = argument(1);  // type: oop
3043   Node* offset   = argument(2);  // type: long
3044   Node* val      = argument(4);  // type: oop, int, or long
3045 
3046   // Null check receiver.
3047   receiver = null_check(receiver);
3048   if (stopped()) {
3049     return true;
3050   }
3051 
3052   // Build field offset expression.
3053   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3054   // 32-bit machines ignore the high half of long offsets
3055   offset = ConvL2X(offset);
3056   Node* adr = make_unsafe_address(base, offset);
3057   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3058   const Type *value_type = Type::get_const_basic_type(type);
3059   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3060 
3061   insert_mem_bar(Op_MemBarRelease);
3062   insert_mem_bar(Op_MemBarCPUOrder);
3063   // Ensure that the store is atomic for longs:
3064   const bool require_atomic_access = true;
3065   Node* store;
3066   if (type == T_OBJECT) // reference stores need a store barrier.
3067     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3068   else {
3069     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3070   }
3071   insert_mem_bar(Op_MemBarCPUOrder);
3072   return true;
3073 }
3074 
3075 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3076   // Regardless of form, don't allow previous ld/st to move down,
3077   // then issue acquire, release, or volatile mem_bar.
3078   insert_mem_bar(Op_MemBarCPUOrder);
3079   switch(id) {
3080     case vmIntrinsics::_loadFence:
3081       insert_mem_bar(Op_LoadFence);
3082       return true;
3083     case vmIntrinsics::_storeFence:
3084       insert_mem_bar(Op_StoreFence);
3085       return true;
3086     case vmIntrinsics::_fullFence:
3087       insert_mem_bar(Op_MemBarVolatile);
3088       return true;
3089     default:
3090       fatal_unexpected_iid(id);
3091       return false;
3092   }
3093 }
3094 
3095 bool LibraryCallKit::inline_spinloophint() {
3096   insert_mem_bar(Op_SpinLoopHint);
3097   return true;
3098 }
3099 
3100 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3101   if (!kls-&gt;is_Con()) {
3102     return true;
3103   }
3104   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3105   if (klsptr == NULL) {
3106     return true;
3107   }
3108   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3109   // don't need a guard for a klass that is already initialized
3110   return !ik-&gt;is_initialized();
3111 }
3112 
3113 //----------------------------inline_unsafe_allocate---------------------------
3114 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
3115 bool LibraryCallKit::inline_unsafe_allocate() {
3116   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3117 
3118   null_check_receiver();  // null-check, then ignore
3119   Node* cls = null_check(argument(1));
3120   if (stopped())  return true;
3121 
3122   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3123   kls = null_check(kls);
3124   if (stopped())  return true;  // argument was like int.class
3125 
3126   Node* test = NULL;
3127   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3128     // Note:  The argument might still be an illegal value like
3129     // Serializable.class or Object[].class.   The runtime will handle it.
3130     // But we must make an explicit check for initialization.
3131     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3132     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3133     // can generate code to load it as unsigned byte.
3134     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3135     Node* bits = intcon(InstanceKlass::fully_initialized);
3136     test = _gvn.transform(new SubINode(inst, bits));
3137     // The 'test' is non-zero if we need to take a slow path.
3138   }
3139 
3140   Node* obj = new_instance(kls, test);
3141   set_result(obj);
3142   return true;
3143 }
3144 
3145 #ifdef TRACE_HAVE_INTRINSICS
3146 /*
3147  * oop -&gt; myklass
3148  * myklass-&gt;trace_id |= USED
3149  * return myklass-&gt;trace_id &amp; ~0x3
3150  */
3151 bool LibraryCallKit::inline_native_classID() {
3152   null_check_receiver();  // null-check, then ignore
3153   Node* cls = null_check(argument(1), T_OBJECT);
3154   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3155   kls = null_check(kls, T_OBJECT);
3156   ByteSize offset = TRACE_ID_OFFSET;
3157   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3158   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3159   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3160   Node* andl = _gvn.transform(new AndLNode(tvalue, bits));
3161   Node* clsused = longcon(0x01l); // set the class bit
3162   Node* orl = _gvn.transform(new OrLNode(tvalue, clsused));
3163 
3164   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3165   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3166   set_result(andl);
3167   return true;
3168 }
3169 
3170 bool LibraryCallKit::inline_native_threadID() {
3171   Node* tls_ptr = NULL;
3172   Node* cur_thr = generate_current_thread(tls_ptr);
3173   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3174   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3175   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3176 
3177   Node* threadid = NULL;
3178   size_t thread_id_size = OSThread::thread_id_size();
3179   if (thread_id_size == (size_t) BytesPerLong) {
3180     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3181   } else if (thread_id_size == (size_t) BytesPerInt) {
3182     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3183   } else {
3184     ShouldNotReachHere();
3185   }
3186   set_result(threadid);
3187   return true;
3188 }
3189 #endif
3190 
3191 //------------------------inline_native_time_funcs--------------
3192 // inline code for System.currentTimeMillis() and System.nanoTime()
3193 // these have the same type and signature
3194 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3195   const TypeFunc* tf = OptoRuntime::void_long_Type();
3196   const TypePtr* no_memory_effects = NULL;
3197   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3198   Node* value = _gvn.transform(new ProjNode(time, TypeFunc::Parms+0));
3199 #ifdef ASSERT
3200   Node* value_top = _gvn.transform(new ProjNode(time, TypeFunc::Parms+1));
3201   assert(value_top == top(), "second value must be top");
3202 #endif
3203   set_result(value);
3204   return true;
3205 }
3206 
3207 //------------------------inline_native_currentThread------------------
3208 bool LibraryCallKit::inline_native_currentThread() {
3209   Node* junk = NULL;
3210   set_result(generate_current_thread(junk));
3211   return true;
3212 }
3213 
3214 //------------------------inline_native_isInterrupted------------------
3215 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3216 bool LibraryCallKit::inline_native_isInterrupted() {
3217   // Add a fast path to t.isInterrupted(clear_int):
3218   //   (t == Thread.current() &amp;&amp;
3219   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3220   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3221   // So, in the common case that the interrupt bit is false,
3222   // we avoid making a call into the VM.  Even if the interrupt bit
3223   // is true, if the clear_int argument is false, we avoid the VM call.
3224   // However, if the receiver is not currentThread, we must call the VM,
3225   // because there must be some locking done around the operation.
3226 
3227   // We only go to the fast case code if we pass two guards.
3228   // Paths which do not pass are accumulated in the slow_region.
3229 
3230   enum {
3231     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3232     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3233     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3234     PATH_LIMIT
3235   };
3236 
3237   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3238   // out of the function.
3239   insert_mem_bar(Op_MemBarCPUOrder);
3240 
3241   RegionNode* result_rgn = new RegionNode(PATH_LIMIT);
3242   PhiNode*    result_val = new PhiNode(result_rgn, TypeInt::BOOL);
3243 
3244   RegionNode* slow_region = new RegionNode(1);
3245   record_for_igvn(slow_region);
3246 
3247   // (a) Receiving thread must be the current thread.
3248   Node* rec_thr = argument(0);
3249   Node* tls_ptr = NULL;
3250   Node* cur_thr = generate_current_thread(tls_ptr);
3251   Node* cmp_thr = _gvn.transform(new CmpPNode(cur_thr, rec_thr));
3252   Node* bol_thr = _gvn.transform(new BoolNode(cmp_thr, BoolTest::ne));
3253 
3254   generate_slow_guard(bol_thr, slow_region);
3255 
3256   // (b) Interrupt bit on TLS must be false.
3257   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3258   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3259   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3260 
3261   // Set the control input on the field _interrupted read to prevent it floating up.
3262   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3263   Node* cmp_bit = _gvn.transform(new CmpINode(int_bit, intcon(0)));
3264   Node* bol_bit = _gvn.transform(new BoolNode(cmp_bit, BoolTest::ne));
3265 
3266   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3267 
3268   // First fast path:  if (!TLS._interrupted) return false;
3269   Node* false_bit = _gvn.transform(new IfFalseNode(iff_bit));
3270   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3271   result_val-&gt;init_req(no_int_result_path, intcon(0));
3272 
3273   // drop through to next case
3274   set_control( _gvn.transform(new IfTrueNode(iff_bit)));
3275 
3276 #ifndef TARGET_OS_FAMILY_windows
3277   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3278   Node* clr_arg = argument(1);
3279   Node* cmp_arg = _gvn.transform(new CmpINode(clr_arg, intcon(0)));
3280   Node* bol_arg = _gvn.transform(new BoolNode(cmp_arg, BoolTest::ne));
3281   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3282 
3283   // Second fast path:  ... else if (!clear_int) return true;
3284   Node* false_arg = _gvn.transform(new IfFalseNode(iff_arg));
3285   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3286   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3287 
3288   // drop through to next case
3289   set_control( _gvn.transform(new IfTrueNode(iff_arg)));
3290 #else
3291   // To return true on Windows you must read the _interrupted field
3292   // and check the the event state i.e. take the slow path.
3293 #endif // TARGET_OS_FAMILY_windows
3294 
3295   // (d) Otherwise, go to the slow path.
3296   slow_region-&gt;add_req(control());
3297   set_control( _gvn.transform(slow_region));
3298 
3299   if (stopped()) {
3300     // There is no slow path.
3301     result_rgn-&gt;init_req(slow_result_path, top());
3302     result_val-&gt;init_req(slow_result_path, top());
3303   } else {
3304     // non-virtual because it is a private non-static
3305     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3306 
3307     Node* slow_val = set_results_for_java_call(slow_call);
3308     // this-&gt;control() comes from set_results_for_java_call
3309 
3310     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3311     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3312 
3313     // These two phis are pre-filled with copies of of the fast IO and Memory
3314     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3315     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3316 
3317     result_rgn-&gt;init_req(slow_result_path, control());
3318     result_io -&gt;init_req(slow_result_path, i_o());
3319     result_mem-&gt;init_req(slow_result_path, reset_memory());
3320     result_val-&gt;init_req(slow_result_path, slow_val);
3321 
3322     set_all_memory(_gvn.transform(result_mem));
3323     set_i_o(       _gvn.transform(result_io));
3324   }
3325 
3326   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3327   set_result(result_rgn, result_val);
3328   return true;
3329 }
3330 
3331 //---------------------------load_mirror_from_klass----------------------------
3332 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3333 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3334   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3335   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3336 }
3337 
3338 //-----------------------load_klass_from_mirror_common-------------------------
3339 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3340 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3341 // and branch to the given path on the region.
3342 // If never_see_null, take an uncommon trap on null, so we can optimistically
3343 // compile for the non-null case.
3344 // If the region is NULL, force never_see_null = true.
3345 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3346                                                     bool never_see_null,
3347                                                     RegionNode* region,
3348                                                     int null_path,
3349                                                     int offset) {
3350   if (region == NULL)  never_see_null = true;
3351   Node* p = basic_plus_adr(mirror, offset);
3352   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3353   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3354   Node* null_ctl = top();
3355   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3356   if (region != NULL) {
3357     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3358     region-&gt;init_req(null_path, null_ctl);
3359   } else {
3360     assert(null_ctl == top(), "no loose ends");
3361   }
3362   return kls;
3363 }
3364 
3365 //--------------------(inline_native_Class_query helpers)---------------------
3366 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3367 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3368 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3369   // Branch around if the given klass has the given modifier bit set.
3370   // Like generate_guard, adds a new path onto the region.
3371   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3372   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3373   Node* mask = intcon(modifier_mask);
3374   Node* bits = intcon(modifier_bits);
3375   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3376   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3377   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3378   return generate_fair_guard(bol, region);
3379 }
3380 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3381   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3382 }
3383 
3384 //-------------------------inline_native_Class_query-------------------
3385 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3386   const Type* return_type = TypeInt::BOOL;
3387   Node* prim_return_value = top();  // what happens if it's a primitive class?
3388   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3389   bool expect_prim = false;     // most of these guys expect to work on refs
3390 
3391   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3392 
3393   Node* mirror = argument(0);
3394   Node* obj    = top();
3395 
3396   switch (id) {
3397   case vmIntrinsics::_isInstance:
3398     // nothing is an instance of a primitive type
3399     prim_return_value = intcon(0);
3400     obj = argument(1);
3401     break;
3402   case vmIntrinsics::_getModifiers:
3403     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3404     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3405     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3406     break;
3407   case vmIntrinsics::_isInterface:
3408     prim_return_value = intcon(0);
3409     break;
3410   case vmIntrinsics::_isArray:
3411     prim_return_value = intcon(0);
3412     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3413     break;
3414   case vmIntrinsics::_isPrimitive:
3415     prim_return_value = intcon(1);
3416     expect_prim = true;  // obviously
3417     break;
3418   case vmIntrinsics::_getSuperclass:
3419     prim_return_value = null();
3420     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3421     break;
3422   case vmIntrinsics::_getClassAccessFlags:
3423     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3424     return_type = TypeInt::INT;  // not bool!  6297094
3425     break;
3426   default:
3427     fatal_unexpected_iid(id);
3428     break;
3429   }
3430 
3431   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3432   if (mirror_con == NULL)  return false;  // cannot happen?
3433 
3434 #ifndef PRODUCT
3435   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3436     ciType* k = mirror_con-&gt;java_mirror_type();
3437     if (k) {
3438       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3439       k-&gt;print_name();
3440       tty-&gt;cr();
3441     }
3442   }
3443 #endif
3444 
3445   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3446   RegionNode* region = new RegionNode(PATH_LIMIT);
3447   record_for_igvn(region);
3448   PhiNode* phi = new PhiNode(region, return_type);
3449 
3450   // The mirror will never be null of Reflection.getClassAccessFlags, however
3451   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3452   // if it is. See bug 4774291.
3453 
3454   // For Reflection.getClassAccessFlags(), the null check occurs in
3455   // the wrong place; see inline_unsafe_access(), above, for a similar
3456   // situation.
3457   mirror = null_check(mirror);
3458   // If mirror or obj is dead, only null-path is taken.
3459   if (stopped())  return true;
3460 
3461   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3462 
3463   // Now load the mirror's klass metaobject, and null-check it.
3464   // Side-effects region with the control path if the klass is null.
3465   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3466   // If kls is null, we have a primitive mirror.
3467   phi-&gt;init_req(_prim_path, prim_return_value);
3468   if (stopped()) { set_result(region, phi); return true; }
3469   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3470 
3471   Node* p;  // handy temp
3472   Node* null_ctl;
3473 
3474   // Now that we have the non-null klass, we can perform the real query.
3475   // For constant classes, the query will constant-fold in LoadNode::Value.
3476   Node* query_value = top();
3477   switch (id) {
3478   case vmIntrinsics::_isInstance:
3479     // nothing is an instance of a primitive type
3480     query_value = gen_instanceof(obj, kls, safe_for_replace);
3481     break;
3482 
3483   case vmIntrinsics::_getModifiers:
3484     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3485     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3486     break;
3487 
3488   case vmIntrinsics::_isInterface:
3489     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3490     if (generate_interface_guard(kls, region) != NULL)
3491       // A guard was added.  If the guard is taken, it was an interface.
3492       phi-&gt;add_req(intcon(1));
3493     // If we fall through, it's a plain class.
3494     query_value = intcon(0);
3495     break;
3496 
3497   case vmIntrinsics::_isArray:
3498     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3499     if (generate_array_guard(kls, region) != NULL)
3500       // A guard was added.  If the guard is taken, it was an array.
3501       phi-&gt;add_req(intcon(1));
3502     // If we fall through, it's a plain class.
3503     query_value = intcon(0);
3504     break;
3505 
3506   case vmIntrinsics::_isPrimitive:
3507     query_value = intcon(0); // "normal" path produces false
3508     break;
3509 
3510   case vmIntrinsics::_getSuperclass:
3511     // The rules here are somewhat unfortunate, but we can still do better
3512     // with random logic than with a JNI call.
3513     // Interfaces store null or Object as _super, but must report null.
3514     // Arrays store an intermediate super as _super, but must report Object.
3515     // Other types can report the actual _super.
3516     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3517     if (generate_interface_guard(kls, region) != NULL)
3518       // A guard was added.  If the guard is taken, it was an interface.
3519       phi-&gt;add_req(null());
3520     if (generate_array_guard(kls, region) != NULL)
3521       // A guard was added.  If the guard is taken, it was an array.
3522       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3523     // If we fall through, it's a plain class.  Get its _super.
3524     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3525     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3526     null_ctl = top();
3527     kls = null_check_oop(kls, &amp;null_ctl);
3528     if (null_ctl != top()) {
3529       // If the guard is taken, Object.superClass is null (both klass and mirror).
3530       region-&gt;add_req(null_ctl);
3531       phi   -&gt;add_req(null());
3532     }
3533     if (!stopped()) {
3534       query_value = load_mirror_from_klass(kls);
3535     }
3536     break;
3537 
3538   case vmIntrinsics::_getClassAccessFlags:
3539     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3540     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3541     break;
3542 
3543   default:
3544     fatal_unexpected_iid(id);
3545     break;
3546   }
3547 
3548   // Fall-through is the normal case of a query to a real class.
3549   phi-&gt;init_req(1, query_value);
3550   region-&gt;init_req(1, control());
3551 
3552   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3553   set_result(region, phi);
3554   return true;
3555 }
3556 
3557 //-------------------------inline_Class_cast-------------------
3558 bool LibraryCallKit::inline_Class_cast() {
3559   Node* mirror = argument(0); // Class
3560   Node* obj    = argument(1);
3561   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3562   if (mirror_con == NULL) {
3563     return false;  // dead path (mirror-&gt;is_top()).
3564   }
3565   if (obj == NULL || obj-&gt;is_top()) {
3566     return false;  // dead path
3567   }
3568   const TypeOopPtr* tp = _gvn.type(obj)-&gt;isa_oopptr();
3569 
3570   // First, see if Class.cast() can be folded statically.
3571   // java_mirror_type() returns non-null for compile-time Class constants.
3572   ciType* tm = mirror_con-&gt;java_mirror_type();
3573   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp;
3574       tp != NULL &amp;&amp; tp-&gt;klass() != NULL) {
3575     if (!tp-&gt;klass()-&gt;is_loaded()) {
3576       // Don't use intrinsic when class is not loaded.
3577       return false;
3578     } else {
3579       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), tp-&gt;klass());
3580       if (static_res == Compile::SSC_always_true) {
3581         // isInstance() is true - fold the code.
3582         set_result(obj);
3583         return true;
3584       } else if (static_res == Compile::SSC_always_false) {
3585         // Don't use intrinsic, have to throw ClassCastException.
3586         // If the reference is null, the non-intrinsic bytecode will
3587         // be optimized appropriately.
3588         return false;
3589       }
3590     }
3591   }
3592 
3593   // Bailout intrinsic and do normal inlining if exception path is frequent.
3594   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3595     return false;
3596   }
3597 
3598   // Generate dynamic checks.
3599   // Class.cast() is java implementation of _checkcast bytecode.
3600   // Do checkcast (Parse::do_checkcast()) optimizations here.
3601 
3602   mirror = null_check(mirror);
3603   // If mirror is dead, only null-path is taken.
3604   if (stopped()) {
3605     return true;
3606   }
3607 
3608   // Not-subtype or the mirror's klass ptr is NULL (in case it is a primitive).
3609   enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };
3610   RegionNode* region = new RegionNode(PATH_LIMIT);
3611   record_for_igvn(region);
3612 
3613   // Now load the mirror's klass metaobject, and null-check it.
3614   // If kls is null, we have a primitive mirror and
3615   // nothing is an instance of a primitive type.
3616   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3617 
3618   Node* res = top();
3619   if (!stopped()) {
3620     Node* bad_type_ctrl = top();
3621     // Do checkcast optimizations.
3622     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3623     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3624   }
3625   if (region-&gt;in(_prim_path) != top() ||
3626       region-&gt;in(_bad_type_path) != top()) {
3627     // Let Interpreter throw ClassCastException.
3628     PreserveJVMState pjvms(this);
3629     set_control(_gvn.transform(region));
3630     uncommon_trap(Deoptimization::Reason_intrinsic,
3631                   Deoptimization::Action_maybe_recompile);
3632   }
3633   if (!stopped()) {
3634     set_result(res);
3635   }
3636   return true;
3637 }
3638 
3639 
3640 //--------------------------inline_native_subtype_check------------------------
3641 // This intrinsic takes the JNI calls out of the heart of
3642 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3643 bool LibraryCallKit::inline_native_subtype_check() {
3644   // Pull both arguments off the stack.
3645   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3646   args[0] = argument(0);
3647   args[1] = argument(1);
3648   Node* klasses[2];             // corresponding Klasses: superk, subk
3649   klasses[0] = klasses[1] = top();
3650 
3651   enum {
3652     // A full decision tree on {superc is prim, subc is prim}:
3653     _prim_0_path = 1,           // {P,N} =&gt; false
3654                                 // {P,P} &amp; superc!=subc =&gt; false
3655     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3656     _prim_1_path,               // {N,P} =&gt; false
3657     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3658     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3659     PATH_LIMIT
3660   };
3661 
3662   RegionNode* region = new RegionNode(PATH_LIMIT);
3663   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3664   record_for_igvn(region);
3665 
3666   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3667   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3668   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3669 
3670   // First null-check both mirrors and load each mirror's klass metaobject.
3671   int which_arg;
3672   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3673     Node* arg = args[which_arg];
3674     arg = null_check(arg);
3675     if (stopped())  break;
3676     args[which_arg] = arg;
3677 
3678     Node* p = basic_plus_adr(arg, class_klass_offset);
3679     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3680     klasses[which_arg] = _gvn.transform(kls);
3681   }
3682 
3683   // Having loaded both klasses, test each for null.
3684   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3685   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3686     Node* kls = klasses[which_arg];
3687     Node* null_ctl = top();
3688     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3689     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3690     region-&gt;init_req(prim_path, null_ctl);
3691     if (stopped())  break;
3692     klasses[which_arg] = kls;
3693   }
3694 
3695   if (!stopped()) {
3696     // now we have two reference types, in klasses[0..1]
3697     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3698     Node* superk = klasses[0];  // the receiver
3699     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3700     // now we have a successful reference subtype check
3701     region-&gt;set_req(_ref_subtype_path, control());
3702   }
3703 
3704   // If both operands are primitive (both klasses null), then
3705   // we must return true when they are identical primitives.
3706   // It is convenient to test this after the first null klass check.
3707   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3708   if (!stopped()) {
3709     // Since superc is primitive, make a guard for the superc==subc case.
3710     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3711     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
3712     generate_guard(bol_eq, region, PROB_FAIR);
3713     if (region-&gt;req() == PATH_LIMIT+1) {
3714       // A guard was added.  If the added guard is taken, superc==subc.
3715       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3716       region-&gt;del_req(PATH_LIMIT);
3717     }
3718     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3719   }
3720 
3721   // these are the only paths that produce 'true':
3722   phi-&gt;set_req(_prim_same_path,   intcon(1));
3723   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3724 
3725   // pull together the cases:
3726   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3727   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3728     Node* ctl = region-&gt;in(i);
3729     if (ctl == NULL || ctl == top()) {
3730       region-&gt;set_req(i, top());
3731       phi   -&gt;set_req(i, top());
3732     } else if (phi-&gt;in(i) == NULL) {
3733       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3734     }
3735   }
3736 
3737   set_control(_gvn.transform(region));
3738   set_result(_gvn.transform(phi));
3739   return true;
3740 }
3741 
3742 //---------------------generate_array_guard_common------------------------
3743 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3744                                                   bool obj_array, bool not_array) {
3745 
3746   if (stopped()) {
3747     return NULL;
3748   }
3749 
3750   // If obj_array/non_array==false/false:
3751   // Branch around if the given klass is in fact an array (either obj or prim).
3752   // If obj_array/non_array==false/true:
3753   // Branch around if the given klass is not an array klass of any kind.
3754   // If obj_array/non_array==true/true:
3755   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3756   // If obj_array/non_array==true/false:
3757   // Branch around if the kls is an oop array (Object[] or subtype)
3758   //
3759   // Like generate_guard, adds a new path onto the region.
3760   jint  layout_con = 0;
3761   Node* layout_val = get_layout_helper(kls, layout_con);
3762   if (layout_val == NULL) {
3763     bool query = (obj_array
3764                   ? Klass::layout_helper_is_objArray(layout_con)
3765                   : Klass::layout_helper_is_array(layout_con));
3766     if (query == not_array) {
3767       return NULL;                       // never a branch
3768     } else {                             // always a branch
3769       Node* always_branch = control();
3770       if (region != NULL)
3771         region-&gt;add_req(always_branch);
3772       set_control(top());
3773       return always_branch;
3774     }
3775   }
3776   // Now test the correct condition.
3777   jint  nval = (obj_array
3778                 ? ((jint)Klass::_lh_array_tag_type_value
3779                    &lt;&lt;    Klass::_lh_array_tag_shift)
3780                 : Klass::_lh_neutral_value);
3781   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3782   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3783   // invert the test if we are looking for a non-array
3784   if (not_array)  btest = BoolTest(btest).negate();
3785   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3786   return generate_fair_guard(bol, region);
3787 }
3788 
3789 
3790 //-----------------------inline_native_newArray--------------------------
3791 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3792 bool LibraryCallKit::inline_native_newArray() {
3793   Node* mirror    = argument(0);
3794   Node* count_val = argument(1);
3795 
3796   mirror = null_check(mirror);
3797   // If mirror or obj is dead, only null-path is taken.
3798   if (stopped())  return true;
3799 
3800   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3801   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3802   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3803   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3804   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3805 
3806   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3807   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3808                                                   result_reg, _slow_path);
3809   Node* normal_ctl   = control();
3810   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3811 
3812   // Generate code for the slow case.  We make a call to newArray().
3813   set_control(no_array_ctl);
3814   if (!stopped()) {
3815     // Either the input type is void.class, or else the
3816     // array klass has not yet been cached.  Either the
3817     // ensuing call will throw an exception, or else it
3818     // will cache the array klass for next time.
3819     PreserveJVMState pjvms(this);
3820     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3821     Node* slow_result = set_results_for_java_call(slow_call);
3822     // this-&gt;control() comes from set_results_for_java_call
3823     result_reg-&gt;set_req(_slow_path, control());
3824     result_val-&gt;set_req(_slow_path, slow_result);
3825     result_io -&gt;set_req(_slow_path, i_o());
3826     result_mem-&gt;set_req(_slow_path, reset_memory());
3827   }
3828 
3829   set_control(normal_ctl);
3830   if (!stopped()) {
3831     // Normal case:  The array type has been cached in the java.lang.Class.
3832     // The following call works fine even if the array type is polymorphic.
3833     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3834     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3835     result_reg-&gt;init_req(_normal_path, control());
3836     result_val-&gt;init_req(_normal_path, obj);
3837     result_io -&gt;init_req(_normal_path, i_o());
3838     result_mem-&gt;init_req(_normal_path, reset_memory());
3839   }
3840 
3841   // Return the combined state.
3842   set_i_o(        _gvn.transform(result_io)  );
3843   set_all_memory( _gvn.transform(result_mem));
3844 
3845   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3846   set_result(result_reg, result_val);
3847   return true;
3848 }
3849 
3850 //----------------------inline_native_getLength--------------------------
3851 // public static native int java.lang.reflect.Array.getLength(Object array);
3852 bool LibraryCallKit::inline_native_getLength() {
3853   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3854 
3855   Node* array = null_check(argument(0));
3856   // If array is dead, only null-path is taken.
3857   if (stopped())  return true;
3858 
3859   // Deoptimize if it is a non-array.
3860   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3861 
3862   if (non_array != NULL) {
3863     PreserveJVMState pjvms(this);
3864     set_control(non_array);
3865     uncommon_trap(Deoptimization::Reason_intrinsic,
3866                   Deoptimization::Action_maybe_recompile);
3867   }
3868 
3869   // If control is dead, only non-array-path is taken.
3870   if (stopped())  return true;
3871 
3872   // The works fine even if the array type is polymorphic.
3873   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3874   Node* result = load_array_length(array);
3875 
3876   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3877   set_result(result);
3878   return true;
3879 }
3880 
3881 //------------------------inline_array_copyOf----------------------------
3882 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3883 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3884 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3885   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3886 
3887   // Get the arguments.
3888   Node* original          = argument(0);
3889   Node* start             = is_copyOfRange? argument(1): intcon(0);
3890   Node* end               = is_copyOfRange? argument(2): argument(1);
3891   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3892 
3893   Node* newcopy;
3894 
3895   // Set the original stack and the reexecute bit for the interpreter to reexecute
3896   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3897   { PreserveReexecuteState preexecs(this);
3898     jvms()-&gt;set_should_reexecute(true);
3899 
3900     array_type_mirror = null_check(array_type_mirror);
3901     original          = null_check(original);
3902 
3903     // Check if a null path was taken unconditionally.
3904     if (stopped())  return true;
3905 
3906     Node* orig_length = load_array_length(original);
3907 
3908     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3909     klass_node = null_check(klass_node);
3910 
3911     RegionNode* bailout = new RegionNode(1);
3912     record_for_igvn(bailout);
3913 
3914     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3915     // Bail out if that is so.
3916     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3917     if (not_objArray != NULL) {
3918       // Improve the klass node's type from the new optimistic assumption:
3919       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3920       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3921       Node* cast = new CastPPNode(klass_node, akls);
3922       cast-&gt;init_req(0, control());
3923       klass_node = _gvn.transform(cast);
3924     }
3925 
3926     // Bail out if either start or end is negative.
3927     generate_negative_guard(start, bailout, &amp;start);
3928     generate_negative_guard(end,   bailout, &amp;end);
3929 
3930     Node* length = end;
3931     if (_gvn.type(start) != TypeInt::ZERO) {
3932       length = _gvn.transform(new SubINode(end, start));
3933     }
3934 
3935     // Bail out if length is negative.
3936     // Without this the new_array would throw
3937     // NegativeArraySizeException but IllegalArgumentException is what
3938     // should be thrown
3939     generate_negative_guard(length, bailout, &amp;length);
3940 
3941     if (bailout-&gt;req() &gt; 1) {
3942       PreserveJVMState pjvms(this);
3943       set_control(_gvn.transform(bailout));
3944       uncommon_trap(Deoptimization::Reason_intrinsic,
3945                     Deoptimization::Action_maybe_recompile);
3946     }
3947 
3948     if (!stopped()) {
3949       // How many elements will we copy from the original?
3950       // The answer is MinI(orig_length - start, length).
3951       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
3952       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3953 
3954       // Generate a direct call to the right arraycopy function(s).
3955       // We know the copy is disjoint but we might not know if the
3956       // oop stores need checking.
3957       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3958       // This will fail a store-check if x contains any non-nulls.
3959 
3960       // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3961       // loads/stores but it is legal only if we're sure the
3962       // Arrays.copyOf would succeed. So we need all input arguments
3963       // to the copyOf to be validated, including that the copy to the
3964       // new array won't trigger an ArrayStoreException. That subtype
3965       // check can be optimized if we know something on the type of
3966       // the input array from type speculation.
3967       if (_gvn.type(klass_node)-&gt;singleton()) {
3968         ciKlass* subk   = _gvn.type(load_object_klass(original))-&gt;is_klassptr()-&gt;klass();
3969         ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3970 
3971         int test = C-&gt;static_subtype_check(superk, subk);
3972         if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3973           const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3974           if (t_original-&gt;speculative_type() != NULL) {
3975             original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3976           }
3977         }
3978       }
3979 
3980       bool validated = false;
3981       // Reason_class_check rather than Reason_intrinsic because we
3982       // want to intrinsify even if this traps.
3983       if (!too_many_traps(Deoptimization::Reason_class_check)) {
3984         Node* not_subtype_ctrl = gen_subtype_check(load_object_klass(original),
3985                                                    klass_node);
3986 
3987         if (not_subtype_ctrl != top()) {
3988           PreserveJVMState pjvms(this);
3989           set_control(not_subtype_ctrl);
3990           uncommon_trap(Deoptimization::Reason_class_check,
3991                         Deoptimization::Action_make_not_entrant);
3992           assert(stopped(), "Should be stopped");
3993         }
3994         validated = true;
3995       }
3996 
3997       if (!stopped()) {
3998         newcopy = new_array(klass_node, length, 0);  // no arguments to push
3999 
4000         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true,
4001                                                 load_object_klass(original), klass_node);
4002         if (!is_copyOfRange) {
4003           ac-&gt;set_copyof(validated);
4004         } else {
4005           ac-&gt;set_copyofrange(validated);
4006         }
4007         Node* n = _gvn.transform(ac);
4008         if (n == ac) {
4009           ac-&gt;connect_outputs(this);
4010         } else {
4011           assert(validated, "shouldn't transform if all arguments not validated");
4012           set_all_memory(n);
4013         }
4014       }
4015     }
4016   } // original reexecute is set back here
4017 
4018   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4019   if (!stopped()) {
4020     set_result(newcopy);
4021   }
4022   return true;
4023 }
4024 
4025 
4026 //----------------------generate_virtual_guard---------------------------
4027 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
4028 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
4029                                              RegionNode* slow_region) {
4030   ciMethod* method = callee();
4031   int vtable_index = method-&gt;vtable_index();
4032   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4033          err_msg_res("bad index %d", vtable_index));
4034   // Get the Method* out of the appropriate vtable entry.
4035   int entry_offset  = (InstanceKlass::vtable_start_offset() +
4036                      vtable_index*vtableEntry::size()) * wordSize +
4037                      vtableEntry::method_offset_in_bytes();
4038   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
4039   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4040 
4041   // Compare the target method with the expected method (e.g., Object.hashCode).
4042   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
4043 
4044   Node* native_call = makecon(native_call_addr);
4045   Node* chk_native  = _gvn.transform(new CmpPNode(target_call, native_call));
4046   Node* test_native = _gvn.transform(new BoolNode(chk_native, BoolTest::ne));
4047 
4048   return generate_slow_guard(test_native, slow_region);
4049 }
4050 
4051 //-----------------------generate_method_call----------------------------
4052 // Use generate_method_call to make a slow-call to the real
4053 // method if the fast path fails.  An alternative would be to
4054 // use a stub like OptoRuntime::slow_arraycopy_Java.
4055 // This only works for expanding the current library call,
4056 // not another intrinsic.  (E.g., don't use this for making an
4057 // arraycopy call inside of the copyOf intrinsic.)
4058 CallJavaNode*
4059 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
4060   // When compiling the intrinsic method itself, do not use this technique.
4061   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
4062 
4063   ciMethod* method = callee();
4064   // ensure the JVMS we have will be correct for this call
4065   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
4066 
4067   const TypeFunc* tf = TypeFunc::make(method);
4068   CallJavaNode* slow_call;
4069   if (is_static) {
4070     assert(!is_virtual, "");
4071     slow_call = new CallStaticJavaNode(C, tf,
4072                            SharedRuntime::get_resolve_static_call_stub(),
4073                            method, bci());
4074   } else if (is_virtual) {
4075     null_check_receiver();
4076     int vtable_index = Method::invalid_vtable_index;
4077     if (UseInlineCaches) {
4078       // Suppress the vtable call
4079     } else {
4080       // hashCode and clone are not a miranda methods,
4081       // so the vtable index is fixed.
4082       // No need to use the linkResolver to get it.
4083        vtable_index = method-&gt;vtable_index();
4084        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4085               err_msg_res("bad index %d", vtable_index));
4086     }
4087     slow_call = new CallDynamicJavaNode(tf,
4088                           SharedRuntime::get_resolve_virtual_call_stub(),
4089                           method, vtable_index, bci());
4090   } else {  // neither virtual nor static:  opt_virtual
4091     null_check_receiver();
4092     slow_call = new CallStaticJavaNode(C, tf,
4093                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
4094                                 method, bci());
4095     slow_call-&gt;set_optimized_virtual(true);
4096   }
4097   set_arguments_for_java_call(slow_call);
4098   set_edges_for_java_call(slow_call);
4099   return slow_call;
4100 }
4101 
4102 
4103 /**
4104  * Build special case code for calls to hashCode on an object. This call may
4105  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4106  * slightly different code.
4107  */
4108 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4109   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
4110   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
4111 
4112   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4113 
4114   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4115   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
4116   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
4117   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4118   Node* obj = NULL;
4119   if (!is_static) {
4120     // Check for hashing null object
4121     obj = null_check_receiver();
4122     if (stopped())  return true;        // unconditionally null
4123     result_reg-&gt;init_req(_null_path, top());
4124     result_val-&gt;init_req(_null_path, top());
4125   } else {
4126     // Do a null check, and return zero if null.
4127     // System.identityHashCode(null) == 0
4128     obj = argument(0);
4129     Node* null_ctl = top();
4130     obj = null_check_oop(obj, &amp;null_ctl);
4131     result_reg-&gt;init_req(_null_path, null_ctl);
4132     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4133   }
4134 
4135   // Unconditionally null?  Then return right away.
4136   if (stopped()) {
4137     set_control( result_reg-&gt;in(_null_path));
4138     if (!stopped())
4139       set_result(result_val-&gt;in(_null_path));
4140     return true;
4141   }
4142 
4143   // We only go to the fast case code if we pass a number of guards.  The
4144   // paths which do not pass are accumulated in the slow_region.
4145   RegionNode* slow_region = new RegionNode(1);
4146   record_for_igvn(slow_region);
4147 
4148   // If this is a virtual call, we generate a funny guard.  We pull out
4149   // the vtable entry corresponding to hashCode() from the target object.
4150   // If the target method which we are calling happens to be the native
4151   // Object hashCode() method, we pass the guard.  We do not need this
4152   // guard for non-virtual calls -- the caller is known to be the native
4153   // Object hashCode().
4154   if (is_virtual) {
4155     // After null check, get the object's klass.
4156     Node* obj_klass = load_object_klass(obj);
4157     generate_virtual_guard(obj_klass, slow_region);
4158   }
4159 
4160   // Get the header out of the object, use LoadMarkNode when available
4161   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4162   // The control of the load must be NULL. Otherwise, the load can move before
4163   // the null check after castPP removal.
4164   Node* no_ctrl = NULL;
4165   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4166 
4167   // Test the header to see if it is unlocked.
4168   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4169   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
4170   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4171   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
4172   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
4173 
4174   generate_slow_guard(test_unlocked, slow_region);
4175 
4176   // Get the hash value and check to see that it has been properly assigned.
4177   // We depend on hash_mask being at most 32 bits and avoid the use of
4178   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4179   // vm: see markOop.hpp.
4180   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4181   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4182   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
4183   // This hack lets the hash bits live anywhere in the mark object now, as long
4184   // as the shift drops the relevant bits into the low 32 bits.  Note that
4185   // Java spec says that HashCode is an int so there's no point in capturing
4186   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4187   hshifted_header      = ConvX2I(hshifted_header);
4188   Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));
4189 
4190   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4191   Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));
4192   Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));
4193 
4194   generate_slow_guard(test_assigned, slow_region);
4195 
4196   Node* init_mem = reset_memory();
4197   // fill in the rest of the null path:
4198   result_io -&gt;init_req(_null_path, i_o());
4199   result_mem-&gt;init_req(_null_path, init_mem);
4200 
4201   result_val-&gt;init_req(_fast_path, hash_val);
4202   result_reg-&gt;init_req(_fast_path, control());
4203   result_io -&gt;init_req(_fast_path, i_o());
4204   result_mem-&gt;init_req(_fast_path, init_mem);
4205 
4206   // Generate code for the slow case.  We make a call to hashCode().
4207   set_control(_gvn.transform(slow_region));
4208   if (!stopped()) {
4209     // No need for PreserveJVMState, because we're using up the present state.
4210     set_all_memory(init_mem);
4211     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4212     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4213     Node* slow_result = set_results_for_java_call(slow_call);
4214     // this-&gt;control() comes from set_results_for_java_call
4215     result_reg-&gt;init_req(_slow_path, control());
4216     result_val-&gt;init_req(_slow_path, slow_result);
4217     result_io  -&gt;set_req(_slow_path, i_o());
4218     result_mem -&gt;set_req(_slow_path, reset_memory());
4219   }
4220 
4221   // Return the combined state.
4222   set_i_o(        _gvn.transform(result_io)  );
4223   set_all_memory( _gvn.transform(result_mem));
4224 
4225   set_result(result_reg, result_val);
4226   return true;
4227 }
4228 
4229 //---------------------------inline_native_getClass----------------------------
4230 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4231 //
4232 // Build special case code for calls to getClass on an object.
4233 bool LibraryCallKit::inline_native_getClass() {
4234   Node* obj = null_check_receiver();
4235   if (stopped())  return true;
4236   set_result(load_mirror_from_klass(load_object_klass(obj)));
4237   return true;
4238 }
4239 
4240 //-----------------inline_native_Reflection_getCallerClass---------------------
4241 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4242 //
4243 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4244 //
4245 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4246 // in that it must skip particular security frames and checks for
4247 // caller sensitive methods.
4248 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4249 #ifndef PRODUCT
4250   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4251     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4252   }
4253 #endif
4254 
4255   if (!jvms()-&gt;has_method()) {
4256 #ifndef PRODUCT
4257     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4258       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4259     }
4260 #endif
4261     return false;
4262   }
4263 
4264   // Walk back up the JVM state to find the caller at the required
4265   // depth.
4266   JVMState* caller_jvms = jvms();
4267 
4268   // Cf. JVM_GetCallerClass
4269   // NOTE: Start the loop at depth 1 because the current JVM state does
4270   // not include the Reflection.getCallerClass() frame.
4271   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4272     ciMethod* m = caller_jvms-&gt;method();
4273     switch (n) {
4274     case 0:
4275       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4276       break;
4277     case 1:
4278       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4279       if (!m-&gt;caller_sensitive()) {
4280 #ifndef PRODUCT
4281         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4282           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4283         }
4284 #endif
4285         return false;  // bail-out; let JVM_GetCallerClass do the work
4286       }
4287       break;
4288     default:
4289       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4290         // We have reached the desired frame; return the holder class.
4291         // Acquire method holder as java.lang.Class and push as constant.
4292         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4293         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4294         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4295 
4296 #ifndef PRODUCT
4297         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4298           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4299           tty-&gt;print_cr("  JVM state at this point:");
4300           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4301             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4302             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4303           }
4304         }
4305 #endif
4306         return true;
4307       }
4308       break;
4309     }
4310   }
4311 
4312 #ifndef PRODUCT
4313   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4314     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4315     tty-&gt;print_cr("  JVM state at this point:");
4316     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4317       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4318       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4319     }
4320   }
4321 #endif
4322 
4323   return false;  // bail-out; let JVM_GetCallerClass do the work
4324 }
4325 
4326 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4327   Node* arg = argument(0);
4328   Node* result;
4329 
4330   switch (id) {
4331   case vmIntrinsics::_floatToRawIntBits:    result = new MoveF2INode(arg);  break;
4332   case vmIntrinsics::_intBitsToFloat:       result = new MoveI2FNode(arg);  break;
4333   case vmIntrinsics::_doubleToRawLongBits:  result = new MoveD2LNode(arg);  break;
4334   case vmIntrinsics::_longBitsToDouble:     result = new MoveL2DNode(arg);  break;
4335 
4336   case vmIntrinsics::_doubleToLongBits: {
4337     // two paths (plus control) merge in a wood
4338     RegionNode *r = new RegionNode(3);
4339     Node *phi = new PhiNode(r, TypeLong::LONG);
4340 
4341     Node *cmpisnan = _gvn.transform(new CmpDNode(arg, arg));
4342     // Build the boolean node
4343     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4344 
4345     // Branch either way.
4346     // NaN case is less traveled, which makes all the difference.
4347     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4348     Node *opt_isnan = _gvn.transform(ifisnan);
4349     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4350     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4351     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4352 
4353     set_control(iftrue);
4354 
4355     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4356     Node *slow_result = longcon(nan_bits); // return NaN
4357     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4358     r-&gt;init_req(1, iftrue);
4359 
4360     // Else fall through
4361     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4362     set_control(iffalse);
4363 
4364     phi-&gt;init_req(2, _gvn.transform(new MoveD2LNode(arg)));
4365     r-&gt;init_req(2, iffalse);
4366 
4367     // Post merge
4368     set_control(_gvn.transform(r));
4369     record_for_igvn(r);
4370 
4371     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4372     result = phi;
4373     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4374     break;
4375   }
4376 
4377   case vmIntrinsics::_floatToIntBits: {
4378     // two paths (plus control) merge in a wood
4379     RegionNode *r = new RegionNode(3);
4380     Node *phi = new PhiNode(r, TypeInt::INT);
4381 
4382     Node *cmpisnan = _gvn.transform(new CmpFNode(arg, arg));
4383     // Build the boolean node
4384     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4385 
4386     // Branch either way.
4387     // NaN case is less traveled, which makes all the difference.
4388     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4389     Node *opt_isnan = _gvn.transform(ifisnan);
4390     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4391     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4392     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4393 
4394     set_control(iftrue);
4395 
4396     static const jint nan_bits = 0x7fc00000;
4397     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4398     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4399     r-&gt;init_req(1, iftrue);
4400 
4401     // Else fall through
4402     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4403     set_control(iffalse);
4404 
4405     phi-&gt;init_req(2, _gvn.transform(new MoveF2INode(arg)));
4406     r-&gt;init_req(2, iffalse);
4407 
4408     // Post merge
4409     set_control(_gvn.transform(r));
4410     record_for_igvn(r);
4411 
4412     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4413     result = phi;
4414     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4415     break;
4416   }
4417 
4418   default:
4419     fatal_unexpected_iid(id);
4420     break;
4421   }
4422   set_result(_gvn.transform(result));
4423   return true;
4424 }
4425 
4426 #ifdef _LP64
4427 #define XTOP ,top() /*additional argument*/
4428 #else  //_LP64
4429 #define XTOP        /*no additional argument*/
4430 #endif //_LP64
4431 
4432 //----------------------inline_unsafe_copyMemory-------------------------
4433 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4434 bool LibraryCallKit::inline_unsafe_copyMemory() {
4435   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4436   null_check_receiver();  // null-check receiver
4437   if (stopped())  return true;
4438 
4439   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4440 
4441   Node* src_ptr =         argument(1);   // type: oop
4442   Node* src_off = ConvL2X(argument(2));  // type: long
4443   Node* dst_ptr =         argument(4);   // type: oop
4444   Node* dst_off = ConvL2X(argument(5));  // type: long
4445   Node* size    = ConvL2X(argument(7));  // type: long
4446 
4447   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4448          "fieldOffset must be byte-scaled");
4449 
4450   Node* src = make_unsafe_address(src_ptr, src_off);
4451   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4452 
4453   // Conservatively insert a memory barrier on all memory slices.
4454   // Do not let writes of the copy source or destination float below the copy.
4455   insert_mem_bar(Op_MemBarCPUOrder);
4456 
4457   // Call it.  Note that the length argument is not scaled.
4458   make_runtime_call(RC_LEAF|RC_NO_FP,
4459                     OptoRuntime::fast_arraycopy_Type(),
4460                     StubRoutines::unsafe_arraycopy(),
4461                     "unsafe_arraycopy",
4462                     TypeRawPtr::BOTTOM,
4463                     src, dst, size XTOP);
4464 
4465   // Do not let reads of the copy destination float above the copy.
4466   insert_mem_bar(Op_MemBarCPUOrder);
4467 
4468   return true;
4469 }
4470 
4471 //------------------------clone_coping-----------------------------------
4472 // Helper function for inline_native_clone.
4473 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4474   assert(obj_size != NULL, "");
4475   Node* raw_obj = alloc_obj-&gt;in(1);
4476   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4477 
4478   AllocateNode* alloc = NULL;
4479   if (ReduceBulkZeroing) {
4480     // We will be completely responsible for initializing this object -
4481     // mark Initialize node as complete.
4482     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4483     // The object was just allocated - there should be no any stores!
4484     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4485     // Mark as complete_with_arraycopy so that on AllocateNode
4486     // expansion, we know this AllocateNode is initialized by an array
4487     // copy and a StoreStore barrier exists after the array copy.
4488     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4489   }
4490 
4491   // Copy the fastest available way.
4492   // TODO: generate fields copies for small objects instead.
4493   Node* src  = obj;
4494   Node* dest = alloc_obj;
4495   Node* size = _gvn.transform(obj_size);
4496 
4497   // Exclude the header but include array length to copy by 8 bytes words.
4498   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4499   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4500                             instanceOopDesc::base_offset_in_bytes();
4501   // base_off:
4502   // 8  - 32-bit VM
4503   // 12 - 64-bit VM, compressed klass
4504   // 16 - 64-bit VM, normal klass
4505   if (base_off % BytesPerLong != 0) {
4506     assert(UseCompressedClassPointers, "");
4507     if (is_array) {
4508       // Exclude length to copy by 8 bytes words.
4509       base_off += sizeof(int);
4510     } else {
4511       // Include klass to copy by 8 bytes words.
4512       base_off = instanceOopDesc::klass_offset_in_bytes();
4513     }
4514     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4515   }
4516   src  = basic_plus_adr(src,  base_off);
4517   dest = basic_plus_adr(dest, base_off);
4518 
4519   // Compute the length also, if needed:
4520   Node* countx = size;
4521   countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
4522   countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong) ));
4523 
4524   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4525 
4526   ArrayCopyNode* ac = ArrayCopyNode::make(this, false, src, NULL, dest, NULL, countx, false);
4527   ac-&gt;set_clonebasic();
4528   Node* n = _gvn.transform(ac);
4529   if (n == ac) {
4530     set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
4531   } else {
4532     set_all_memory(n);
4533   }
4534 
4535   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4536   if (card_mark) {
4537     assert(!is_array, "");
4538     // Put in store barrier for any and all oops we are sticking
4539     // into this object.  (We could avoid this if we could prove
4540     // that the object type contains no oop fields at all.)
4541     Node* no_particular_value = NULL;
4542     Node* no_particular_field = NULL;
4543     int raw_adr_idx = Compile::AliasIdxRaw;
4544     post_barrier(control(),
4545                  memory(raw_adr_type),
4546                  alloc_obj,
4547                  no_particular_field,
4548                  raw_adr_idx,
4549                  no_particular_value,
4550                  T_OBJECT,
4551                  false);
4552   }
4553 
4554   // Do not let reads from the cloned object float above the arraycopy.
4555   if (alloc != NULL) {
4556     // Do not let stores that initialize this object be reordered with
4557     // a subsequent store that would make this object accessible by
4558     // other threads.
4559     // Record what AllocateNode this StoreStore protects so that
4560     // escape analysis can go from the MemBarStoreStoreNode to the
4561     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4562     // based on the escape status of the AllocateNode.
4563     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4564   } else {
4565     insert_mem_bar(Op_MemBarCPUOrder);
4566   }
4567 }
4568 
4569 //------------------------inline_native_clone----------------------------
4570 // protected native Object java.lang.Object.clone();
4571 //
4572 // Here are the simple edge cases:
4573 //  null receiver =&gt; normal trap
4574 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4575 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4576 //
4577 // The general case has two steps, allocation and copying.
4578 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4579 //
4580 // Copying also has two cases, oop arrays and everything else.
4581 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4582 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4583 //
4584 // These steps fold up nicely if and when the cloned object's klass
4585 // can be sharply typed as an object array, a type array, or an instance.
4586 //
4587 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4588   PhiNode* result_val;
4589 
4590   // Set the reexecute bit for the interpreter to reexecute
4591   // the bytecode that invokes Object.clone if deoptimization happens.
4592   { PreserveReexecuteState preexecs(this);
4593     jvms()-&gt;set_should_reexecute(true);
4594 
4595     Node* obj = null_check_receiver();
4596     if (stopped())  return true;
4597 
4598     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4599 
4600     // If we are going to clone an instance, we need its exact type to
4601     // know the number and types of fields to convert the clone to
4602     // loads/stores. Maybe a speculative type can help us.
4603     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4604         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4605         obj_type-&gt;speculative_type()-&gt;is_instance_klass()) {
4606       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4607       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4608           !spec_ik-&gt;has_injected_fields()) {
4609         ciKlass* k = obj_type-&gt;klass();
4610         if (!k-&gt;is_instance_klass() ||
4611             k-&gt;as_instance_klass()-&gt;is_interface() ||
4612             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4613           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4614         }
4615       }
4616     }
4617 
4618     Node* obj_klass = load_object_klass(obj);
4619     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4620     const TypeOopPtr*   toop   = ((tklass != NULL)
4621                                 ? tklass-&gt;as_instance_type()
4622                                 : TypeInstPtr::NOTNULL);
4623 
4624     // Conservatively insert a memory barrier on all memory slices.
4625     // Do not let writes into the original float below the clone.
4626     insert_mem_bar(Op_MemBarCPUOrder);
4627 
4628     // paths into result_reg:
4629     enum {
4630       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4631       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4632       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4633       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4634       PATH_LIMIT
4635     };
4636     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4637     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4638     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4639     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4640     record_for_igvn(result_reg);
4641 
4642     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4643     int raw_adr_idx = Compile::AliasIdxRaw;
4644 
4645     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4646     if (array_ctl != NULL) {
4647       // It's an array.
4648       PreserveJVMState pjvms(this);
4649       set_control(array_ctl);
4650       Node* obj_length = load_array_length(obj);
4651       Node* obj_size  = NULL;
4652       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4653 
4654       if (!use_ReduceInitialCardMarks()) {
4655         // If it is an oop array, it requires very special treatment,
4656         // because card marking is required on each card of the array.
4657         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4658         if (is_obja != NULL) {
4659           PreserveJVMState pjvms2(this);
4660           set_control(is_obja);
4661           // Generate a direct call to the right arraycopy function(s).
4662           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4663           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL);
4664           ac-&gt;set_cloneoop();
4665           Node* n = _gvn.transform(ac);
4666           assert(n == ac, "cannot disappear");
4667           ac-&gt;connect_outputs(this);
4668 
4669           result_reg-&gt;init_req(_objArray_path, control());
4670           result_val-&gt;init_req(_objArray_path, alloc_obj);
4671           result_i_o -&gt;set_req(_objArray_path, i_o());
4672           result_mem -&gt;set_req(_objArray_path, reset_memory());
4673         }
4674       }
4675       // Otherwise, there are no card marks to worry about.
4676       // (We can dispense with card marks if we know the allocation
4677       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4678       //  causes the non-eden paths to take compensating steps to
4679       //  simulate a fresh allocation, so that no further
4680       //  card marks are required in compiled code to initialize
4681       //  the object.)
4682 
4683       if (!stopped()) {
4684         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4685 
4686         // Present the results of the copy.
4687         result_reg-&gt;init_req(_array_path, control());
4688         result_val-&gt;init_req(_array_path, alloc_obj);
4689         result_i_o -&gt;set_req(_array_path, i_o());
4690         result_mem -&gt;set_req(_array_path, reset_memory());
4691       }
4692     }
4693 
4694     // We only go to the instance fast case code if we pass a number of guards.
4695     // The paths which do not pass are accumulated in the slow_region.
4696     RegionNode* slow_region = new RegionNode(1);
4697     record_for_igvn(slow_region);
4698     if (!stopped()) {
4699       // It's an instance (we did array above).  Make the slow-path tests.
4700       // If this is a virtual call, we generate a funny guard.  We grab
4701       // the vtable entry corresponding to clone() from the target object.
4702       // If the target method which we are calling happens to be the
4703       // Object clone() method, we pass the guard.  We do not need this
4704       // guard for non-virtual calls; the caller is known to be the native
4705       // Object clone().
4706       if (is_virtual) {
4707         generate_virtual_guard(obj_klass, slow_region);
4708       }
4709 
4710       // The object must be cloneable and must not have a finalizer.
4711       // Both of these conditions may be checked in a single test.
4712       // We could optimize the cloneable test further, but we don't care.
4713       generate_access_flags_guard(obj_klass,
4714                                   // Test both conditions:
4715                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4716                                   // Must be cloneable but not finalizer:
4717                                   JVM_ACC_IS_CLONEABLE,
4718                                   slow_region);
4719     }
4720 
4721     if (!stopped()) {
4722       // It's an instance, and it passed the slow-path tests.
4723       PreserveJVMState pjvms(this);
4724       Node* obj_size  = NULL;
4725       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4726       // is reexecuted if deoptimization occurs and there could be problems when merging
4727       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4728       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4729 
4730       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4731 
4732       // Present the results of the slow call.
4733       result_reg-&gt;init_req(_instance_path, control());
4734       result_val-&gt;init_req(_instance_path, alloc_obj);
4735       result_i_o -&gt;set_req(_instance_path, i_o());
4736       result_mem -&gt;set_req(_instance_path, reset_memory());
4737     }
4738 
4739     // Generate code for the slow case.  We make a call to clone().
4740     set_control(_gvn.transform(slow_region));
4741     if (!stopped()) {
4742       PreserveJVMState pjvms(this);
4743       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4744       Node* slow_result = set_results_for_java_call(slow_call);
4745       // this-&gt;control() comes from set_results_for_java_call
4746       result_reg-&gt;init_req(_slow_path, control());
4747       result_val-&gt;init_req(_slow_path, slow_result);
4748       result_i_o -&gt;set_req(_slow_path, i_o());
4749       result_mem -&gt;set_req(_slow_path, reset_memory());
4750     }
4751 
4752     // Return the combined state.
4753     set_control(    _gvn.transform(result_reg));
4754     set_i_o(        _gvn.transform(result_i_o));
4755     set_all_memory( _gvn.transform(result_mem));
4756   } // original reexecute is set back here
4757 
4758   set_result(_gvn.transform(result_val));
4759   return true;
4760 }
4761 
4762 // If we have a tighly coupled allocation, the arraycopy may take care
4763 // of the array initialization. If one of the guards we insert between
4764 // the allocation and the arraycopy causes a deoptimization, an
4765 // unitialized array will escape the compiled method. To prevent that
4766 // we set the JVM state for uncommon traps between the allocation and
4767 // the arraycopy to the state before the allocation so, in case of
4768 // deoptimization, we'll reexecute the allocation and the
4769 // initialization.
4770 JVMState* LibraryCallKit::arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp) {
4771   if (alloc != NULL) {
4772     ciMethod* trap_method = alloc-&gt;jvms()-&gt;method();
4773     int trap_bci = alloc-&gt;jvms()-&gt;bci();
4774 
4775     if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;
4776           !C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
4777       // Make sure there's no store between the allocation and the
4778       // arraycopy otherwise visible side effects could be rexecuted
4779       // in case of deoptimization and cause incorrect execution.
4780       bool no_interfering_store = true;
4781       Node* mem = alloc-&gt;in(TypeFunc::Memory);
4782       if (mem-&gt;is_MergeMem()) {
4783         for (MergeMemStream mms(merged_memory(), mem-&gt;as_MergeMem()); mms.next_non_empty2(); ) {
4784           Node* n = mms.memory();
4785           if (n != mms.memory2() &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4786             assert(n-&gt;is_Store(), "what else?");
4787             no_interfering_store = false;
4788             break;
4789           }
4790         }
4791       } else {
4792         for (MergeMemStream mms(merged_memory()); mms.next_non_empty(); ) {
4793           Node* n = mms.memory();
4794           if (n != mem &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4795             assert(n-&gt;is_Store(), "what else?");
4796             no_interfering_store = false;
4797             break;
4798           }
4799         }
4800       }
4801 
4802       if (no_interfering_store) {
4803         JVMState* old_jvms = alloc-&gt;jvms()-&gt;clone_shallow(C);
4804         uint size = alloc-&gt;req();
4805         SafePointNode* sfpt = new SafePointNode(size, old_jvms);
4806         old_jvms-&gt;set_map(sfpt);
4807         for (uint i = 0; i &lt; size; i++) {
4808           sfpt-&gt;init_req(i, alloc-&gt;in(i));
4809         }
4810         // re-push array length for deoptimization
4811         sfpt-&gt;ins_req(old_jvms-&gt;stkoff() + old_jvms-&gt;sp(), alloc-&gt;in(AllocateNode::ALength));
4812         old_jvms-&gt;set_sp(old_jvms-&gt;sp()+1);
4813         old_jvms-&gt;set_monoff(old_jvms-&gt;monoff()+1);
4814         old_jvms-&gt;set_scloff(old_jvms-&gt;scloff()+1);
4815         old_jvms-&gt;set_endoff(old_jvms-&gt;endoff()+1);
4816         old_jvms-&gt;set_should_reexecute(true);
4817 
4818         sfpt-&gt;set_i_o(map()-&gt;i_o());
4819         sfpt-&gt;set_memory(map()-&gt;memory());
4820         sfpt-&gt;set_control(map()-&gt;control());
4821 
4822         JVMState* saved_jvms = jvms();
4823         saved_reexecute_sp = _reexecute_sp;
4824 
4825         set_jvms(sfpt-&gt;jvms());
4826         _reexecute_sp = jvms()-&gt;sp();
4827 
4828         return saved_jvms;
4829       }
4830     }
4831   }
4832   return NULL;
4833 }
4834 
4835 // In case of a deoptimization, we restart execution at the
4836 // allocation, allocating a new array. We would leave an uninitialized
4837 // array in the heap that GCs wouldn't expect. Move the allocation
4838 // after the traps so we don't allocate the array if we
4839 // deoptimize. This is possible because tightly_coupled_allocation()
4840 // guarantees there's no observer of the allocated array at this point
4841 // and the control flow is simple enough.
4842 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp) {
4843   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4844     assert(alloc != NULL, "only with a tightly coupled allocation");
4845     // restore JVM state to the state at the arraycopy
4846     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4847     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), "memory state changed?");
4848     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), "IO state changed?");
4849     // If we've improved the types of some nodes (null check) while
4850     // emitting the guards, propagate them to the current state
4851     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map());
4852     set_jvms(saved_jvms);
4853     _reexecute_sp = saved_reexecute_sp;
4854 
4855     // Remove the allocation from above the guards
4856     CallProjections callprojs;
4857     alloc-&gt;extract_projections(&amp;callprojs, true);
4858     InitializeNode* init = alloc-&gt;initialization();
4859     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
4860     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));
4861     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4862     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4863 
4864     // move the allocation here (after the guards)
4865     _gvn.hash_delete(alloc);
4866     alloc-&gt;set_req(TypeFunc::Control, control());
4867     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4868     Node *mem = reset_memory();
4869     set_all_memory(mem);
4870     alloc-&gt;set_req(TypeFunc::Memory, mem);
4871     set_control(init-&gt;proj_out(TypeFunc::Control));
4872     set_i_o(callprojs.fallthrough_ioproj);
4873 
4874     // Update memory as done in GraphKit::set_output_for_allocation()
4875     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4876     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4877     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4878       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4879     }
4880     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4881     int            elemidx  = C-&gt;get_alias_index(telemref);
4882     set_memory(init-&gt;proj_out(TypeFunc::Memory), Compile::AliasIdxRaw);
4883     set_memory(init-&gt;proj_out(TypeFunc::Memory), elemidx);
4884 
4885     Node* allocx = _gvn.transform(alloc);
4886     assert(allocx == alloc, "where has the allocation gone?");
4887     assert(dest-&gt;is_CheckCastPP(), "not an allocation result?");
4888 
4889     _gvn.hash_delete(dest);
4890     dest-&gt;set_req(0, control());
4891     Node* destx = _gvn.transform(dest);
4892     assert(destx == dest, "where has the allocation result gone?");
4893   }
4894 }
4895 
4896 
4897 //------------------------------inline_arraycopy-----------------------
4898 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4899 //                                                      Object dest, int destPos,
4900 //                                                      int length);
4901 bool LibraryCallKit::inline_arraycopy() {
4902   // Get the arguments.
4903   Node* src         = argument(0);  // type: oop
4904   Node* src_offset  = argument(1);  // type: int
4905   Node* dest        = argument(2);  // type: oop
4906   Node* dest_offset = argument(3);  // type: int
4907   Node* length      = argument(4);  // type: int
4908 
4909 
4910   // Check for allocation before we add nodes that would confuse
4911   // tightly_coupled_allocation()
4912   AllocateArrayNode* alloc = tightly_coupled_allocation(dest, NULL);
4913 
4914   int saved_reexecute_sp = -1;
4915   JVMState* saved_jvms = arraycopy_restore_alloc_state(alloc, saved_reexecute_sp);
4916   // See arraycopy_restore_alloc_state() comment
4917   // if alloc == NULL we don't have to worry about a tightly coupled allocation so we can emit all needed guards
4918   // if saved_jvms != NULL (then alloc != NULL) then we can handle guards and a tightly coupled allocation
4919   // if saved_jvms == NULL and alloc != NULL, we can’t emit any guards
4920   bool can_emit_guards = (alloc == NULL || saved_jvms != NULL);
4921 
4922   // The following tests must be performed
4923   // (1) src and dest are arrays.
4924   // (2) src and dest arrays must have elements of the same BasicType
4925   // (3) src and dest must not be null.
4926   // (4) src_offset must not be negative.
4927   // (5) dest_offset must not be negative.
4928   // (6) length must not be negative.
4929   // (7) src_offset + length must not exceed length of src.
4930   // (8) dest_offset + length must not exceed length of dest.
4931   // (9) each element of an oop array must be assignable
4932 
4933   // (3) src and dest must not be null.
4934   // always do this here because we need the JVM state for uncommon traps
4935   Node* null_ctl = top();
4936   src  = saved_jvms != NULL ? null_check_oop(src, &amp;null_ctl, true, true) : null_check(src,  T_ARRAY);
4937   assert(null_ctl-&gt;is_top(), "no null control here");
4938   dest = null_check(dest, T_ARRAY);
4939 
4940   if (!can_emit_guards) {
4941     // if saved_jvms == NULL and alloc != NULL, we don't emit any
4942     // guards but the arraycopy node could still take advantage of a
4943     // tightly allocated allocation. tightly_coupled_allocation() is
4944     // called again to make sure it takes the null check above into
4945     // account: the null check is mandatory and if it caused an
4946     // uncommon trap to be emitted then the allocation can't be
4947     // considered tightly coupled in this context.
4948     alloc = tightly_coupled_allocation(dest, NULL);
4949   }
4950 
4951   bool validated = false;
4952 
4953   const Type* src_type  = _gvn.type(src);
4954   const Type* dest_type = _gvn.type(dest);
4955   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4956   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4957 
4958   // Do we have the type of src?
4959   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4960   // Do we have the type of dest?
4961   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4962   // Is the type for src from speculation?
4963   bool src_spec = false;
4964   // Is the type for dest from speculation?
4965   bool dest_spec = false;
4966 
4967   if ((!has_src || !has_dest) &amp;&amp; can_emit_guards) {
4968     // We don't have sufficient type information, let's see if
4969     // speculative types can help. We need to have types for both src
4970     // and dest so that it pays off.
4971 
4972     // Do we already have or could we have type information for src
4973     bool could_have_src = has_src;
4974     // Do we already have or could we have type information for dest
4975     bool could_have_dest = has_dest;
4976 
4977     ciKlass* src_k = NULL;
4978     if (!has_src) {
4979       src_k = src_type-&gt;speculative_type_not_null();
4980       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4981         could_have_src = true;
4982       }
4983     }
4984 
4985     ciKlass* dest_k = NULL;
4986     if (!has_dest) {
4987       dest_k = dest_type-&gt;speculative_type_not_null();
4988       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4989         could_have_dest = true;
4990       }
4991     }
4992 
4993     if (could_have_src &amp;&amp; could_have_dest) {
4994       // This is going to pay off so emit the required guards
4995       if (!has_src) {
4996         src = maybe_cast_profiled_obj(src, src_k, true);
4997         src_type  = _gvn.type(src);
4998         top_src  = src_type-&gt;isa_aryptr();
4999         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
5000         src_spec = true;
5001       }
5002       if (!has_dest) {
5003         dest = maybe_cast_profiled_obj(dest, dest_k, true);
5004         dest_type  = _gvn.type(dest);
5005         top_dest  = dest_type-&gt;isa_aryptr();
5006         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
5007         dest_spec = true;
5008       }
5009     }
5010   }
5011 
5012   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
5013     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5014     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5015     if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
5016     if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
5017 
5018     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
5019       // If both arrays are object arrays then having the exact types
5020       // for both will remove the need for a subtype check at runtime
5021       // before the call and may make it possible to pick a faster copy
5022       // routine (without a subtype check on every element)
5023       // Do we have the exact type of src?
5024       bool could_have_src = src_spec;
5025       // Do we have the exact type of dest?
5026       bool could_have_dest = dest_spec;
5027       ciKlass* src_k = top_src-&gt;klass();
5028       ciKlass* dest_k = top_dest-&gt;klass();
5029       if (!src_spec) {
5030         src_k = src_type-&gt;speculative_type_not_null();
5031         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
5032           could_have_src = true;
5033         }
5034       }
5035       if (!dest_spec) {
5036         dest_k = dest_type-&gt;speculative_type_not_null();
5037         if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
5038           could_have_dest = true;
5039         }
5040       }
5041       if (could_have_src &amp;&amp; could_have_dest) {
5042         // If we can have both exact types, emit the missing guards
5043         if (could_have_src &amp;&amp; !src_spec) {
5044           src = maybe_cast_profiled_obj(src, src_k, true);
5045         }
5046         if (could_have_dest &amp;&amp; !dest_spec) {
5047           dest = maybe_cast_profiled_obj(dest, dest_k, true);
5048         }
5049       }
5050     }
5051   }
5052 
5053   ciMethod* trap_method = method();
5054   int trap_bci = bci();
5055   if (saved_jvms != NULL) {
5056     trap_method = alloc-&gt;jvms()-&gt;method();
5057     trap_bci = alloc-&gt;jvms()-&gt;bci();
5058   }
5059 
5060   if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;&amp;
5061       can_emit_guards &amp;&amp;
5062       !src-&gt;is_top() &amp;&amp; !dest-&gt;is_top()) {
5063     // validate arguments: enables transformation the ArrayCopyNode
5064     validated = true;
5065 
5066     RegionNode* slow_region = new RegionNode(1);
5067     record_for_igvn(slow_region);
5068 
5069     // (1) src and dest are arrays.
5070     generate_non_array_guard(load_object_klass(src), slow_region);
5071     generate_non_array_guard(load_object_klass(dest), slow_region);
5072 
5073     // (2) src and dest arrays must have elements of the same BasicType
5074     // done at macro expansion or at Ideal transformation time
5075 
5076     // (4) src_offset must not be negative.
5077     generate_negative_guard(src_offset, slow_region);
5078 
5079     // (5) dest_offset must not be negative.
5080     generate_negative_guard(dest_offset, slow_region);
5081 
5082     // (7) src_offset + length must not exceed length of src.
5083     generate_limit_guard(src_offset, length,
5084                          load_array_length(src),
5085                          slow_region);
5086 
5087     // (8) dest_offset + length must not exceed length of dest.
5088     generate_limit_guard(dest_offset, length,
5089                          load_array_length(dest),
5090                          slow_region);
5091 
5092     // (9) each element of an oop array must be assignable
5093     Node* src_klass  = load_object_klass(src);
5094     Node* dest_klass = load_object_klass(dest);
5095     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5096 
5097     if (not_subtype_ctrl != top()) {
5098       PreserveJVMState pjvms(this);
5099       set_control(not_subtype_ctrl);
5100       uncommon_trap(Deoptimization::Reason_intrinsic,
5101                     Deoptimization::Action_make_not_entrant);
5102       assert(stopped(), "Should be stopped");
5103     }
5104     {
5105       PreserveJVMState pjvms(this);
5106       set_control(_gvn.transform(slow_region));
5107       uncommon_trap(Deoptimization::Reason_intrinsic,
5108                     Deoptimization::Action_make_not_entrant);
5109       assert(stopped(), "Should be stopped");
5110     }
5111   }
5112 
5113   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp);
5114 
5115   if (stopped()) {
5116     return true;
5117   }
5118 
5119   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL,
5120                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
5121                                           // so the compiler has a chance to eliminate them: during macro expansion,
5122                                           // we have to set their control (CastPP nodes are eliminated).
5123                                           load_object_klass(src), load_object_klass(dest),
5124                                           load_array_length(src), load_array_length(dest));
5125 
5126   ac-&gt;set_arraycopy(validated);
5127 
5128   Node* n = _gvn.transform(ac);
5129   if (n == ac) {
5130     ac-&gt;connect_outputs(this);
5131   } else {
5132     assert(validated, "shouldn't transform if all arguments not validated");
5133     set_all_memory(n);
5134   }
5135 
5136   return true;
5137 }
5138 
5139 
5140 // Helper function which determines if an arraycopy immediately follows
5141 // an allocation, with no intervening tests or other escapes for the object.
5142 AllocateArrayNode*
5143 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5144                                            RegionNode* slow_region) {
5145   if (stopped())             return NULL;  // no fast path
5146   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5147 
5148   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5149   if (alloc == NULL)  return NULL;
5150 
5151   Node* rawmem = memory(Compile::AliasIdxRaw);
5152   // Is the allocation's memory state untouched?
5153   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5154     // Bail out if there have been raw-memory effects since the allocation.
5155     // (Example:  There might have been a call or safepoint.)
5156     return NULL;
5157   }
5158   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5159   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5160     return NULL;
5161   }
5162 
5163   // There must be no unexpected observers of this allocation.
5164   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5165     Node* obs = ptr-&gt;fast_out(i);
5166     if (obs != this-&gt;map()) {
5167       return NULL;
5168     }
5169   }
5170 
5171   // This arraycopy must unconditionally follow the allocation of the ptr.
5172   Node* alloc_ctl = ptr-&gt;in(0);
5173   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5174 
5175   Node* ctl = control();
5176   while (ctl != alloc_ctl) {
5177     // There may be guards which feed into the slow_region.
5178     // Any other control flow means that we might not get a chance
5179     // to finish initializing the allocated object.
5180     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5181       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5182       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5183       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5184       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5185         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5186         continue;
5187       }
5188       // One more try:  Various low-level checks bottom out in
5189       // uncommon traps.  If the debug-info of the trap omits
5190       // any reference to the allocation, as we've already
5191       // observed, then there can be no objection to the trap.
5192       bool found_trap = false;
5193       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5194         Node* obs = not_ctl-&gt;fast_out(j);
5195         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5196             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5197           found_trap = true; break;
5198         }
5199       }
5200       if (found_trap) {
5201         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5202         continue;
5203       }
5204     }
5205     return NULL;
5206   }
5207 
5208   // If we get this far, we have an allocation which immediately
5209   // precedes the arraycopy, and we can take over zeroing the new object.
5210   // The arraycopy will finish the initialization, and provide
5211   // a new control state to which we will anchor the destination pointer.
5212 
5213   return alloc;
5214 }
5215 
5216 //-------------inline_encodeISOArray-----------------------------------
5217 // encode char[] to byte[] in ISO_8859_1
5218 bool LibraryCallKit::inline_encodeISOArray() {
5219   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5220   // no receiver since it is static method
5221   Node *src         = argument(0);
5222   Node *src_offset  = argument(1);
5223   Node *dst         = argument(2);
5224   Node *dst_offset  = argument(3);
5225   Node *length      = argument(4);
5226 
5227   const Type* src_type = src-&gt;Value(&amp;_gvn);
5228   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5229   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5230   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5231   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5232       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5233     // failed array check
5234     return false;
5235   }
5236 
5237   // Figure out the size and type of the elements we will be copying.
5238   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5239   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5240   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5241     return false;
5242   }
5243   Node* src_start = array_element_address(src, src_offset, src_elem);
5244   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5245   // 'src_start' points to src array + scaled offset
5246   // 'dst_start' points to dst array + scaled offset
5247 
5248   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5249   Node* enc = new EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5250   enc = _gvn.transform(enc);
5251   Node* res_mem = _gvn.transform(new SCMemProjNode(enc));
5252   set_memory(res_mem, mtype);
5253   set_result(enc);
5254   return true;
5255 }
5256 
5257 //-------------inline_multiplyToLen-----------------------------------
5258 bool LibraryCallKit::inline_multiplyToLen() {
5259   assert(UseMultiplyToLenIntrinsic, "not implementated on this platform");
5260 
5261   address stubAddr = StubRoutines::multiplyToLen();
5262   if (stubAddr == NULL) {
5263     return false; // Intrinsic's stub is not implemented on this platform
5264   }
5265   const char* stubName = "multiplyToLen";
5266 
5267   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5268 
5269   Node* x    = argument(1);
5270   Node* xlen = argument(2);
5271   Node* y    = argument(3);
5272   Node* ylen = argument(4);
5273   Node* z    = argument(5);
5274 
5275   const Type* x_type = x-&gt;Value(&amp;_gvn);
5276   const Type* y_type = y-&gt;Value(&amp;_gvn);
5277   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5278   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5279   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5280       top_y == NULL || top_y-&gt;klass() == NULL) {
5281     // failed array check
5282     return false;
5283   }
5284 
5285   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5286   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5287   if (x_elem != T_INT || y_elem != T_INT) {
5288     return false;
5289   }
5290 
5291   // Set the original stack and the reexecute bit for the interpreter to reexecute
5292   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5293   // on the return from z array allocation in runtime.
5294   { PreserveReexecuteState preexecs(this);
5295     jvms()-&gt;set_should_reexecute(true);
5296 
5297     Node* x_start = array_element_address(x, intcon(0), x_elem);
5298     Node* y_start = array_element_address(y, intcon(0), y_elem);
5299     // 'x_start' points to x array + scaled xlen
5300     // 'y_start' points to y array + scaled ylen
5301 
5302     // Allocate the result array
5303     Node* zlen = _gvn.transform(new AddINode(xlen, ylen));
5304     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5305     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5306 
5307     IdealKit ideal(this);
5308 
5309 #define __ ideal.
5310      Node* one = __ ConI(1);
5311      Node* zero = __ ConI(0);
5312      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5313      __ set(need_alloc, zero);
5314      __ set(z_alloc, z);
5315      __ if_then(z, BoolTest::eq, null()); {
5316        __ increment (need_alloc, one);
5317      } __ else_(); {
5318        // Update graphKit memory and control from IdealKit.
5319        sync_kit(ideal);
5320        Node* zlen_arg = load_array_length(z);
5321        // Update IdealKit memory and control from graphKit.
5322        __ sync_kit(this);
5323        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5324          __ increment (need_alloc, one);
5325        } __ end_if();
5326      } __ end_if();
5327 
5328      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5329        // Update graphKit memory and control from IdealKit.
5330        sync_kit(ideal);
5331        Node * narr = new_array(klass_node, zlen, 1);
5332        // Update IdealKit memory and control from graphKit.
5333        __ sync_kit(this);
5334        __ set(z_alloc, narr);
5335      } __ end_if();
5336 
5337      sync_kit(ideal);
5338      z = __ value(z_alloc);
5339      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5340      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5341      // Final sync IdealKit and GraphKit.
5342      final_sync(ideal);
5343 #undef __
5344 
5345     Node* z_start = array_element_address(z, intcon(0), T_INT);
5346 
5347     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5348                                    OptoRuntime::multiplyToLen_Type(),
5349                                    stubAddr, stubName, TypePtr::BOTTOM,
5350                                    x_start, xlen, y_start, ylen, z_start, zlen);
5351   } // original reexecute is set back here
5352 
5353   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5354   set_result(z);
5355   return true;
5356 }
5357 
5358 //-------------inline_squareToLen------------------------------------
5359 bool LibraryCallKit::inline_squareToLen() {
5360   assert(UseSquareToLenIntrinsic, "not implementated on this platform");
5361 
5362   address stubAddr = StubRoutines::squareToLen();
5363   if (stubAddr == NULL) {
5364     return false; // Intrinsic's stub is not implemented on this platform
5365   }
5366   const char* stubName = "squareToLen";
5367 
5368   assert(callee()-&gt;signature()-&gt;size() == 4, "implSquareToLen has 4 parameters");
5369 
5370   Node* x    = argument(0);
5371   Node* len  = argument(1);
5372   Node* z    = argument(2);
5373   Node* zlen = argument(3);
5374 
5375   const Type* x_type = x-&gt;Value(&amp;_gvn);
5376   const Type* z_type = z-&gt;Value(&amp;_gvn);
5377   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5378   const TypeAryPtr* top_z = z_type-&gt;isa_aryptr();
5379   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5380       top_z  == NULL || top_z-&gt;klass()  == NULL) {
5381     // failed array check
5382     return false;
5383   }
5384 
5385   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5386   BasicType z_elem = z_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5387   if (x_elem != T_INT || z_elem != T_INT) {
5388     return false;
5389   }
5390 
5391 
5392   Node* x_start = array_element_address(x, intcon(0), x_elem);
5393   Node* z_start = array_element_address(z, intcon(0), z_elem);
5394 
5395   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5396                                   OptoRuntime::squareToLen_Type(),
5397                                   stubAddr, stubName, TypePtr::BOTTOM,
5398                                   x_start, len, z_start, zlen);
5399 
5400   set_result(z);
5401   return true;
5402 }
5403 
5404 //-------------inline_mulAdd------------------------------------------
5405 bool LibraryCallKit::inline_mulAdd() {
5406   assert(UseMulAddIntrinsic, "not implementated on this platform");
5407 
5408   address stubAddr = StubRoutines::mulAdd();
5409   if (stubAddr == NULL) {
5410     return false; // Intrinsic's stub is not implemented on this platform
5411   }
5412   const char* stubName = "mulAdd";
5413 
5414   assert(callee()-&gt;signature()-&gt;size() == 5, "mulAdd has 5 parameters");
5415 
5416   Node* out      = argument(0);
5417   Node* in       = argument(1);
5418   Node* offset   = argument(2);
5419   Node* len      = argument(3);
5420   Node* k        = argument(4);
5421 
5422   const Type* out_type = out-&gt;Value(&amp;_gvn);
5423   const Type* in_type = in-&gt;Value(&amp;_gvn);
5424   const TypeAryPtr* top_out = out_type-&gt;isa_aryptr();
5425   const TypeAryPtr* top_in = in_type-&gt;isa_aryptr();
5426   if (top_out  == NULL || top_out-&gt;klass()  == NULL ||
5427       top_in == NULL || top_in-&gt;klass() == NULL) {
5428     // failed array check
5429     return false;
5430   }
5431 
5432   BasicType out_elem = out_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5433   BasicType in_elem = in_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5434   if (out_elem != T_INT || in_elem != T_INT) {
5435     return false;
5436   }
5437 
5438   Node* outlen = load_array_length(out);
5439   Node* new_offset = _gvn.transform(new SubINode(outlen, offset));
5440   Node* out_start = array_element_address(out, intcon(0), out_elem);
5441   Node* in_start = array_element_address(in, intcon(0), in_elem);
5442 
5443   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5444                                   OptoRuntime::mulAdd_Type(),
5445                                   stubAddr, stubName, TypePtr::BOTTOM,
5446                                   out_start,in_start, new_offset, len, k);
5447   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5448   set_result(result);
5449   return true;
5450 }
5451 
5452 
5453 /**
5454  * Calculate CRC32 for byte.
5455  * int java.util.zip.CRC32.update(int crc, int b)
5456  */
5457 bool LibraryCallKit::inline_updateCRC32() {
5458   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5459   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5460   // no receiver since it is static method
5461   Node* crc  = argument(0); // type: int
5462   Node* b    = argument(1); // type: int
5463 
5464   /*
5465    *    int c = ~ crc;
5466    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5467    *    b = b ^ (c &gt;&gt;&gt; 8);
5468    *    crc = ~b;
5469    */
5470 
5471   Node* M1 = intcon(-1);
5472   crc = _gvn.transform(new XorINode(crc, M1));
5473   Node* result = _gvn.transform(new XorINode(crc, b));
5474   result = _gvn.transform(new AndINode(result, intcon(0xFF)));
5475 
5476   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5477   Node* offset = _gvn.transform(new LShiftINode(result, intcon(0x2)));
5478   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5479   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5480 
5481   crc = _gvn.transform(new URShiftINode(crc, intcon(8)));
5482   result = _gvn.transform(new XorINode(crc, result));
5483   result = _gvn.transform(new XorINode(result, M1));
5484   set_result(result);
5485   return true;
5486 }
5487 
5488 /**
5489  * Calculate CRC32 for byte[] array.
5490  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5491  */
5492 bool LibraryCallKit::inline_updateBytesCRC32() {
5493   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5494   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5495   // no receiver since it is static method
5496   Node* crc     = argument(0); // type: int
5497   Node* src     = argument(1); // type: oop
5498   Node* offset  = argument(2); // type: int
5499   Node* length  = argument(3); // type: int
5500 
5501   const Type* src_type = src-&gt;Value(&amp;_gvn);
5502   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5503   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5504     // failed array check
5505     return false;
5506   }
5507 
5508   // Figure out the size and type of the elements we will be copying.
5509   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5510   if (src_elem != T_BYTE) {
5511     return false;
5512   }
5513 
5514   // 'src_start' points to src array + scaled offset
5515   Node* src_start = array_element_address(src, offset, src_elem);
5516 
5517   // We assume that range check is done by caller.
5518   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5519 
5520   // Call the stub.
5521   address stubAddr = StubRoutines::updateBytesCRC32();
5522   const char *stubName = "updateBytesCRC32";
5523 
5524   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5525                                  stubAddr, stubName, TypePtr::BOTTOM,
5526                                  crc, src_start, length);
5527   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5528   set_result(result);
5529   return true;
5530 }
5531 
5532 /**
5533  * Calculate CRC32 for ByteBuffer.
5534  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5535  */
5536 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5537   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5538   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5539   // no receiver since it is static method
5540   Node* crc     = argument(0); // type: int
5541   Node* src     = argument(1); // type: long
5542   Node* offset  = argument(3); // type: int
5543   Node* length  = argument(4); // type: int
5544 
5545   src = ConvL2X(src);  // adjust Java long to machine word
5546   Node* base = _gvn.transform(new CastX2PNode(src));
5547   offset = ConvI2X(offset);
5548 
5549   // 'src_start' points to src array + scaled offset
5550   Node* src_start = basic_plus_adr(top(), base, offset);
5551 
5552   // Call the stub.
5553   address stubAddr = StubRoutines::updateBytesCRC32();
5554   const char *stubName = "updateBytesCRC32";
5555 
5556   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5557                                  stubAddr, stubName, TypePtr::BOTTOM,
5558                                  crc, src_start, length);
5559   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5560   set_result(result);
5561   return true;
5562 }
5563 
5564 //------------------------------get_table_from_crc32c_class-----------------------
5565 Node * LibraryCallKit::get_table_from_crc32c_class(ciInstanceKlass *crc32c_class) {
5566   Node* table = load_field_from_object(NULL, "byteTable", "[I", /*is_exact*/ false, /*is_static*/ true, crc32c_class);
5567   assert (table != NULL, "wrong version of java.util.zip.CRC32C");
5568 
5569   return table;
5570 }
5571 
5572 //------------------------------inline_updateBytesCRC32C-----------------------
5573 //
5574 // Calculate CRC32C for byte[] array.
5575 // int java.util.zip.CRC32C.updateBytes(int crc, byte[] buf, int off, int end)
5576 //
5577 bool LibraryCallKit::inline_updateBytesCRC32C() {
5578   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5579   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5580   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5581   // no receiver since it is a static method
5582   Node* crc     = argument(0); // type: int
5583   Node* src     = argument(1); // type: oop
5584   Node* offset  = argument(2); // type: int
5585   Node* end     = argument(3); // type: int
5586 
5587   Node* length = _gvn.transform(new SubINode(end, offset));
5588 
5589   const Type* src_type = src-&gt;Value(&amp;_gvn);
5590   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5591   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5592     // failed array check
5593     return false;
5594   }
5595 
5596   // Figure out the size and type of the elements we will be copying.
5597   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5598   if (src_elem != T_BYTE) {
5599     return false;
5600   }
5601 
5602   // 'src_start' points to src array + scaled offset
5603   Node* src_start = array_element_address(src, offset, src_elem);
5604 
5605   // static final int[] byteTable in class CRC32C
5606   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5607   Node* table_start = array_element_address(table, intcon(0), T_INT);
5608 
5609   // We assume that range check is done by caller.
5610   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5611 
5612   // Call the stub.
5613   address stubAddr = StubRoutines::updateBytesCRC32C();
5614   const char *stubName = "updateBytesCRC32C";
5615 
5616   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5617                                  stubAddr, stubName, TypePtr::BOTTOM,
5618                                  crc, src_start, length, table_start);
5619   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5620   set_result(result);
5621   return true;
5622 }
5623 
5624 //------------------------------inline_updateDirectByteBufferCRC32C-----------------------
5625 //
5626 // Calculate CRC32C for DirectByteBuffer.
5627 // int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)
5628 //
5629 bool LibraryCallKit::inline_updateDirectByteBufferCRC32C() {
5630   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5631   assert(callee()-&gt;signature()-&gt;size() == 5, "updateDirectByteBuffer has 4 parameters and one is long");
5632   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5633   // no receiver since it is a static method
5634   Node* crc     = argument(0); // type: int
5635   Node* src     = argument(1); // type: long
5636   Node* offset  = argument(3); // type: int
5637   Node* end     = argument(4); // type: int
5638 
5639   Node* length = _gvn.transform(new SubINode(end, offset));
5640 
5641   src = ConvL2X(src);  // adjust Java long to machine word
5642   Node* base = _gvn.transform(new CastX2PNode(src));
5643   offset = ConvI2X(offset);
5644 
5645   // 'src_start' points to src array + scaled offset
5646   Node* src_start = basic_plus_adr(top(), base, offset);
5647 
5648   // static final int[] byteTable in class CRC32C
5649   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5650   Node* table_start = array_element_address(table, intcon(0), T_INT);
5651 
5652   // Call the stub.
5653   address stubAddr = StubRoutines::updateBytesCRC32C();
5654   const char *stubName = "updateBytesCRC32C";
5655 
5656   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5657                                  stubAddr, stubName, TypePtr::BOTTOM,
5658                                  crc, src_start, length, table_start);
5659   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5660   set_result(result);
5661   return true;
5662 }
5663 
5664 //----------------------------inline_reference_get----------------------------
5665 // public T java.lang.ref.Reference.get();
5666 bool LibraryCallKit::inline_reference_get() {
5667   const int referent_offset = java_lang_ref_Reference::referent_offset;
5668   guarantee(referent_offset &gt; 0, "should have already been set");
5669 
5670   // Get the argument:
5671   Node* reference_obj = null_check_receiver();
5672   if (stopped()) return true;
5673 
5674   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5675 
5676   ciInstanceKlass* klass = env()-&gt;Object_klass();
5677   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5678 
5679   Node* no_ctrl = NULL;
5680   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5681 
5682   // Use the pre-barrier to record the value in the referent field
5683   pre_barrier(false /* do_load */,
5684               control(),
5685               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5686               result /* pre_val */,
5687               T_OBJECT);
5688 
5689   // Add memory barrier to prevent commoning reads from this field
5690   // across safepoint since GC can change its value.
5691   insert_mem_bar(Op_MemBarCPUOrder);
5692 
5693   set_result(result);
5694   return true;
5695 }
5696 
5697 
5698 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5699                                               bool is_exact=true, bool is_static=false,
5700                                               ciInstanceKlass * fromKls=NULL) {
5701   if (fromKls == NULL) {
5702     const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5703     assert(tinst != NULL, "obj is null");
5704     assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5705     assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5706     fromKls = tinst-&gt;klass()-&gt;as_instance_klass();
5707   } else {
5708     assert(is_static, "only for static field access");
5709   }
5710   ciField* field = fromKls-&gt;get_field_by_name(ciSymbol::make(fieldName),
5711                                               ciSymbol::make(fieldTypeString),
5712                                               is_static);
5713 
5714   assert (field != NULL, "undefined field");
5715   if (field == NULL) return (Node *) NULL;
5716 
5717   if (is_static) {
5718     const TypeInstPtr* tip = TypeInstPtr::make(fromKls-&gt;java_mirror());
5719     fromObj = makecon(tip);
5720   }
5721 
5722   // Next code  copied from Parse::do_get_xxx():
5723 
5724   // Compute address and memory type.
5725   int offset  = field-&gt;offset_in_bytes();
5726   bool is_vol = field-&gt;is_volatile();
5727   ciType* field_klass = field-&gt;type();
5728   assert(field_klass-&gt;is_loaded(), "should be loaded");
5729   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5730   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5731   BasicType bt = field-&gt;layout_type();
5732 
5733   // Build the resultant type of the load
5734   const Type *type;
5735   if (bt == T_OBJECT) {
5736     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5737   } else {
5738     type = Type::get_const_basic_type(bt);
5739   }
5740 
5741   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
5742     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
5743   }
5744   // Build the load.
5745   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
5746   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, LoadNode::DependsOnlyOnTest, is_vol);
5747   // If reference is volatile, prevent following memory ops from
5748   // floating up past the volatile read.  Also prevents commoning
5749   // another volatile read.
5750   if (is_vol) {
5751     // Memory barrier includes bogus read of value to force load BEFORE membar
5752     insert_mem_bar(Op_MemBarAcquire, loadedField);
5753   }
5754   return loadedField;
5755 }
5756 
5757 
5758 //------------------------------inline_aescrypt_Block-----------------------
5759 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
5760   address stubAddr;
5761   const char *stubName;
5762   assert(UseAES, "need AES instruction support");
5763 
5764   switch(id) {
5765   case vmIntrinsics::_aescrypt_encryptBlock:
5766     stubAddr = StubRoutines::aescrypt_encryptBlock();
5767     stubName = "aescrypt_encryptBlock";
5768     break;
5769   case vmIntrinsics::_aescrypt_decryptBlock:
5770     stubAddr = StubRoutines::aescrypt_decryptBlock();
5771     stubName = "aescrypt_decryptBlock";
5772     break;
5773   }
5774   if (stubAddr == NULL) return false;
5775 
5776   Node* aescrypt_object = argument(0);
5777   Node* src             = argument(1);
5778   Node* src_offset      = argument(2);
5779   Node* dest            = argument(3);
5780   Node* dest_offset     = argument(4);
5781 
5782   // (1) src and dest are arrays.
5783   const Type* src_type = src-&gt;Value(&amp;_gvn);
5784   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5785   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5786   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5787   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5788 
5789   // for the quick and dirty code we will skip all the checks.
5790   // we are just trying to get the call to be generated.
5791   Node* src_start  = src;
5792   Node* dest_start = dest;
5793   if (src_offset != NULL || dest_offset != NULL) {
5794     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5795     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5796     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5797   }
5798 
5799   // now need to get the start of its expanded key array
5800   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5801   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5802   if (k_start == NULL) return false;
5803 
5804   if (Matcher::pass_original_key_for_aes()) {
5805     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5806     // compatibility issues between Java key expansion and SPARC crypto instructions
5807     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5808     if (original_k_start == NULL) return false;
5809 
5810     // Call the stub.
5811     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5812                       stubAddr, stubName, TypePtr::BOTTOM,
5813                       src_start, dest_start, k_start, original_k_start);
5814   } else {
5815     // Call the stub.
5816     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5817                       stubAddr, stubName, TypePtr::BOTTOM,
5818                       src_start, dest_start, k_start);
5819   }
5820 
5821   return true;
5822 }
5823 
5824 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
5825 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
5826   address stubAddr;
5827   const char *stubName;
5828 
5829   assert(UseAES, "need AES instruction support");
5830 
5831   switch(id) {
5832   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
5833     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
5834     stubName = "cipherBlockChaining_encryptAESCrypt";
5835     break;
5836   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
5837     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
5838     stubName = "cipherBlockChaining_decryptAESCrypt";
5839     break;
5840   }
5841   if (stubAddr == NULL) return false;
5842 
5843   Node* cipherBlockChaining_object = argument(0);
5844   Node* src                        = argument(1);
5845   Node* src_offset                 = argument(2);
5846   Node* len                        = argument(3);
5847   Node* dest                       = argument(4);
5848   Node* dest_offset                = argument(5);
5849 
5850   // (1) src and dest are arrays.
5851   const Type* src_type = src-&gt;Value(&amp;_gvn);
5852   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5853   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5854   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5855   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
5856           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5857 
5858   // checks are the responsibility of the caller
5859   Node* src_start  = src;
5860   Node* dest_start = dest;
5861   if (src_offset != NULL || dest_offset != NULL) {
5862     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5863     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5864     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5865   }
5866 
5867   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
5868   // (because of the predicated logic executed earlier).
5869   // so we cast it here safely.
5870   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5871 
5872   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5873   if (embeddedCipherObj == NULL) return false;
5874 
5875   // cast it to what we know it will be at runtime
5876   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
5877   assert(tinst != NULL, "CBC obj is null");
5878   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
5879   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5880   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
5881 
5882   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5883   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
5884   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
5885   Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
5886   aescrypt_object = _gvn.transform(aescrypt_object);
5887 
5888   // we need to get the start of the aescrypt_object's expanded key array
5889   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5890   if (k_start == NULL) return false;
5891 
5892   // similarly, get the start address of the r vector
5893   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
5894   if (objRvec == NULL) return false;
5895   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
5896 
5897   Node* cbcCrypt;
5898   if (Matcher::pass_original_key_for_aes()) {
5899     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5900     // compatibility issues between Java key expansion and SPARC crypto instructions
5901     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5902     if (original_k_start == NULL) return false;
5903 
5904     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
5905     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5906                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5907                                  stubAddr, stubName, TypePtr::BOTTOM,
5908                                  src_start, dest_start, k_start, r_start, len, original_k_start);
5909   } else {
5910     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
5911     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5912                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5913                                  stubAddr, stubName, TypePtr::BOTTOM,
5914                                  src_start, dest_start, k_start, r_start, len);
5915   }
5916 
5917   // return cipher length (int)
5918   Node* retvalue = _gvn.transform(new ProjNode(cbcCrypt, TypeFunc::Parms));
5919   set_result(retvalue);
5920   return true;
5921 }
5922 
5923 //------------------------------get_key_start_from_aescrypt_object-----------------------
5924 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
5925   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
5926   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5927   if (objAESCryptKey == NULL) return (Node *) NULL;
5928 
5929   // now have the array, need to get the start address of the K array
5930   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
5931   return k_start;
5932 }
5933 
5934 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
5935 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
5936   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
5937   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
5938   if (objAESCryptKey == NULL) return (Node *) NULL;
5939 
5940   // now have the array, need to get the start address of the lastKey array
5941   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
5942   return original_k_start;
5943 }
5944 
5945 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
5946 // Return node representing slow path of predicate check.
5947 // the pseudo code we want to emulate with this predicate is:
5948 // for encryption:
5949 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
5950 // for decryption:
5951 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
5952 //    note cipher==plain is more conservative than the original java code but that's OK
5953 //
5954 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
5955   // The receiver was checked for NULL already.
5956   Node* objCBC = argument(0);
5957 
5958   // Load embeddedCipher field of CipherBlockChaining object.
5959   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5960 
5961   // get AESCrypt klass for instanceOf check
5962   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
5963   // will have same classloader as CipherBlockChaining object
5964   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
5965   assert(tinst != NULL, "CBCobj is null");
5966   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
5967 
5968   // we want to do an instanceof comparison against the AESCrypt class
5969   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5970   if (!klass_AESCrypt-&gt;is_loaded()) {
5971     // if AESCrypt is not even loaded, we never take the intrinsic fast path
5972     Node* ctrl = control();
5973     set_control(top()); // no regular fast path
5974     return ctrl;
5975   }
5976   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5977 
5978   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
5979   Node* cmp_instof  = _gvn.transform(new CmpINode(instof, intcon(1)));
5980   Node* bool_instof  = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
5981 
5982   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
5983 
5984   // for encryption, we are done
5985   if (!decrypting)
5986     return instof_false;  // even if it is NULL
5987 
5988   // for decryption, we need to add a further check to avoid
5989   // taking the intrinsic path when cipher and plain are the same
5990   // see the original java code for why.
5991   RegionNode* region = new RegionNode(3);
5992   region-&gt;init_req(1, instof_false);
5993   Node* src = argument(1);
5994   Node* dest = argument(4);
5995   Node* cmp_src_dest = _gvn.transform(new CmpPNode(src, dest));
5996   Node* bool_src_dest = _gvn.transform(new BoolNode(cmp_src_dest, BoolTest::eq));
5997   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
5998   region-&gt;init_req(2, src_dest_conjoint);
5999 
6000   record_for_igvn(region);
6001   return _gvn.transform(region);
6002 }
6003 
6004 //------------------------------inline_ghash_processBlocks
6005 bool LibraryCallKit::inline_ghash_processBlocks() {
6006   address stubAddr;
6007   const char *stubName;
6008   assert(UseGHASHIntrinsics, "need GHASH intrinsics support");
6009 
6010   stubAddr = StubRoutines::ghash_processBlocks();
6011   stubName = "ghash_processBlocks";
6012 
6013   Node* data           = argument(0);
6014   Node* offset         = argument(1);
6015   Node* len            = argument(2);
6016   Node* state          = argument(3);
6017   Node* subkeyH        = argument(4);
6018 
6019   Node* state_start  = array_element_address(state, intcon(0), T_LONG);
6020   assert(state_start, "state is NULL");
6021   Node* subkeyH_start  = array_element_address(subkeyH, intcon(0), T_LONG);
6022   assert(subkeyH_start, "subkeyH is NULL");
6023   Node* data_start  = array_element_address(data, offset, T_BYTE);
6024   assert(data_start, "data is NULL");
6025 
6026   Node* ghash = make_runtime_call(RC_LEAF|RC_NO_FP,
6027                                   OptoRuntime::ghash_processBlocks_Type(),
6028                                   stubAddr, stubName, TypePtr::BOTTOM,
6029                                   state_start, subkeyH_start, data_start, len);
6030   return true;
6031 }
6032 
6033 //------------------------------inline_sha_implCompress-----------------------
6034 //
6035 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6036 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6037 //
6038 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6039 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6040 //
6041 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6042 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6043 //
6044 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6045   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6046 
6047   Node* sha_obj = argument(0);
6048   Node* src     = argument(1); // type oop
6049   Node* ofs     = argument(2); // type int
6050 
6051   const Type* src_type = src-&gt;Value(&amp;_gvn);
6052   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6053   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6054     // failed array check
6055     return false;
6056   }
6057   // Figure out the size and type of the elements we will be copying.
6058   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6059   if (src_elem != T_BYTE) {
6060     return false;
6061   }
6062   // 'src_start' points to src array + offset
6063   Node* src_start = array_element_address(src, ofs, src_elem);
6064   Node* state = NULL;
6065   address stubAddr;
6066   const char *stubName;
6067 
6068   switch(id) {
6069   case vmIntrinsics::_sha_implCompress:
6070     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6071     state = get_state_from_sha_object(sha_obj);
6072     stubAddr = StubRoutines::sha1_implCompress();
6073     stubName = "sha1_implCompress";
6074     break;
6075   case vmIntrinsics::_sha2_implCompress:
6076     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6077     state = get_state_from_sha_object(sha_obj);
6078     stubAddr = StubRoutines::sha256_implCompress();
6079     stubName = "sha256_implCompress";
6080     break;
6081   case vmIntrinsics::_sha5_implCompress:
6082     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6083     state = get_state_from_sha5_object(sha_obj);
6084     stubAddr = StubRoutines::sha512_implCompress();
6085     stubName = "sha512_implCompress";
6086     break;
6087   default:
6088     fatal_unexpected_iid(id);
6089     return false;
6090   }
6091   if (state == NULL) return false;
6092 
6093   // Call the stub.
6094   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6095                                  stubAddr, stubName, TypePtr::BOTTOM,
6096                                  src_start, state);
6097 
6098   return true;
6099 }
6100 
6101 //------------------------------inline_digestBase_implCompressMB-----------------------
6102 //
6103 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6104 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6105 //
6106 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6107   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6108          "need SHA1/SHA256/SHA512 instruction support");
6109   assert((uint)predicate &lt; 3, "sanity");
6110   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6111 
6112   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6113   Node* src            = argument(1); // byte[] array
6114   Node* ofs            = argument(2); // type int
6115   Node* limit          = argument(3); // type int
6116 
6117   const Type* src_type = src-&gt;Value(&amp;_gvn);
6118   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6119   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6120     // failed array check
6121     return false;
6122   }
6123   // Figure out the size and type of the elements we will be copying.
6124   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6125   if (src_elem != T_BYTE) {
6126     return false;
6127   }
6128   // 'src_start' points to src array + offset
6129   Node* src_start = array_element_address(src, ofs, src_elem);
6130 
6131   const char* klass_SHA_name = NULL;
6132   const char* stub_name = NULL;
6133   address     stub_addr = NULL;
6134   bool        long_state = false;
6135 
6136   switch (predicate) {
6137   case 0:
6138     if (UseSHA1Intrinsics) {
6139       klass_SHA_name = "sun/security/provider/SHA";
6140       stub_name = "sha1_implCompressMB";
6141       stub_addr = StubRoutines::sha1_implCompressMB();
6142     }
6143     break;
6144   case 1:
6145     if (UseSHA256Intrinsics) {
6146       klass_SHA_name = "sun/security/provider/SHA2";
6147       stub_name = "sha256_implCompressMB";
6148       stub_addr = StubRoutines::sha256_implCompressMB();
6149     }
6150     break;
6151   case 2:
6152     if (UseSHA512Intrinsics) {
6153       klass_SHA_name = "sun/security/provider/SHA5";
6154       stub_name = "sha512_implCompressMB";
6155       stub_addr = StubRoutines::sha512_implCompressMB();
6156       long_state = true;
6157     }
6158     break;
6159   default:
6160     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6161   }
6162   if (klass_SHA_name != NULL) {
6163     // get DigestBase klass to lookup for SHA klass
6164     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6165     assert(tinst != NULL, "digestBase_obj is not instance???");
6166     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6167 
6168     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6169     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6170     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6171     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6172   }
6173   return false;
6174 }
6175 //------------------------------inline_sha_implCompressMB-----------------------
6176 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6177                                                bool long_state, address stubAddr, const char *stubName,
6178                                                Node* src_start, Node* ofs, Node* limit) {
6179   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6180   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6181   Node* sha_obj = new CheckCastPPNode(control(), digestBase_obj, xtype);
6182   sha_obj = _gvn.transform(sha_obj);
6183 
6184   Node* state;
6185   if (long_state) {
6186     state = get_state_from_sha5_object(sha_obj);
6187   } else {
6188     state = get_state_from_sha_object(sha_obj);
6189   }
6190   if (state == NULL) return false;
6191 
6192   // Call the stub.
6193   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6194                                  OptoRuntime::digestBase_implCompressMB_Type(),
6195                                  stubAddr, stubName, TypePtr::BOTTOM,
6196                                  src_start, state, ofs, limit);
6197   // return ofs (int)
6198   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
6199   set_result(result);
6200 
6201   return true;
6202 }
6203 
6204 //------------------------------get_state_from_sha_object-----------------------
6205 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6206   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6207   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6208   if (sha_state == NULL) return (Node *) NULL;
6209 
6210   // now have the array, need to get the start address of the state array
6211   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6212   return state;
6213 }
6214 
6215 //------------------------------get_state_from_sha5_object-----------------------
6216 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6217   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6218   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6219   if (sha_state == NULL) return (Node *) NULL;
6220 
6221   // now have the array, need to get the start address of the state array
6222   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6223   return state;
6224 }
6225 
6226 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6227 // Return node representing slow path of predicate check.
6228 // the pseudo code we want to emulate with this predicate is:
6229 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6230 //
6231 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6232   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6233          "need SHA1/SHA256/SHA512 instruction support");
6234   assert((uint)predicate &lt; 3, "sanity");
6235 
6236   // The receiver was checked for NULL already.
6237   Node* digestBaseObj = argument(0);
6238 
6239   // get DigestBase klass for instanceOf check
6240   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6241   assert(tinst != NULL, "digestBaseObj is null");
6242   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6243 
6244   const char* klass_SHA_name = NULL;
6245   switch (predicate) {
6246   case 0:
6247     if (UseSHA1Intrinsics) {
6248       // we want to do an instanceof comparison against the SHA class
6249       klass_SHA_name = "sun/security/provider/SHA";
6250     }
6251     break;
6252   case 1:
6253     if (UseSHA256Intrinsics) {
6254       // we want to do an instanceof comparison against the SHA2 class
6255       klass_SHA_name = "sun/security/provider/SHA2";
6256     }
6257     break;
6258   case 2:
6259     if (UseSHA512Intrinsics) {
6260       // we want to do an instanceof comparison against the SHA5 class
6261       klass_SHA_name = "sun/security/provider/SHA5";
6262     }
6263     break;
6264   default:
6265     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6266   }
6267 
6268   ciKlass* klass_SHA = NULL;
6269   if (klass_SHA_name != NULL) {
6270     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6271   }
6272   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6273     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6274     Node* ctrl = control();
6275     set_control(top()); // no intrinsic path
6276     return ctrl;
6277   }
6278   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6279 
6280   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6281   Node* cmp_instof = _gvn.transform(new CmpINode(instofSHA, intcon(1)));
6282   Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6283   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6284 
6285   return instof_false;  // even if it is NULL
6286 }
6287 
6288 bool LibraryCallKit::inline_profileBoolean() {
6289   Node* counts = argument(1);
6290   const TypeAryPtr* ary = NULL;
6291   ciArray* aobj = NULL;
6292   if (counts-&gt;is_Con()
6293       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL
6294       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL
6295       &amp;&amp; (aobj-&gt;length() == 2)) {
6296     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
6297     jint false_cnt = aobj-&gt;element_value(0).as_int();
6298     jint  true_cnt = aobj-&gt;element_value(1).as_int();
6299 
6300     if (C-&gt;log() != NULL) {
6301       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",
6302                      false_cnt, true_cnt);
6303     }
6304 
6305     if (false_cnt + true_cnt == 0) {
6306       // According to profile, never executed.
6307       uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6308                           Deoptimization::Action_reinterpret);
6309       return true;
6310     }
6311 
6312     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)
6313     // is a number of each value occurrences.
6314     Node* result = argument(0);
6315     if (false_cnt == 0 || true_cnt == 0) {
6316       // According to profile, one value has been never seen.
6317       int expected_val = (false_cnt == 0) ? 1 : 0;
6318 
6319       Node* cmp  = _gvn.transform(new CmpINode(result, intcon(expected_val)));
6320       Node* test = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
6321 
6322       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
6323       Node* fast_path = _gvn.transform(new IfTrueNode(check));
6324       Node* slow_path = _gvn.transform(new IfFalseNode(check));
6325 
6326       { // Slow path: uncommon trap for never seen value and then reexecute
6327         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
6328         // the value has been seen at least once.
6329         PreserveJVMState pjvms(this);
6330         PreserveReexecuteState preexecs(this);
6331         jvms()-&gt;set_should_reexecute(true);
6332 
6333         set_control(slow_path);
6334         set_i_o(i_o());
6335 
6336         uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6337                             Deoptimization::Action_reinterpret);
6338       }
6339       // The guard for never seen value enables sharpening of the result and
6340       // returning a constant. It allows to eliminate branches on the same value
6341       // later on.
6342       set_control(fast_path);
6343       result = intcon(expected_val);
6344     }
6345     // Stop profiling.
6346     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
6347     // By replacing method body with profile data (represented as ProfileBooleanNode
6348     // on IR level) we effectively disable profiling.
6349     // It enables full speed execution once optimized code is generated.
6350     Node* profile = _gvn.transform(new ProfileBooleanNode(result, false_cnt, true_cnt));
6351     C-&gt;record_for_igvn(profile);
6352     set_result(profile);
6353     return true;
6354   } else {
6355     // Continue profiling.
6356     // Profile data isn't available at the moment. So, execute method's bytecode version.
6357     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
6358     // is compiled and counters aren't available since corresponding MethodHandle
6359     // isn't a compile-time constant.
6360     return false;
6361   }
6362 }
6363 
6364 bool LibraryCallKit::inline_isCompileConstant() {
6365   Node* n = argument(0);
6366   set_result(n-&gt;is_Con() ? intcon(1) : intcon(0));
6367   return true;
6368 }
</pre></body></html>
