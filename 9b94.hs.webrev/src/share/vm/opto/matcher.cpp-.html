<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/opto/matcher.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "memory/allocation.inline.hpp"
  27 #include "opto/ad.hpp"
  28 #include "opto/addnode.hpp"
  29 #include "opto/callnode.hpp"
  30 #include "opto/idealGraphPrinter.hpp"
  31 #include "opto/matcher.hpp"
  32 #include "opto/memnode.hpp"
  33 #include "opto/movenode.hpp"
  34 #include "opto/opcodes.hpp"
  35 #include "opto/regmask.hpp"
  36 #include "opto/rootnode.hpp"
  37 #include "opto/runtime.hpp"
  38 #include "opto/type.hpp"
  39 #include "opto/vectornode.hpp"
  40 #include "runtime/os.hpp"
  41 #include "runtime/sharedRuntime.hpp"
  42 
  43 OptoReg::Name OptoReg::c_frame_pointer;
  44 
  45 const RegMask *Matcher::idealreg2regmask[_last_machine_leaf];
  46 RegMask Matcher::mreg2regmask[_last_Mach_Reg];
  47 RegMask Matcher::STACK_ONLY_mask;
  48 RegMask Matcher::c_frame_ptr_mask;
  49 const uint Matcher::_begin_rematerialize = _BEGIN_REMATERIALIZE;
  50 const uint Matcher::_end_rematerialize   = _END_REMATERIALIZE;
  51 
  52 //---------------------------Matcher-------------------------------------------
  53 Matcher::Matcher()
  54 : PhaseTransform( Phase::Ins_Select ),
  55 #ifdef ASSERT
  56   _old2new_map(C-&gt;comp_arena()),
  57   _new2old_map(C-&gt;comp_arena()),
  58 #endif
  59   _shared_nodes(C-&gt;comp_arena()),
  60   _reduceOp(reduceOp), _leftOp(leftOp), _rightOp(rightOp),
  61   _swallowed(swallowed),
  62   _begin_inst_chain_rule(_BEGIN_INST_CHAIN_RULE),
  63   _end_inst_chain_rule(_END_INST_CHAIN_RULE),
  64   _must_clone(must_clone),
  65   _register_save_policy(register_save_policy),
  66   _c_reg_save_policy(c_reg_save_policy),
  67   _register_save_type(register_save_type),
  68   _ruleName(ruleName),
  69   _allocation_started(false),
  70   _states_arena(Chunk::medium_size),
  71   _visited(&amp;_states_arena),
  72   _shared(&amp;_states_arena),
  73   _dontcare(&amp;_states_arena) {
  74   C-&gt;set_matcher(this);
  75 
  76   idealreg2spillmask  [Op_RegI] = NULL;
  77   idealreg2spillmask  [Op_RegN] = NULL;
  78   idealreg2spillmask  [Op_RegL] = NULL;
  79   idealreg2spillmask  [Op_RegF] = NULL;
  80   idealreg2spillmask  [Op_RegD] = NULL;
  81   idealreg2spillmask  [Op_RegP] = NULL;
  82   idealreg2spillmask  [Op_VecS] = NULL;
  83   idealreg2spillmask  [Op_VecD] = NULL;
  84   idealreg2spillmask  [Op_VecX] = NULL;
  85   idealreg2spillmask  [Op_VecY] = NULL;
  86   idealreg2spillmask  [Op_VecZ] = NULL;
  87 
  88   idealreg2debugmask  [Op_RegI] = NULL;
  89   idealreg2debugmask  [Op_RegN] = NULL;
  90   idealreg2debugmask  [Op_RegL] = NULL;
  91   idealreg2debugmask  [Op_RegF] = NULL;
  92   idealreg2debugmask  [Op_RegD] = NULL;
  93   idealreg2debugmask  [Op_RegP] = NULL;
  94   idealreg2debugmask  [Op_VecS] = NULL;
  95   idealreg2debugmask  [Op_VecD] = NULL;
  96   idealreg2debugmask  [Op_VecX] = NULL;
  97   idealreg2debugmask  [Op_VecY] = NULL;
  98   idealreg2debugmask  [Op_VecZ] = NULL;
  99 
 100   idealreg2mhdebugmask[Op_RegI] = NULL;
 101   idealreg2mhdebugmask[Op_RegN] = NULL;
 102   idealreg2mhdebugmask[Op_RegL] = NULL;
 103   idealreg2mhdebugmask[Op_RegF] = NULL;
 104   idealreg2mhdebugmask[Op_RegD] = NULL;
 105   idealreg2mhdebugmask[Op_RegP] = NULL;
 106   idealreg2mhdebugmask[Op_VecS] = NULL;
 107   idealreg2mhdebugmask[Op_VecD] = NULL;
 108   idealreg2mhdebugmask[Op_VecX] = NULL;
 109   idealreg2mhdebugmask[Op_VecY] = NULL;
 110   idealreg2mhdebugmask[Op_VecZ] = NULL;
 111 
 112   debug_only(_mem_node = NULL;)   // Ideal memory node consumed by mach node
 113 }
 114 
 115 //------------------------------warp_incoming_stk_arg------------------------
 116 // This warps a VMReg into an OptoReg::Name
 117 OptoReg::Name Matcher::warp_incoming_stk_arg( VMReg reg ) {
 118   OptoReg::Name warped;
 119   if( reg-&gt;is_stack() ) {  // Stack slot argument?
 120     warped = OptoReg::add(_old_SP, reg-&gt;reg2stack() );
 121     warped = OptoReg::add(warped, C-&gt;out_preserve_stack_slots());
 122     if( warped &gt;= _in_arg_limit )
 123       _in_arg_limit = OptoReg::add(warped, 1); // Bump max stack slot seen
 124     if (!RegMask::can_represent_arg(warped)) {
 125       // the compiler cannot represent this method's calling sequence
 126       C-&gt;record_method_not_compilable_all_tiers("unsupported incoming calling sequence");
 127       return OptoReg::Bad;
 128     }
 129     return warped;
 130   }
 131   return OptoReg::as_OptoReg(reg);
 132 }
 133 
 134 //---------------------------compute_old_SP------------------------------------
 135 OptoReg::Name Compile::compute_old_SP() {
 136   int fixed    = fixed_slots();
 137   int preserve = in_preserve_stack_slots();
 138   return OptoReg::stack2reg(round_to(fixed + preserve, Matcher::stack_alignment_in_slots()));
 139 }
 140 
 141 
 142 
 143 #ifdef ASSERT
 144 void Matcher::verify_new_nodes_only(Node* xroot) {
 145   // Make sure that the new graph only references new nodes
 146   ResourceMark rm;
 147   Unique_Node_List worklist;
 148   VectorSet visited(Thread::current()-&gt;resource_area());
 149   worklist.push(xroot);
 150   while (worklist.size() &gt; 0) {
 151     Node* n = worklist.pop();
 152     visited &lt;&lt;= n-&gt;_idx;
 153     assert(C-&gt;node_arena()-&gt;contains(n), "dead node");
 154     for (uint j = 0; j &lt; n-&gt;req(); j++) {
 155       Node* in = n-&gt;in(j);
 156       if (in != NULL) {
 157         assert(C-&gt;node_arena()-&gt;contains(in), "dead node");
 158         if (!visited.test(in-&gt;_idx)) {
 159           worklist.push(in);
 160         }
 161       }
 162     }
 163   }
 164 }
 165 #endif
 166 
 167 
 168 //---------------------------match---------------------------------------------
 169 void Matcher::match( ) {
 170   if( MaxLabelRootDepth &lt; 100 ) { // Too small?
 171     assert(false, "invalid MaxLabelRootDepth, increase it to 100 minimum");
 172     MaxLabelRootDepth = 100;
 173   }
 174   // One-time initialization of some register masks.
 175   init_spill_mask( C-&gt;root()-&gt;in(1) );
 176   _return_addr_mask = return_addr();
 177 #ifdef _LP64
 178   // Pointers take 2 slots in 64-bit land
 179   _return_addr_mask.Insert(OptoReg::add(return_addr(),1));
 180 #endif
 181 
 182   // Map a Java-signature return type into return register-value
 183   // machine registers for 0, 1 and 2 returned values.
 184   const TypeTuple *range = C-&gt;tf()-&gt;range();
 185   if( range-&gt;cnt() &gt; TypeFunc::Parms ) { // If not a void function
 186     // Get ideal-register return type
 187     int ireg = range-&gt;field_at(TypeFunc::Parms)-&gt;ideal_reg();
 188     // Get machine return register
 189     uint sop = C-&gt;start()-&gt;Opcode();
 190     OptoRegPair regs = return_value(ireg, false);
 191 
 192     // And mask for same
 193     _return_value_mask = RegMask(regs.first());
 194     if( OptoReg::is_valid(regs.second()) )
 195       _return_value_mask.Insert(regs.second());
 196   }
 197 
 198   // ---------------
 199   // Frame Layout
 200 
 201   // Need the method signature to determine the incoming argument types,
 202   // because the types determine which registers the incoming arguments are
 203   // in, and this affects the matched code.
 204   const TypeTuple *domain = C-&gt;tf()-&gt;domain();
 205   uint             argcnt = domain-&gt;cnt() - TypeFunc::Parms;
 206   BasicType *sig_bt        = NEW_RESOURCE_ARRAY( BasicType, argcnt );
 207   VMRegPair *vm_parm_regs  = NEW_RESOURCE_ARRAY( VMRegPair, argcnt );
 208   _parm_regs               = NEW_RESOURCE_ARRAY( OptoRegPair, argcnt );
 209   _calling_convention_mask = NEW_RESOURCE_ARRAY( RegMask, argcnt );
 210   uint i;
 211   for( i = 0; i&lt;argcnt; i++ ) {
 212     sig_bt[i] = domain-&gt;field_at(i+TypeFunc::Parms)-&gt;basic_type();
 213   }
 214 
 215   // Pass array of ideal registers and length to USER code (from the AD file)
 216   // that will convert this to an array of register numbers.
 217   const StartNode *start = C-&gt;start();
 218   start-&gt;calling_convention( sig_bt, vm_parm_regs, argcnt );
 219 #ifdef ASSERT
 220   // Sanity check users' calling convention.  Real handy while trying to
 221   // get the initial port correct.
 222   { for (uint i = 0; i&lt;argcnt; i++) {
 223       if( !vm_parm_regs[i].first()-&gt;is_valid() &amp;&amp; !vm_parm_regs[i].second()-&gt;is_valid() ) {
 224         assert(domain-&gt;field_at(i+TypeFunc::Parms)==Type::HALF, "only allowed on halve" );
 225         _parm_regs[i].set_bad();
 226         continue;
 227       }
 228       VMReg parm_reg = vm_parm_regs[i].first();
 229       assert(parm_reg-&gt;is_valid(), "invalid arg?");
 230       if (parm_reg-&gt;is_reg()) {
 231         OptoReg::Name opto_parm_reg = OptoReg::as_OptoReg(parm_reg);
 232         assert(can_be_java_arg(opto_parm_reg) ||
 233                C-&gt;stub_function() == CAST_FROM_FN_PTR(address, OptoRuntime::rethrow_C) ||
 234                opto_parm_reg == inline_cache_reg(),
 235                "parameters in register must be preserved by runtime stubs");
 236       }
 237       for (uint j = 0; j &lt; i; j++) {
 238         assert(parm_reg != vm_parm_regs[j].first(),
 239                "calling conv. must produce distinct regs");
 240       }
 241     }
 242   }
 243 #endif
 244 
 245   // Do some initial frame layout.
 246 
 247   // Compute the old incoming SP (may be called FP) as
 248   //   OptoReg::stack0() + locks + in_preserve_stack_slots + pad2.
 249   _old_SP = C-&gt;compute_old_SP();
 250   assert( is_even(_old_SP), "must be even" );
 251 
 252   // Compute highest incoming stack argument as
 253   //   _old_SP + out_preserve_stack_slots + incoming argument size.
 254   _in_arg_limit = OptoReg::add(_old_SP, C-&gt;out_preserve_stack_slots());
 255   assert( is_even(_in_arg_limit), "out_preserve must be even" );
 256   for( i = 0; i &lt; argcnt; i++ ) {
 257     // Permit args to have no register
 258     _calling_convention_mask[i].Clear();
 259     if( !vm_parm_regs[i].first()-&gt;is_valid() &amp;&amp; !vm_parm_regs[i].second()-&gt;is_valid() ) {
 260       continue;
 261     }
 262     // calling_convention returns stack arguments as a count of
 263     // slots beyond OptoReg::stack0()/VMRegImpl::stack0.  We need to convert this to
 264     // the allocators point of view, taking into account all the
 265     // preserve area, locks &amp; pad2.
 266 
 267     OptoReg::Name reg1 = warp_incoming_stk_arg(vm_parm_regs[i].first());
 268     if( OptoReg::is_valid(reg1))
 269       _calling_convention_mask[i].Insert(reg1);
 270 
 271     OptoReg::Name reg2 = warp_incoming_stk_arg(vm_parm_regs[i].second());
 272     if( OptoReg::is_valid(reg2))
 273       _calling_convention_mask[i].Insert(reg2);
 274 
 275     // Saved biased stack-slot register number
 276     _parm_regs[i].set_pair(reg2, reg1);
 277   }
 278 
 279   // Finally, make sure the incoming arguments take up an even number of
 280   // words, in case the arguments or locals need to contain doubleword stack
 281   // slots.  The rest of the system assumes that stack slot pairs (in
 282   // particular, in the spill area) which look aligned will in fact be
 283   // aligned relative to the stack pointer in the target machine.  Double
 284   // stack slots will always be allocated aligned.
 285   _new_SP = OptoReg::Name(round_to(_in_arg_limit, RegMask::SlotsPerLong));
 286 
 287   // Compute highest outgoing stack argument as
 288   //   _new_SP + out_preserve_stack_slots + max(outgoing argument size).
 289   _out_arg_limit = OptoReg::add(_new_SP, C-&gt;out_preserve_stack_slots());
 290   assert( is_even(_out_arg_limit), "out_preserve must be even" );
 291 
 292   if (!RegMask::can_represent_arg(OptoReg::add(_out_arg_limit,-1))) {
 293     // the compiler cannot represent this method's calling sequence
 294     C-&gt;record_method_not_compilable("must be able to represent all call arguments in reg mask");
 295   }
 296 
 297   if (C-&gt;failing())  return;  // bailed out on incoming arg failure
 298 
 299   // ---------------
 300   // Collect roots of matcher trees.  Every node for which
 301   // _shared[_idx] is cleared is guaranteed to not be shared, and thus
 302   // can be a valid interior of some tree.
 303   find_shared( C-&gt;root() );
 304   find_shared( C-&gt;top() );
 305 
 306   C-&gt;print_method(PHASE_BEFORE_MATCHING);
 307 
 308   // Create new ideal node ConP #NULL even if it does exist in old space
 309   // to avoid false sharing if the corresponding mach node is not used.
 310   // The corresponding mach node is only used in rare cases for derived
 311   // pointers.
 312   Node* new_ideal_null = ConNode::make(TypePtr::NULL_PTR);
 313 
 314   // Swap out to old-space; emptying new-space
 315   Arena *old = C-&gt;node_arena()-&gt;move_contents(C-&gt;old_arena());
 316 
 317   // Save debug and profile information for nodes in old space:
 318   _old_node_note_array = C-&gt;node_note_array();
 319   if (_old_node_note_array != NULL) {
 320     C-&gt;set_node_note_array(new(C-&gt;comp_arena()) GrowableArray&lt;Node_Notes*&gt;
 321                            (C-&gt;comp_arena(), _old_node_note_array-&gt;length(),
 322                             0, NULL));
 323   }
 324 
 325   // Pre-size the new_node table to avoid the need for range checks.
 326   grow_new_node_array(C-&gt;unique());
 327 
 328   // Reset node counter so MachNodes start with _idx at 0
 329   int live_nodes = C-&gt;live_nodes();
 330   C-&gt;set_unique(0);
 331   C-&gt;reset_dead_node_list();
 332 
 333   // Recursively match trees from old space into new space.
 334   // Correct leaves of new-space Nodes; they point to old-space.
 335   _visited.Clear();             // Clear visit bits for xform call
 336   C-&gt;set_cached_top_node(xform( C-&gt;top(), live_nodes ));
 337   if (!C-&gt;failing()) {
 338     Node* xroot =        xform( C-&gt;root(), 1 );
 339     if (xroot == NULL) {
 340       Matcher::soft_match_failure();  // recursive matching process failed
 341       C-&gt;record_method_not_compilable("instruction match failed");
 342     } else {
 343       // During matching shared constants were attached to C-&gt;root()
 344       // because xroot wasn't available yet, so transfer the uses to
 345       // the xroot.
 346       for( DUIterator_Fast jmax, j = C-&gt;root()-&gt;fast_outs(jmax); j &lt; jmax; j++ ) {
 347         Node* n = C-&gt;root()-&gt;fast_out(j);
 348         if (C-&gt;node_arena()-&gt;contains(n)) {
 349           assert(n-&gt;in(0) == C-&gt;root(), "should be control user");
 350           n-&gt;set_req(0, xroot);
 351           --j;
 352           --jmax;
 353         }
 354       }
 355 
 356       // Generate new mach node for ConP #NULL
 357       assert(new_ideal_null != NULL, "sanity");
 358       _mach_null = match_tree(new_ideal_null);
 359       // Don't set control, it will confuse GCM since there are no uses.
 360       // The control will be set when this node is used first time
 361       // in find_base_for_derived().
 362       assert(_mach_null != NULL, "");
 363 
 364       C-&gt;set_root(xroot-&gt;is_Root() ? xroot-&gt;as_Root() : NULL);
 365 
 366 #ifdef ASSERT
 367       verify_new_nodes_only(xroot);
 368 #endif
 369     }
 370   }
 371   if (C-&gt;top() == NULL || C-&gt;root() == NULL) {
 372     C-&gt;record_method_not_compilable("graph lost"); // %%% cannot happen?
 373   }
 374   if (C-&gt;failing()) {
 375     // delete old;
 376     old-&gt;destruct_contents();
 377     return;
 378   }
 379   assert( C-&gt;top(), "" );
 380   assert( C-&gt;root(), "" );
 381   validate_null_checks();
 382 
 383   // Now smoke old-space
 384   NOT_DEBUG( old-&gt;destruct_contents() );
 385 
 386   // ------------------------
 387   // Set up save-on-entry registers
 388   Fixup_Save_On_Entry( );
 389 }
 390 
 391 
 392 //------------------------------Fixup_Save_On_Entry----------------------------
 393 // The stated purpose of this routine is to take care of save-on-entry
 394 // registers.  However, the overall goal of the Match phase is to convert into
 395 // machine-specific instructions which have RegMasks to guide allocation.
 396 // So what this procedure really does is put a valid RegMask on each input
 397 // to the machine-specific variations of all Return, TailCall and Halt
 398 // instructions.  It also adds edgs to define the save-on-entry values (and of
 399 // course gives them a mask).
 400 
 401 static RegMask *init_input_masks( uint size, RegMask &amp;ret_adr, RegMask &amp;fp ) {
 402   RegMask *rms = NEW_RESOURCE_ARRAY( RegMask, size );
 403   // Do all the pre-defined register masks
 404   rms[TypeFunc::Control  ] = RegMask::Empty;
 405   rms[TypeFunc::I_O      ] = RegMask::Empty;
 406   rms[TypeFunc::Memory   ] = RegMask::Empty;
 407   rms[TypeFunc::ReturnAdr] = ret_adr;
 408   rms[TypeFunc::FramePtr ] = fp;
 409   return rms;
 410 }
 411 
 412 //---------------------------init_first_stack_mask-----------------------------
 413 // Create the initial stack mask used by values spilling to the stack.
 414 // Disallow any debug info in outgoing argument areas by setting the
 415 // initial mask accordingly.
 416 void Matcher::init_first_stack_mask() {
 417 
 418   // Allocate storage for spill masks as masks for the appropriate load type.
 419   RegMask *rms = (RegMask*)C-&gt;comp_arena()-&gt;Amalloc_D(sizeof(RegMask) * (3*6+5));
 420 
 421   idealreg2spillmask  [Op_RegN] = &amp;rms[0];
 422   idealreg2spillmask  [Op_RegI] = &amp;rms[1];
 423   idealreg2spillmask  [Op_RegL] = &amp;rms[2];
 424   idealreg2spillmask  [Op_RegF] = &amp;rms[3];
 425   idealreg2spillmask  [Op_RegD] = &amp;rms[4];
 426   idealreg2spillmask  [Op_RegP] = &amp;rms[5];
 427 
 428   idealreg2debugmask  [Op_RegN] = &amp;rms[6];
 429   idealreg2debugmask  [Op_RegI] = &amp;rms[7];
 430   idealreg2debugmask  [Op_RegL] = &amp;rms[8];
 431   idealreg2debugmask  [Op_RegF] = &amp;rms[9];
 432   idealreg2debugmask  [Op_RegD] = &amp;rms[10];
 433   idealreg2debugmask  [Op_RegP] = &amp;rms[11];
 434 
 435   idealreg2mhdebugmask[Op_RegN] = &amp;rms[12];
 436   idealreg2mhdebugmask[Op_RegI] = &amp;rms[13];
 437   idealreg2mhdebugmask[Op_RegL] = &amp;rms[14];
 438   idealreg2mhdebugmask[Op_RegF] = &amp;rms[15];
 439   idealreg2mhdebugmask[Op_RegD] = &amp;rms[16];
 440   idealreg2mhdebugmask[Op_RegP] = &amp;rms[17];
 441 
 442   idealreg2spillmask  [Op_VecS] = &amp;rms[18];
 443   idealreg2spillmask  [Op_VecD] = &amp;rms[19];
 444   idealreg2spillmask  [Op_VecX] = &amp;rms[20];
 445   idealreg2spillmask  [Op_VecY] = &amp;rms[21];
 446   idealreg2spillmask  [Op_VecZ] = &amp;rms[22];
 447 
 448   OptoReg::Name i;
 449 
 450   // At first, start with the empty mask
 451   C-&gt;FIRST_STACK_mask().Clear();
 452 
 453   // Add in the incoming argument area
 454   OptoReg::Name init_in = OptoReg::add(_old_SP, C-&gt;out_preserve_stack_slots());
 455   for (i = init_in; i &lt; _in_arg_limit; i = OptoReg::add(i,1)) {
 456     C-&gt;FIRST_STACK_mask().Insert(i);
 457   }
 458   // Add in all bits past the outgoing argument area
 459   guarantee(RegMask::can_represent_arg(OptoReg::add(_out_arg_limit,-1)),
 460             "must be able to represent all call arguments in reg mask");
 461   OptoReg::Name init = _out_arg_limit;
 462   for (i = init; RegMask::can_represent(i); i = OptoReg::add(i,1)) {
 463     C-&gt;FIRST_STACK_mask().Insert(i);
 464   }
 465   // Finally, set the "infinite stack" bit.
 466   C-&gt;FIRST_STACK_mask().set_AllStack();
 467 
 468   // Make spill masks.  Registers for their class, plus FIRST_STACK_mask.
 469   RegMask aligned_stack_mask = C-&gt;FIRST_STACK_mask();
 470   // Keep spill masks aligned.
 471   aligned_stack_mask.clear_to_pairs();
 472   assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 473 
 474   *idealreg2spillmask[Op_RegP] = *idealreg2regmask[Op_RegP];
 475 #ifdef _LP64
 476   *idealreg2spillmask[Op_RegN] = *idealreg2regmask[Op_RegN];
 477    idealreg2spillmask[Op_RegN]-&gt;OR(C-&gt;FIRST_STACK_mask());
 478    idealreg2spillmask[Op_RegP]-&gt;OR(aligned_stack_mask);
 479 #else
 480    idealreg2spillmask[Op_RegP]-&gt;OR(C-&gt;FIRST_STACK_mask());
 481 #endif
 482   *idealreg2spillmask[Op_RegI] = *idealreg2regmask[Op_RegI];
 483    idealreg2spillmask[Op_RegI]-&gt;OR(C-&gt;FIRST_STACK_mask());
 484   *idealreg2spillmask[Op_RegL] = *idealreg2regmask[Op_RegL];
 485    idealreg2spillmask[Op_RegL]-&gt;OR(aligned_stack_mask);
 486   *idealreg2spillmask[Op_RegF] = *idealreg2regmask[Op_RegF];
 487    idealreg2spillmask[Op_RegF]-&gt;OR(C-&gt;FIRST_STACK_mask());
 488   *idealreg2spillmask[Op_RegD] = *idealreg2regmask[Op_RegD];
 489    idealreg2spillmask[Op_RegD]-&gt;OR(aligned_stack_mask);
 490 
 491   if (Matcher::vector_size_supported(T_BYTE,4)) {
 492     *idealreg2spillmask[Op_VecS] = *idealreg2regmask[Op_VecS];
 493      idealreg2spillmask[Op_VecS]-&gt;OR(C-&gt;FIRST_STACK_mask());
 494   }
 495   if (Matcher::vector_size_supported(T_FLOAT,2)) {
 496     // For VecD we need dual alignment and 8 bytes (2 slots) for spills.
 497     // RA guarantees such alignment since it is needed for Double and Long values.
 498     *idealreg2spillmask[Op_VecD] = *idealreg2regmask[Op_VecD];
 499      idealreg2spillmask[Op_VecD]-&gt;OR(aligned_stack_mask);
 500   }
 501   if (Matcher::vector_size_supported(T_FLOAT,4)) {
 502     // For VecX we need quadro alignment and 16 bytes (4 slots) for spills.
 503     //
 504     // RA can use input arguments stack slots for spills but until RA
 505     // we don't know frame size and offset of input arg stack slots.
 506     //
 507     // Exclude last input arg stack slots to avoid spilling vectors there
 508     // otherwise vector spills could stomp over stack slots in caller frame.
 509     OptoReg::Name in = OptoReg::add(_in_arg_limit, -1);
 510     for (int k = 1; (in &gt;= init_in) &amp;&amp; (k &lt; RegMask::SlotsPerVecX); k++) {
 511       aligned_stack_mask.Remove(in);
 512       in = OptoReg::add(in, -1);
 513     }
 514      aligned_stack_mask.clear_to_sets(RegMask::SlotsPerVecX);
 515      assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 516     *idealreg2spillmask[Op_VecX] = *idealreg2regmask[Op_VecX];
 517      idealreg2spillmask[Op_VecX]-&gt;OR(aligned_stack_mask);
 518   }
 519   if (Matcher::vector_size_supported(T_FLOAT,8)) {
 520     // For VecY we need octo alignment and 32 bytes (8 slots) for spills.
 521     OptoReg::Name in = OptoReg::add(_in_arg_limit, -1);
 522     for (int k = 1; (in &gt;= init_in) &amp;&amp; (k &lt; RegMask::SlotsPerVecY); k++) {
 523       aligned_stack_mask.Remove(in);
 524       in = OptoReg::add(in, -1);
 525     }
 526      aligned_stack_mask.clear_to_sets(RegMask::SlotsPerVecY);
 527      assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 528     *idealreg2spillmask[Op_VecY] = *idealreg2regmask[Op_VecY];
 529      idealreg2spillmask[Op_VecY]-&gt;OR(aligned_stack_mask);
 530   }
 531   if (Matcher::vector_size_supported(T_FLOAT,16)) {
 532     // For VecZ we need enough alignment and 64 bytes (16 slots) for spills.
 533     OptoReg::Name in = OptoReg::add(_in_arg_limit, -1);
 534     for (int k = 1; (in &gt;= init_in) &amp;&amp; (k &lt; RegMask::SlotsPerVecZ); k++) {
 535       aligned_stack_mask.Remove(in);
 536       in = OptoReg::add(in, -1);
 537     }
 538      aligned_stack_mask.clear_to_sets(RegMask::SlotsPerVecZ);
 539      assert(aligned_stack_mask.is_AllStack(), "should be infinite stack");
 540     *idealreg2spillmask[Op_VecZ] = *idealreg2regmask[Op_VecZ];
 541      idealreg2spillmask[Op_VecZ]-&gt;OR(aligned_stack_mask);
 542   }
 543    if (UseFPUForSpilling) {
 544      // This mask logic assumes that the spill operations are
 545      // symmetric and that the registers involved are the same size.
 546      // On sparc for instance we may have to use 64 bit moves will
 547      // kill 2 registers when used with F0-F31.
 548      idealreg2spillmask[Op_RegI]-&gt;OR(*idealreg2regmask[Op_RegF]);
 549      idealreg2spillmask[Op_RegF]-&gt;OR(*idealreg2regmask[Op_RegI]);
 550 #ifdef _LP64
 551      idealreg2spillmask[Op_RegN]-&gt;OR(*idealreg2regmask[Op_RegF]);
 552      idealreg2spillmask[Op_RegL]-&gt;OR(*idealreg2regmask[Op_RegD]);
 553      idealreg2spillmask[Op_RegD]-&gt;OR(*idealreg2regmask[Op_RegL]);
 554      idealreg2spillmask[Op_RegP]-&gt;OR(*idealreg2regmask[Op_RegD]);
 555 #else
 556      idealreg2spillmask[Op_RegP]-&gt;OR(*idealreg2regmask[Op_RegF]);
 557 #ifdef ARM
 558      // ARM has support for moving 64bit values between a pair of
 559      // integer registers and a double register
 560      idealreg2spillmask[Op_RegL]-&gt;OR(*idealreg2regmask[Op_RegD]);
 561      idealreg2spillmask[Op_RegD]-&gt;OR(*idealreg2regmask[Op_RegL]);
 562 #endif
 563 #endif
 564    }
 565 
 566   // Make up debug masks.  Any spill slot plus callee-save registers.
 567   // Caller-save registers are assumed to be trashable by the various
 568   // inline-cache fixup routines.
 569   *idealreg2debugmask  [Op_RegN]= *idealreg2spillmask[Op_RegN];
 570   *idealreg2debugmask  [Op_RegI]= *idealreg2spillmask[Op_RegI];
 571   *idealreg2debugmask  [Op_RegL]= *idealreg2spillmask[Op_RegL];
 572   *idealreg2debugmask  [Op_RegF]= *idealreg2spillmask[Op_RegF];
 573   *idealreg2debugmask  [Op_RegD]= *idealreg2spillmask[Op_RegD];
 574   *idealreg2debugmask  [Op_RegP]= *idealreg2spillmask[Op_RegP];
 575 
 576   *idealreg2mhdebugmask[Op_RegN]= *idealreg2spillmask[Op_RegN];
 577   *idealreg2mhdebugmask[Op_RegI]= *idealreg2spillmask[Op_RegI];
 578   *idealreg2mhdebugmask[Op_RegL]= *idealreg2spillmask[Op_RegL];
 579   *idealreg2mhdebugmask[Op_RegF]= *idealreg2spillmask[Op_RegF];
 580   *idealreg2mhdebugmask[Op_RegD]= *idealreg2spillmask[Op_RegD];
 581   *idealreg2mhdebugmask[Op_RegP]= *idealreg2spillmask[Op_RegP];
 582 
 583   // Prevent stub compilations from attempting to reference
 584   // callee-saved registers from debug info
 585   bool exclude_soe = !Compile::current()-&gt;is_method_compilation();
 586 
 587   for( i=OptoReg::Name(0); i&lt;OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {
 588     // registers the caller has to save do not work
 589     if( _register_save_policy[i] == 'C' ||
 590         _register_save_policy[i] == 'A' ||
 591         (_register_save_policy[i] == 'E' &amp;&amp; exclude_soe) ) {
 592       idealreg2debugmask  [Op_RegN]-&gt;Remove(i);
 593       idealreg2debugmask  [Op_RegI]-&gt;Remove(i); // Exclude save-on-call
 594       idealreg2debugmask  [Op_RegL]-&gt;Remove(i); // registers from debug
 595       idealreg2debugmask  [Op_RegF]-&gt;Remove(i); // masks
 596       idealreg2debugmask  [Op_RegD]-&gt;Remove(i);
 597       idealreg2debugmask  [Op_RegP]-&gt;Remove(i);
 598 
 599       idealreg2mhdebugmask[Op_RegN]-&gt;Remove(i);
 600       idealreg2mhdebugmask[Op_RegI]-&gt;Remove(i);
 601       idealreg2mhdebugmask[Op_RegL]-&gt;Remove(i);
 602       idealreg2mhdebugmask[Op_RegF]-&gt;Remove(i);
 603       idealreg2mhdebugmask[Op_RegD]-&gt;Remove(i);
 604       idealreg2mhdebugmask[Op_RegP]-&gt;Remove(i);
 605     }
 606   }
 607 
 608   // Subtract the register we use to save the SP for MethodHandle
 609   // invokes to from the debug mask.
 610   const RegMask save_mask = method_handle_invoke_SP_save_mask();
 611   idealreg2mhdebugmask[Op_RegN]-&gt;SUBTRACT(save_mask);
 612   idealreg2mhdebugmask[Op_RegI]-&gt;SUBTRACT(save_mask);
 613   idealreg2mhdebugmask[Op_RegL]-&gt;SUBTRACT(save_mask);
 614   idealreg2mhdebugmask[Op_RegF]-&gt;SUBTRACT(save_mask);
 615   idealreg2mhdebugmask[Op_RegD]-&gt;SUBTRACT(save_mask);
 616   idealreg2mhdebugmask[Op_RegP]-&gt;SUBTRACT(save_mask);
 617 }
 618 
 619 //---------------------------is_save_on_entry----------------------------------
 620 bool Matcher::is_save_on_entry( int reg ) {
 621   return
 622     _register_save_policy[reg] == 'E' ||
 623     _register_save_policy[reg] == 'A' || // Save-on-entry register?
 624     // Also save argument registers in the trampolining stubs
 625     (C-&gt;save_argument_registers() &amp;&amp; is_spillable_arg(reg));
 626 }
 627 
 628 //---------------------------Fixup_Save_On_Entry-------------------------------
 629 void Matcher::Fixup_Save_On_Entry( ) {
 630   init_first_stack_mask();
 631 
 632   Node *root = C-&gt;root();       // Short name for root
 633   // Count number of save-on-entry registers.
 634   uint soe_cnt = number_of_saved_registers();
 635   uint i;
 636 
 637   // Find the procedure Start Node
 638   StartNode *start = C-&gt;start();
 639   assert( start, "Expect a start node" );
 640 
 641   // Save argument registers in the trampolining stubs
 642   if( C-&gt;save_argument_registers() )
 643     for( i = 0; i &lt; _last_Mach_Reg; i++ )
 644       if( is_spillable_arg(i) )
 645         soe_cnt++;
 646 
 647   // Input RegMask array shared by all Returns.
 648   // The type for doubles and longs has a count of 2, but
 649   // there is only 1 returned value
 650   uint ret_edge_cnt = TypeFunc::Parms + ((C-&gt;tf()-&gt;range()-&gt;cnt() == TypeFunc::Parms) ? 0 : 1);
 651   RegMask *ret_rms  = init_input_masks( ret_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 652   // Returns have 0 or 1 returned values depending on call signature.
 653   // Return register is specified by return_value in the AD file.
 654   if (ret_edge_cnt &gt; TypeFunc::Parms)
 655     ret_rms[TypeFunc::Parms+0] = _return_value_mask;
 656 
 657   // Input RegMask array shared by all Rethrows.
 658   uint reth_edge_cnt = TypeFunc::Parms+1;
 659   RegMask *reth_rms  = init_input_masks( reth_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 660   // Rethrow takes exception oop only, but in the argument 0 slot.
 661   reth_rms[TypeFunc::Parms] = mreg2regmask[find_receiver(false)];
 662 #ifdef _LP64
 663   // Need two slots for ptrs in 64-bit land
 664   reth_rms[TypeFunc::Parms].Insert(OptoReg::add(OptoReg::Name(find_receiver(false)),1));
 665 #endif
 666 
 667   // Input RegMask array shared by all TailCalls
 668   uint tail_call_edge_cnt = TypeFunc::Parms+2;
 669   RegMask *tail_call_rms = init_input_masks( tail_call_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 670 
 671   // Input RegMask array shared by all TailJumps
 672   uint tail_jump_edge_cnt = TypeFunc::Parms+2;
 673   RegMask *tail_jump_rms = init_input_masks( tail_jump_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 674 
 675   // TailCalls have 2 returned values (target &amp; moop), whose masks come
 676   // from the usual MachNode/MachOper mechanism.  Find a sample
 677   // TailCall to extract these masks and put the correct masks into
 678   // the tail_call_rms array.
 679   for( i=1; i &lt; root-&gt;req(); i++ ) {
 680     MachReturnNode *m = root-&gt;in(i)-&gt;as_MachReturn();
 681     if( m-&gt;ideal_Opcode() == Op_TailCall ) {
 682       tail_call_rms[TypeFunc::Parms+0] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+0);
 683       tail_call_rms[TypeFunc::Parms+1] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+1);
 684       break;
 685     }
 686   }
 687 
 688   // TailJumps have 2 returned values (target &amp; ex_oop), whose masks come
 689   // from the usual MachNode/MachOper mechanism.  Find a sample
 690   // TailJump to extract these masks and put the correct masks into
 691   // the tail_jump_rms array.
 692   for( i=1; i &lt; root-&gt;req(); i++ ) {
 693     MachReturnNode *m = root-&gt;in(i)-&gt;as_MachReturn();
 694     if( m-&gt;ideal_Opcode() == Op_TailJump ) {
 695       tail_jump_rms[TypeFunc::Parms+0] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+0);
 696       tail_jump_rms[TypeFunc::Parms+1] = m-&gt;MachNode::in_RegMask(TypeFunc::Parms+1);
 697       break;
 698     }
 699   }
 700 
 701   // Input RegMask array shared by all Halts
 702   uint halt_edge_cnt = TypeFunc::Parms;
 703   RegMask *halt_rms = init_input_masks( halt_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
 704 
 705   // Capture the return input masks into each exit flavor
 706   for( i=1; i &lt; root-&gt;req(); i++ ) {
 707     MachReturnNode *exit = root-&gt;in(i)-&gt;as_MachReturn();
 708     switch( exit-&gt;ideal_Opcode() ) {
 709       case Op_Return   : exit-&gt;_in_rms = ret_rms;  break;
 710       case Op_Rethrow  : exit-&gt;_in_rms = reth_rms; break;
 711       case Op_TailCall : exit-&gt;_in_rms = tail_call_rms; break;
 712       case Op_TailJump : exit-&gt;_in_rms = tail_jump_rms; break;
 713       case Op_Halt     : exit-&gt;_in_rms = halt_rms; break;
 714       default          : ShouldNotReachHere();
 715     }
 716   }
 717 
 718   // Next unused projection number from Start.
 719   int proj_cnt = C-&gt;tf()-&gt;domain()-&gt;cnt();
 720 
 721   // Do all the save-on-entry registers.  Make projections from Start for
 722   // them, and give them a use at the exit points.  To the allocator, they
 723   // look like incoming register arguments.
 724   for( i = 0; i &lt; _last_Mach_Reg; i++ ) {
 725     if( is_save_on_entry(i) ) {
 726 
 727       // Add the save-on-entry to the mask array
 728       ret_rms      [      ret_edge_cnt] = mreg2regmask[i];
 729       reth_rms     [     reth_edge_cnt] = mreg2regmask[i];
 730       tail_call_rms[tail_call_edge_cnt] = mreg2regmask[i];
 731       tail_jump_rms[tail_jump_edge_cnt] = mreg2regmask[i];
 732       // Halts need the SOE registers, but only in the stack as debug info.
 733       // A just-prior uncommon-trap or deoptimization will use the SOE regs.
 734       halt_rms     [     halt_edge_cnt] = *idealreg2spillmask[_register_save_type[i]];
 735 
 736       Node *mproj;
 737 
 738       // Is this a RegF low half of a RegD?  Double up 2 adjacent RegF's
 739       // into a single RegD.
 740       if( (i&amp;1) == 0 &amp;&amp;
 741           _register_save_type[i  ] == Op_RegF &amp;&amp;
 742           _register_save_type[i+1] == Op_RegF &amp;&amp;
 743           is_save_on_entry(i+1) ) {
 744         // Add other bit for double
 745         ret_rms      [      ret_edge_cnt].Insert(OptoReg::Name(i+1));
 746         reth_rms     [     reth_edge_cnt].Insert(OptoReg::Name(i+1));
 747         tail_call_rms[tail_call_edge_cnt].Insert(OptoReg::Name(i+1));
 748         tail_jump_rms[tail_jump_edge_cnt].Insert(OptoReg::Name(i+1));
 749         halt_rms     [     halt_edge_cnt].Insert(OptoReg::Name(i+1));
 750         mproj = new MachProjNode( start, proj_cnt, ret_rms[ret_edge_cnt], Op_RegD );
 751         proj_cnt += 2;          // Skip 2 for doubles
 752       }
 753       else if( (i&amp;1) == 1 &amp;&amp;    // Else check for high half of double
 754                _register_save_type[i-1] == Op_RegF &amp;&amp;
 755                _register_save_type[i  ] == Op_RegF &amp;&amp;
 756                is_save_on_entry(i-1) ) {
 757         ret_rms      [      ret_edge_cnt] = RegMask::Empty;
 758         reth_rms     [     reth_edge_cnt] = RegMask::Empty;
 759         tail_call_rms[tail_call_edge_cnt] = RegMask::Empty;
 760         tail_jump_rms[tail_jump_edge_cnt] = RegMask::Empty;
 761         halt_rms     [     halt_edge_cnt] = RegMask::Empty;
 762         mproj = C-&gt;top();
 763       }
 764       // Is this a RegI low half of a RegL?  Double up 2 adjacent RegI's
 765       // into a single RegL.
 766       else if( (i&amp;1) == 0 &amp;&amp;
 767           _register_save_type[i  ] == Op_RegI &amp;&amp;
 768           _register_save_type[i+1] == Op_RegI &amp;&amp;
 769         is_save_on_entry(i+1) ) {
 770         // Add other bit for long
 771         ret_rms      [      ret_edge_cnt].Insert(OptoReg::Name(i+1));
 772         reth_rms     [     reth_edge_cnt].Insert(OptoReg::Name(i+1));
 773         tail_call_rms[tail_call_edge_cnt].Insert(OptoReg::Name(i+1));
 774         tail_jump_rms[tail_jump_edge_cnt].Insert(OptoReg::Name(i+1));
 775         halt_rms     [     halt_edge_cnt].Insert(OptoReg::Name(i+1));
 776         mproj = new MachProjNode( start, proj_cnt, ret_rms[ret_edge_cnt], Op_RegL );
 777         proj_cnt += 2;          // Skip 2 for longs
 778       }
 779       else if( (i&amp;1) == 1 &amp;&amp;    // Else check for high half of long
 780                _register_save_type[i-1] == Op_RegI &amp;&amp;
 781                _register_save_type[i  ] == Op_RegI &amp;&amp;
 782                is_save_on_entry(i-1) ) {
 783         ret_rms      [      ret_edge_cnt] = RegMask::Empty;
 784         reth_rms     [     reth_edge_cnt] = RegMask::Empty;
 785         tail_call_rms[tail_call_edge_cnt] = RegMask::Empty;
 786         tail_jump_rms[tail_jump_edge_cnt] = RegMask::Empty;
 787         halt_rms     [     halt_edge_cnt] = RegMask::Empty;
 788         mproj = C-&gt;top();
 789       } else {
 790         // Make a projection for it off the Start
 791         mproj = new MachProjNode( start, proj_cnt++, ret_rms[ret_edge_cnt], _register_save_type[i] );
 792       }
 793 
 794       ret_edge_cnt ++;
 795       reth_edge_cnt ++;
 796       tail_call_edge_cnt ++;
 797       tail_jump_edge_cnt ++;
 798       halt_edge_cnt ++;
 799 
 800       // Add a use of the SOE register to all exit paths
 801       for( uint j=1; j &lt; root-&gt;req(); j++ )
 802         root-&gt;in(j)-&gt;add_req(mproj);
 803     } // End of if a save-on-entry register
 804   } // End of for all machine registers
 805 }
 806 
 807 //------------------------------init_spill_mask--------------------------------
 808 void Matcher::init_spill_mask( Node *ret ) {
 809   if( idealreg2regmask[Op_RegI] ) return; // One time only init
 810 
 811   OptoReg::c_frame_pointer = c_frame_pointer();
 812   c_frame_ptr_mask = c_frame_pointer();
 813 #ifdef _LP64
 814   // pointers are twice as big
 815   c_frame_ptr_mask.Insert(OptoReg::add(c_frame_pointer(),1));
 816 #endif
 817 
 818   // Start at OptoReg::stack0()
 819   STACK_ONLY_mask.Clear();
 820   OptoReg::Name init = OptoReg::stack2reg(0);
 821   // STACK_ONLY_mask is all stack bits
 822   OptoReg::Name i;
 823   for (i = init; RegMask::can_represent(i); i = OptoReg::add(i,1))
 824     STACK_ONLY_mask.Insert(i);
 825   // Also set the "infinite stack" bit.
 826   STACK_ONLY_mask.set_AllStack();
 827 
 828   // Copy the register names over into the shared world
 829   for( i=OptoReg::Name(0); i&lt;OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {
 830     // SharedInfo::regName[i] = regName[i];
 831     // Handy RegMasks per machine register
 832     mreg2regmask[i].Insert(i);
 833   }
 834 
 835   // Grab the Frame Pointer
 836   Node *fp  = ret-&gt;in(TypeFunc::FramePtr);
 837   Node *mem = ret-&gt;in(TypeFunc::Memory);
 838   const TypePtr* atp = TypePtr::BOTTOM;
 839   // Share frame pointer while making spill ops
 840   set_shared(fp);
 841 
 842   // Compute generic short-offset Loads
 843 #ifdef _LP64
 844   MachNode *spillCP = match_tree(new LoadNNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));
 845 #endif
 846   MachNode *spillI  = match_tree(new LoadINode(NULL,mem,fp,atp,TypeInt::INT,MemNode::unordered));
 847   MachNode *spillL  = match_tree(new LoadLNode(NULL,mem,fp,atp,TypeLong::LONG,MemNode::unordered, LoadNode::DependsOnlyOnTest, false));
 848   MachNode *spillF  = match_tree(new LoadFNode(NULL,mem,fp,atp,Type::FLOAT,MemNode::unordered));
 849   MachNode *spillD  = match_tree(new LoadDNode(NULL,mem,fp,atp,Type::DOUBLE,MemNode::unordered));
 850   MachNode *spillP  = match_tree(new LoadPNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));
 851   assert(spillI != NULL &amp;&amp; spillL != NULL &amp;&amp; spillF != NULL &amp;&amp;
 852          spillD != NULL &amp;&amp; spillP != NULL, "");
 853   // Get the ADLC notion of the right regmask, for each basic type.
 854 #ifdef _LP64
 855   idealreg2regmask[Op_RegN] = &amp;spillCP-&gt;out_RegMask();
 856 #endif
 857   idealreg2regmask[Op_RegI] = &amp;spillI-&gt;out_RegMask();
 858   idealreg2regmask[Op_RegL] = &amp;spillL-&gt;out_RegMask();
 859   idealreg2regmask[Op_RegF] = &amp;spillF-&gt;out_RegMask();
 860   idealreg2regmask[Op_RegD] = &amp;spillD-&gt;out_RegMask();
 861   idealreg2regmask[Op_RegP] = &amp;spillP-&gt;out_RegMask();
 862 
 863   // Vector regmasks.
 864   if (Matcher::vector_size_supported(T_BYTE,4)) {
 865     TypeVect::VECTS = TypeVect::make(T_BYTE, 4);
 866     MachNode *spillVectS = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTS));
 867     idealreg2regmask[Op_VecS] = &amp;spillVectS-&gt;out_RegMask();
 868   }
 869   if (Matcher::vector_size_supported(T_FLOAT,2)) {
 870     MachNode *spillVectD = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTD));
 871     idealreg2regmask[Op_VecD] = &amp;spillVectD-&gt;out_RegMask();
 872   }
 873   if (Matcher::vector_size_supported(T_FLOAT,4)) {
 874     MachNode *spillVectX = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTX));
 875     idealreg2regmask[Op_VecX] = &amp;spillVectX-&gt;out_RegMask();
 876   }
 877   if (Matcher::vector_size_supported(T_FLOAT,8)) {
 878     MachNode *spillVectY = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTY));
 879     idealreg2regmask[Op_VecY] = &amp;spillVectY-&gt;out_RegMask();
 880   }
 881   if (Matcher::vector_size_supported(T_FLOAT,16)) {
 882     MachNode *spillVectZ = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTZ));
 883     idealreg2regmask[Op_VecZ] = &amp;spillVectZ-&gt;out_RegMask();
 884   }
 885 }
 886 
 887 #ifdef ASSERT
 888 static void match_alias_type(Compile* C, Node* n, Node* m) {
 889   if (!VerifyAliases)  return;  // do not go looking for trouble by default
 890   const TypePtr* nat = n-&gt;adr_type();
 891   const TypePtr* mat = m-&gt;adr_type();
 892   int nidx = C-&gt;get_alias_index(nat);
 893   int midx = C-&gt;get_alias_index(mat);
 894   // Detune the assert for cases like (AndI 0xFF (LoadB p)).
 895   if (nidx == Compile::AliasIdxTop &amp;&amp; midx &gt;= Compile::AliasIdxRaw) {
 896     for (uint i = 1; i &lt; n-&gt;req(); i++) {
 897       Node* n1 = n-&gt;in(i);
 898       const TypePtr* n1at = n1-&gt;adr_type();
 899       if (n1at != NULL) {
 900         nat = n1at;
 901         nidx = C-&gt;get_alias_index(n1at);
 902       }
 903     }
 904   }
 905   // %%% Kludgery.  Instead, fix ideal adr_type methods for all these cases:
 906   if (nidx == Compile::AliasIdxTop &amp;&amp; midx == Compile::AliasIdxRaw) {
 907     switch (n-&gt;Opcode()) {
 908     case Op_PrefetchAllocation:
 909       nidx = Compile::AliasIdxRaw;
 910       nat = TypeRawPtr::BOTTOM;
 911       break;
 912     }
 913   }
 914   if (nidx == Compile::AliasIdxRaw &amp;&amp; midx == Compile::AliasIdxTop) {
 915     switch (n-&gt;Opcode()) {
 916     case Op_ClearArray:
 917       midx = Compile::AliasIdxRaw;
 918       mat = TypeRawPtr::BOTTOM;
 919       break;
 920     }
 921   }
 922   if (nidx == Compile::AliasIdxTop &amp;&amp; midx == Compile::AliasIdxBot) {
 923     switch (n-&gt;Opcode()) {
 924     case Op_Return:
 925     case Op_Rethrow:
 926     case Op_Halt:
 927     case Op_TailCall:
 928     case Op_TailJump:
 929       nidx = Compile::AliasIdxBot;
 930       nat = TypePtr::BOTTOM;
 931       break;
 932     }
 933   }
 934   if (nidx == Compile::AliasIdxBot &amp;&amp; midx == Compile::AliasIdxTop) {
 935     switch (n-&gt;Opcode()) {
 936     case Op_StrComp:
 937     case Op_StrEquals:
 938     case Op_StrIndexOf:
 939     case Op_StrIndexOfChar:
 940     case Op_AryEq:
 941     case Op_HasNegatives:
 942     case Op_MemBarVolatile:
 943     case Op_MemBarCPUOrder: // %%% these ideals should have narrower adr_type?
 944     case Op_StrInflatedCopy:
 945     case Op_StrCompressedCopy:
 946     case Op_EncodeISOArray:
 947       nidx = Compile::AliasIdxTop;
 948       nat = NULL;
 949       break;
 950     }
 951   }
 952   if (nidx != midx) {
 953     if (PrintOpto || (PrintMiscellaneous &amp;&amp; (WizardMode || Verbose))) {
 954       tty-&gt;print_cr("==== Matcher alias shift %d =&gt; %d", nidx, midx);
 955       n-&gt;dump();
 956       m-&gt;dump();
 957     }
 958     assert(C-&gt;subsume_loads() &amp;&amp; C-&gt;must_alias(nat, midx),
 959            "must not lose alias info when matching");
 960   }
 961 }
 962 #endif
 963 
 964 
 965 //------------------------------MStack-----------------------------------------
 966 // State and MStack class used in xform() and find_shared() iterative methods.
 967 enum Node_State { Pre_Visit,  // node has to be pre-visited
 968                       Visit,  // visit node
 969                  Post_Visit,  // post-visit node
 970              Alt_Post_Visit   // alternative post-visit path
 971                 };
 972 
 973 class MStack: public Node_Stack {
 974   public:
 975     MStack(int size) : Node_Stack(size) { }
 976 
 977     void push(Node *n, Node_State ns) {
 978       Node_Stack::push(n, (uint)ns);
 979     }
 980     void push(Node *n, Node_State ns, Node *parent, int indx) {
 981       ++_inode_top;
 982       if ((_inode_top + 1) &gt;= _inode_max) grow();
 983       _inode_top-&gt;node = parent;
 984       _inode_top-&gt;indx = (uint)indx;
 985       ++_inode_top;
 986       _inode_top-&gt;node = n;
 987       _inode_top-&gt;indx = (uint)ns;
 988     }
 989     Node *parent() {
 990       pop();
 991       return node();
 992     }
 993     Node_State state() const {
 994       return (Node_State)index();
 995     }
 996     void set_state(Node_State ns) {
 997       set_index((uint)ns);
 998     }
 999 };
1000 
1001 
1002 //------------------------------xform------------------------------------------
1003 // Given a Node in old-space, Match him (Label/Reduce) to produce a machine
1004 // Node in new-space.  Given a new-space Node, recursively walk his children.
1005 Node *Matcher::transform( Node *n ) { ShouldNotCallThis(); return n; }
1006 Node *Matcher::xform( Node *n, int max_stack ) {
1007   // Use one stack to keep both: child's node/state and parent's node/index
1008   MStack mstack(max_stack * 2 * 2); // usually: C-&gt;live_nodes() * 2 * 2
1009   mstack.push(n, Visit, NULL, -1);  // set NULL as parent to indicate root
1010 
1011   while (mstack.is_nonempty()) {
1012     C-&gt;check_node_count(NodeLimitFudgeFactor, "too many nodes matching instructions");
1013     if (C-&gt;failing()) return NULL;
1014     n = mstack.node();          // Leave node on stack
1015     Node_State nstate = mstack.state();
1016     if (nstate == Visit) {
1017       mstack.set_state(Post_Visit);
1018       Node *oldn = n;
1019       // Old-space or new-space check
1020       if (!C-&gt;node_arena()-&gt;contains(n)) {
1021         // Old space!
1022         Node* m;
1023         if (has_new_node(n)) {  // Not yet Label/Reduced
1024           m = new_node(n);
1025         } else {
1026           if (!is_dontcare(n)) { // Matcher can match this guy
1027             // Calls match special.  They match alone with no children.
1028             // Their children, the incoming arguments, match normally.
1029             m = n-&gt;is_SafePoint() ? match_sfpt(n-&gt;as_SafePoint()):match_tree(n);
1030             if (C-&gt;failing())  return NULL;
1031             if (m == NULL) { Matcher::soft_match_failure(); return NULL; }
1032           } else {                  // Nothing the matcher cares about
1033             if( n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;is_Multi()) {       // Projections?
1034               // Convert to machine-dependent projection
1035               m = n-&gt;in(0)-&gt;as_Multi()-&gt;match( n-&gt;as_Proj(), this );
1036 #ifdef ASSERT
1037               _new2old_map.map(m-&gt;_idx, n);
1038 #endif
1039               if (m-&gt;in(0) != NULL) // m might be top
1040                 collect_null_checks(m, n);
1041             } else {                // Else just a regular 'ol guy
1042               m = n-&gt;clone();       // So just clone into new-space
1043 #ifdef ASSERT
1044               _new2old_map.map(m-&gt;_idx, n);
1045 #endif
1046               // Def-Use edges will be added incrementally as Uses
1047               // of this node are matched.
1048               assert(m-&gt;outcnt() == 0, "no Uses of this clone yet");
1049             }
1050           }
1051 
1052           set_new_node(n, m);       // Map old to new
1053           if (_old_node_note_array != NULL) {
1054             Node_Notes* nn = C-&gt;locate_node_notes(_old_node_note_array,
1055                                                   n-&gt;_idx);
1056             C-&gt;set_node_notes_at(m-&gt;_idx, nn);
1057           }
1058           debug_only(match_alias_type(C, n, m));
1059         }
1060         n = m;    // n is now a new-space node
1061         mstack.set_node(n);
1062       }
1063 
1064       // New space!
1065       if (_visited.test_set(n-&gt;_idx)) continue; // while(mstack.is_nonempty())
1066 
1067       int i;
1068       // Put precedence edges on stack first (match them last).
1069       for (i = oldn-&gt;req(); (uint)i &lt; oldn-&gt;len(); i++) {
1070         Node *m = oldn-&gt;in(i);
1071         if (m == NULL) break;
1072         // set -1 to call add_prec() instead of set_req() during Step1
1073         mstack.push(m, Visit, n, -1);
1074       }
1075 
1076       // Handle precedence edges for interior nodes
1077       for (i = n-&gt;len()-1; (uint)i &gt;= n-&gt;req(); i--) {
1078         Node *m = n-&gt;in(i);
1079         if (m == NULL || C-&gt;node_arena()-&gt;contains(m)) continue;
1080         n-&gt;rm_prec(i);
1081         // set -1 to call add_prec() instead of set_req() during Step1
1082         mstack.push(m, Visit, n, -1);
1083       }
1084 
1085       // For constant debug info, I'd rather have unmatched constants.
1086       int cnt = n-&gt;req();
1087       JVMState* jvms = n-&gt;jvms();
1088       int debug_cnt = jvms ? jvms-&gt;debug_start() : cnt;
1089 
1090       // Now do only debug info.  Clone constants rather than matching.
1091       // Constants are represented directly in the debug info without
1092       // the need for executable machine instructions.
1093       // Monitor boxes are also represented directly.
1094       for (i = cnt - 1; i &gt;= debug_cnt; --i) { // For all debug inputs do
1095         Node *m = n-&gt;in(i);          // Get input
1096         int op = m-&gt;Opcode();
1097         assert((op == Op_BoxLock) == jvms-&gt;is_monitor_use(i), "boxes only at monitor sites");
1098         if( op == Op_ConI || op == Op_ConP || op == Op_ConN || op == Op_ConNKlass ||
1099             op == Op_ConF || op == Op_ConD || op == Op_ConL
1100             // || op == Op_BoxLock  // %%%% enable this and remove (+++) in chaitin.cpp
1101             ) {
1102           m = m-&gt;clone();
1103 #ifdef ASSERT
1104           _new2old_map.map(m-&gt;_idx, n);
1105 #endif
1106           mstack.push(m, Post_Visit, n, i); // Don't need to visit
1107           mstack.push(m-&gt;in(0), Visit, m, 0);
1108         } else {
1109           mstack.push(m, Visit, n, i);
1110         }
1111       }
1112 
1113       // And now walk his children, and convert his inputs to new-space.
1114       for( ; i &gt;= 0; --i ) { // For all normal inputs do
1115         Node *m = n-&gt;in(i);  // Get input
1116         if(m != NULL)
1117           mstack.push(m, Visit, n, i);
1118       }
1119 
1120     }
1121     else if (nstate == Post_Visit) {
1122       // Set xformed input
1123       Node *p = mstack.parent();
1124       if (p != NULL) { // root doesn't have parent
1125         int i = (int)mstack.index();
1126         if (i &gt;= 0)
1127           p-&gt;set_req(i, n); // required input
1128         else if (i == -1)
1129           p-&gt;add_prec(n);   // precedence input
1130         else
1131           ShouldNotReachHere();
1132       }
1133       mstack.pop(); // remove processed node from stack
1134     }
1135     else {
1136       ShouldNotReachHere();
1137     }
1138   } // while (mstack.is_nonempty())
1139   return n; // Return new-space Node
1140 }
1141 
1142 //------------------------------warp_outgoing_stk_arg------------------------
1143 OptoReg::Name Matcher::warp_outgoing_stk_arg( VMReg reg, OptoReg::Name begin_out_arg_area, OptoReg::Name &amp;out_arg_limit_per_call ) {
1144   // Convert outgoing argument location to a pre-biased stack offset
1145   if (reg-&gt;is_stack()) {
1146     OptoReg::Name warped = reg-&gt;reg2stack();
1147     // Adjust the stack slot offset to be the register number used
1148     // by the allocator.
1149     warped = OptoReg::add(begin_out_arg_area, warped);
1150     // Keep track of the largest numbered stack slot used for an arg.
1151     // Largest used slot per call-site indicates the amount of stack
1152     // that is killed by the call.
1153     if( warped &gt;= out_arg_limit_per_call )
1154       out_arg_limit_per_call = OptoReg::add(warped,1);
1155     if (!RegMask::can_represent_arg(warped)) {
1156       C-&gt;record_method_not_compilable_all_tiers("unsupported calling sequence");
1157       return OptoReg::Bad;
1158     }
1159     return warped;
1160   }
1161   return OptoReg::as_OptoReg(reg);
1162 }
1163 
1164 
1165 //------------------------------match_sfpt-------------------------------------
1166 // Helper function to match call instructions.  Calls match special.
1167 // They match alone with no children.  Their children, the incoming
1168 // arguments, match normally.
1169 MachNode *Matcher::match_sfpt( SafePointNode *sfpt ) {
1170   MachSafePointNode *msfpt = NULL;
1171   MachCallNode      *mcall = NULL;
1172   uint               cnt;
1173   // Split out case for SafePoint vs Call
1174   CallNode *call;
1175   const TypeTuple *domain;
1176   ciMethod*        method = NULL;
1177   bool             is_method_handle_invoke = false;  // for special kill effects
1178   if( sfpt-&gt;is_Call() ) {
1179     call = sfpt-&gt;as_Call();
1180     domain = call-&gt;tf()-&gt;domain();
1181     cnt = domain-&gt;cnt();
1182 
1183     // Match just the call, nothing else
1184     MachNode *m = match_tree(call);
1185     if (C-&gt;failing())  return NULL;
1186     if( m == NULL ) { Matcher::soft_match_failure(); return NULL; }
1187 
1188     // Copy data from the Ideal SafePoint to the machine version
1189     mcall = m-&gt;as_MachCall();
1190 
1191     mcall-&gt;set_tf(         call-&gt;tf());
1192     mcall-&gt;set_entry_point(call-&gt;entry_point());
1193     mcall-&gt;set_cnt(        call-&gt;cnt());
1194 
1195     if( mcall-&gt;is_MachCallJava() ) {
1196       MachCallJavaNode *mcall_java  = mcall-&gt;as_MachCallJava();
1197       const CallJavaNode *call_java =  call-&gt;as_CallJava();
1198       method = call_java-&gt;method();
1199       mcall_java-&gt;_method = method;
1200       mcall_java-&gt;_bci = call_java-&gt;_bci;
1201       mcall_java-&gt;_optimized_virtual = call_java-&gt;is_optimized_virtual();
1202       is_method_handle_invoke = call_java-&gt;is_method_handle_invoke();
1203       mcall_java-&gt;_method_handle_invoke = is_method_handle_invoke;
1204       if (is_method_handle_invoke) {
1205         C-&gt;set_has_method_handle_invokes(true);
1206       }
1207       if( mcall_java-&gt;is_MachCallStaticJava() )
1208         mcall_java-&gt;as_MachCallStaticJava()-&gt;_name =
1209          call_java-&gt;as_CallStaticJava()-&gt;_name;
1210       if( mcall_java-&gt;is_MachCallDynamicJava() )
1211         mcall_java-&gt;as_MachCallDynamicJava()-&gt;_vtable_index =
1212          call_java-&gt;as_CallDynamicJava()-&gt;_vtable_index;
1213     }
1214     else if( mcall-&gt;is_MachCallRuntime() ) {
1215       mcall-&gt;as_MachCallRuntime()-&gt;_name = call-&gt;as_CallRuntime()-&gt;_name;
1216     }
1217     msfpt = mcall;
1218   }
1219   // This is a non-call safepoint
1220   else {
1221     call = NULL;
1222     domain = NULL;
1223     MachNode *mn = match_tree(sfpt);
1224     if (C-&gt;failing())  return NULL;
1225     msfpt = mn-&gt;as_MachSafePoint();
1226     cnt = TypeFunc::Parms;
1227   }
1228 
1229   // Advertise the correct memory effects (for anti-dependence computation).
1230   msfpt-&gt;set_adr_type(sfpt-&gt;adr_type());
1231 
1232   // Allocate a private array of RegMasks.  These RegMasks are not shared.
1233   msfpt-&gt;_in_rms = NEW_RESOURCE_ARRAY( RegMask, cnt );
1234   // Empty them all.
1235   memset( msfpt-&gt;_in_rms, 0, sizeof(RegMask)*cnt );
1236 
1237   // Do all the pre-defined non-Empty register masks
1238   msfpt-&gt;_in_rms[TypeFunc::ReturnAdr] = _return_addr_mask;
1239   msfpt-&gt;_in_rms[TypeFunc::FramePtr ] = c_frame_ptr_mask;
1240 
1241   // Place first outgoing argument can possibly be put.
1242   OptoReg::Name begin_out_arg_area = OptoReg::add(_new_SP, C-&gt;out_preserve_stack_slots());
1243   assert( is_even(begin_out_arg_area), "" );
1244   // Compute max outgoing register number per call site.
1245   OptoReg::Name out_arg_limit_per_call = begin_out_arg_area;
1246   // Calls to C may hammer extra stack slots above and beyond any arguments.
1247   // These are usually backing store for register arguments for varargs.
1248   if( call != NULL &amp;&amp; call-&gt;is_CallRuntime() )
1249     out_arg_limit_per_call = OptoReg::add(out_arg_limit_per_call,C-&gt;varargs_C_out_slots_killed());
1250 
1251 
1252   // Do the normal argument list (parameters) register masks
1253   int argcnt = cnt - TypeFunc::Parms;
1254   if( argcnt &gt; 0 ) {          // Skip it all if we have no args
1255     BasicType *sig_bt  = NEW_RESOURCE_ARRAY( BasicType, argcnt );
1256     VMRegPair *parm_regs = NEW_RESOURCE_ARRAY( VMRegPair, argcnt );
1257     int i;
1258     for( i = 0; i &lt; argcnt; i++ ) {
1259       sig_bt[i] = domain-&gt;field_at(i+TypeFunc::Parms)-&gt;basic_type();
1260     }
1261     // V-call to pick proper calling convention
1262     call-&gt;calling_convention( sig_bt, parm_regs, argcnt );
1263 
1264 #ifdef ASSERT
1265     // Sanity check users' calling convention.  Really handy during
1266     // the initial porting effort.  Fairly expensive otherwise.
1267     { for (int i = 0; i&lt;argcnt; i++) {
1268       if( !parm_regs[i].first()-&gt;is_valid() &amp;&amp;
1269           !parm_regs[i].second()-&gt;is_valid() ) continue;
1270       VMReg reg1 = parm_regs[i].first();
1271       VMReg reg2 = parm_regs[i].second();
1272       for (int j = 0; j &lt; i; j++) {
1273         if( !parm_regs[j].first()-&gt;is_valid() &amp;&amp;
1274             !parm_regs[j].second()-&gt;is_valid() ) continue;
1275         VMReg reg3 = parm_regs[j].first();
1276         VMReg reg4 = parm_regs[j].second();
1277         if( !reg1-&gt;is_valid() ) {
1278           assert( !reg2-&gt;is_valid(), "valid halvsies" );
1279         } else if( !reg3-&gt;is_valid() ) {
1280           assert( !reg4-&gt;is_valid(), "valid halvsies" );
1281         } else {
1282           assert( reg1 != reg2, "calling conv. must produce distinct regs");
1283           assert( reg1 != reg3, "calling conv. must produce distinct regs");
1284           assert( reg1 != reg4, "calling conv. must produce distinct regs");
1285           assert( reg2 != reg3, "calling conv. must produce distinct regs");
1286           assert( reg2 != reg4 || !reg2-&gt;is_valid(), "calling conv. must produce distinct regs");
1287           assert( reg3 != reg4, "calling conv. must produce distinct regs");
1288         }
1289       }
1290     }
1291     }
1292 #endif
1293 
1294     // Visit each argument.  Compute its outgoing register mask.
1295     // Return results now can have 2 bits returned.
1296     // Compute max over all outgoing arguments both per call-site
1297     // and over the entire method.
1298     for( i = 0; i &lt; argcnt; i++ ) {
1299       // Address of incoming argument mask to fill in
1300       RegMask *rm = &amp;mcall-&gt;_in_rms[i+TypeFunc::Parms];
1301       if( !parm_regs[i].first()-&gt;is_valid() &amp;&amp;
1302           !parm_regs[i].second()-&gt;is_valid() ) {
1303         continue;               // Avoid Halves
1304       }
1305       // Grab first register, adjust stack slots and insert in mask.
1306       OptoReg::Name reg1 = warp_outgoing_stk_arg(parm_regs[i].first(), begin_out_arg_area, out_arg_limit_per_call );
1307       if (OptoReg::is_valid(reg1))
1308         rm-&gt;Insert( reg1 );
1309       // Grab second register (if any), adjust stack slots and insert in mask.
1310       OptoReg::Name reg2 = warp_outgoing_stk_arg(parm_regs[i].second(), begin_out_arg_area, out_arg_limit_per_call );
1311       if (OptoReg::is_valid(reg2))
1312         rm-&gt;Insert( reg2 );
1313     } // End of for all arguments
1314 
1315     // Compute number of stack slots needed to restore stack in case of
1316     // Pascal-style argument popping.
1317     mcall-&gt;_argsize = out_arg_limit_per_call - begin_out_arg_area;
1318   }
1319 
1320   // Compute the max stack slot killed by any call.  These will not be
1321   // available for debug info, and will be used to adjust FIRST_STACK_mask
1322   // after all call sites have been visited.
1323   if( _out_arg_limit &lt; out_arg_limit_per_call)
1324     _out_arg_limit = out_arg_limit_per_call;
1325 
1326   if (mcall) {
1327     // Kill the outgoing argument area, including any non-argument holes and
1328     // any legacy C-killed slots.  Use Fat-Projections to do the killing.
1329     // Since the max-per-method covers the max-per-call-site and debug info
1330     // is excluded on the max-per-method basis, debug info cannot land in
1331     // this killed area.
1332     uint r_cnt = mcall-&gt;tf()-&gt;range()-&gt;cnt();
1333     MachProjNode *proj = new MachProjNode( mcall, r_cnt+10000, RegMask::Empty, MachProjNode::fat_proj );
1334     if (!RegMask::can_represent_arg(OptoReg::Name(out_arg_limit_per_call-1))) {
1335       C-&gt;record_method_not_compilable_all_tiers("unsupported outgoing calling sequence");
1336     } else {
1337       for (int i = begin_out_arg_area; i &lt; out_arg_limit_per_call; i++)
1338         proj-&gt;_rout.Insert(OptoReg::Name(i));
1339     }
1340     if (proj-&gt;_rout.is_NotEmpty()) {
1341       push_projection(proj);
1342     }
1343   }
1344   // Transfer the safepoint information from the call to the mcall
1345   // Move the JVMState list
1346   msfpt-&gt;set_jvms(sfpt-&gt;jvms());
1347   for (JVMState* jvms = msfpt-&gt;jvms(); jvms; jvms = jvms-&gt;caller()) {
1348     jvms-&gt;set_map(sfpt);
1349   }
1350 
1351   // Debug inputs begin just after the last incoming parameter
1352   assert((mcall == NULL) || (mcall-&gt;jvms() == NULL) ||
1353          (mcall-&gt;jvms()-&gt;debug_start() + mcall-&gt;_jvmadj == mcall-&gt;tf()-&gt;domain()-&gt;cnt()), "");
1354 
1355   // Move the OopMap
1356   msfpt-&gt;_oop_map = sfpt-&gt;_oop_map;
1357 
1358   // Add additional edges.
1359   if (msfpt-&gt;mach_constant_base_node_input() != (uint)-1 &amp;&amp; !msfpt-&gt;is_MachCallLeaf()) {
1360     // For these calls we can not add MachConstantBase in expand(), as the
1361     // ins are not complete then.
1362     msfpt-&gt;ins_req(msfpt-&gt;mach_constant_base_node_input(), C-&gt;mach_constant_base_node());
1363     if (msfpt-&gt;jvms() &amp;&amp;
1364         msfpt-&gt;mach_constant_base_node_input() &lt;= msfpt-&gt;jvms()-&gt;debug_start() + msfpt-&gt;_jvmadj) {
1365       // We added an edge before jvms, so we must adapt the position of the ins.
1366       msfpt-&gt;jvms()-&gt;adapt_position(+1);
1367     }
1368   }
1369 
1370   // Registers killed by the call are set in the local scheduling pass
1371   // of Global Code Motion.
1372   return msfpt;
1373 }
1374 
1375 //---------------------------match_tree----------------------------------------
1376 // Match a Ideal Node DAG - turn it into a tree; Label &amp; Reduce.  Used as part
1377 // of the whole-sale conversion from Ideal to Mach Nodes.  Also used for
1378 // making GotoNodes while building the CFG and in init_spill_mask() to identify
1379 // a Load's result RegMask for memoization in idealreg2regmask[]
1380 MachNode *Matcher::match_tree( const Node *n ) {
1381   assert( n-&gt;Opcode() != Op_Phi, "cannot match" );
1382   assert( !n-&gt;is_block_start(), "cannot match" );
1383   // Set the mark for all locally allocated State objects.
1384   // When this call returns, the _states_arena arena will be reset
1385   // freeing all State objects.
1386   ResourceMark rm( &amp;_states_arena );
1387 
1388   LabelRootDepth = 0;
1389 
1390   // StoreNodes require their Memory input to match any LoadNodes
1391   Node *mem = n-&gt;is_Store() ? n-&gt;in(MemNode::Memory) : (Node*)1 ;
1392 #ifdef ASSERT
1393   Node* save_mem_node = _mem_node;
1394   _mem_node = n-&gt;is_Store() ? (Node*)n : NULL;
1395 #endif
1396   // State object for root node of match tree
1397   // Allocate it on _states_arena - stack allocation can cause stack overflow.
1398   State *s = new (&amp;_states_arena) State;
1399   s-&gt;_kids[0] = NULL;
1400   s-&gt;_kids[1] = NULL;
1401   s-&gt;_leaf = (Node*)n;
1402   // Label the input tree, allocating labels from top-level arena
1403   Label_Root( n, s, n-&gt;in(0), mem );
1404   if (C-&gt;failing())  return NULL;
1405 
1406   // The minimum cost match for the whole tree is found at the root State
1407   uint mincost = max_juint;
1408   uint cost = max_juint;
1409   uint i;
1410   for( i = 0; i &lt; NUM_OPERANDS; i++ ) {
1411     if( s-&gt;valid(i) &amp;&amp;                // valid entry and
1412         s-&gt;_cost[i] &lt; cost &amp;&amp;         // low cost and
1413         s-&gt;_rule[i] &gt;= NUM_OPERANDS ) // not an operand
1414       cost = s-&gt;_cost[mincost=i];
1415   }
1416   if (mincost == max_juint) {
1417 #ifndef PRODUCT
1418     tty-&gt;print("No matching rule for:");
1419     s-&gt;dump();
1420 #endif
1421     Matcher::soft_match_failure();
1422     return NULL;
1423   }
1424   // Reduce input tree based upon the state labels to machine Nodes
1425   MachNode *m = ReduceInst( s, s-&gt;_rule[mincost], mem );
1426 #ifdef ASSERT
1427   _old2new_map.map(n-&gt;_idx, m);
1428   _new2old_map.map(m-&gt;_idx, (Node*)n);
1429 #endif
1430 
1431   // Add any Matcher-ignored edges
1432   uint cnt = n-&gt;req();
1433   uint start = 1;
1434   if( mem != (Node*)1 ) start = MemNode::Memory+1;
1435   if( n-&gt;is_AddP() ) {
1436     assert( mem == (Node*)1, "" );
1437     start = AddPNode::Base+1;
1438   }
1439   for( i = start; i &lt; cnt; i++ ) {
1440     if( !n-&gt;match_edge(i) ) {
1441       if( i &lt; m-&gt;req() )
1442         m-&gt;ins_req( i, n-&gt;in(i) );
1443       else
1444         m-&gt;add_req( n-&gt;in(i) );
1445     }
1446   }
1447 
1448   debug_only( _mem_node = save_mem_node; )
1449   return m;
1450 }
1451 
1452 
1453 //------------------------------match_into_reg---------------------------------
1454 // Choose to either match this Node in a register or part of the current
1455 // match tree.  Return true for requiring a register and false for matching
1456 // as part of the current match tree.
1457 static bool match_into_reg( const Node *n, Node *m, Node *control, int i, bool shared ) {
1458 
1459   const Type *t = m-&gt;bottom_type();
1460 
1461   if (t-&gt;singleton()) {
1462     // Never force constants into registers.  Allow them to match as
1463     // constants or registers.  Copies of the same value will share
1464     // the same register.  See find_shared_node.
1465     return false;
1466   } else {                      // Not a constant
1467     // Stop recursion if they have different Controls.
1468     Node* m_control = m-&gt;in(0);
1469     // Control of load's memory can post-dominates load's control.
1470     // So use it since load can't float above its memory.
1471     Node* mem_control = (m-&gt;is_Load()) ? m-&gt;in(MemNode::Memory)-&gt;in(0) : NULL;
1472     if (control &amp;&amp; m_control &amp;&amp; control != m_control &amp;&amp; control != mem_control) {
1473 
1474       // Actually, we can live with the most conservative control we
1475       // find, if it post-dominates the others.  This allows us to
1476       // pick up load/op/store trees where the load can float a little
1477       // above the store.
1478       Node *x = control;
1479       const uint max_scan = 6;  // Arbitrary scan cutoff
1480       uint j;
1481       for (j=0; j&lt;max_scan; j++) {
1482         if (x-&gt;is_Region())     // Bail out at merge points
1483           return true;
1484         x = x-&gt;in(0);
1485         if (x == m_control)     // Does 'control' post-dominate
1486           break;                // m-&gt;in(0)?  If so, we can use it
1487         if (x == mem_control)   // Does 'control' post-dominate
1488           break;                // mem_control?  If so, we can use it
1489       }
1490       if (j == max_scan)        // No post-domination before scan end?
1491         return true;            // Then break the match tree up
1492     }
1493     if ((m-&gt;is_DecodeN() &amp;&amp; Matcher::narrow_oop_use_complex_address()) ||
1494         (m-&gt;is_DecodeNKlass() &amp;&amp; Matcher::narrow_klass_use_complex_address())) {
1495       // These are commonly used in address expressions and can
1496       // efficiently fold into them on X64 in some cases.
1497       return false;
1498     }
1499   }
1500 
1501   // Not forceable cloning.  If shared, put it into a register.
1502   return shared;
1503 }
1504 
1505 
1506 //------------------------------Instruction Selection--------------------------
1507 // Label method walks a "tree" of nodes, using the ADLC generated DFA to match
1508 // ideal nodes to machine instructions.  Trees are delimited by shared Nodes,
1509 // things the Matcher does not match (e.g., Memory), and things with different
1510 // Controls (hence forced into different blocks).  We pass in the Control
1511 // selected for this entire State tree.
1512 
1513 // The Matcher works on Trees, but an Intel add-to-memory requires a DAG: the
1514 // Store and the Load must have identical Memories (as well as identical
1515 // pointers).  Since the Matcher does not have anything for Memory (and
1516 // does not handle DAGs), I have to match the Memory input myself.  If the
1517 // Tree root is a Store, I require all Loads to have the identical memory.
1518 Node *Matcher::Label_Root( const Node *n, State *svec, Node *control, const Node *mem){
1519   // Since Label_Root is a recursive function, its possible that we might run
1520   // out of stack space.  See bugs 6272980 &amp; 6227033 for more info.
1521   LabelRootDepth++;
1522   if (LabelRootDepth &gt; MaxLabelRootDepth) {
1523     C-&gt;record_method_not_compilable_all_tiers("Out of stack space, increase MaxLabelRootDepth");
1524     return NULL;
1525   }
1526   uint care = 0;                // Edges matcher cares about
1527   uint cnt = n-&gt;req();
1528   uint i = 0;
1529 
1530   // Examine children for memory state
1531   // Can only subsume a child into your match-tree if that child's memory state
1532   // is not modified along the path to another input.
1533   // It is unsafe even if the other inputs are separate roots.
1534   Node *input_mem = NULL;
1535   for( i = 1; i &lt; cnt; i++ ) {
1536     if( !n-&gt;match_edge(i) ) continue;
1537     Node *m = n-&gt;in(i);         // Get ith input
1538     assert( m, "expect non-null children" );
1539     if( m-&gt;is_Load() ) {
1540       if( input_mem == NULL ) {
1541         input_mem = m-&gt;in(MemNode::Memory);
1542       } else if( input_mem != m-&gt;in(MemNode::Memory) ) {
1543         input_mem = NodeSentinel;
1544       }
1545     }
1546   }
1547 
1548   for( i = 1; i &lt; cnt; i++ ){// For my children
1549     if( !n-&gt;match_edge(i) ) continue;
1550     Node *m = n-&gt;in(i);         // Get ith input
1551     // Allocate states out of a private arena
1552     State *s = new (&amp;_states_arena) State;
1553     svec-&gt;_kids[care++] = s;
1554     assert( care &lt;= 2, "binary only for now" );
1555 
1556     // Recursively label the State tree.
1557     s-&gt;_kids[0] = NULL;
1558     s-&gt;_kids[1] = NULL;
1559     s-&gt;_leaf = m;
1560 
1561     // Check for leaves of the State Tree; things that cannot be a part of
1562     // the current tree.  If it finds any, that value is matched as a
1563     // register operand.  If not, then the normal matching is used.
1564     if( match_into_reg(n, m, control, i, is_shared(m)) ||
1565         //
1566         // Stop recursion if this is LoadNode and the root of this tree is a
1567         // StoreNode and the load &amp; store have different memories.
1568         ((mem!=(Node*)1) &amp;&amp; m-&gt;is_Load() &amp;&amp; m-&gt;in(MemNode::Memory) != mem) ||
1569         // Can NOT include the match of a subtree when its memory state
1570         // is used by any of the other subtrees
1571         (input_mem == NodeSentinel) ) {
1572 #ifndef PRODUCT
1573       // Print when we exclude matching due to different memory states at input-loads
1574       if( PrintOpto &amp;&amp; (Verbose &amp;&amp; WizardMode) &amp;&amp; (input_mem == NodeSentinel)
1575         &amp;&amp; !((mem!=(Node*)1) &amp;&amp; m-&gt;is_Load() &amp;&amp; m-&gt;in(MemNode::Memory) != mem) ) {
1576         tty-&gt;print_cr("invalid input_mem");
1577       }
1578 #endif
1579       // Switch to a register-only opcode; this value must be in a register
1580       // and cannot be subsumed as part of a larger instruction.
1581       s-&gt;DFA( m-&gt;ideal_reg(), m );
1582 
1583     } else {
1584       // If match tree has no control and we do, adopt it for entire tree
1585       if( control == NULL &amp;&amp; m-&gt;in(0) != NULL &amp;&amp; m-&gt;req() &gt; 1 )
1586         control = m-&gt;in(0);         // Pick up control
1587       // Else match as a normal part of the match tree.
1588       control = Label_Root(m,s,control,mem);
1589       if (C-&gt;failing()) return NULL;
1590     }
1591   }
1592 
1593 
1594   // Call DFA to match this node, and return
1595   svec-&gt;DFA( n-&gt;Opcode(), n );
1596 
1597 #ifdef ASSERT
1598   uint x;
1599   for( x = 0; x &lt; _LAST_MACH_OPER; x++ )
1600     if( svec-&gt;valid(x) )
1601       break;
1602 
1603   if (x &gt;= _LAST_MACH_OPER) {
1604     n-&gt;dump();
1605     svec-&gt;dump();
1606     assert( false, "bad AD file" );
1607   }
1608 #endif
1609   return control;
1610 }
1611 
1612 
1613 // Con nodes reduced using the same rule can share their MachNode
1614 // which reduces the number of copies of a constant in the final
1615 // program.  The register allocator is free to split uses later to
1616 // split live ranges.
1617 MachNode* Matcher::find_shared_node(Node* leaf, uint rule) {
1618   if (!leaf-&gt;is_Con() &amp;&amp; !leaf-&gt;is_DecodeNarrowPtr()) return NULL;
1619 
1620   // See if this Con has already been reduced using this rule.
1621   if (_shared_nodes.Size() &lt;= leaf-&gt;_idx) return NULL;
1622   MachNode* last = (MachNode*)_shared_nodes.at(leaf-&gt;_idx);
1623   if (last != NULL &amp;&amp; rule == last-&gt;rule()) {
1624     // Don't expect control change for DecodeN
1625     if (leaf-&gt;is_DecodeNarrowPtr())
1626       return last;
1627     // Get the new space root.
1628     Node* xroot = new_node(C-&gt;root());
1629     if (xroot == NULL) {
1630       // This shouldn't happen give the order of matching.
1631       return NULL;
1632     }
1633 
1634     // Shared constants need to have their control be root so they
1635     // can be scheduled properly.
1636     Node* control = last-&gt;in(0);
1637     if (control != xroot) {
1638       if (control == NULL || control == C-&gt;root()) {
1639         last-&gt;set_req(0, xroot);
1640       } else {
1641         assert(false, "unexpected control");
1642         return NULL;
1643       }
1644     }
1645     return last;
1646   }
1647   return NULL;
1648 }
1649 
1650 
1651 //------------------------------ReduceInst-------------------------------------
1652 // Reduce a State tree (with given Control) into a tree of MachNodes.
1653 // This routine (and it's cohort ReduceOper) convert Ideal Nodes into
1654 // complicated machine Nodes.  Each MachNode covers some tree of Ideal Nodes.
1655 // Each MachNode has a number of complicated MachOper operands; each
1656 // MachOper also covers a further tree of Ideal Nodes.
1657 
1658 // The root of the Ideal match tree is always an instruction, so we enter
1659 // the recursion here.  After building the MachNode, we need to recurse
1660 // the tree checking for these cases:
1661 // (1) Child is an instruction -
1662 //     Build the instruction (recursively), add it as an edge.
1663 //     Build a simple operand (register) to hold the result of the instruction.
1664 // (2) Child is an interior part of an instruction -
1665 //     Skip over it (do nothing)
1666 // (3) Child is the start of a operand -
1667 //     Build the operand, place it inside the instruction
1668 //     Call ReduceOper.
1669 MachNode *Matcher::ReduceInst( State *s, int rule, Node *&amp;mem ) {
1670   assert( rule &gt;= NUM_OPERANDS, "called with operand rule" );
1671 
1672   MachNode* shared_node = find_shared_node(s-&gt;_leaf, rule);
1673   if (shared_node != NULL) {
1674     return shared_node;
1675   }
1676 
1677   // Build the object to represent this state &amp; prepare for recursive calls
1678   MachNode *mach = s-&gt;MachNodeGenerator(rule);
1679   mach-&gt;_opnds[0] = s-&gt;MachOperGenerator(_reduceOp[rule]);
1680   assert( mach-&gt;_opnds[0] != NULL, "Missing result operand" );
1681   Node *leaf = s-&gt;_leaf;
1682   // Check for instruction or instruction chain rule
1683   if( rule &gt;= _END_INST_CHAIN_RULE || rule &lt; _BEGIN_INST_CHAIN_RULE ) {
1684     assert(C-&gt;node_arena()-&gt;contains(s-&gt;_leaf) || !has_new_node(s-&gt;_leaf),
1685            "duplicating node that's already been matched");
1686     // Instruction
1687     mach-&gt;add_req( leaf-&gt;in(0) ); // Set initial control
1688     // Reduce interior of complex instruction
1689     ReduceInst_Interior( s, rule, mem, mach, 1 );
1690   } else {
1691     // Instruction chain rules are data-dependent on their inputs
1692     mach-&gt;add_req(0);             // Set initial control to none
1693     ReduceInst_Chain_Rule( s, rule, mem, mach );
1694   }
1695 
1696   // If a Memory was used, insert a Memory edge
1697   if( mem != (Node*)1 ) {
1698     mach-&gt;ins_req(MemNode::Memory,mem);
1699 #ifdef ASSERT
1700     // Verify adr type after matching memory operation
1701     const MachOper* oper = mach-&gt;memory_operand();
1702     if (oper != NULL &amp;&amp; oper != (MachOper*)-1) {
1703       // It has a unique memory operand.  Find corresponding ideal mem node.
1704       Node* m = NULL;
1705       if (leaf-&gt;is_Mem()) {
1706         m = leaf;
1707       } else {
1708         m = _mem_node;
1709         assert(m != NULL &amp;&amp; m-&gt;is_Mem(), "expecting memory node");
1710       }
1711       const Type* mach_at = mach-&gt;adr_type();
1712       // DecodeN node consumed by an address may have different type
1713       // then its input. Don't compare types for such case.
1714       if (m-&gt;adr_type() != mach_at &amp;&amp;
1715           (m-&gt;in(MemNode::Address)-&gt;is_DecodeNarrowPtr() ||
1716            m-&gt;in(MemNode::Address)-&gt;is_AddP() &amp;&amp;
1717            m-&gt;in(MemNode::Address)-&gt;in(AddPNode::Address)-&gt;is_DecodeNarrowPtr() ||
1718            m-&gt;in(MemNode::Address)-&gt;is_AddP() &amp;&amp;
1719            m-&gt;in(MemNode::Address)-&gt;in(AddPNode::Address)-&gt;is_AddP() &amp;&amp;
1720            m-&gt;in(MemNode::Address)-&gt;in(AddPNode::Address)-&gt;in(AddPNode::Address)-&gt;is_DecodeNarrowPtr())) {
1721         mach_at = m-&gt;adr_type();
1722       }
1723       if (m-&gt;adr_type() != mach_at) {
1724         m-&gt;dump();
1725         tty-&gt;print_cr("mach:");
1726         mach-&gt;dump(1);
1727       }
1728       assert(m-&gt;adr_type() == mach_at, "matcher should not change adr type");
1729     }
1730 #endif
1731   }
1732 
1733   // If the _leaf is an AddP, insert the base edge
1734   if (leaf-&gt;is_AddP()) {
1735     mach-&gt;ins_req(AddPNode::Base,leaf-&gt;in(AddPNode::Base));
1736   }
1737 
1738   uint number_of_projections_prior = number_of_projections();
1739 
1740   // Perform any 1-to-many expansions required
1741   MachNode *ex = mach-&gt;Expand(s, _projection_list, mem);
1742   if (ex != mach) {
1743     assert(ex-&gt;ideal_reg() == mach-&gt;ideal_reg(), "ideal types should match");
1744     if( ex-&gt;in(1)-&gt;is_Con() )
1745       ex-&gt;in(1)-&gt;set_req(0, C-&gt;root());
1746     // Remove old node from the graph
1747     for( uint i=0; i&lt;mach-&gt;req(); i++ ) {
1748       mach-&gt;set_req(i,NULL);
1749     }
1750 #ifdef ASSERT
1751     _new2old_map.map(ex-&gt;_idx, s-&gt;_leaf);
1752 #endif
1753   }
1754 
1755   // PhaseChaitin::fixup_spills will sometimes generate spill code
1756   // via the matcher.  By the time, nodes have been wired into the CFG,
1757   // and any further nodes generated by expand rules will be left hanging
1758   // in space, and will not get emitted as output code.  Catch this.
1759   // Also, catch any new register allocation constraints ("projections")
1760   // generated belatedly during spill code generation.
1761   if (_allocation_started) {
1762     guarantee(ex == mach, "no expand rules during spill generation");
1763     guarantee(number_of_projections_prior == number_of_projections(), "no allocation during spill generation");
1764   }
1765 
1766   if (leaf-&gt;is_Con() || leaf-&gt;is_DecodeNarrowPtr()) {
1767     // Record the con for sharing
1768     _shared_nodes.map(leaf-&gt;_idx, ex);
1769   }
1770 
1771   return ex;
1772 }
1773 
1774 void Matcher::handle_precedence_edges(Node* n, MachNode *mach) {
1775   for (uint i = n-&gt;req(); i &lt; n-&gt;len(); i++) {
1776     if (n-&gt;in(i) != NULL) {
1777       mach-&gt;add_prec(n-&gt;in(i));
1778     }
1779   }
1780 }
1781 
1782 void Matcher::ReduceInst_Chain_Rule( State *s, int rule, Node *&amp;mem, MachNode *mach ) {
1783   // 'op' is what I am expecting to receive
1784   int op = _leftOp[rule];
1785   // Operand type to catch childs result
1786   // This is what my child will give me.
1787   int opnd_class_instance = s-&gt;_rule[op];
1788   // Choose between operand class or not.
1789   // This is what I will receive.
1790   int catch_op = (FIRST_OPERAND_CLASS &lt;= op &amp;&amp; op &lt; NUM_OPERANDS) ? opnd_class_instance : op;
1791   // New rule for child.  Chase operand classes to get the actual rule.
1792   int newrule = s-&gt;_rule[catch_op];
1793 
1794   if( newrule &lt; NUM_OPERANDS ) {
1795     // Chain from operand or operand class, may be output of shared node
1796     assert( 0 &lt;= opnd_class_instance &amp;&amp; opnd_class_instance &lt; NUM_OPERANDS,
1797             "Bad AD file: Instruction chain rule must chain from operand");
1798     // Insert operand into array of operands for this instruction
1799     mach-&gt;_opnds[1] = s-&gt;MachOperGenerator(opnd_class_instance);
1800 
1801     ReduceOper( s, newrule, mem, mach );
1802   } else {
1803     // Chain from the result of an instruction
1804     assert( newrule &gt;= _LAST_MACH_OPER, "Do NOT chain from internal operand");
1805     mach-&gt;_opnds[1] = s-&gt;MachOperGenerator(_reduceOp[catch_op]);
1806     Node *mem1 = (Node*)1;
1807     debug_only(Node *save_mem_node = _mem_node;)
1808     mach-&gt;add_req( ReduceInst(s, newrule, mem1) );
1809     debug_only(_mem_node = save_mem_node;)
1810   }
1811   return;
1812 }
1813 
1814 
1815 uint Matcher::ReduceInst_Interior( State *s, int rule, Node *&amp;mem, MachNode *mach, uint num_opnds ) {
1816   handle_precedence_edges(s-&gt;_leaf, mach);
1817 
1818   if( s-&gt;_leaf-&gt;is_Load() ) {
1819     Node *mem2 = s-&gt;_leaf-&gt;in(MemNode::Memory);
1820     assert( mem == (Node*)1 || mem == mem2, "multiple Memories being matched at once?" );
1821     debug_only( if( mem == (Node*)1 ) _mem_node = s-&gt;_leaf;)
1822     mem = mem2;
1823   }
1824   if( s-&gt;_leaf-&gt;in(0) != NULL &amp;&amp; s-&gt;_leaf-&gt;req() &gt; 1) {
1825     if( mach-&gt;in(0) == NULL )
1826       mach-&gt;set_req(0, s-&gt;_leaf-&gt;in(0));
1827   }
1828 
1829   // Now recursively walk the state tree &amp; add operand list.
1830   for( uint i=0; i&lt;2; i++ ) {   // binary tree
1831     State *newstate = s-&gt;_kids[i];
1832     if( newstate == NULL ) break;      // Might only have 1 child
1833     // 'op' is what I am expecting to receive
1834     int op;
1835     if( i == 0 ) {
1836       op = _leftOp[rule];
1837     } else {
1838       op = _rightOp[rule];
1839     }
1840     // Operand type to catch childs result
1841     // This is what my child will give me.
1842     int opnd_class_instance = newstate-&gt;_rule[op];
1843     // Choose between operand class or not.
1844     // This is what I will receive.
1845     int catch_op = (op &gt;= FIRST_OPERAND_CLASS &amp;&amp; op &lt; NUM_OPERANDS) ? opnd_class_instance : op;
1846     // New rule for child.  Chase operand classes to get the actual rule.
1847     int newrule = newstate-&gt;_rule[catch_op];
1848 
1849     if( newrule &lt; NUM_OPERANDS ) { // Operand/operandClass or internalOp/instruction?
1850       // Operand/operandClass
1851       // Insert operand into array of operands for this instruction
1852       mach-&gt;_opnds[num_opnds++] = newstate-&gt;MachOperGenerator(opnd_class_instance);
1853       ReduceOper( newstate, newrule, mem, mach );
1854 
1855     } else {                    // Child is internal operand or new instruction
1856       if( newrule &lt; _LAST_MACH_OPER ) { // internal operand or instruction?
1857         // internal operand --&gt; call ReduceInst_Interior
1858         // Interior of complex instruction.  Do nothing but recurse.
1859         num_opnds = ReduceInst_Interior( newstate, newrule, mem, mach, num_opnds );
1860       } else {
1861         // instruction --&gt; call build operand(  ) to catch result
1862         //             --&gt; ReduceInst( newrule )
1863         mach-&gt;_opnds[num_opnds++] = s-&gt;MachOperGenerator(_reduceOp[catch_op]);
1864         Node *mem1 = (Node*)1;
1865         debug_only(Node *save_mem_node = _mem_node;)
1866         mach-&gt;add_req( ReduceInst( newstate, newrule, mem1 ) );
1867         debug_only(_mem_node = save_mem_node;)
1868       }
1869     }
1870     assert( mach-&gt;_opnds[num_opnds-1], "" );
1871   }
1872   return num_opnds;
1873 }
1874 
1875 // This routine walks the interior of possible complex operands.
1876 // At each point we check our children in the match tree:
1877 // (1) No children -
1878 //     We are a leaf; add _leaf field as an input to the MachNode
1879 // (2) Child is an internal operand -
1880 //     Skip over it ( do nothing )
1881 // (3) Child is an instruction -
1882 //     Call ReduceInst recursively and
1883 //     and instruction as an input to the MachNode
1884 void Matcher::ReduceOper( State *s, int rule, Node *&amp;mem, MachNode *mach ) {
1885   assert( rule &lt; _LAST_MACH_OPER, "called with operand rule" );
1886   State *kid = s-&gt;_kids[0];
1887   assert( kid == NULL || s-&gt;_leaf-&gt;in(0) == NULL, "internal operands have no control" );
1888 
1889   // Leaf?  And not subsumed?
1890   if( kid == NULL &amp;&amp; !_swallowed[rule] ) {
1891     mach-&gt;add_req( s-&gt;_leaf );  // Add leaf pointer
1892     return;                     // Bail out
1893   }
1894 
1895   if( s-&gt;_leaf-&gt;is_Load() ) {
1896     assert( mem == (Node*)1, "multiple Memories being matched at once?" );
1897     mem = s-&gt;_leaf-&gt;in(MemNode::Memory);
1898     debug_only(_mem_node = s-&gt;_leaf;)
1899   }
1900 
1901   handle_precedence_edges(s-&gt;_leaf, mach);
1902 
1903   if( s-&gt;_leaf-&gt;in(0) &amp;&amp; s-&gt;_leaf-&gt;req() &gt; 1) {
1904     if( !mach-&gt;in(0) )
1905       mach-&gt;set_req(0,s-&gt;_leaf-&gt;in(0));
1906     else {
1907       assert( s-&gt;_leaf-&gt;in(0) == mach-&gt;in(0), "same instruction, differing controls?" );
1908     }
1909   }
1910 
1911   for( uint i=0; kid != NULL &amp;&amp; i&lt;2; kid = s-&gt;_kids[1], i++ ) {   // binary tree
1912     int newrule;
1913     if( i == 0)
1914       newrule = kid-&gt;_rule[_leftOp[rule]];
1915     else
1916       newrule = kid-&gt;_rule[_rightOp[rule]];
1917 
1918     if( newrule &lt; _LAST_MACH_OPER ) { // Operand or instruction?
1919       // Internal operand; recurse but do nothing else
1920       ReduceOper( kid, newrule, mem, mach );
1921 
1922     } else {                    // Child is a new instruction
1923       // Reduce the instruction, and add a direct pointer from this
1924       // machine instruction to the newly reduced one.
1925       Node *mem1 = (Node*)1;
1926       debug_only(Node *save_mem_node = _mem_node;)
1927       mach-&gt;add_req( ReduceInst( kid, newrule, mem1 ) );
1928       debug_only(_mem_node = save_mem_node;)
1929     }
1930   }
1931 }
1932 
1933 
1934 // -------------------------------------------------------------------------
1935 // Java-Java calling convention
1936 // (what you use when Java calls Java)
1937 
1938 //------------------------------find_receiver----------------------------------
1939 // For a given signature, return the OptoReg for parameter 0.
1940 OptoReg::Name Matcher::find_receiver( bool is_outgoing ) {
1941   VMRegPair regs;
1942   BasicType sig_bt = T_OBJECT;
1943   calling_convention(&amp;sig_bt, &amp;regs, 1, is_outgoing);
1944   // Return argument 0 register.  In the LP64 build pointers
1945   // take 2 registers, but the VM wants only the 'main' name.
1946   return OptoReg::as_OptoReg(regs.first());
1947 }
1948 
1949 // This function identifies sub-graphs in which a 'load' node is
1950 // input to two different nodes, and such that it can be matched
1951 // with BMI instructions like blsi, blsr, etc.
1952 // Example : for b = -a[i] &amp; a[i] can be matched to blsi r32, m32.
1953 // The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*
1954 // refers to the same node.
1955 #ifdef X86
1956 // Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)
1957 // This is a temporary solution until we make DAGs expressible in ADL.
1958 template&lt;typename ConType&gt;
1959 class FusedPatternMatcher {
1960   Node* _op1_node;
1961   Node* _mop_node;
1962   int _con_op;
1963 
1964   static int match_next(Node* n, int next_op, int next_op_idx) {
1965     if (n-&gt;in(1) == NULL || n-&gt;in(2) == NULL) {
1966       return -1;
1967     }
1968 
1969     if (next_op_idx == -1) { // n is commutative, try rotations
1970       if (n-&gt;in(1)-&gt;Opcode() == next_op) {
1971         return 1;
1972       } else if (n-&gt;in(2)-&gt;Opcode() == next_op) {
1973         return 2;
1974       }
1975     } else {
1976       assert(next_op_idx &gt; 0 &amp;&amp; next_op_idx &lt;= 2, "Bad argument index");
1977       if (n-&gt;in(next_op_idx)-&gt;Opcode() == next_op) {
1978         return next_op_idx;
1979       }
1980     }
1981     return -1;
1982   }
1983 public:
1984   FusedPatternMatcher(Node* op1_node, Node *mop_node, int con_op) :
1985     _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }
1986 
1987   bool match(int op1, int op1_op2_idx,  // op1 and the index of the op1-&gt;op2 edge, -1 if op1 is commutative
1988              int op2, int op2_con_idx,  // op2 and the index of the op2-&gt;con edge, -1 if op2 is commutative
1989              typename ConType::NativeType con_value) {
1990     if (_op1_node-&gt;Opcode() != op1) {
1991       return false;
1992     }
1993     if (_mop_node-&gt;outcnt() &gt; 2) {
1994       return false;
1995     }
1996     op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);
1997     if (op1_op2_idx == -1) {
1998       return false;
1999     }
2000     // Memory operation must be the other edge
2001     int op1_mop_idx = (op1_op2_idx &amp; 1) + 1;
2002 
2003     // Check that the mop node is really what we want
2004     if (_op1_node-&gt;in(op1_mop_idx) == _mop_node) {
2005       Node *op2_node = _op1_node-&gt;in(op1_op2_idx);
2006       if (op2_node-&gt;outcnt() &gt; 1) {
2007         return false;
2008       }
2009       assert(op2_node-&gt;Opcode() == op2, "Should be");
2010       op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);
2011       if (op2_con_idx == -1) {
2012         return false;
2013       }
2014       // Memory operation must be the other edge
2015       int op2_mop_idx = (op2_con_idx &amp; 1) + 1;
2016       // Check that the memory operation is the same node
2017       if (op2_node-&gt;in(op2_mop_idx) == _mop_node) {
2018         // Now check the constant
2019         const Type* con_type = op2_node-&gt;in(op2_con_idx)-&gt;bottom_type();
2020         if (con_type != Type::TOP &amp;&amp; ConType::as_self(con_type)-&gt;get_con() == con_value) {
2021           return true;
2022         }
2023       }
2024     }
2025     return false;
2026   }
2027 };
2028 
2029 
2030 bool Matcher::is_bmi_pattern(Node *n, Node *m) {
2031   if (n != NULL &amp;&amp; m != NULL) {
2032     if (m-&gt;Opcode() == Op_LoadI) {
2033       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
2034       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
2035              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
2036              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
2037     } else if (m-&gt;Opcode() == Op_LoadL) {
2038       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
2039       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
2040              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
2041              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
2042     }
2043   }
2044   return false;
2045 }
2046 #endif // X86
2047 
2048 // A method-klass-holder may be passed in the inline_cache_reg
2049 // and then expanded into the inline_cache_reg and a method_oop register
2050 //   defined in ad_&lt;arch&gt;.cpp
2051 
2052 // Check for shift by small constant as well
2053 static bool clone_shift(Node* shift, Matcher* matcher, MStack&amp; mstack, VectorSet&amp; address_visited) {
2054   if (shift-&gt;Opcode() == Op_LShiftX &amp;&amp; shift-&gt;in(2)-&gt;is_Con() &amp;&amp;
2055       shift-&gt;in(2)-&gt;get_int() &lt;= 3 &amp;&amp;
2056       // Are there other uses besides address expressions?
2057       !matcher-&gt;is_visited(shift)) {
2058     address_visited.set(shift-&gt;_idx); // Flag as address_visited
2059     mstack.push(shift-&gt;in(2), Visit);
2060     Node *conv = shift-&gt;in(1);
2061 #ifdef _LP64
2062     // Allow Matcher to match the rule which bypass
2063     // ConvI2L operation for an array index on LP64
2064     // if the index value is positive.
2065     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
2066         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
2067         // Are there other uses besides address expressions?
2068         !matcher-&gt;is_visited(conv)) {
2069       address_visited.set(conv-&gt;_idx); // Flag as address_visited
2070       mstack.push(conv-&gt;in(1), Pre_Visit);
2071     } else
2072 #endif
2073       mstack.push(conv, Pre_Visit);
2074     return true;
2075   }
2076   return false;
2077 }
2078 
2079 
2080 //------------------------------find_shared------------------------------------
2081 // Set bits if Node is shared or otherwise a root
2082 void Matcher::find_shared( Node *n ) {
2083   // Allocate stack of size C-&gt;live_nodes() * 2 to avoid frequent realloc
2084   MStack mstack(C-&gt;live_nodes() * 2);
2085   // Mark nodes as address_visited if they are inputs to an address expression
2086   VectorSet address_visited(Thread::current()-&gt;resource_area());
2087   mstack.push(n, Visit);     // Don't need to pre-visit root node
2088   while (mstack.is_nonempty()) {
2089     n = mstack.node();       // Leave node on stack
2090     Node_State nstate = mstack.state();
2091     uint nop = n-&gt;Opcode();
2092     if (nstate == Pre_Visit) {
2093       if (address_visited.test(n-&gt;_idx)) { // Visited in address already?
2094         // Flag as visited and shared now.
2095         set_visited(n);
2096       }
2097       if (is_visited(n)) {   // Visited already?
2098         // Node is shared and has no reason to clone.  Flag it as shared.
2099         // This causes it to match into a register for the sharing.
2100         set_shared(n);       // Flag as shared and
2101         mstack.pop();        // remove node from stack
2102         continue;
2103       }
2104       nstate = Visit; // Not already visited; so visit now
2105     }
2106     if (nstate == Visit) {
2107       mstack.set_state(Post_Visit);
2108       set_visited(n);   // Flag as visited now
2109       bool mem_op = false;
2110 
2111       switch( nop ) {  // Handle some opcodes special
2112       case Op_Phi:             // Treat Phis as shared roots
2113       case Op_Parm:
2114       case Op_Proj:            // All handled specially during matching
2115       case Op_SafePointScalarObject:
2116         set_shared(n);
2117         set_dontcare(n);
2118         break;
2119       case Op_If:
2120       case Op_CountedLoopEnd:
2121         mstack.set_state(Alt_Post_Visit); // Alternative way
2122         // Convert (If (Bool (CmpX A B))) into (If (Bool) (CmpX A B)).  Helps
2123         // with matching cmp/branch in 1 instruction.  The Matcher needs the
2124         // Bool and CmpX side-by-side, because it can only get at constants
2125         // that are at the leaves of Match trees, and the Bool's condition acts
2126         // as a constant here.
2127         mstack.push(n-&gt;in(1), Visit);         // Clone the Bool
2128         mstack.push(n-&gt;in(0), Pre_Visit);     // Visit control input
2129         continue; // while (mstack.is_nonempty())
2130       case Op_ConvI2D:         // These forms efficiently match with a prior
2131       case Op_ConvI2F:         //   Load but not a following Store
2132         if( n-&gt;in(1)-&gt;is_Load() &amp;&amp;        // Prior load
2133             n-&gt;outcnt() == 1 &amp;&amp;           // Not already shared
2134             n-&gt;unique_out()-&gt;is_Store() ) // Following store
2135           set_shared(n);       // Force it to be a root
2136         break;
2137       case Op_ReverseBytesI:
2138       case Op_ReverseBytesL:
2139         if( n-&gt;in(1)-&gt;is_Load() &amp;&amp;        // Prior load
2140             n-&gt;outcnt() == 1 )            // Not already shared
2141           set_shared(n);                  // Force it to be a root
2142         break;
2143       case Op_BoxLock:         // Cant match until we get stack-regs in ADLC
2144       case Op_IfFalse:
2145       case Op_IfTrue:
2146       case Op_MachProj:
2147       case Op_MergeMem:
2148       case Op_Catch:
2149       case Op_CatchProj:
2150       case Op_CProj:
2151       case Op_JumpProj:
2152       case Op_JProj:
2153       case Op_NeverBranch:
2154         set_dontcare(n);
2155         break;
2156       case Op_Jump:
2157         mstack.push(n-&gt;in(1), Pre_Visit);     // Switch Value (could be shared)
2158         mstack.push(n-&gt;in(0), Pre_Visit);     // Visit Control input
2159         continue;                             // while (mstack.is_nonempty())
2160       case Op_StrComp:
2161       case Op_StrEquals:
2162       case Op_StrIndexOf:
2163       case Op_StrIndexOfChar:
2164       case Op_AryEq:
2165       case Op_HasNegatives:
2166       case Op_StrInflatedCopy:
2167       case Op_StrCompressedCopy:
2168       case Op_EncodeISOArray:
2169         set_shared(n); // Force result into register (it will be anyways)
2170         break;
2171       case Op_ConP: {  // Convert pointers above the centerline to NUL
2172         TypeNode *tn = n-&gt;as_Type(); // Constants derive from type nodes
2173         const TypePtr* tp = tn-&gt;type()-&gt;is_ptr();
2174         if (tp-&gt;_ptr == TypePtr::AnyNull) {
2175           tn-&gt;set_type(TypePtr::NULL_PTR);
2176         }
2177         break;
2178       }
2179       case Op_ConN: {  // Convert narrow pointers above the centerline to NUL
2180         TypeNode *tn = n-&gt;as_Type(); // Constants derive from type nodes
2181         const TypePtr* tp = tn-&gt;type()-&gt;make_ptr();
2182         if (tp &amp;&amp; tp-&gt;_ptr == TypePtr::AnyNull) {
2183           tn-&gt;set_type(TypeNarrowOop::NULL_PTR);
2184         }
2185         break;
2186       }
2187       case Op_Binary:         // These are introduced in the Post_Visit state.
2188         ShouldNotReachHere();
2189         break;
2190       case Op_ClearArray:
2191       case Op_SafePoint:
2192         mem_op = true;
2193         break;
2194       default:
2195         if( n-&gt;is_Store() ) {
2196           // Do match stores, despite no ideal reg
2197           mem_op = true;
2198           break;
2199         }
2200         if( n-&gt;is_Mem() ) { // Loads and LoadStores
2201           mem_op = true;
2202           // Loads must be root of match tree due to prior load conflict
2203           if( C-&gt;subsume_loads() == false )
2204             set_shared(n);
2205         }
2206         // Fall into default case
2207         if( !n-&gt;ideal_reg() )
2208           set_dontcare(n);  // Unmatchable Nodes
2209       } // end_switch
2210 
2211       for(int i = n-&gt;req() - 1; i &gt;= 0; --i) { // For my children
2212         Node *m = n-&gt;in(i); // Get ith input
2213         if (m == NULL) continue;  // Ignore NULLs
2214         uint mop = m-&gt;Opcode();
2215 
2216         // Must clone all producers of flags, or we will not match correctly.
2217         // Suppose a compare setting int-flags is shared (e.g., a switch-tree)
2218         // then it will match into an ideal Op_RegFlags.  Alas, the fp-flags
2219         // are also there, so we may match a float-branch to int-flags and
2220         // expect the allocator to haul the flags from the int-side to the
2221         // fp-side.  No can do.
2222         if( _must_clone[mop] ) {
2223           mstack.push(m, Visit);
2224           continue; // for(int i = ...)
2225         }
2226 
2227         if( mop == Op_AddP &amp;&amp; m-&gt;in(AddPNode::Base)-&gt;is_DecodeNarrowPtr()) {
2228           // Bases used in addresses must be shared but since
2229           // they are shared through a DecodeN they may appear
2230           // to have a single use so force sharing here.
2231           set_shared(m-&gt;in(AddPNode::Base)-&gt;in(1));
2232         }
2233 
2234         // if 'n' and 'm' are part of a graph for BMI instruction, clone this node.
2235 #ifdef X86
2236         if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
2237           mstack.push(m, Visit);
2238           continue;
2239         }
2240 #endif
2241 
2242         // Clone addressing expressions as they are "free" in memory access instructions
2243         if (mem_op &amp;&amp; i == MemNode::Address &amp;&amp; mop == Op_AddP &amp;&amp;
2244             // When there are other uses besides address expressions
2245             // put it on stack and mark as shared.
2246             !is_visited(m)) {
2247           // Some inputs for address expression are not put on stack
2248           // to avoid marking them as shared and forcing them into register
2249           // if they are used only in address expressions.
2250           // But they should be marked as shared if there are other uses
2251           // besides address expressions.
2252 
2253           Node *off = m-&gt;in(AddPNode::Offset);
2254           if (off-&gt;is_Con()) {
2255             address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2256             Node *adr = m-&gt;in(AddPNode::Address);
2257 
2258             // Intel, ARM and friends can handle 2 adds in addressing mode
2259             if( clone_shift_expressions &amp;&amp; adr-&gt;is_AddP() &amp;&amp;
2260                 // AtomicAdd is not an addressing expression.
2261                 // Cheap to find it by looking for screwy base.
2262                 !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
2263                 // Are there other uses besides address expressions?
2264                 !is_visited(adr) ) {
2265               address_visited.set(adr-&gt;_idx); // Flag as address_visited
2266               Node *shift = adr-&gt;in(AddPNode::Offset);
2267               if (!clone_shift(shift, this, mstack, address_visited)) {
2268                 mstack.push(shift, Pre_Visit);
2269               }
2270               mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
2271               mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
2272             } else {  // Sparc, Alpha, PPC and friends
2273               mstack.push(adr, Pre_Visit);
2274             }
2275 
2276             // Clone X+offset as it also folds into most addressing expressions
2277             mstack.push(off, Visit);
2278             mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2279             continue; // for(int i = ...)
2280           } else if (clone_shift_expressions &amp;&amp;
2281                      clone_shift(off, this, mstack, address_visited)) {
2282               address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2283               mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
2284               mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2285               continue;
2286           } // if( off-&gt;is_Con() )
2287         }   // if( mem_op &amp;&amp;
2288         mstack.push(m, Pre_Visit);
2289       }     // for(int i = ...)
2290     }
2291     else if (nstate == Alt_Post_Visit) {
2292       mstack.pop(); // Remove node from stack
2293       // We cannot remove the Cmp input from the Bool here, as the Bool may be
2294       // shared and all users of the Bool need to move the Cmp in parallel.
2295       // This leaves both the Bool and the If pointing at the Cmp.  To
2296       // prevent the Matcher from trying to Match the Cmp along both paths
2297       // BoolNode::match_edge always returns a zero.
2298 
2299       // We reorder the Op_If in a pre-order manner, so we can visit without
2300       // accidentally sharing the Cmp (the Bool and the If make 2 users).
2301       n-&gt;add_req( n-&gt;in(1)-&gt;in(1) ); // Add the Cmp next to the Bool
2302     }
2303     else if (nstate == Post_Visit) {
2304       mstack.pop(); // Remove node from stack
2305 
2306       // Now hack a few special opcodes
2307       switch( n-&gt;Opcode() ) {       // Handle some opcodes special
2308       case Op_StorePConditional:
2309       case Op_StoreIConditional:
2310       case Op_StoreLConditional:
2311       case Op_CompareAndSwapI:
2312       case Op_CompareAndSwapL:
2313       case Op_CompareAndSwapP:
2314       case Op_CompareAndSwapN: {   // Convert trinary to binary-tree
2315         Node *newval = n-&gt;in(MemNode::ValueIn );
2316         Node *oldval  = n-&gt;in(LoadStoreConditionalNode::ExpectedIn);
2317         Node *pair = new BinaryNode( oldval, newval );
2318         n-&gt;set_req(MemNode::ValueIn,pair);
2319         n-&gt;del_req(LoadStoreConditionalNode::ExpectedIn);
2320         break;
2321       }
2322       case Op_CMoveD:              // Convert trinary to binary-tree
2323       case Op_CMoveF:
2324       case Op_CMoveI:
2325       case Op_CMoveL:
2326       case Op_CMoveN:
2327       case Op_CMoveP:
2328       case Op_CMoveVD:  {
2329         // Restructure into a binary tree for Matching.  It's possible that
2330         // we could move this code up next to the graph reshaping for IfNodes
2331         // or vice-versa, but I do not want to debug this for Ladybird.
2332         // 10/2/2000 CNC.
2333         Node *pair1 = new BinaryNode(n-&gt;in(1),n-&gt;in(1)-&gt;in(1));
2334         n-&gt;set_req(1,pair1);
2335         Node *pair2 = new BinaryNode(n-&gt;in(2),n-&gt;in(3));
2336         n-&gt;set_req(2,pair2);
2337         n-&gt;del_req(3);
2338         break;
2339       }
2340       case Op_LoopLimit: {
2341         Node *pair1 = new BinaryNode(n-&gt;in(1),n-&gt;in(2));
2342         n-&gt;set_req(1,pair1);
2343         n-&gt;set_req(2,n-&gt;in(3));
2344         n-&gt;del_req(3);
2345         break;
2346       }
2347       case Op_StrEquals:
2348       case Op_StrIndexOfChar: {
2349         Node *pair1 = new BinaryNode(n-&gt;in(2),n-&gt;in(3));
2350         n-&gt;set_req(2,pair1);
2351         n-&gt;set_req(3,n-&gt;in(4));
2352         n-&gt;del_req(4);
2353         break;
2354       }
2355       case Op_StrComp:
2356       case Op_StrIndexOf: {
2357         Node *pair1 = new BinaryNode(n-&gt;in(2),n-&gt;in(3));
2358         n-&gt;set_req(2,pair1);
2359         Node *pair2 = new BinaryNode(n-&gt;in(4),n-&gt;in(5));
2360         n-&gt;set_req(3,pair2);
2361         n-&gt;del_req(5);
2362         n-&gt;del_req(4);
2363         break;
2364       }
2365       case Op_StrCompressedCopy:
2366       case Op_StrInflatedCopy:
2367       case Op_EncodeISOArray: {
2368         // Restructure into a binary tree for Matching.
2369         Node* pair = new BinaryNode(n-&gt;in(3), n-&gt;in(4));
2370         n-&gt;set_req(3, pair);
2371         n-&gt;del_req(4);
2372         break;
2373       }
2374       default:
2375         break;
2376       }
2377     }
2378     else {
2379       ShouldNotReachHere();
2380     }
2381   } // end of while (mstack.is_nonempty())
2382 }
2383 
2384 #ifdef ASSERT
2385 // machine-independent root to machine-dependent root
2386 void Matcher::dump_old2new_map() {
2387   _old2new_map.dump();
2388 }
2389 #endif
2390 
2391 //---------------------------collect_null_checks-------------------------------
2392 // Find null checks in the ideal graph; write a machine-specific node for
2393 // it.  Used by later implicit-null-check handling.  Actually collects
2394 // either an IfTrue or IfFalse for the common NOT-null path, AND the ideal
2395 // value being tested.
2396 void Matcher::collect_null_checks( Node *proj, Node *orig_proj ) {
2397   Node *iff = proj-&gt;in(0);
2398   if( iff-&gt;Opcode() == Op_If ) {
2399     // During matching If's have Bool &amp; Cmp side-by-side
2400     BoolNode *b = iff-&gt;in(1)-&gt;as_Bool();
2401     Node *cmp = iff-&gt;in(2);
2402     int opc = cmp-&gt;Opcode();
2403     if (opc != Op_CmpP &amp;&amp; opc != Op_CmpN) return;
2404 
2405     const Type* ct = cmp-&gt;in(2)-&gt;bottom_type();
2406     if (ct == TypePtr::NULL_PTR ||
2407         (opc == Op_CmpN &amp;&amp; ct == TypeNarrowOop::NULL_PTR)) {
2408 
2409       bool push_it = false;
2410       if( proj-&gt;Opcode() == Op_IfTrue ) {
2411         extern int all_null_checks_found;
2412         all_null_checks_found++;
2413         if( b-&gt;_test._test == BoolTest::ne ) {
2414           push_it = true;
2415         }
2416       } else {
2417         assert( proj-&gt;Opcode() == Op_IfFalse, "" );
2418         if( b-&gt;_test._test == BoolTest::eq ) {
2419           push_it = true;
2420         }
2421       }
2422       if( push_it ) {
2423         _null_check_tests.push(proj);
2424         Node* val = cmp-&gt;in(1);
2425 #ifdef _LP64
2426         if (val-&gt;bottom_type()-&gt;isa_narrowoop() &amp;&amp;
2427             !Matcher::narrow_oop_use_complex_address()) {
2428           //
2429           // Look for DecodeN node which should be pinned to orig_proj.
2430           // On platforms (Sparc) which can not handle 2 adds
2431           // in addressing mode we have to keep a DecodeN node and
2432           // use it to do implicit NULL check in address.
2433           //
2434           // DecodeN node was pinned to non-null path (orig_proj) during
2435           // CastPP transformation in final_graph_reshaping_impl().
2436           //
2437           uint cnt = orig_proj-&gt;outcnt();
2438           for (uint i = 0; i &lt; orig_proj-&gt;outcnt(); i++) {
2439             Node* d = orig_proj-&gt;raw_out(i);
2440             if (d-&gt;is_DecodeN() &amp;&amp; d-&gt;in(1) == val) {
2441               val = d;
2442               val-&gt;set_req(0, NULL); // Unpin now.
2443               // Mark this as special case to distinguish from
2444               // a regular case: CmpP(DecodeN, NULL).
2445               val = (Node*)(((intptr_t)val) | 1);
2446               break;
2447             }
2448           }
2449         }
2450 #endif
2451         _null_check_tests.push(val);
2452       }
2453     }
2454   }
2455 }
2456 
2457 //---------------------------validate_null_checks------------------------------
2458 // Its possible that the value being NULL checked is not the root of a match
2459 // tree.  If so, I cannot use the value in an implicit null check.
2460 void Matcher::validate_null_checks( ) {
2461   uint cnt = _null_check_tests.size();
2462   for( uint i=0; i &lt; cnt; i+=2 ) {
2463     Node *test = _null_check_tests[i];
2464     Node *val = _null_check_tests[i+1];
2465     bool is_decoden = ((intptr_t)val) &amp; 1;
2466     val = (Node*)(((intptr_t)val) &amp; ~1);
2467     if (has_new_node(val)) {
2468       Node* new_val = new_node(val);
2469       if (is_decoden) {
2470         assert(val-&gt;is_DecodeNarrowPtr() &amp;&amp; val-&gt;in(0) == NULL, "sanity");
2471         // Note: new_val may have a control edge if
2472         // the original ideal node DecodeN was matched before
2473         // it was unpinned in Matcher::collect_null_checks().
2474         // Unpin the mach node and mark it.
2475         new_val-&gt;set_req(0, NULL);
2476         new_val = (Node*)(((intptr_t)new_val) | 1);
2477       }
2478       // Is a match-tree root, so replace with the matched value
2479       _null_check_tests.map(i+1, new_val);
2480     } else {
2481       // Yank from candidate list
2482       _null_check_tests.map(i+1,_null_check_tests[--cnt]);
2483       _null_check_tests.map(i,_null_check_tests[--cnt]);
2484       _null_check_tests.pop();
2485       _null_check_tests.pop();
2486       i-=2;
2487     }
2488   }
2489 }
2490 
2491 // Used by the DFA in dfa_xxx.cpp.  Check for a following barrier or
2492 // atomic instruction acting as a store_load barrier without any
2493 // intervening volatile load, and thus we don't need a barrier here.
2494 // We retain the Node to act as a compiler ordering barrier.
2495 bool Matcher::post_store_load_barrier(const Node* vmb) {
2496   Compile* C = Compile::current();
2497   assert(vmb-&gt;is_MemBar(), "");
2498   assert(vmb-&gt;Opcode() != Op_MemBarAcquire &amp;&amp; vmb-&gt;Opcode() != Op_LoadFence, "");
2499   const MemBarNode* membar = vmb-&gt;as_MemBar();
2500 
2501   // Get the Ideal Proj node, ctrl, that can be used to iterate forward
2502   Node* ctrl = NULL;
2503   for (DUIterator_Fast imax, i = membar-&gt;fast_outs(imax); i &lt; imax; i++) {
2504     Node* p = membar-&gt;fast_out(i);
2505     assert(p-&gt;is_Proj(), "only projections here");
2506     if ((p-&gt;as_Proj()-&gt;_con == TypeFunc::Control) &amp;&amp;
2507         !C-&gt;node_arena()-&gt;contains(p)) { // Unmatched old-space only
2508       ctrl = p;
2509       break;
2510     }
2511   }
2512   assert((ctrl != NULL), "missing control projection");
2513 
2514   for (DUIterator_Fast jmax, j = ctrl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2515     Node *x = ctrl-&gt;fast_out(j);
2516     int xop = x-&gt;Opcode();
2517 
2518     // We don't need current barrier if we see another or a lock
2519     // before seeing volatile load.
2520     //
2521     // Op_Fastunlock previously appeared in the Op_* list below.
2522     // With the advent of 1-0 lock operations we're no longer guaranteed
2523     // that a monitor exit operation contains a serializing instruction.
2524 
2525     if (xop == Op_MemBarVolatile ||
2526         xop == Op_CompareAndSwapL ||
2527         xop == Op_CompareAndSwapP ||
2528         xop == Op_CompareAndSwapN ||
2529         xop == Op_CompareAndSwapI) {
2530       return true;
2531     }
2532 
2533     // Op_FastLock previously appeared in the Op_* list above.
2534     // With biased locking we're no longer guaranteed that a monitor
2535     // enter operation contains a serializing instruction.
2536     if ((xop == Op_FastLock) &amp;&amp; !UseBiasedLocking) {
2537       return true;
2538     }
2539 
2540     if (x-&gt;is_MemBar()) {
2541       // We must retain this membar if there is an upcoming volatile
2542       // load, which will be followed by acquire membar.
2543       if (xop == Op_MemBarAcquire || xop == Op_LoadFence) {
2544         return false;
2545       } else {
2546         // For other kinds of barriers, check by pretending we
2547         // are them, and seeing if we can be removed.
2548         return post_store_load_barrier(x-&gt;as_MemBar());
2549       }
2550     }
2551 
2552     // probably not necessary to check for these
2553     if (x-&gt;is_Call() || x-&gt;is_SafePoint() || x-&gt;is_block_proj()) {
2554       return false;
2555     }
2556   }
2557   return false;
2558 }
2559 
2560 // Check whether node n is a branch to an uncommon trap that we could
2561 // optimize as test with very high branch costs in case of going to
2562 // the uncommon trap. The code must be able to be recompiled to use
2563 // a cheaper test.
2564 bool Matcher::branches_to_uncommon_trap(const Node *n) {
2565   // Don't do it for natives, adapters, or runtime stubs
2566   Compile *C = Compile::current();
2567   if (!C-&gt;is_method_compilation()) return false;
2568 
2569   assert(n-&gt;is_If(), "You should only call this on if nodes.");
2570   IfNode *ifn = n-&gt;as_If();
2571 
2572   Node *ifFalse = NULL;
2573   for (DUIterator_Fast imax, i = ifn-&gt;fast_outs(imax); i &lt; imax; i++) {
2574     if (ifn-&gt;fast_out(i)-&gt;is_IfFalse()) {
2575       ifFalse = ifn-&gt;fast_out(i);
2576       break;
2577     }
2578   }
2579   assert(ifFalse, "An If should have an ifFalse. Graph is broken.");
2580 
2581   Node *reg = ifFalse;
2582   int cnt = 4; // We must protect against cycles.  Limit to 4 iterations.
2583                // Alternatively use visited set?  Seems too expensive.
2584   while (reg != NULL &amp;&amp; cnt &gt; 0) {
2585     CallNode *call = NULL;
2586     RegionNode *nxt_reg = NULL;
2587     for (DUIterator_Fast imax, i = reg-&gt;fast_outs(imax); i &lt; imax; i++) {
2588       Node *o = reg-&gt;fast_out(i);
2589       if (o-&gt;is_Call()) {
2590         call = o-&gt;as_Call();
2591       }
2592       if (o-&gt;is_Region()) {
2593         nxt_reg = o-&gt;as_Region();
2594       }
2595     }
2596 
2597     if (call &amp;&amp;
2598         call-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point()) {
2599       const Type* trtype = call-&gt;in(TypeFunc::Parms)-&gt;bottom_type();
2600       if (trtype-&gt;isa_int() &amp;&amp; trtype-&gt;is_int()-&gt;is_con()) {
2601         jint tr_con = trtype-&gt;is_int()-&gt;get_con();
2602         Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(tr_con);
2603         Deoptimization::DeoptAction action = Deoptimization::trap_request_action(tr_con);
2604         assert((int)reason &lt; (int)BitsPerInt, "recode bit map");
2605 
2606         if (is_set_nth_bit(C-&gt;allowed_deopt_reasons(), (int)reason)
2607             &amp;&amp; action != Deoptimization::Action_none) {
2608           // This uncommon trap is sure to recompile, eventually.
2609           // When that happens, C-&gt;too_many_traps will prevent
2610           // this transformation from happening again.
2611           return true;
2612         }
2613       }
2614     }
2615 
2616     reg = nxt_reg;
2617     cnt--;
2618   }
2619 
2620   return false;
2621 }
2622 
2623 //=============================================================================
2624 //---------------------------State---------------------------------------------
2625 State::State(void) {
2626 #ifdef ASSERT
2627   _id = 0;
2628   _kids[0] = _kids[1] = (State*)(intptr_t) CONST64(0xcafebabecafebabe);
2629   _leaf = (Node*)(intptr_t) CONST64(0xbaadf00dbaadf00d);
2630   //memset(_cost, -1, sizeof(_cost));
2631   //memset(_rule, -1, sizeof(_rule));
2632 #endif
2633   memset(_valid, 0, sizeof(_valid));
2634 }
2635 
2636 #ifdef ASSERT
2637 State::~State() {
2638   _id = 99;
2639   _kids[0] = _kids[1] = (State*)(intptr_t) CONST64(0xcafebabecafebabe);
2640   _leaf = (Node*)(intptr_t) CONST64(0xbaadf00dbaadf00d);
2641   memset(_cost, -3, sizeof(_cost));
2642   memset(_rule, -3, sizeof(_rule));
2643 }
2644 #endif
2645 
2646 #ifndef PRODUCT
2647 //---------------------------dump----------------------------------------------
2648 void State::dump() {
2649   tty-&gt;print("\n");
2650   dump(0);
2651 }
2652 
2653 void State::dump(int depth) {
2654   for( int j = 0; j &lt; depth; j++ )
2655     tty-&gt;print("   ");
2656   tty-&gt;print("--N: ");
2657   _leaf-&gt;dump();
2658   uint i;
2659   for( i = 0; i &lt; _LAST_MACH_OPER; i++ )
2660     // Check for valid entry
2661     if( valid(i) ) {
2662       for( int j = 0; j &lt; depth; j++ )
2663         tty-&gt;print("   ");
2664         assert(_cost[i] != max_juint, "cost must be a valid value");
2665         assert(_rule[i] &lt; _last_Mach_Node, "rule[i] must be valid rule");
2666         tty-&gt;print_cr("%s  %d  %s",
2667                       ruleName[i], _cost[i], ruleName[_rule[i]] );
2668       }
2669   tty-&gt;cr();
2670 
2671   for( i=0; i&lt;2; i++ )
2672     if( _kids[i] )
2673       _kids[i]-&gt;dump(depth+1);
2674 }
2675 #endif
</pre></body></html>
